[
  {
    "objectID": "7_study.html",
    "href": "7_study.html",
    "title": "STUDY",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nNov 4, 2023\n\n\n[Vis] pheatmap\n\n\nJiyunLim \n\n\n\n\nOct 12, 2023\n\n\n6wk-2 데이터 다루기\n\n\nJiyunLim \n\n\n\n\nSep 26, 2023\n\n\n4wk-1 선형대수\n\n\nJiyunLim \n\n\n\n\nSep 19, 2023\n\n\n3wk-1 그래프2\n\n\nJiyunLim \n\n\n\n\nSep 14, 2023\n\n\n2wk-2 그래프\n\n\nJiyunLim \n\n\n\n\nSep 13, 2023\n\n\n3wk-1\n\n\nJiyunLim \n\n\n\n\nSep 13, 2023\n\n\n2wk-2 웹 기초\n\n\nJiyunLim \n\n\n\n\nSep 12, 2023\n\n\n2wk 환경설정 및 파이썬 기초\n\n\nJiyunLim \n\n\n\n\nSep 11, 2023\n\n\n2wk 파이썬\n\n\nJiyunLim \n\n\n\n\nAug 10, 2023\n\n\n트랜스포머 (전력사용량 데이터)\n\n\nJiyunLim \n\n\n\n\nAug 10, 2023\n\n\nTime Series Transformere (Stock Price)\n\n\nJiyunLim \n\n\n\n\nAug 9, 2023\n\n\nAttention is all you need\n\n\nJiyunLim \n\n\n\n\nJul 11, 2023\n\n\n[Fourier] 퓨리에변환(detailed)\n\n\nJiyunLim \n\n\n\n\nJun 25, 2023\n\n\n[Fourier] 푸리에변환 코드실습 (블로그)\n\n\nJiyunLim \n\n\n\n\nJun 23, 2023\n\n\n[Fourier] 퓨리에변환4jy\n\n\n신록예찬 \n\n\n\n\nJun 14, 2023\n\n\n[수리통계학] 추정 for JY\n\n\n신록예찬 \n\n\n\n\nFeb 19, 2023\n\n\nsimultaneous equation\n\n\njiyun Lim\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "1_ip2022.html",
    "href": "1_ip2022.html",
    "title": "IP2022",
    "section": "",
    "text": "This page is organized based on the contents of the Introduction to Python (2022-1) and lecture notes of Professor Guebin Choi of Jeonbuk National University.\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 21, 2023\n\n\n04wk-2: 파이썬의 자료형 (7) – O\n\n\njiyunLim \n\n\n\n\nSep 20, 2023\n\n\n04wk-1: 파이썬의 자료형 (6) – O\n\n\njiyunLim \n\n\n\n\nSep 19, 2023\n\n\n03wk-2: 파이썬의 자료형 (5) – O\n\n\nJiyunLim \n\n\n\n\nSep 18, 2023\n\n\n03wk-1: 파이썬의 자료형 (4) – O\n\n\njiyunLim \n\n\n\n\nSep 17, 2023\n\n\n02wk-2: 파이썬의 자료형 (3) – O\n\n\nJiyunLim \n\n\n\n\nSep 16, 2023\n\n\n02wk-1: 파이썬의 자료형 (2) – O\n\n\nJiyunLim \n\n\n\n\nSep 15, 2023\n\n\n01wk-2: 파이썬의 자료형 (1) – O\n\n\nJiyunLim \n\n\n\n\nJun 21, 2023\n\n\n[IP2023] 13wk-1: 깊은복사와 얕은복사\n\n\n최규빈 \n\n\n\n\nMar 14, 2023\n\n\n[IP2022] Pandas 2단계\n\n\njiyun Lim\n\n\n\n\nMar 13, 2023\n\n\n[IP2022] Pandas 1단계\n\n\njiyun Lim\n\n\n\n\nMar 12, 2023\n\n\n[IP2022] Pandas 0단계\n\n\njiyun Lim\n\n\n\n\nFeb 27, 2023\n\n\n[2022 EXAM] 2022 final exam\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\n[IP2022] class 08단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\n[IP2022] class 07단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\n[IP2022] class 09단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\n[IP2022] class 06단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\n[IP2022] class 05단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\n[IP2022] class 10단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\n[IP2022] Numpy 4단계(concat, stack)\n\n\njiyun Lim\n\n\n\n\nFeb 15, 2023\n\n\n[IP2022] class 02단계\n\n\njiyun Lim\n\n\n\n\nFeb 15, 2023\n\n\n[IP2022] class 04단계\n\n\njiyun Lim\n\n\n\n\nFeb 15, 2023\n\n\n[IP2022] class 03단계\n\n\njiyun Lim\n\n\n\n\nFeb 15, 2023\n\n\n[IP2022] class 01단계\n\n\njiyun Lim\n\n\n\n\nJun 9, 2022\n\n\n[2021 EXAM] 2021 final exam solution\n\n\nGuebinChoi \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "5_stbda2023.html",
    "href": "5_stbda2023.html",
    "title": "STBDA2023",
    "section": "",
    "text": "This page is organized based on the contents of the 2023-2 Special Topics in Big Data Analysis lectures and lecture notes of Professor Guebin Choi of Jeonbuk National University.\n\n2023STBDA Hompage\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nNov 16, 2023\n\n\n11wk-040: Medical Cost / 의사결정나무의 시각화\n\n\nJiyunLim \n\n\n\n\nNov 16, 2023\n\n\n11wk-042: Weighted_Data / 의사결정나무 weights\n\n\nJiyunLim \n\n\n\n\nNov 16, 2023\n\n\n11wk-043: 아이스크림 판매량 / 배깅\n\n\nJiyunLim \n\n\n\n\nNov 16, 2023\n\n\n11wk-041: Medical Cost / 의사결정나무 max_feature,random_state\n\n\nJiyunLim \n\n\n\n\nNov 14, 2023\n\n\nA3: 개발환경의 변천사\n\n\n최규빈 \n\n\n\n\nNov 10, 2023\n\n\n10wk-036: 애니메이션\n\n\nJiyunLim \n\n\n\n\nNov 10, 2023\n\n\n10wk-038: 아이스크림 – 의사결정나무 원리\n\n\nJiyunLim \n\n\n\n\nNov 10, 2023\n\n\n10wk-037: 아이스크림 – 의사결정나무, max_depth\n\n\nJiyunLim \n\n\n\n\nNov 10, 2023\n\n\n10wk-039: 의사결정나무 Discussion\n\n\nJiyunLim \n\n\n\n\nOct 24, 2023\n\n\n08wk-supp: 중간점검\n\n\n최규빈 \n\n\n\n\nOct 17, 2023\n\n\n07wk-030: 아이스크림(교호작용) / 선형회귀\n\n\nJiyunLim \n\n\n\n\nOct 17, 2023\n\n\n07wk-033: 취업(다중공선성) / 의사결정나무\n\n\nJiyunLim \n\n\n\n\nOct 17, 2023\n\n\n07wk-028: 선형모형의 적\n\n\nJiyunLim \n\n\n\n\nOct 17, 2023\n\n\n07wk-031: 체중감량(교호작용) / 의사결정나무\n\n\nJiyunLim \n\n\n\n\nOct 17, 2023\n\n\n07wk-029: 체중감량(교호작용) / 회귀분석\n\n\nJiyunLim \n\n\n\n\nOct 17, 2023\n\n\n07wk-032: 아이스크림(교호작용) / 의사결정나무\n\n\nJiyunLim \n\n\n\n\nOct 17, 2023\n\n\n07wk-027: 아이스크림(이상치) / 회귀분석\n\n\nJiyunLim \n\n\n\n\nOct 16, 2023\n\n\n07wk-035: 아이스크림(이상치) / 의사결정나무\n\n\nJiyunLim \n\n\n\n\nOct 16, 2023\n\n\n07wk-034: 취업(오버피팅) / 의사결정나무\n\n\nJiyunLim \n\n\n\n\nOct 5, 2023\n\n\n06wk-025: 취업+각종영어점수, Lasso\n\n\nJiyunLim \n\n\n\n\nOct 5, 2023\n\n\n06wk-026: 취업+각종영어점수, LassoCV\n\n\nJiyunLim \n\n\n\n\nOct 5, 2023\n\n\n06wk-022: 취업+각종영어점수, 다중공선성\n\n\nJiyunLim \n\n\n\n\nOct 5, 2023\n\n\n06wk-024: 취업+각종영어점수, RidgeCV\n\n\nJiyunLim \n\n\n\n\nOct 5, 2023\n\n\n05wk-020: StandardScaler를 이용한 전처리\n\n\nJiyunLim \n\n\n\n\nOct 5, 2023\n\n\n05wk-021: 취업+밸런스게임, 오버피팅\n\n\nJiyunLim \n\n\n\n\nOct 5, 2023\n\n\n05wk-019: MinMaxScaler를 이용한 전처리\n\n\nJiyunLim \n\n\n\n\nOct 5, 2023\n\n\n06wk-023: 취업+각종영어점수, Ridge\n\n\nJiyunLim \n\n\n\n\nSep 26, 2023\n\n\n04wk-015: 결측치 처리, sklearn.impute\n\n\nJiyunLim \n\n\n\n\nSep 26, 2023\n\n\n04wk-014: 결측치 시각화, msno\n\n\nJiyunLim \n\n\n\n\nSep 26, 2023\n\n\n04wk-017: 취업, 로지스틱을 더 깊게\n\n\nJiyunLim \n\n\n\n\nSep 26, 2023\n\n\n04wk-016: 타이타닉, 결측치처리+로지스틱\n\n\nJiyunLim \n\n\n\n\nSep 26, 2023\n\n\n04wk-018: Predictor 깊은 이해 + 기호정리\n\n\nJiyunLim \n\n\n\n\nSep 21, 2023\n\n\n03wk-011: Medical Cost, 회귀분석\n\n\nJiyunLim \n\n\n\n\nSep 21, 2023\n\n\n03wk-010: 아이스크림(초코/바닐라), 회귀분석\n\n\nJiyunLim \n\n\n\n\nSep 21, 2023\n\n\n03wk-009: 아이스크림, 회귀분석\n\n\nJiyunLim \n\n\n\n\nSep 21, 2023\n\n\n03wk-012: 취업, 로지스틱\n\n\nJiyunLim \n\n\n\n\nSep 21, 2023\n\n\n03wk-013: 타이타닉, 로지스틱\n\n\nJiyunLim \n\n\n\n\nSep 12, 2023\n\n\n02wk-007: 타이타닉, Autogluon (Fsize,Drop)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-006: 타이타닉, Autogluon (Fsize)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-003: 타이타닉, 첫 제출\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-004: 타이타닉, Alexis Cook의 코드\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-005: 타이타닉, Autogluon\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-008: 타이타닉, Autogluon (best_quality)\n\n\n최규빈 \n\n\n\n\nSep 10, 2023\n\n\n타이타닉 튜토리얼\n\n\nJiyunLim \n\n\n\n\nSep 5, 2023\n\n\n01wk-002: 타이타닉, 데이터의 이해\n\n\n최규빈 \n\n\n\n\nSep 4, 2023\n\n\ntitanic\n\n\nJiyunLim \n\n\n\n\nSep 12, 2000\n\n\n[STBDA] Quarto Blog 만들기\n\n\n최규빈 \n\n\n\n\nJan 1, 2000\n\n\nKaggle 리눅스 세팅\n\n\nJiyunLim \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "",
    "text": "list 고급내용2,3 // 리스트컴프리헨션 // 튜플기본내용"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-원소-추가",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-원소-추가",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "리스트 원소 추가",
    "text": "리스트 원소 추가\n(예제) 비어있는 리스트를 만들고 원소 0,1,2를 차례로 추가하여 보자.\n(풀이1) + 연산이용\n\na=[]\na\n\n[]\n\n\n\na= a+[0]\na\n\n[0]\n\n\n\na= a+[1] # a = [0]+[1]\na\n\n[0, 1]\n\n\n\na= a+[2] # a = [0,1] + [2]\na\n\n[0, 1, 2]\n\n\n(풀이2) += 이용\n\na=[]\na+=[0]\na+=[1] \na+=[2] \na\n\n[0, 1, 2]\n\n\n\n반복되는 문자를 제거하고 연산의 순서를 바꾼다.\n\n(풀이3) 리스트 특수기능 .append()를 이용\n\na=[] \n\n\na.append(0)\na.append(1)\na.append(2)\na\n\n[0, 1, 2]\n\n\n- 아래는 불가능하다.\n\na.append(0).append(1).append(2)\n\nAttributeError: 'NoneType' object has no attribute 'append'\n\n\n\na.append(0,1,2)\n\nTypeError: append() takes exactly one argument (3 given)"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#a4와-a.append4의-차이점은",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#a4와-a.append4의-차이점은",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "a+[4]와 a.append(4)의 차이점은?",
    "text": "a+[4]와 a.append(4)의 차이점은?\n(관찰1)\n\na=[1,2,3]\na+[4] ## 리스트 a와 리스트 [4]의 연산결과를 알려줘 \n\n[1, 2, 3, 4]\n\n\n\na ## a는 그대로임. 변화없음 \n\n[1, 2, 3]\n\n\n(관찰2)\n\na=[1,2,3]\na.append(4)\n\n\na ## a자체가 변화함 \n\n[1, 2, 3, 4]\n\n\n비슷해보이지만 굉장히 미묘한 차이가 있음\na.append(4): a에 4를 append하라 \\(\\to\\) a가 변함\na+[4]: a와 [4]를 연산하라"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-특수기능",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-특수기능",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "리스트 특수기능",
    "text": "리스트 특수기능\n(append)\n\na=[1,2,3,4]\na.append?\n\n\nSignature: a.append(object, /)\nDocstring: Append object to the end of the list.\nType:      builtin_function_or_method\n\n\n\n\na.append(5)\na\n\n[1, 2, 3, 4, 5]\n\n\n(clear)\n\na=[1,2,3,4]\na.clear?\n\n\nSignature: a.clear()\nDocstring: Remove all items from list.\nType:      builtin_function_or_method\n\n\n\n\na.clear()\na\n\n[]\n\n\n(copy)\n\na=[1,2,3,4]\na.copy?\n\n\nSignature: a.copy()\nDocstring: Return a shallow copy of the list.\nType:      builtin_function_or_method\n\n\n\n\nb=a.copy()\nb\n\n[1, 2, 3, 4]\n\n\n(count)\n\na=['a','a','b','b','b','c']\na.count?\n\n\nSignature: a.count(value, /)\nDocstring: Return number of occurrences of value.\nType:      builtin_function_or_method\n\n\n\n\na.count('a')\n\n2\n\n\n\na.count('b')\n\n3\n\n\n\na.count('c')\n\n1\n\n\n(extend)\n\na=[1,2,3,4]\nb=[-1,-2,-3,-4]\n\n\na.extend(b)\na\n\n[1, 2, 3, 4, -1, -2, -3, -4]\n\n\n\na=[1,2,3,4]\nb=[-1,-2,-3,-4]\n\n\na.append(b)\n\n\na\n\n[1, 2, 3, 4, [-1, -2, -3, -4]]\n\n\n(index)\n\na=[11,22,'a',True, 22,'a']\na.index?\n\n\nSignature: a.index(value, start=0, stop=9223372036854775807, /)\nDocstring:\nReturn first index of value.\nRaises ValueError if the value is not present.\nType:      builtin_function_or_method\n\n\n\n\na.index(11)\n\n0\n\n\n\na.index(22)\n\n1\n\n\n\na.index('a')\n\n2\n\n\n\na.index(True)\n\n3\n\n\n(insert)\n\na=[1,2,3]\na.insert?\n\n\nSignature: a.insert(index, object, /)\nDocstring: Insert object before index.\nType:      builtin_function_or_method\n\n\n\n\na.insert(1,88) \na\n\n[1, 88, 2, 3]\n\n\n(pop)\n\na=['a',1,2,'d']\na.pop?\n\n\nSignature: a.pop(index=-1, /)\nDocstring:\nRemove and return item at index (default last).\nRaises IndexError if list is empty or index is out of range.\nType:      builtin_function_or_method\n\n\n\n\na.pop() # index=-1 이므로 마지막원소가 나타남\n\n'd'\n\n\n\na # a는 마지막 원소가 사라진 상태\n\n['a', 1, 2]\n\n\n\na.pop(0) # index=0 이므로 첫번쨰 원소가 나타남\n\n'a'\n\n\n\na # a에는 첫번째 원소가 사라진 상태\n\n[1, 2]\n\n\n(remove)\n\na=['a',2,3,'d']\na.remove?\n\n\nSignature: a.remove(value, /)\nDocstring:\nRemove first occurrence of value.\nRaises ValueError if the value is not present.\nType:      builtin_function_or_method\n\n\n\n\na.remove('d')\n\n\na\n\n['a', 2, 3]\n\n\n\na.remove('a')\n\n\na\n\n[2, 3]\n\n\n(reverse)\n\na=[1,2,3,4]\na.reverse?\n\n\nSignature: a.reverse()\nDocstring: Reverse *IN PLACE*.\nType:      builtin_function_or_method\n\n\n\n\na.reverse()\na\n\n[4, 3, 2, 1]\n\n\n(sort)\n\na=[1,3,2,4]\na.sort?\n\n\nSignature: a.sort(*, key=None, reverse=False)\nDocstring:\nSort the list in ascending order and return None.\nThe sort is in-place (i.e. the list itself is modified) and stable (i.e. the\norder of two equal elements is maintained).\nIf a key function is given, apply it once to each list item and sort them,\nascending or descending, according to their function values.\nThe reverse flag can be set to sort in descending order.\nType:      builtin_function_or_method\n\n\n\n\na.sort()\na\n\n[1, 2, 3, 4]\n\n\n(다른예제들)\n\na=list('guebin')\na\n\n['g', 'u', 'e', 'b', 'i', 'n']\n\n\n\na.sort()\na\n\n['b', 'e', 'g', 'i', 'n', 'u']\n\n\n\na.sort(reverse=True)\na\n\n['u', 'n', 'i', 'g', 'e', 'b']"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#중첩리스트",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#중첩리스트",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "중첩리스트",
    "text": "중첩리스트\n- 리스트는 리스트를 원소로 받을 수 있으므로 아래와 같이 중첩된 리스트를 만들 수 있다.\n\nA=[[1,2,3],\n   [4,5,6],\n   [7,8,9]]\nA\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n- A는 1차원인 벡터가 아니라 2차원인 매트릭스로 이해할 수 있다. 구체적으로는 아래와 같은 매트릭스로 이해할 수 있다\n$\n\\[\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\\]\n$\n- A에서 (2,1)의 원소를 뽑고싶다 = 4를 뽑고싶다\n\nA[1,0] # R에서는 이게 가능했죠\n\nTypeError: list indices must be integers or slices, not tuple\n\n\n\n실패\n\n\nA[1][0]\n\n4\n\n\n\n성공\n\n- 성공의 이유를 분석해보자.\n\nA\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n\nA[1]\n\n[4, 5, 6]\n\n\n\nA[1][0]\n\n4\n\n\n- 매트릭스는 아니지만 매트릭스 같음! - 1차원 배열을 다차원 배열로 확장할 수 있는 기본 아이디어를 제공함"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#예비학습1-for문-벼락치기",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#예비학습1-for문-벼락치기",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "예비학습1: for문 벼락치기",
    "text": "예비학습1: for문 벼락치기\n- 리스트 컴프리헨션을 이해하기 전에 for문에 대하여 알아보자.\n프로그램안에서 반복해서 무엇인가를 하고싶다 \\(\\to\\) for\n\nfor i in [0,1,2,3]: ## 반복실행계획\n    print(i) ## 반복실행할내용, 탭을이용하여 들여쓰기해야한다. \n\n0\n1\n2\n3\n\n\n(예제) 1,2,3,4의 합을 for문을 이용하여 구해보자.\n\n_sum = 0 \n\n\n_sum = 0\nfor i in [1,2,3,4]: \n    _sum = _sum + i \n\n\n_sum\n\n10\n\n\n\n_sum = 0\ni=1 \n_sum = _sum + i ## 1 &lt;= 0+1\ni=2\n_sum = _sum + i ## 3 &lt;= 1+2 \ni=3 \n_sum = _sum + i ## 6 &lt;= 3+3\ni=4\n_sum = _sum + i ## 10 &lt;= 6+4 \n\n\n_sum\n\n10\n\n\n- 궁금: 아래와 같은 코드가 있다고 하자.\nfor i in ????: \n    print(i)\n??? 자리에 올 수 있는건 무엇일까?\n\n대답하기 어려움.\n일단 list는 가능했음.\n\n(예시1)\n\nfor i in [1,2,3,4]: \n    print(i)\n\n1\n2\n3\n4\n\n\n(예시2)\n\nfor i in ['a','b','c','d']: \n    print(i)\n\na\nb\nc\nd\n\n\n(예시3)\n\nfor i in 'abcd': \n    print(i)\n\na\nb\nc\nd\n\n\n(예시4)\n\nfor i in '1': \n    print(i)\n\n1\n\n\n(예시5)\n\nfor i in 1: \n    print(i)\n\nTypeError: 'int' object is not iterable\n\n\n(예시6)\n\nfor i in range(10): \n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#예비학습2-range",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#예비학습2-range",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "예비학습2: range",
    "text": "예비학습2: range\n- range(0,10) 선언해보기\n\nrange(0,10)\n\nrange(0, 10)\n\n\n\n이게뭐야?\n\n- 도움말 확인하기\n\n_tmp = range(0,10)\n_tmp?\n\n\nType:        range\nString form: range(0, 10)\nLength:      10\nDocstring:  \nrange(stop) -&gt; range object\nrange(start, stop[, step]) -&gt; range object\nReturn an object that produces a sequence of integers from start (inclusive)\nto stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\nstart defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\nThese are exactly the valid indices for a list of 4 elements.\nWhen step is given, it specifies the increment (or decrement).\n\n\n\n\n우리가 아는 범위에서는 모르겠음.. 이런게 있나보다 하고 넘어가야 하겠음\n\n- 형태변환으로 range(0,10)의 느낌 찾기\n\nlist(range(0,10)) # 0을 포함, 10을 미포함 \n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n이게 중요한것임. range(0,10)를 리스트화시키면 [0,1,…,9] 와 같은 리스트를 얻을 수 있음. \\(\\Rightarrow\\) range(0,10)은 [0,1,…,9] 와 “비슷한 것” 임\n\n- range()의 활용\n\nlist(range(10)) # 0은 생략가능\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nlist(range(2,10)) # 2는 포함, 10은 미포함 \n\n[2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nlist(range(1,10,2)) # 2는 포함, 10은 미포함 \n\n[1, 3, 5, 7, 9]"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-컴프리헨션",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-컴프리헨션",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "리스트 컴프리헨션",
    "text": "리스트 컴프리헨션\n- 예제: \\(2^0, 2^1, 2^2, 2^3\\)를 원소로 가지는 리스트를 생성하라.\n(풀이1) 직접입력\n\nx= [2**0, 2**1, 2**2, 2**3] \nx\n\n[1, 2, 4, 8]\n\n\n(풀이2) for문을 이용함\n\nx=[] \nfor i in [0,1,2,3]:\n    x.append(2**i) \n\n\nx\n\n[1, 2, 4, 8]\n\n\n(풀이3) for문을 이용함\n\nx=[] \nfor i in [0,1,2,3]:\n    x = x+[2**i]\n\n\nx\n\n[1, 2, 4, 8]\n\n\n(풀이4) for문을 이용함\n\nx=[] \nfor i in [0,1,2,3]:\n    x += [2**i]\n\n\nx\n\n[1, 2, 4, 8]\n\n\n(풀이5) 리스트컴프리헨션을 이용한 풀이\n\n[2**i for i in [0,1,2,3]]\n\n[1, 2, 4, 8]\n\n\n- 리스트컴프리헨션의 문법 암기방법\n\n집합에서 조건제시법을 연상\n\\(\\{2^0,2^1,2^2,2^3\\}=\\{2^i: i \\in \\{0,1,2,3\\} \\}\\)\n\n- 리스트컴프리헨션이란?\n\n리스트를 매우 효율적으로 만드는 테크닉\nfor문에 비하여 가지고 있는 장점: (1) 코드가 간결하다 (2) 빠르다"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-컴프리헨션-연습",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-컴프리헨션-연습",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "리스트 컴프리헨션 연습",
    "text": "리스트 컴프리헨션 연습\n- 예제1: 리스트 컴프리헨션을 이용하여 아래와 같은 리스트를 만들어라.\n\n['SSSS','PPPP','AAAA','MMMM']\n\n['SSSS', 'PPPP', 'AAAA', 'MMMM']\n\n\n(풀이)\n\n[i*4 for i in 'SPAM']\n\n['SSSS', 'PPPP', 'AAAA', 'MMMM']\n\n\n- 예제2: 리스트컴프리헨션을 이용하여 아래와 같은 리스트를 만들어라.\n\n['X1','X2','X3','Y1','Y2','Y3']\n\n['X1', 'X2', 'X3', 'Y1', 'Y2', 'Y3']\n\n\n(풀이)\n\n[i+j for i in 'XY' for j in '123']\n\n['X1', 'X2', 'X3', 'Y1', 'Y2', 'Y3']\n\n\n- 예제: 리스트컴프리헨션을 이용하여 아래와 같은 리스트를 만들어라.\n\n['stat1', 'stat2', 'stat3', 'math1', 'math2', 'math3']\n\n['stat1', 'stat2', 'stat3', 'math1', 'math2', 'math3']\n\n\n(풀이)\n\n[i+j for i in ['stat', 'math'] for j in '123']\n\n['stat1', 'stat2', 'stat3', 'math1', 'math2', 'math3']\n\n\n(다른풀이) 참고로 for문을 쓰면 좀 복잡해진다.\n\n_lst = [] \nfor x in ['stat','math']: \n    for y in '123': \n        _lst = _lst + [x+y] \n\n\n_lst \n\n['stat1', 'stat2', 'stat3', 'math1', 'math2', 'math3']\n\n\n- 예제: 리스트컴프리헨션과 문자열 'jbnu'를 이용하여 아래와 같은 리스트를 만들어라.\n\n['j','b','n','u']\n\n['j', 'b', 'n', 'u']\n\n\n(다른풀이) 아래와 같이 풀면 된다는것은 알고 있음\n\nlist('jbnu')\n\n['j', 'b', 'n', 'u']\n\n\n(풀이)\n\n[i for i in 'jbnu']\n\n['j', 'b', 'n', 'u']\n\n\n- 예제: 리스트컴프리헨션을 이용하여 아래와 같은 리스트를 만들어라.\n\n['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12']\n\n['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12']\n\n\n(풀이)\n\n['X'+str(i) for i in range(1,13)]\n#['X'+str(i) for i in list(range(1,13))]\n\n['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12']"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-컴프리헨션과-for문의-미묘한-차이",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-컴프리헨션과-for문의-미묘한-차이",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "리스트 컴프리헨션과 for문의 미묘한 차이",
    "text": "리스트 컴프리헨션과 for문의 미묘한 차이\n(경우1)\n\nx=777 \nlst = [] \nfor x in 'jbnu': \n    lst = lst + [x]\nlst    \n\n['j', 'b', 'n', 'u']\n\n\n\nx\n\n'u'\n\n\n(경우2)\n\nx=777\nlst = [x for x in 'jbnu'] \nlst \n\n['j', 'b', 'n', 'u']\n\n\n\nx\n\n777\n\n\n\n진짜 미묘하게 다르죠?"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-vs-튜플",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#리스트-vs-튜플",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "리스트 vs 튜플",
    "text": "리스트 vs 튜플\n- 컨테이너형타입이라는 점, 그리고 연산 및 인덱싱을 하는 방법은 리스트와 같음 - 차이점1: [] 대신에 ()를 사용한다. - 차이점2: 불변형이다. (원소의 값을 바꿀 수 없음) - 차이점3: 하나의 원소를 선언할 때는 (1,)와 같이 해야 한다. - 차이점4: 의미가 명확할때는 튜플의 ()를 생략가능하다.\n- 컨테이너형이라는 것이 무슨의미?\n\na=(4,6,'pencil', 3.2+4.6j, [3,4]) \n\n\ntype(a[2])\n\nstr\n\n\n\ntype(a[3])\n\ncomplex\n\n\n- 불변형이라는 것은 무슨의미?\n\na[2] = 'Pencil'\n\nTypeError: 'tuple' object does not support item assignment\n\n\n참고로 a를 튜플이 아니라 리스트로 선언하면 값이 잘 바뀐다.\n\na=[4,6,'pencil', 3.2+4.6j, [3,4]]\n\n\na[2]\n\n'pencil'\n\n\n\na[2]='Pencil'\n\n\na\n\n[4, 6, 'Pencil', (3.2+4.6j), [3, 4]]\n\n\n- 하나의 원소로 이루어진 튜플을 만들때는 쉼표를 붙여야 함.\n\n[1]+[2,3,4]\n\n[1, 2, 3, 4]\n\n\n\n(1,)+(2,3,4)\n\n(1, 2, 3, 4)\n\n\n- 마지막차이점! 의미가 명확할때 튜플의 괄호는 생략가능하다. (이게 중요합니다)\n\na=1,2\na\n\n(1, 2)\n\n\n의미가 명확할때 생략해야함\n\n1,2 + 3,4,5 \n\n(1, 5, 4, 5)\n\n\n\n(1,2) + (3,4,5) \n\n(1, 2, 3, 4, 5)"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#선언",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#선언",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "선언",
    "text": "선언\n- 소괄호를 이용\n\na=(1,2,3)\na\n\n(1, 2, 3)\n\n\n\ntype(a)\n\ntuple\n\n\n- 생략가능하다는 점이 포인트\n\na=1,2,3\na\n\n(1, 2, 3)\n\n\n\ntype(a)\n\ntuple\n\n\n- 원소가 하나인 튜플을 만들고 싶다면?\n\na=(1,)\na\n\n(1,)"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#연산",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#연산",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "연산",
    "text": "연산\n- 리스트와 동일\n\n(1,2)+(3,4,5)\n\n(1, 2, 3, 4, 5)\n\n\n\n(1,2)*2\n\n(1, 2, 1, 2)"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#인덱싱",
    "href": "posts/1_IP2022/01_자료형/2023-03-20-3wk-1.out.html#인덱싱",
    "title": "03wk-1: 파이썬의 자료형 (4) – O",
    "section": "인덱싱",
    "text": "인덱싱\n- 리스트와 동일\n\na=(1,2,3,-4,-5)\na\n\n(1, 2, 3, -4, -5)\n\n\n\na[-1]\n\n-5\n\n\n\na[-3:]\n\n(3, -4, -5)"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "",
    "text": "딕셔너리 컴프리헨션, 셋 컴프리헨션"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#key의-조건",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#key의-조건",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "key의 조건",
    "text": "key의 조건\n- 조건1: 키로 쓸 수 있는 자료형은 정해져 있다.\n\nint O, float O, bool O, str O, list X, tuple O, dict X, set X\n\n(예시1) dict의 키로 int를 사용\n\ndct = {0:[1,2,3], 1:[2,3,4]} \ndct[0] # 인덱싱 하는거 같네?\n\n[1, 2, 3]\n\n\n\ndct[-1] # 속았지?\n\nKeyError: -1\n\n\n(예시2) dict의 키로 float을 사용 &lt;– 이렇게 쓰는 사람 본적이 없어요\n\ndct = {3.14:'π', 2.178:'e'}\ndct[3.14]\n\n'π'\n\n\n(예시3) dict의 키로 bool을 사용\n\ndct = {True: '참이다', False: '거짓이다.'} \ndct\n\n{True: '참이다', False: '거짓이다.'}\n\n\n\ndct[1&lt;2]\n\n'참이다'\n\n\n(예시4) dict의 키로 str을 사용 (\\(\\star\\))\n\ndct = {'guebin':[10,20,30,30], 'hanni':[10,20,25,40]}\ndct['guebin']\n\n[10, 20, 30, 30]\n\n\n(예시5) dict의 키로 list를 사용 \\(\\Rightarrow\\) 불가능\n\ndct = {[10,20,30,40]: 'guebin', [10,20,25,40]: 'hanni'} \ndct\n\nTypeError: unhashable type: 'list'\n\n\n(예시6) dict의 키로 tuple 사용 (\\(\\star\\))\n\ndct = {(10,20,30,40): 'guebin', (10,20,25,40): 'hanni'} \ndct\n\n\ndct[(10,20,30,40)]\n\n\ndct[10,20,30,40]\n\n(예시7) dict의 키로 dict사용 \\(\\Rightarrow\\) 불가능\n\ndct = {{0:1}: 'guebin', {1:2}: 'hanni'} \ndct\n\n(예시8) dict의 키로 set사용 \\(\\Rightarrow\\) 불가능\n\ndct = {{'샌드위치','딸기우유'}:'점심', {'불고기','된장찌개','김','콩자반'}: '저녁'}\ndct\n\n- 조건2: 키는 중복해서 쓸 수 없다.\n(예시1)\n\ndct = {0:[1,2,3], 1:[2,3,4], 0:[3,4,5]} # 이렇게 쓰지 마세요\ndct"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#value의-조건",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#value의-조건",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "value의 조건",
    "text": "value의 조건\n- 없다… \\(\\Rightarrow\\) dict는 컨테이너형!!"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#딕셔너리-컴프리헨션",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#딕셔너리-컴프리헨션",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "딕셔너리 컴프리헨션",
    "text": "딕셔너리 컴프리헨션\n- 예시1\n\nlst = [['딸기','사과'],['오토바이','자동차'],['컴퓨터','아이패드','마우스']]\nlst  \n\n[['딸기', '사과'], ['오토바이', '자동차'], ['컴퓨터', '아이패드', '마우스']]\n\n\n\nlst[0]\n\n['딸기', '사과']\n\n\n\n{i:lst[i] for i in range(3)}\n\n{0: ['딸기', '사과'], 1: ['오토바이', '자동차'], 2: ['컴퓨터', '아이패드', '마우스']}\n\n\n- 예시2: key, val을 서로 바꾸는 예시\n\ndct = {'a':(1,0,0,0), 'b':(0,1,0,0), 'c':(0,0,1,0), 'd':(0,0,0,1)}\ndct\n\n{'a': (1, 0, 0, 0), 'b': (0, 1, 0, 0), 'c': (0, 0, 1, 0), 'd': (0, 0, 0, 1)}\n\n\n\n[(k,v) for k,v in dct.items()]\n\n[('a', (1, 0, 0, 0)),\n ('b', (0, 1, 0, 0)),\n ('c', (0, 0, 1, 0)),\n ('d', (0, 0, 0, 1))]\n\n\n\n{k:v for k,v in dct.items()}\n\n{'a': (1, 0, 0, 0), 'b': (0, 1, 0, 0), 'c': (0, 0, 1, 0), 'd': (0, 0, 0, 1)}\n\n\n\n# key, value 서로 바꾸는 예시\n{v:k for k,v in dct.items() }\n\n{(1, 0, 0, 0): 'a', (0, 1, 0, 0): 'b', (0, 0, 1, 0): 'c', (0, 0, 0, 1): 'd'}"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#바꿔치기-3",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#바꿔치기-3",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "바꿔치기 (3)",
    "text": "바꿔치기 (3)\n- 예제1: 아래와 같은 리스트가 있다고 하자.\n\nlst = list('abcd'*2)\nlst\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd']\n\n\n아래의 규칙에 의하여 lst의 각 원소의 값을 바꾸고 싶다고 하자.\n\n\n\n변환전\n변환후\n\n\n\n\n‘a’\n[1,0,0,0]\n\n\n‘b’\n[0,1,0,0]\n\n\n‘c’\n[0,0,1,0]\n\n\n‘d’\n[0,0,0,1]\n\n\n\n이를 구현하는 코드를 작성하고, 역변환하는 코드를 작성하라.\nhint: 아래의 dct를 이용할 것\n\ndct = {'a':[1,0,0,0], 'b':[0,1,0,0], 'c':[0,0,1,0], 'd':[0,0,0,1]}\ndct\n\n{'a': [1, 0, 0, 0], 'b': [0, 1, 0, 0], 'c': [0, 0, 1, 0], 'd': [0, 0, 0, 1]}\n\n\n(풀이)\n변환하는 코드를 구현하면\n\nlst2= [dct[l] for l in lst] \nlst2\n\n[[1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 0, 1],\n [1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 0, 1]]\n\n\n역변환하는 코드를 구현하면\n(1단계)\n\ndct.items()\n\ndict_items([('a', [1, 0, 0, 0]), ('b', [0, 1, 0, 0]), ('c', [0, 0, 1, 0]), ('d', [0, 0, 0, 1])])\n\n\n\n{k:v for k,v in dct.items()} \n\n{'a': [1, 0, 0, 0], 'b': [0, 1, 0, 0], 'c': [0, 0, 1, 0], 'd': [0, 0, 0, 1]}\n\n\n\n{v:k for k,v in dct.items()} # 리스트 타입은 키로 쓸수없다.\n\nTypeError: unhashable type: 'list'\n\n\n\n{tuple(v):k for k,v in dct.items()} # 억지로 키를 튜플로 바꿔서.\n\n{(1, 0, 0, 0): 'a', (0, 1, 0, 0): 'b', (0, 0, 1, 0): 'c', (0, 0, 0, 1): 'd'}\n\n\n역변환\n\ndct_inv = {tuple(v):k for k,v in dct.items()}\ndct_inv\n\n{(1, 0, 0, 0): 'a', (0, 1, 0, 0): 'b', (0, 0, 1, 0): 'c', (0, 0, 0, 1): 'd'}\n\n\n\nlst2\n\n[[1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 0, 1],\n [1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 0, 1]]\n\n\n\n[dct_inv[l] for l in lst2] # 키로 list 타입을 넣을 수 없다.\n\nTypeError: unhashable type: 'list'\n\n\n\n[dct_inv[tuple(l)] for l in lst2]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd']\n\n\n(2단계)\n\n[dct_inv[tuple(l)] for l in lst2]\n\n내생각\n위와 같은 코드는 경우에 따라서 아래와 같은 복잡합 코드를 피할 수 있는 장점이 있다.\n\n[x for l in lst2 for x,y in dct.items() if l==y]\n\n- 예제2: 아래와 같은 리스트가 있다고 하자. – 강의를 재촬영 했습니다.\n\nlst = ['딸기', '사과', '바나나', '바나나', '오토바이', '자동차', '기차']\nlst\n\n['딸기', '사과', '바나나', '바나나', '오토바이', '자동차', '기차']\n\n\n아래와 같은 규칙에 따라서 바꾸고 싶다고 하자.\n\n\n\n변환전\n변환후\n\n\n\n\n딸기\n과일\n\n\n사과\n과일\n\n\n바나나\n과일\n\n\n오토바이\n탈것\n\n\n자동차\n탈것\n\n\n버스\n탈것\n\n\n기차\n탈것\n\n\n\n(풀이1)\n\ndct = {'딸기':'과일', '사과':'과일', '바나나':'과일', \n       '오토바이':'탈것', '자동차':'탈것', '버스':'탈것', '기차':'탈것'}\ndct\n\n\n[dct[l] for l in lst]    \n\n(풀이2) – 지난시간에 한 것\n\ndct = {'과일':['딸기','사과','바나나'], '탈것':['오토바이','자동차', '버스', '기차']} \ndct\n\n{'과일': ['딸기', '사과', '바나나'], '탈것': ['오토바이', '자동차', '버스', '기차']}\n\n\n\nfor l in lst:\n    # print(l) \n    for k,v in dct.items():\n        if l in v: \n            print(k) # list안에 있는 것이 과일이냐? 탈것이냐?\n\n과일\n과일\n과일\n과일\n탈것\n탈것\n탈것\n\n\n\n# lst = [딸기, 사과, 바나나, 바나나, 오토바이, 자동차, 기차]\n\n위에 코드를 중첩구조로 만들면 아래와 같다.\n\n[k for l in lst for k,v in dct.items() if l in v]\n\n['과일', '과일', '과일', '과일', '탈것', '탈것', '탈것']\n\n\n(풀이3)\n\n{k:v for k,v in dct.items()}\n\n{'과일': ['딸기', '사과', '바나나'], '탈것': ['오토바이', '자동차', '버스', '기차']}\n\n\n\n{l:k for k,v in dct.items() for l in v}\n\n{'딸기': '과일',\n '사과': '과일',\n '바나나': '과일',\n '오토바이': '탈것',\n '자동차': '탈것',\n '버스': '탈것',\n '기차': '탈것'}\n\n\n\n_dct = {l:k for k,v in dct.items() for l in v}\n_dct \n\n{'딸기': '과일',\n '사과': '과일',\n '바나나': '과일',\n '오토바이': '탈것',\n '자동차': '탈것',\n '버스': '탈것',\n '기차': '탈것'}\n\n\n\n[_dct[l] for l in lst]\n\n['과일', '과일', '과일', '과일', '탈것', '탈것', '탈것']"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#선언",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#선언",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "선언",
    "text": "선언\n\nwishlist={'notebook','desktop'}\nwishlist\n\n{'desktop', 'notebook'}"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#원소추출",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#원소추출",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "원소추출",
    "text": "원소추출\n- 일단 인덱스로는 못합니다.\n\nwishlist={'notebook','desktop'}\nwishlist[0] # 원소추출을 안됨.\n\nTypeError: 'set' object is not subscriptable\n\n\n- 딱히 하는 방법이 없어요.. 그리고 이걸 하는 의미가 없어요.. (원소에 접근해서 뭐하려고??)"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#원소추가",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#원소추가",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "원소추가",
    "text": "원소추가\n- 이건 의미가 있음\n\nwishlist={'notebook','desktop'} \nwishlist\n\n{'desktop', 'notebook'}\n\n\n\nwishlist.add('ipad')\nwishlist\n\n{'desktop', 'ipad', 'notebook'}\n\n\n\nwishlist.add('notebook') # 이미 원소로 있는건 추가되지 않음. \nwishlist\n\n{'desktop', 'ipad', 'notebook'}"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#원소삭제",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#원소삭제",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "원소삭제",
    "text": "원소삭제\n\nwishlist={'desktop', 'ipad', 'notebook'}\nwishlist\n\n{'desktop', 'ipad', 'notebook'}\n\n\n\nwishlist.remove('notebook')\n\n\nwishlist\n\n{'desktop', 'ipad'}"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#연산",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#연산",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "연산",
    "text": "연산\n- in 연산자\n\nwishlist={'desktop', 'ipad', 'notebook'}\nwishlist\n\n{'desktop', 'ipad', 'notebook'}\n\n\n\n'notebook' in wishlist\n\nTrue\n\n\n\n참고로 in연산자는 집합에서만 쓰는것은 아님\n\n- 합집합, 교집합, 차집합\n\nday1 = {'notebook','desktop'}\nday2 = {'notebook','ipad'}\n\n\nday1 | day2 # 합집합\n\n{'desktop', 'ipad', 'notebook'}\n\n\n\nday1 & day2 # 교집합\n\n{'notebook'}\n\n\n\nday1 - day2 # 차집합 \n\n{'desktop'}\n\n\n\nday2 - day1 # 차집합\n\n{'ipad'}\n\n\n- 부분집합\n\nday1 = {'notebook', 'desktop'}\nday2 = day1 | {'ipad'} \n\n\nday2\n\n{'desktop', 'ipad', 'notebook'}\n\n\n\nday1 &lt; day2  # day1는 day2의 부분집합인가? \n\nTrue\n\n\n\nday2 &lt; day1\n\nFalse"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#집합-특수기능",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#집합-특수기능",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "집합 특수기능",
    "text": "집합 특수기능\n- 합집합\n\nday1 = {'notebook', 'desktop'}\nday2 = {'notebook','ipad'}\n\n\nday1.union(day2)\n\n{'desktop', 'ipad', 'notebook'}\n\n\n- 나머지 메소드는 스스로 찾아보세요"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#for문과-set",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#for문과-set",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "for문과 set",
    "text": "for문과 set\n\nday1 = {'notebook', 'desktop'}\nday2 = {'notebook', 'ipad'}\n\n\nfor i in day1|day2:\n    print(i)\n\nnotebook\nipad\ndesktop\n\n\n\nfor i in day1|day2: \n    print(i)\n\nnotebook\nipad\ndesktop"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#set-컴프리헨션",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#set-컴프리헨션",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "set 컴프리헨션",
    "text": "set 컴프리헨션\n- 예시1\n\nlst = [1,2,1,1,3,4,5]\n{l for l in lst}\n\n{1, 2, 3, 4, 5}"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#유니크한-원소",
    "href": "posts/1_IP2022/01_자료형/2023-03-29-4wk-2.out.html#유니크한-원소",
    "title": "04wk-2: 파이썬의 자료형 (7) – O",
    "section": "유니크한 원소",
    "text": "유니크한 원소\n\n\n\n\n\n\nTip\n\n\n\n중복되는 원소가 사라진다는 것이 집합의 장점!\n\n\n- 예제1: 아래의 list는 모두 몇 종류의 문자로 이루어져 있는가?\n\nlst=list('asdfasssdfdsasdfasdfasdfasdf')\n\n\n{s for s in lst}\n\n{'a', 'd', 'f', 's'}\n\n\n참고 : .count\n\nlst.count('a') # 'a'가 몇개있냐?\n\n6\n\n\n\nlst.count('s') # 's'가 몇개?\n\n9\n\n\n\nlst.count('f') # 'f'는 몇개?\n\n6\n\n\n\n{'a':lst.count('a'), 's':lst.count('s'), 'f':lst.count('f')}\n\n{'a': 6, 's': 9, 'f': 6}\n\n\n(풀이)\n\nset(lst) \n\n{'a', 'd', 'f', 's'}\n\n\n\nlen(set(lst))\n\n4\n\n\n\n{k:lst.count(k) for k in set(lst)}  # {'a':lst.count('a'), 's':lst.count('s'), 'f':lst.count('f')}와 동일.\n\n{'f': 6, 's': 9, 'a': 6, 'd': 7}\n\n\n- 예제2: 아래의 txt에서 어떠한 종류의 문자가 각각 몇번씩 사용되었는지 빈도를 구하는 코드를 작성하라.\n\ntxt = 'asdkflkjahsdlkjfhlaksglkjdhflkgjhlskdfjhglkajhsdlkfjhalsdkf'\ntxt\n\n'asdkflkjahsdlkjfhlaksglkjdhflkgjhlskdfjhglkajhsdlkfjhalsdkf'\n\n\n\n{t for t in txt}\n\n{'a', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 's'}\n\n\n\n{k:list(txt).count(k) for k in set(txt)}\n\n{'k': 10, 'g': 3, 'a': 5, 'l': 9, 'j': 7, 'h': 7, 'f': 6, 's': 6, 'd': 6}\n\n\n(풀이)\n\n{k:list(txt).count(k) for k in set(txt)}\n\n{'k': 10, 'g': 3, 'a': 5, 'l': 9, 'j': 7, 'h': 7, 'f': 6, 's': 6, 'd': 6}"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-08-1wk-2.out.html",
    "href": "posts/1_IP2022/01_자료형/2023-03-08-1wk-2.out.html",
    "title": "01wk-2: 파이썬의 자료형 (1) – O",
    "section": "",
    "text": "파이썬의 기본자료형은 int, float, bool, str, list, tuple, dict, set 등이 있다.\n\n\n강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-w17wsQ3-WMvDNNWEX52GOX\n\n\n\nIntro\n- 파이썬의 기본자료형은 int, float, bool, str, list, tuple, dict, set 등이 있다.\n\n0차원 자료형: int, float, bool\n1차원 자료형: str, list, tuple, dict, set\n\n\n\nint, float, bool\n- int형\n\na=100\n\n\ntype(a)\n\nint\n\n\n- float형\n\na=1.2*3\na\n\n3.5999999999999996\n\n\n\ntype(a)\n\nfloat\n\n\n\na?\n\n\nType:        float\nString form: 3.5999999999999996\nDocstring:   Convert a string or number to a floating point number, if possible.\n\n\n\n- bool형\n\na=True ## 숫자1으로 생각할 수 있음 \nb=False ## 숫자0으로 생각할 수 있음\n\n\ntype(a)\n\nbool\n\n\n\ntype(b)\n\nbool\n\n\n\na?\n\n\nb?\n\n- bool형의 연산\n\na=True ## 1\nb=False ## 0 \n\n\na+b\n\n1\n\n\n\na*b \n\n0\n\n\n- complex형\n\na=1+2j\nb=2-2j\n\n\ntype(a)\n\ncomplex\n\n\n\ntype(b)\n\ncomplex\n\n\n\na?\n\n\nType:        complex\nString form: (1+2j)\nDocstring:  \nCreate a complex number from a real part and an optional imaginary part.\nThis is equivalent to (real + imag*1j) where imag defaults to 0.\n\n\n\n\nb?\n\n\nType:        complex\nString form: (2-2j)\nDocstring:  \nCreate a complex number from a real part and an optional imaginary part.\nThis is equivalent to (real + imag*1j) where imag defaults to 0.\n\n\n\n\nc=a+b\n\n\nc\n\n(3+0j)\n\n\n- 형태변환: float \\(\\to\\) int\n(예시1)\n\na=3.0\ntype(a)\n\nfloat\n\n\n\na=int(a)\n\n\ntype(a)\n\nint\n\n\n(예시2) 이경우는 정보의 손실이 발생\n\na=3.14 \nint(a)\n\n3\n\n\n- 형태변환: int \\(\\to\\) float\n\na=3\ntype(a)\n\nint\n\n\n\na=float(a)\ntype(a)\n\nfloat\n\n\n- 형태변환: bool \\(\\to\\) int/float, int/float \\(\\to\\) bool\n(예시1)\n\na=True\ntype(a)\n\nbool\n\n\n\nint(a)\n\n1\n\n\n\nfloat(a)\n\n1.0\n\n\n(예시2)\n\na=1 \nbool(a)\n\nTrue\n\n\n\na=0\nbool(a)\n\nFalse\n\n\n(예시3)\n\na=1.0\nbool(a)\n\nTrue\n\n\n\na=0.0\nbool(a)\n\nFalse\n\n\n- 이상한 형태변환도 가능하다. (이런것도 바꿔주나 싶은것도 바꿔줌)\n\nbool(-3.14)\n\nTrue\n\n\n\n저는 이런 코드를 의도적으로 사용하지 않아요..\n\n\nint(3.14)\n\n3\n\n\n- 형태변환이 항상가능한것도 아님\n\nfloat(3+0j) # 사실상 3+0j=3 이므로 float으로 형변환하면 3.0이 되어야 할 것 같은데 변환불가능하다. \n\nTypeError: can't convert complex to float\n\n\n- 암묵적형변환 (implicit)\n(예비학습) implicit의 의미\n\n추운날씨 -&gt; 보일러좀 틀자! (explicit) / 오늘 날씨 좀 추운 것 같지 않아? (implicit)\n짜장면 먹을래? -&gt; 싫어! (explicit) / 난 어제 짜장면 먹었는데.. (implicit)\n\n(예제)\n\nTrue * 1 # 1을 곱할건데 너 계속 True로 있을꺼야? \n\n1\n\n\n\n1 * 1.0 # 1.0을 곱할건데 너 계속 int로 있을꺼야? \n\n1.0\n\n\n\nTrue+True # +연산을 할건데 계속 True로 있을꺼야? \n\n2\n\n\n\n\n숙제\n아래 강의노트의 영상 1-3을 참고하여 주피터랩을 설치하고 설치성공한 화면을 스크린샷으로 LMS에 제출\nhttps://guebin.github.io/IP2022/2022/03/07/(1주차)-3월7일.html"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-14-pandas2.html",
    "href": "posts/1_IP2022/02_DataScience/2023-03-14-pandas2.html",
    "title": "[IP2022] Pandas 2단계",
    "section": "",
    "text": "하나 혹은 여러개의 col\\(\\cdot\\)row 선택하는 법\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n- 데이터\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\n\n\ndf=pd.DataFrame({'att':att,'rep':rep,'mid':mid,'fin':fin},index=key)\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.att\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 방법2: dict 스타일\n\ndf['att']\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n\ntype(df['att'])\n\npandas.core.series.Series\n\n\n- 방법3: dict 스타일\n\ndf[['att']]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ntype(df[['att']])\n\npandas.core.frame.DataFrame\n\n\n\ndf.att나 df['att']는 series를 리턴하고 df[['att']]는 dataframe을 리턴한다.\n\n- 방법4: ndarray스타일\n\ndf.iloc[:,0] \n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n\ntype(df.iloc[:,0] )\n\npandas.core.series.Series\n\n\n- 방법5: ndarray 스타일\n\ndf.iloc[:,[0]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ntype(df.iloc[:,[0]])\n\npandas.core.frame.DataFrame\n\n\n\ndf.iloc[:,0]은 series를 리턴하고 df.iloc[:,[0]]은 dataframe을 리턴한다.\n\n- 방법6: ndarray 스타일과 dict스타일의 혼합\n\ndf.loc[:,'att'] \n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 방법7: ndarray 스타일과 dict스타일의 혼합\n\ndf.loc[:,['att']] \n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ndf.loc[:,'att']은 series를 리턴하고 df.loc[:,['att']] 은 dataframe을 리턴한다.\n\n- 방법7: ndarray 스타일 + bool 인덱싱\n\ndf.iloc[:,[True,False,False,False]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n- 방법8: ndarray와 dict의 혼합형 + bool 인덱싱\n\ndf.loc[:,[True,False,False,False]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\n\n\n- 방법1: dict 스타일\n\ndf[['att','fin']]\n\n\n\n\n\n\n\n\natt\nfin\n\n\n\n\n202212380\n65\n40\n\n\n202212370\n95\n80\n\n\n202212363\n65\n30\n\n\n202212488\n55\n80\n\n\n202212312\n80\n100\n\n\n202212377\n75\n15\n\n\n202212463\n65\n90\n\n\n202212471\n60\n0\n\n\n202212400\n95\n10\n\n\n202212469\n90\n20\n\n\n202212318\n55\n25\n\n\n202212432\n95\n0\n\n\n202212443\n95\n35\n\n\n202212367\n50\n30\n\n\n202212458\n50\n85\n\n\n202212396\n95\n95\n\n\n202212482\n50\n10\n\n\n202212452\n65\n45\n\n\n202212387\n70\n35\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n- 방법2: ndarray 스타일 (정수리스트로 인덱싱, 슬라이싱, 스트라이딩)\n\ndf.iloc[:,[0,1]] # 정수의 리스트를 전달하여 컬럼추출\n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,range(2)] \n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,:2]  # 슬라이싱 , 0,1,2에서 마지막 2는 제외되고 0,1에 해당하는 것만 추출\n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,::2]  # 스트라이딩\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n- 방법3: ndarray와 dict의 혼합형\n\ndf.loc[:,['att','mid']] \n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,'att':'mid']  # 마지막의 mid도 포함된다. \n\n\n\n\n\n\n\n\natt\nrep\nmid\n\n\n\n\n202212380\n65\n55\n50\n\n\n202212370\n95\n100\n50\n\n\n202212363\n65\n90\n60\n\n\n202212488\n55\n80\n75\n\n\n202212312\n80\n30\n30\n\n\n202212377\n75\n40\n100\n\n\n202212463\n65\n45\n45\n\n\n202212471\n60\n60\n25\n\n\n202212400\n95\n65\n20\n\n\n202212469\n90\n80\n80\n\n\n202212318\n55\n75\n35\n\n\n202212432\n95\n95\n45\n\n\n202212443\n95\n55\n15\n\n\n202212367\n50\n80\n40\n\n\n202212458\n50\n55\n15\n\n\n202212396\n95\n30\n30\n\n\n202212482\n50\n50\n45\n\n\n202212452\n65\n55\n15\n\n\n202212387\n70\n70\n40\n\n\n202212354\n90\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,'rep':] \n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212370\n100\n50\n80\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212312\n30\n30\n100\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212400\n65\n20\n10\n\n\n202212469\n80\n80\n20\n\n\n202212318\n75\n35\n25\n\n\n202212432\n95\n45\n0\n\n\n202212443\n55\n15\n35\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212396\n30\n30\n95\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n202212354\n90\n80\n90\n\n\n\n\n\n\n\n- 방법4: bool을 이용한 인덱싱\n\ndf.iloc[:,[True,False,True,False]]\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,[True,False,True,False]]\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.iloc[0]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.iloc[0])\n\npandas.core.series.Series\n\n\n- 방법2\n\ndf.iloc[[0]]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.iloc[[0]])\n\npandas.core.frame.DataFrame\n\n\n- 방법3\n\ndf.iloc[0,:]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.iloc[0,:])\n\npandas.core.series.Series\n\n\n- 방법4\n\ndf.iloc[[0],:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.iloc[[0],:])\n\npandas.core.frame.DataFrame\n\n\n- 방법5\n\ndf.loc['202212380']\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.loc['202212380'])\n\npandas.core.series.Series\n\n\n- 방법6\n\ndf.loc[['202212380']]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.loc[['202212380']])\n\npandas.core.frame.DataFrame\n\n\n- 방법7\n\ndf.loc['202212380',:]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.loc['202212380',:])\n\npandas.core.series.Series\n\n\n- 방법8\n\ndf.loc[['202212380'],:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.loc[['202212380'],:])\n\npandas.core.frame.DataFrame\n\n\n- 방법9\n\nlen(df)\n\n20\n\n\n\n_lst = [True]+[False]*19\n\n\ndf.iloc[_lst] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.iloc[_lst,:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.loc[_lst] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.loc[_lst,:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.iloc[[0,2]]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n\ndf.iloc[[0,2],:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n- 방법2\n\ndf.loc[['202212380','202212363']] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n\ndf.loc[['202212380','202212363'],:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n- 그 밖의 방법들\n\ndf.iloc[::3] # 스트라이딩\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212488\n55\n80\n75\n80\n\n\n202212463\n65\n45\n45\n90\n\n\n202212469\n90\n80\n80\n20\n\n\n202212443\n95\n55\n15\n35\n\n\n202212396\n95\n30\n30\n95\n\n\n202212387\n70\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.iloc[:5]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ndf.loc[:'202212312']\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ndf.loc[list(df.att&lt;80),'rep':]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.loc[df.att&lt;80,'rep':]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.iloc[list(df.att&lt;80),1:]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n- 아래는 에러가 난다 주의!\n\ndf.iloc[df.att&lt;80, 1:]\n\nValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-14-pandas2.html#pandas-공부-2단계",
    "href": "posts/1_IP2022/02_DataScience/2023-03-14-pandas2.html#pandas-공부-2단계",
    "title": "[IP2022] Pandas 2단계",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n- 데이터\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\n\n\ndf=pd.DataFrame({'att':att,'rep':rep,'mid':mid,'fin':fin},index=key)\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.att\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 방법2: dict 스타일\n\ndf['att']\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n\ntype(df['att'])\n\npandas.core.series.Series\n\n\n- 방법3: dict 스타일\n\ndf[['att']]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ntype(df[['att']])\n\npandas.core.frame.DataFrame\n\n\n\ndf.att나 df['att']는 series를 리턴하고 df[['att']]는 dataframe을 리턴한다.\n\n- 방법4: ndarray스타일\n\ndf.iloc[:,0] \n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n\ntype(df.iloc[:,0] )\n\npandas.core.series.Series\n\n\n- 방법5: ndarray 스타일\n\ndf.iloc[:,[0]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ntype(df.iloc[:,[0]])\n\npandas.core.frame.DataFrame\n\n\n\ndf.iloc[:,0]은 series를 리턴하고 df.iloc[:,[0]]은 dataframe을 리턴한다.\n\n- 방법6: ndarray 스타일과 dict스타일의 혼합\n\ndf.loc[:,'att'] \n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 방법7: ndarray 스타일과 dict스타일의 혼합\n\ndf.loc[:,['att']] \n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ndf.loc[:,'att']은 series를 리턴하고 df.loc[:,['att']] 은 dataframe을 리턴한다.\n\n- 방법7: ndarray 스타일 + bool 인덱싱\n\ndf.iloc[:,[True,False,False,False]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n- 방법8: ndarray와 dict의 혼합형 + bool 인덱싱\n\ndf.loc[:,[True,False,False,False]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\n\n\n- 방법1: dict 스타일\n\ndf[['att','fin']]\n\n\n\n\n\n\n\n\natt\nfin\n\n\n\n\n202212380\n65\n40\n\n\n202212370\n95\n80\n\n\n202212363\n65\n30\n\n\n202212488\n55\n80\n\n\n202212312\n80\n100\n\n\n202212377\n75\n15\n\n\n202212463\n65\n90\n\n\n202212471\n60\n0\n\n\n202212400\n95\n10\n\n\n202212469\n90\n20\n\n\n202212318\n55\n25\n\n\n202212432\n95\n0\n\n\n202212443\n95\n35\n\n\n202212367\n50\n30\n\n\n202212458\n50\n85\n\n\n202212396\n95\n95\n\n\n202212482\n50\n10\n\n\n202212452\n65\n45\n\n\n202212387\n70\n35\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n- 방법2: ndarray 스타일 (정수리스트로 인덱싱, 슬라이싱, 스트라이딩)\n\ndf.iloc[:,[0,1]] # 정수의 리스트를 전달하여 컬럼추출\n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,range(2)] \n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,:2]  # 슬라이싱 , 0,1,2에서 마지막 2는 제외되고 0,1에 해당하는 것만 추출\n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,::2]  # 스트라이딩\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n- 방법3: ndarray와 dict의 혼합형\n\ndf.loc[:,['att','mid']] \n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,'att':'mid']  # 마지막의 mid도 포함된다. \n\n\n\n\n\n\n\n\natt\nrep\nmid\n\n\n\n\n202212380\n65\n55\n50\n\n\n202212370\n95\n100\n50\n\n\n202212363\n65\n90\n60\n\n\n202212488\n55\n80\n75\n\n\n202212312\n80\n30\n30\n\n\n202212377\n75\n40\n100\n\n\n202212463\n65\n45\n45\n\n\n202212471\n60\n60\n25\n\n\n202212400\n95\n65\n20\n\n\n202212469\n90\n80\n80\n\n\n202212318\n55\n75\n35\n\n\n202212432\n95\n95\n45\n\n\n202212443\n95\n55\n15\n\n\n202212367\n50\n80\n40\n\n\n202212458\n50\n55\n15\n\n\n202212396\n95\n30\n30\n\n\n202212482\n50\n50\n45\n\n\n202212452\n65\n55\n15\n\n\n202212387\n70\n70\n40\n\n\n202212354\n90\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,'rep':] \n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212370\n100\n50\n80\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212312\n30\n30\n100\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212400\n65\n20\n10\n\n\n202212469\n80\n80\n20\n\n\n202212318\n75\n35\n25\n\n\n202212432\n95\n45\n0\n\n\n202212443\n55\n15\n35\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212396\n30\n30\n95\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n202212354\n90\n80\n90\n\n\n\n\n\n\n\n- 방법4: bool을 이용한 인덱싱\n\ndf.iloc[:,[True,False,True,False]]\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,[True,False,True,False]]\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.iloc[0]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.iloc[0])\n\npandas.core.series.Series\n\n\n- 방법2\n\ndf.iloc[[0]]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.iloc[[0]])\n\npandas.core.frame.DataFrame\n\n\n- 방법3\n\ndf.iloc[0,:]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.iloc[0,:])\n\npandas.core.series.Series\n\n\n- 방법4\n\ndf.iloc[[0],:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.iloc[[0],:])\n\npandas.core.frame.DataFrame\n\n\n- 방법5\n\ndf.loc['202212380']\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.loc['202212380'])\n\npandas.core.series.Series\n\n\n- 방법6\n\ndf.loc[['202212380']]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.loc[['202212380']])\n\npandas.core.frame.DataFrame\n\n\n- 방법7\n\ndf.loc['202212380',:]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.loc['202212380',:])\n\npandas.core.series.Series\n\n\n- 방법8\n\ndf.loc[['202212380'],:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.loc[['202212380'],:])\n\npandas.core.frame.DataFrame\n\n\n- 방법9\n\nlen(df)\n\n20\n\n\n\n_lst = [True]+[False]*19\n\n\ndf.iloc[_lst] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.iloc[_lst,:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.loc[_lst] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.loc[_lst,:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.iloc[[0,2]]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n\ndf.iloc[[0,2],:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n- 방법2\n\ndf.loc[['202212380','202212363']] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n\ndf.loc[['202212380','202212363'],:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n- 그 밖의 방법들\n\ndf.iloc[::3] # 스트라이딩\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212488\n55\n80\n75\n80\n\n\n202212463\n65\n45\n45\n90\n\n\n202212469\n90\n80\n80\n20\n\n\n202212443\n95\n55\n15\n35\n\n\n202212396\n95\n30\n30\n95\n\n\n202212387\n70\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.iloc[:5]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ndf.loc[:'202212312']\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ndf.loc[list(df.att&lt;80),'rep':]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.loc[df.att&lt;80,'rep':]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.iloc[list(df.att&lt;80),1:]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n- 아래는 에러가 난다 주의!\n\ndf.iloc[df.att&lt;80, 1:]\n\nValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html",
    "href": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html",
    "title": "[IP2022] Pandas 0단계",
    "section": "",
    "text": "판다스를 왜 써야할까?, pandas 개발동기\n\n\n\nimport numpy as np\nimport pandas as pd\n\n\n\n\n\n\n- 예제1: 기본인덱싱\n\na = 'asdf'\na[2]\n\n'd'\n\n\n\na[-1]\n\n'f'\n\n\n- 예제2: 슬라이싱\n\na='asdf'\na[1:3]\n\n'sd'\n\n\n- 예제3: 스트라이딩\n\na='asdf'\na[::2] # 1번째, 3번째 원소 출력\n\n'ad'\n\n\n- 예제4: 불가능한 것\n\na = 'asdf'\na[[1,2]] # 정수인덱스를 리스트화 시켜서 인덱싱하는 것을 불가능\n\nTypeError: string indices must be integers\n\n\n\n\n\n- 예제1: 인덱스의 리스트 (혹은 ndarray)를 전달\n\na = np.arange(5)\na,a[[1,2,-1]]\n\n(array([0, 1, 2, 3, 4]), array([1, 2, 4]))\n\n\n\na = np.arange(55,61)\na, a[[1,2,-1]]\n\n(array([55, 56, 57, 58, 59, 60]), array([56, 57, 60]))\n\n\n- 예제2: bool로 이루어진 리스트 (혹은 ndarray)를 전달\n\na[[True, True, False, False, False, False]]\n\narray([55, 56])\n\n\n\na[np.array([True, True, False, False, False, False])] # 꼭 리스트로 전달할 필요는 없음.\n\narray([55, 56])\n\n\n\na[a&lt;58]\n\narray([55, 56, 57])\n\n\n\n\n\n- 예제1\n\na = np.arange(4*3).reshape(4,3)\na\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])\n\n\n\na[0:2,1]\n\narray([1, 4])\n\n\n- 예제2: 차원을 유지하면서 인덱싱을 하고 싶으면?\n\na = np.arange(4*3).reshape(4,3)\na[0:2, [1]]\n\narray([[1],\n       [4]])\n\n\n\n\n\n- 예제1: (key, value)o\n\nd = {'att':65, 'rep':45, 'mid':30, 'fin':100}\nd\n\n{'att': 65, 'rep': 45, 'mid': 30, 'fin': 100}\n\n\n\nd['att'] # key를 넣으면 value가 리턴\n\n65\n\n\n- 예제2: numpy와 비교\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,200)\nrep = np.random.choice(np.arange(5,21)*5,200)\nmid = np.random.choice(np.arange(0,21)*5,200)\nfin = np.random.choice(np.arange(0,21)*5,200)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),200,replace=False)]\ntest_dic = {key[i] : {'att':att[i], 'rep':rep[i], 'mid':mid[i], 'fin':fin[i]} for i in range(200)}\ntest_ndarray = np.array([key,att,rep,mid,fin],dtype=np.int64).T\ndel(att);del(rep);del(mid);del(fin);del(key)\n\n\n#test_dic\n\n학번 202212460에 해당하는 학생의 출석점수를 알고 싶다면?\n- (풀이1)\n\ntest_dic['202212460']['att'] ## 가독성이 좋음.\n\n55\n\n\n- (풀이2)\n\ntest_ndarray[test_ndarray[:,0] == 202212460, 1] ## 가독성이 떨어짐.\n\narray([55])\n\n\n정보를 뽑을 때 Numpy indexing을 이용하는 것보다 딕셔너리를 이용하고 hash 타입으로 접근하는것이 편리할 때가 많이 있다.\n(풀이2)가 (풀이1)에 비하여 불편한 점\n\ntest_ndarray의 첫칼럼은 student id 이고 두번째 칼럼은 att라는 사실을 암기하고 있어야 한다.\nstudent id가 아니고 만약에 학생이름을 써서 데이터를 정리한다면 모든 자료형은 문자형이 되어야 한다.\n작성한 코드의 가독성이 없다. (위치로 접근하기 때문)\n\n- 요약: hash 스타일로 정보를 추출하는 것이 유용할 때가 있다. 그리고 보통 hash 스타일로 정보를 뽑는 것이 유리하다. (사실 Numpy는 정보추출을 위해 개발된 자료형이 아니라 행렬 및 벡터의 수학연산을 지원하기 위해 개발된 자료형이다.)\n- 소망: 정보를 추출할때는 hash 스타일도 유용하다는 것은 이해함 \\(\\to\\) 하지만 나는 넘파이스타일로 정보를 뽑고 싶은걸? 그리고 딕셔너리 형태가 아니고 엑셀처럼(행렬처럼) 데이터를 보고 싶은걸? \\(\\to\\) pandas의 개발\n\n\n\n\n\n\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\ntest_dic = {key[i] : {'att':att[i], 'rep':rep[i], 'mid':mid[i], 'fin':fin[i]} for i in range(20)}\n\n\ntest_dic\n\n{'202212380': {'att': 65, 'rep': 55, 'mid': 50, 'fin': 40},\n '202212370': {'att': 95, 'rep': 100, 'mid': 50, 'fin': 80},\n '202212363': {'att': 65, 'rep': 90, 'mid': 60, 'fin': 30},\n '202212488': {'att': 55, 'rep': 80, 'mid': 75, 'fin': 80},\n '202212312': {'att': 80, 'rep': 30, 'mid': 30, 'fin': 100},\n '202212377': {'att': 75, 'rep': 40, 'mid': 100, 'fin': 15},\n '202212463': {'att': 65, 'rep': 45, 'mid': 45, 'fin': 90},\n '202212471': {'att': 60, 'rep': 60, 'mid': 25, 'fin': 0},\n '202212400': {'att': 95, 'rep': 65, 'mid': 20, 'fin': 10},\n '202212469': {'att': 90, 'rep': 80, 'mid': 80, 'fin': 20},\n '202212318': {'att': 55, 'rep': 75, 'mid': 35, 'fin': 25},\n '202212432': {'att': 95, 'rep': 95, 'mid': 45, 'fin': 0},\n '202212443': {'att': 95, 'rep': 55, 'mid': 15, 'fin': 35},\n '202212367': {'att': 50, 'rep': 80, 'mid': 40, 'fin': 30},\n '202212458': {'att': 50, 'rep': 55, 'mid': 15, 'fin': 85},\n '202212396': {'att': 95, 'rep': 30, 'mid': 30, 'fin': 95},\n '202212482': {'att': 50, 'rep': 50, 'mid': 45, 'fin': 10},\n '202212452': {'att': 65, 'rep': 55, 'mid': 15, 'fin': 45},\n '202212387': {'att': 70, 'rep': 70, 'mid': 40, 'fin': 35},\n '202212354': {'att': 90, 'rep': 90, 'mid': 80, 'fin': 90}}\n\n\n\n테이블형태로 보고싶다.\n\n(방법1) – 행렬이기는 하지만 방법 2,3,4,5에 비하여 우리가 원하는 만큼 가독성을 주는 형태는 아님.\n\ntest_ndarray = np.array([key,att,rep,mid,fin],dtype=np.int64).T\ntest_ndarray\n\narray([[202212380,        65,        55,        50,        40],\n       [202212370,        95,       100,        50,        80],\n       [202212363,        65,        90,        60,        30],\n       [202212488,        55,        80,        75,        80],\n       [202212312,        80,        30,        30,       100],\n       [202212377,        75,        40,       100,        15],\n       [202212463,        65,        45,        45,        90],\n       [202212471,        60,        60,        25,         0],\n       [202212400,        95,        65,        20,        10],\n       [202212469,        90,        80,        80,        20],\n       [202212318,        55,        75,        35,        25],\n       [202212432,        95,        95,        45,         0],\n       [202212443,        95,        55,        15,        35],\n       [202212367,        50,        80,        40,        30],\n       [202212458,        50,        55,        15,        85],\n       [202212396,        95,        30,        30,        95],\n       [202212482,        50,        50,        45,        10],\n       [202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n(방법2)\n\npd.DataFrame(test_dic).T\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법3)\n\ntest_dic2 = {'att':{key[i]:att[i] for i in range(20)},\n             'rep':{key[i]:rep[i] for i in range(20)},\n             'mid':{key[i]:mid[i] for i in range(20)},\n             'fin':{key[i]:fin[i] for i in range(20)}}\n\n\npd.DataFrame(test_dic2)\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법4)\n\ndf = pd.DataFrame({'att':att, 'rep':rep, 'mid':mid, 'fin':fin}, index=key)\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법5)\n\ndf = pd.DataFrame({'att':att, 'rep':rep, 'mid':mid, 'fin':fin})\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n0\n65\n55\n50\n40\n\n\n1\n95\n100\n50\n80\n\n\n2\n65\n90\n60\n30\n\n\n3\n55\n80\n75\n80\n\n\n4\n80\n30\n30\n100\n\n\n5\n75\n40\n100\n15\n\n\n6\n65\n45\n45\n90\n\n\n7\n60\n60\n25\n0\n\n\n8\n95\n65\n20\n10\n\n\n9\n90\n80\n80\n20\n\n\n10\n55\n75\n35\n25\n\n\n11\n95\n95\n45\n0\n\n\n12\n95\n55\n15\n35\n\n\n13\n50\n80\n40\n30\n\n\n14\n50\n55\n15\n85\n\n\n15\n95\n30\n30\n95\n\n\n16\n50\n50\n45\n10\n\n\n17\n65\n55\n15\n45\n\n\n18\n70\n70\n40\n35\n\n\n19\n90\n90\n80\n90\n\n\n\n\n\n\n\n\ndf = df.set_index([key])\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\n\n\n- 예제1: 출석점수를 출력\n\ntest_dic2['att']\n\n{'202212380': 65,\n '202212370': 95,\n '202212363': 65,\n '202212488': 55,\n '202212312': 80,\n '202212377': 75,\n '202212463': 65,\n '202212471': 60,\n '202212400': 95,\n '202212469': 90,\n '202212318': 55,\n '202212432': 95,\n '202212443': 95,\n '202212367': 50,\n '202212458': 50,\n '202212396': 95,\n '202212482': 50,\n '202212452': 65,\n '202212387': 70,\n '202212354': 90}\n\n\n\ndf['att']\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 예제2: 학번 202212380의 출석점수 출력\n\ntest_dic2['att']['202212380']\n\n65\n\n\n\ndf['att']['202212380']\n\n65\n\n\n\n\n\n- 예제1: 첫번째 학생의 기말고사 성적을 출력하고 싶다.\n\ntest_ndarray[0,-1]\n\n40\n\n\n\ndf.iloc[0,-1]\n\n40\n\n\n\n벼락치기: df에서 iloc이라는 특수기능을 이용하면 넘파이 인덱싱처럼 원소출력이 가능하다.\n\n- 예제2: 홀수번째 학생의 점수를 뽑고 싶다.\n\ntest_ndarray[::2]\n\narray([[202212380,        65,        55,        50,        40],\n       [202212363,        65,        90,        60,        30],\n       [202212312,        80,        30,        30,       100],\n       [202212463,        65,        45,        45,        90],\n       [202212400,        95,        65,        20,        10],\n       [202212318,        55,        75,        35,        25],\n       [202212443,        95,        55,        15,        35],\n       [202212458,        50,        55,        15,        85],\n       [202212482,        50,        50,        45,        10],\n       [202212387,        70,        70,        40,        35]])\n\n\n\ndf.iloc[::2]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n202212312\n80\n30\n30\n100\n\n\n202212463\n65\n45\n45\n90\n\n\n202212400\n95\n65\n20\n10\n\n\n202212318\n55\n75\n35\n25\n\n\n202212443\n95\n55\n15\n35\n\n\n202212458\n50\n55\n15\n85\n\n\n202212482\n50\n50\n45\n10\n\n\n202212387\n70\n70\n40\n35\n\n\n\n\n\n\n\n- 예제3: 맨 끝에서 3명의 점수를 출력하고 싶다.\n\ntest_ndarray[-3:]\n\narray([[202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n\ndf.iloc[-3:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n- 예제4: 맨 끝에서 3명의 점수 중 마지막 2개의 칼럼만 출력하고 싶다.\n\ntest_ndarray[-3:,-2:]\n\narray([[15, 45],\n       [40, 35],\n       [80, 90]])\n\n\n\ndf.iloc[-3:,-2:]\n\n\n\n\n\n\n\n\nmid\nfin\n\n\n\n\n202212452\n15\n45\n\n\n202212387\n40\n35\n\n\n202212354\n80\n90\n\n\n\n\n\n\n\n\n\n\n- 예제1: 중간고사 점수가 20점 이상이면서 동시에 출석점수가 60점미만인 학생들의 기말고사 점수를 출력\n\ndf.query('mid &gt;= 20 and att &lt; 60')\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212488\n55\n80\n75\n80\n\n\n202212318\n55\n75\n35\n25\n\n\n202212367\n50\n80\n40\n30\n\n\n202212482\n50\n50\n45\n10\n\n\n\n\n\n\n\n\ndf.query('mid &gt;= 20 and att &lt; 60')['fin']\n\n202212488    80\n202212318    25\n202212367    30\n202212482    10\nName: fin, dtype: int64\n\n\n(방법2) 넘파이 스타일이라면?\n\ntest_ndarray\n\narray([[202212380,        65,        55,        50,        40],\n       [202212370,        95,       100,        50,        80],\n       [202212363,        65,        90,        60,        30],\n       [202212488,        55,        80,        75,        80],\n       [202212312,        80,        30,        30,       100],\n       [202212377,        75,        40,       100,        15],\n       [202212463,        65,        45,        45,        90],\n       [202212471,        60,        60,        25,         0],\n       [202212400,        95,        65,        20,        10],\n       [202212469,        90,        80,        80,        20],\n       [202212318,        55,        75,        35,        25],\n       [202212432,        95,        95,        45,         0],\n       [202212443,        95,        55,        15,        35],\n       [202212367,        50,        80,        40,        30],\n       [202212458,        50,        55,        15,        85],\n       [202212396,        95,        30,        30,        95],\n       [202212482,        50,        50,        45,        10],\n       [202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n\ntest_ndarray[:,3] &gt;= 20 ## 중간고사가 20점이상\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True, False,  True, False,  True,  True, False,\n        True,  True])\n\n\n\ntest_ndarray[:,1] &lt; 60 ## 출석이 60미만 \n\narray([False, False, False,  True, False, False, False, False, False,\n       False,  True, False, False,  True,  True, False,  True, False,\n       False, False])\n\n\n\n(test_ndarray[:,3] &gt;= 20) & (test_ndarray[:,1] &lt; 60)\n\narray([False, False, False,  True, False, False, False, False, False,\n       False,  True, False, False,  True, False, False,  True, False,\n       False, False])\n\n\n\nnote: test_ndarray[:,3] &gt;= 20 & test_ndarray[:,1] &gt;= 60와 같이 하면 에러가 난다. 조심하자! 괄호!!!\n\n\ntest_ndarray[(test_ndarray[:,3] &gt;= 20) & (test_ndarray[:,1] &lt; 60),-1]\n\narray([80, 25, 30, 10])\n\n\n\n구현난이도 어려움, 가독성 꽝..\n\n- 예제2: 중간고사점수&lt;기말고사점수인 학생들의 출석점수 평균을 구하자.\n\ndf.query('mid &lt; fin')\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212370\n95\n100\n50\n80\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212463\n65\n45\n45\n90\n\n\n202212443\n95\n55\n15\n35\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212452\n65\n55\n15\n45\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\ndf.query('mid &lt; fin')['att'].mean()\n\n76.66666666666667\n\n\n\n\n\n\n\n- 방법1: dictionary에서 만든다.\n\npd.DataFrame({'att':[30,40,50], 'mid':[50,60,70]}"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#import",
    "href": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#import",
    "title": "[IP2022] Pandas 0단계",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#부분-데이터-꺼내기-판다스를-왜-써야할까",
    "href": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#부분-데이터-꺼내기-판다스를-왜-써야할까",
    "title": "[IP2022] Pandas 0단계",
    "section": "",
    "text": "- 예제1: 기본인덱싱\n\na = 'asdf'\na[2]\n\n'd'\n\n\n\na[-1]\n\n'f'\n\n\n- 예제2: 슬라이싱\n\na='asdf'\na[1:3]\n\n'sd'\n\n\n- 예제3: 스트라이딩\n\na='asdf'\na[::2] # 1번째, 3번째 원소 출력\n\n'ad'\n\n\n- 예제4: 불가능한 것\n\na = 'asdf'\na[[1,2]] # 정수인덱스를 리스트화 시켜서 인덱싱하는 것을 불가능\n\nTypeError: string indices must be integers\n\n\n\n\n\n- 예제1: 인덱스의 리스트 (혹은 ndarray)를 전달\n\na = np.arange(5)\na,a[[1,2,-1]]\n\n(array([0, 1, 2, 3, 4]), array([1, 2, 4]))\n\n\n\na = np.arange(55,61)\na, a[[1,2,-1]]\n\n(array([55, 56, 57, 58, 59, 60]), array([56, 57, 60]))\n\n\n- 예제2: bool로 이루어진 리스트 (혹은 ndarray)를 전달\n\na[[True, True, False, False, False, False]]\n\narray([55, 56])\n\n\n\na[np.array([True, True, False, False, False, False])] # 꼭 리스트로 전달할 필요는 없음.\n\narray([55, 56])\n\n\n\na[a&lt;58]\n\narray([55, 56, 57])\n\n\n\n\n\n- 예제1\n\na = np.arange(4*3).reshape(4,3)\na\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])\n\n\n\na[0:2,1]\n\narray([1, 4])\n\n\n- 예제2: 차원을 유지하면서 인덱싱을 하고 싶으면?\n\na = np.arange(4*3).reshape(4,3)\na[0:2, [1]]\n\narray([[1],\n       [4]])\n\n\n\n\n\n- 예제1: (key, value)o\n\nd = {'att':65, 'rep':45, 'mid':30, 'fin':100}\nd\n\n{'att': 65, 'rep': 45, 'mid': 30, 'fin': 100}\n\n\n\nd['att'] # key를 넣으면 value가 리턴\n\n65\n\n\n- 예제2: numpy와 비교\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,200)\nrep = np.random.choice(np.arange(5,21)*5,200)\nmid = np.random.choice(np.arange(0,21)*5,200)\nfin = np.random.choice(np.arange(0,21)*5,200)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),200,replace=False)]\ntest_dic = {key[i] : {'att':att[i], 'rep':rep[i], 'mid':mid[i], 'fin':fin[i]} for i in range(200)}\ntest_ndarray = np.array([key,att,rep,mid,fin],dtype=np.int64).T\ndel(att);del(rep);del(mid);del(fin);del(key)\n\n\n#test_dic\n\n학번 202212460에 해당하는 학생의 출석점수를 알고 싶다면?\n- (풀이1)\n\ntest_dic['202212460']['att'] ## 가독성이 좋음.\n\n55\n\n\n- (풀이2)\n\ntest_ndarray[test_ndarray[:,0] == 202212460, 1] ## 가독성이 떨어짐.\n\narray([55])\n\n\n정보를 뽑을 때 Numpy indexing을 이용하는 것보다 딕셔너리를 이용하고 hash 타입으로 접근하는것이 편리할 때가 많이 있다.\n(풀이2)가 (풀이1)에 비하여 불편한 점\n\ntest_ndarray의 첫칼럼은 student id 이고 두번째 칼럼은 att라는 사실을 암기하고 있어야 한다.\nstudent id가 아니고 만약에 학생이름을 써서 데이터를 정리한다면 모든 자료형은 문자형이 되어야 한다.\n작성한 코드의 가독성이 없다. (위치로 접근하기 때문)\n\n- 요약: hash 스타일로 정보를 추출하는 것이 유용할 때가 있다. 그리고 보통 hash 스타일로 정보를 뽑는 것이 유리하다. (사실 Numpy는 정보추출을 위해 개발된 자료형이 아니라 행렬 및 벡터의 수학연산을 지원하기 위해 개발된 자료형이다.)\n- 소망: 정보를 추출할때는 hash 스타일도 유용하다는 것은 이해함 \\(\\to\\) 하지만 나는 넘파이스타일로 정보를 뽑고 싶은걸? 그리고 딕셔너리 형태가 아니고 엑셀처럼(행렬처럼) 데이터를 보고 싶은걸? \\(\\to\\) pandas의 개발"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#pandas-개발동기",
    "href": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#pandas-개발동기",
    "title": "[IP2022] Pandas 0단계",
    "section": "",
    "text": "np.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\ntest_dic = {key[i] : {'att':att[i], 'rep':rep[i], 'mid':mid[i], 'fin':fin[i]} for i in range(20)}\n\n\ntest_dic\n\n{'202212380': {'att': 65, 'rep': 55, 'mid': 50, 'fin': 40},\n '202212370': {'att': 95, 'rep': 100, 'mid': 50, 'fin': 80},\n '202212363': {'att': 65, 'rep': 90, 'mid': 60, 'fin': 30},\n '202212488': {'att': 55, 'rep': 80, 'mid': 75, 'fin': 80},\n '202212312': {'att': 80, 'rep': 30, 'mid': 30, 'fin': 100},\n '202212377': {'att': 75, 'rep': 40, 'mid': 100, 'fin': 15},\n '202212463': {'att': 65, 'rep': 45, 'mid': 45, 'fin': 90},\n '202212471': {'att': 60, 'rep': 60, 'mid': 25, 'fin': 0},\n '202212400': {'att': 95, 'rep': 65, 'mid': 20, 'fin': 10},\n '202212469': {'att': 90, 'rep': 80, 'mid': 80, 'fin': 20},\n '202212318': {'att': 55, 'rep': 75, 'mid': 35, 'fin': 25},\n '202212432': {'att': 95, 'rep': 95, 'mid': 45, 'fin': 0},\n '202212443': {'att': 95, 'rep': 55, 'mid': 15, 'fin': 35},\n '202212367': {'att': 50, 'rep': 80, 'mid': 40, 'fin': 30},\n '202212458': {'att': 50, 'rep': 55, 'mid': 15, 'fin': 85},\n '202212396': {'att': 95, 'rep': 30, 'mid': 30, 'fin': 95},\n '202212482': {'att': 50, 'rep': 50, 'mid': 45, 'fin': 10},\n '202212452': {'att': 65, 'rep': 55, 'mid': 15, 'fin': 45},\n '202212387': {'att': 70, 'rep': 70, 'mid': 40, 'fin': 35},\n '202212354': {'att': 90, 'rep': 90, 'mid': 80, 'fin': 90}}\n\n\n\n테이블형태로 보고싶다.\n\n(방법1) – 행렬이기는 하지만 방법 2,3,4,5에 비하여 우리가 원하는 만큼 가독성을 주는 형태는 아님.\n\ntest_ndarray = np.array([key,att,rep,mid,fin],dtype=np.int64).T\ntest_ndarray\n\narray([[202212380,        65,        55,        50,        40],\n       [202212370,        95,       100,        50,        80],\n       [202212363,        65,        90,        60,        30],\n       [202212488,        55,        80,        75,        80],\n       [202212312,        80,        30,        30,       100],\n       [202212377,        75,        40,       100,        15],\n       [202212463,        65,        45,        45,        90],\n       [202212471,        60,        60,        25,         0],\n       [202212400,        95,        65,        20,        10],\n       [202212469,        90,        80,        80,        20],\n       [202212318,        55,        75,        35,        25],\n       [202212432,        95,        95,        45,         0],\n       [202212443,        95,        55,        15,        35],\n       [202212367,        50,        80,        40,        30],\n       [202212458,        50,        55,        15,        85],\n       [202212396,        95,        30,        30,        95],\n       [202212482,        50,        50,        45,        10],\n       [202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n(방법2)\n\npd.DataFrame(test_dic).T\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법3)\n\ntest_dic2 = {'att':{key[i]:att[i] for i in range(20)},\n             'rep':{key[i]:rep[i] for i in range(20)},\n             'mid':{key[i]:mid[i] for i in range(20)},\n             'fin':{key[i]:fin[i] for i in range(20)}}\n\n\npd.DataFrame(test_dic2)\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법4)\n\ndf = pd.DataFrame({'att':att, 'rep':rep, 'mid':mid, 'fin':fin}, index=key)\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법5)\n\ndf = pd.DataFrame({'att':att, 'rep':rep, 'mid':mid, 'fin':fin})\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n0\n65\n55\n50\n40\n\n\n1\n95\n100\n50\n80\n\n\n2\n65\n90\n60\n30\n\n\n3\n55\n80\n75\n80\n\n\n4\n80\n30\n30\n100\n\n\n5\n75\n40\n100\n15\n\n\n6\n65\n45\n45\n90\n\n\n7\n60\n60\n25\n0\n\n\n8\n95\n65\n20\n10\n\n\n9\n90\n80\n80\n20\n\n\n10\n55\n75\n35\n25\n\n\n11\n95\n95\n45\n0\n\n\n12\n95\n55\n15\n35\n\n\n13\n50\n80\n40\n30\n\n\n14\n50\n55\n15\n85\n\n\n15\n95\n30\n30\n95\n\n\n16\n50\n50\n45\n10\n\n\n17\n65\n55\n15\n45\n\n\n18\n70\n70\n40\n35\n\n\n19\n90\n90\n80\n90\n\n\n\n\n\n\n\n\ndf = df.set_index([key])\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\n\n\n- 예제1: 출석점수를 출력\n\ntest_dic2['att']\n\n{'202212380': 65,\n '202212370': 95,\n '202212363': 65,\n '202212488': 55,\n '202212312': 80,\n '202212377': 75,\n '202212463': 65,\n '202212471': 60,\n '202212400': 95,\n '202212469': 90,\n '202212318': 55,\n '202212432': 95,\n '202212443': 95,\n '202212367': 50,\n '202212458': 50,\n '202212396': 95,\n '202212482': 50,\n '202212452': 65,\n '202212387': 70,\n '202212354': 90}\n\n\n\ndf['att']\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 예제2: 학번 202212380의 출석점수 출력\n\ntest_dic2['att']['202212380']\n\n65\n\n\n\ndf['att']['202212380']\n\n65\n\n\n\n\n\n- 예제1: 첫번째 학생의 기말고사 성적을 출력하고 싶다.\n\ntest_ndarray[0,-1]\n\n40\n\n\n\ndf.iloc[0,-1]\n\n40\n\n\n\n벼락치기: df에서 iloc이라는 특수기능을 이용하면 넘파이 인덱싱처럼 원소출력이 가능하다.\n\n- 예제2: 홀수번째 학생의 점수를 뽑고 싶다.\n\ntest_ndarray[::2]\n\narray([[202212380,        65,        55,        50,        40],\n       [202212363,        65,        90,        60,        30],\n       [202212312,        80,        30,        30,       100],\n       [202212463,        65,        45,        45,        90],\n       [202212400,        95,        65,        20,        10],\n       [202212318,        55,        75,        35,        25],\n       [202212443,        95,        55,        15,        35],\n       [202212458,        50,        55,        15,        85],\n       [202212482,        50,        50,        45,        10],\n       [202212387,        70,        70,        40,        35]])\n\n\n\ndf.iloc[::2]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n202212312\n80\n30\n30\n100\n\n\n202212463\n65\n45\n45\n90\n\n\n202212400\n95\n65\n20\n10\n\n\n202212318\n55\n75\n35\n25\n\n\n202212443\n95\n55\n15\n35\n\n\n202212458\n50\n55\n15\n85\n\n\n202212482\n50\n50\n45\n10\n\n\n202212387\n70\n70\n40\n35\n\n\n\n\n\n\n\n- 예제3: 맨 끝에서 3명의 점수를 출력하고 싶다.\n\ntest_ndarray[-3:]\n\narray([[202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n\ndf.iloc[-3:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n- 예제4: 맨 끝에서 3명의 점수 중 마지막 2개의 칼럼만 출력하고 싶다.\n\ntest_ndarray[-3:,-2:]\n\narray([[15, 45],\n       [40, 35],\n       [80, 90]])\n\n\n\ndf.iloc[-3:,-2:]\n\n\n\n\n\n\n\n\nmid\nfin\n\n\n\n\n202212452\n15\n45\n\n\n202212387\n40\n35\n\n\n202212354\n80\n90\n\n\n\n\n\n\n\n\n\n\n- 예제1: 중간고사 점수가 20점 이상이면서 동시에 출석점수가 60점미만인 학생들의 기말고사 점수를 출력\n\ndf.query('mid &gt;= 20 and att &lt; 60')\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212488\n55\n80\n75\n80\n\n\n202212318\n55\n75\n35\n25\n\n\n202212367\n50\n80\n40\n30\n\n\n202212482\n50\n50\n45\n10\n\n\n\n\n\n\n\n\ndf.query('mid &gt;= 20 and att &lt; 60')['fin']\n\n202212488    80\n202212318    25\n202212367    30\n202212482    10\nName: fin, dtype: int64\n\n\n(방법2) 넘파이 스타일이라면?\n\ntest_ndarray\n\narray([[202212380,        65,        55,        50,        40],\n       [202212370,        95,       100,        50,        80],\n       [202212363,        65,        90,        60,        30],\n       [202212488,        55,        80,        75,        80],\n       [202212312,        80,        30,        30,       100],\n       [202212377,        75,        40,       100,        15],\n       [202212463,        65,        45,        45,        90],\n       [202212471,        60,        60,        25,         0],\n       [202212400,        95,        65,        20,        10],\n       [202212469,        90,        80,        80,        20],\n       [202212318,        55,        75,        35,        25],\n       [202212432,        95,        95,        45,         0],\n       [202212443,        95,        55,        15,        35],\n       [202212367,        50,        80,        40,        30],\n       [202212458,        50,        55,        15,        85],\n       [202212396,        95,        30,        30,        95],\n       [202212482,        50,        50,        45,        10],\n       [202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n\ntest_ndarray[:,3] &gt;= 20 ## 중간고사가 20점이상\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True, False,  True, False,  True,  True, False,\n        True,  True])\n\n\n\ntest_ndarray[:,1] &lt; 60 ## 출석이 60미만 \n\narray([False, False, False,  True, False, False, False, False, False,\n       False,  True, False, False,  True,  True, False,  True, False,\n       False, False])\n\n\n\n(test_ndarray[:,3] &gt;= 20) & (test_ndarray[:,1] &lt; 60)\n\narray([False, False, False,  True, False, False, False, False, False,\n       False,  True, False, False,  True, False, False,  True, False,\n       False, False])\n\n\n\nnote: test_ndarray[:,3] &gt;= 20 & test_ndarray[:,1] &gt;= 60와 같이 하면 에러가 난다. 조심하자! 괄호!!!\n\n\ntest_ndarray[(test_ndarray[:,3] &gt;= 20) & (test_ndarray[:,1] &lt; 60),-1]\n\narray([80, 25, 30, 10])\n\n\n\n구현난이도 어려움, 가독성 꽝..\n\n- 예제2: 중간고사점수&lt;기말고사점수인 학생들의 출석점수 평균을 구하자.\n\ndf.query('mid &lt; fin')\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212370\n95\n100\n50\n80\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212463\n65\n45\n45\n90\n\n\n202212443\n95\n55\n15\n35\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212452\n65\n55\n15\n45\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\ndf.query('mid &lt; fin')['att'].mean()\n\n76.66666666666667\n\n\n\n\n\n\n\n- 방법1: dictionary에서 만든다.\n\npd.DataFrame({'att':[30,40,50], 'mid':[50,60,70]}"
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022-06-13-final.html",
    "href": "posts/1_IP2022/04_시험/2022-06-13-final.html",
    "title": "[2022 EXAM] 2022 final exam",
    "section": "",
    "text": "ref: 기말고사 풀이 링크\n\n\n\n아래코드를 이용하여 numpy, matplotlib, pandas를 import하라.\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd\nfrom IPython.display import HTML\n\n\n\n\n(1) 도함수를 구하는 함수 derivate를 선언하라. 이 함수를 이용하여 \\(f(x)=x^2\\)의 그래프와 \\(f'(x)=2x\\)의 그래프를 \\(x \\in (-1,1)\\)의 범위에서 그려라.\n\ndef f(x):\n    return x**2\n\n\ndef derivate(f): \n    def df(x): \n        h=0.00000000000001\n        return (f(x+h)-f(x))/h \n    return df\n\n\nx = np.linspace(-1,1,100)\nplt.plot(x, f(x))  ## f(x)=x**2\nplt.plot(x, derivate(f)(x)) ## f'(x)=2*x\n\n\n\n\n(2) 적당한 클래스 정의하여 인스턴스 a를 만들고 print(a)의 출력결과가 본인의 학번이 나오도록 하라.\n## 코드예시\nclass Klass:\n    ???\n    ???\na=Klass()\nprint(a)\n## 출력결과\n2022-43052\n\nclass Klass:\n    def __str__(self):\n        return('12345678')\n\n\na = Klass()\n\n\nprint(a)\n\n12345678\n\n\n(3) for문이 실행될때마다 [묵,찌,빠] 중에 하나를 내며 빠를 누적 3회 낼경우 for문이 멈추는 이터레이터를 생성하라.\n(나의풀이)\n\nclass Klass: # 빠를 누적 3회 낼 경우 for문이 멈추는 이터레이터를 만들자.\n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n        self.n = 0\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        if action == '빠':\n            self.n += 1\n            print(action,self.n)\n            if self.n == 3:\n                print('빠가 누적3회 나와서 for문을 멈춥니다.')\n                raise StopIteration\n            else:\n                return action\n        else:\n            return action\n\n\na = Klass()\n\n\nfor i in a:\n    print(i)\n\n찌\n찌\n묵\n찌\n찌\n묵\n빠 1\n빠\n찌\n빠 2\n빠\n빠 3\n빠가 누적3회 나와서 for문을 멈춥니다.\n\n\n(모범답안)\n\nclass Klass: \n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n        self.dic = {'묵':0,'찌':0,'빠':0}\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        self.dic[action] += 1\n        if self.dic['빠'] == 3:\n            print('빠가 3번 누적되어 for문을 멈춥니다.')\n            raise StopIteration\n        else:\n            return action\n\n\na = Klass()\nfor i in a:\n    print(i)\n\n묵\n빠\n찌\n찌\n빠\n빠가 3번 누적되어 for문을 멈춥니다.\n\n\n(4)-(6)\nclass GS25: \n    n=0 \n    total_number_of_guests = 0 \n    def __init__(self):\n        self.number_of_guests = 0 \n(4) 위의 클래스를 수정하여 아래와 같이 GS25에서 새로운 인스턴스가 생성될때마다\nGS25의 점포수가 ?개로 늘었습니다.\n라는 메시지가 출력되도록 하라.\n(5) 함수 come를 인스턴스 메소드로 정의하라. 이 메소드가 실행될때마다 각 점포의 손님 인스턴스 변수 number_of_guests와 클래스변수 total_number_of_guests를 1씩 증가시키고 아래의 메시지를 출력하라.\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 n명입니다. \n현재 GS25 점포를 방문한 손님수는 m명입니다. \n(6) 새로운 클래스메서드 show를 만들고 아래와 같은 메시지를 출력하도록 하라.\nGS25의 점포수: ??\nGS25를 방문한 총 손님수: ??\n(사용예시) (4)-(6)을 모두 적용한 경우 사용예시는 아래와 같다.\n\nclass GS25:\n    n = 0\n    total_numer_of_guests = 0\n    def __init__(self):\n        self.number_of_guests = 0\n        GS25.n += 1\n        print('GS25의 점포수가 {}개로 늘었습니다.'.format(GS25.n))\n    def come(self):\n        GS25.total_number_of_guests += 1\n        self.number_of_guests += 1\n        print('새로운 손님이 오셨습니다.')\n        print('GS25를 방문한 총 손님 수는 {}명입니다.'.format(GS25.total_number_of_guests))\n        print('현재 GS25 점포를 방문한 손님수는 {}명입니다.'.format(self.number_of_guests))\n    @classmethod\n    def show(cls):\n        print('GS25의 점포수:{}'.format(cls.n))\n        print('GS를 방문한 총 손님 수: {}'.format(cls.total_number_of_guests))\n\n\na = GS25()\n\nGS25의 점포수가 5개로 늘었습니다.\n\n\n\na=GS25() ## (4)의 사용예시\n\nGS25의 점포수가 1개로 늘었습니다.\n\n\n\nb=GS25() ## (4)의 사용예시\n\nGS25의 점포수가 2개로 늘었습니다.\n\n\n\na.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 1명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다. \n\n\n\na.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 2명입니다.\n현재 GS25 점포를 방문한 손님수는 2명입니다. \n\n\n\nb.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 3명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다. \n\n\n\nGS25.show() ## (6)의 사용예시\n\nGS25의 점포수: 2\nGS25를 방문한 총 손님수: 3\n\n\n(풀이시작)\n\nclass GS25: \n    n=0 \n    total_number_of_guests = 0 \n    def __init__(self):\n        self.number_of_guests = 0\n        GS25.n += 1\n        print('GS25의 점포수가 {}개로 늘었습니다.'.format(GS25.n))\n    def come(self):\n        self.number_of_guests += 1\n        GS25.total_number_of_guests += 1\n        print('새로운 손님이 오셨습니다!')\n        print('GS25를 방문한 총 손님수는 {}명입니다.'.format(GS25.total_number_of_guests))\n        print('현재 GS25 점포를 방문한 손님수는 {}명입니다.'.format(self.number_of_guests))\n    @classmethod\n    def show(cls):\n        print('GS25의 점포수: {}'.format(cls.n))\n        print('GS25를 방문한 총 손님수: {}'.format(cls.total_number_of_guests))\n\n\na = GS25()\n\nGS25의 점포수가 1개로 늘었습니다.\n\n\n\nb = GS25()\n\nGS25의 점포수가 2개로 늘었습니다.\n\n\n\na.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 1명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다.\n\n\n\na.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 2명입니다.\n현재 GS25 점포를 방문한 손님수는 2명입니다.\n\n\n\nb.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 3명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다.\n\n\n\nGS25.show()\n\nGS25의 점포수: 2\nGS25를 방문한 총 손님수: 3\n\n\n(7) __eq__는 연산 == 를 재정의하는 메소드이다. 클래스 RPS_BASE를 상속하여 새로운 클래스 RPS5를 만들라. 연산 ==를 재정의하여 RPS5의 두 인스턴스의 action이 같은 경우 true를 리턴하는 기능을 구현하라.\n\nclass RPS_BASE:\n    def __init__(self):\n        self.action = np.random.choice(['가위','바위','보'])\n\nhint: Appendix를 참고할 것\nhint: RPS5의 선언부분은 아래와 같은 형태를 가지고 있다.\nclass RPS5(???):\n    def __eq__(self,other):\n        return ??????\nhint: RPS5클래스의 사용예시는 아래와 같다.\n\na=RPS5()\na.action\n\n'바위'\n\n\n\nb=RPS5()\nb.action\n\n'보'\n\n\n\na==b\n\nFalse\n\n\n(풀이시작)\n(8) __gt__는 연산 &gt; 를 재정의하는 메소드이다. 클래스 RPS_BASE를 상속하여 새로운 클래스 RPS6를 만들라. 연산 &gt;를 재정의하여 RPS6의 두 인스턴스 a,b의 action이 각각 (‘가위’,‘보’), (‘바위’,‘가위’), (‘보’,‘바위’) 인 경우 true를 리턴하는 기능을 구현하라.\nhint: Appendix를 참고할 것\nhint: RPS6클래스의 사용예시는 아래와 같다.\n\na=RPS6()\na.action\n\n'바위'\n\n\n\nb=RPS6()\nb.action\n\n'보'\n\n\n\na&gt;b, a&lt;b\n\n(False, True)\n\n\n(9)-(10)\n아래와 같은 데이터프레임을 선언하고 물음에 답하라.\n\nnp.random.seed(43052)\ndf=pd.DataFrame({'type':np.random.choice(['A','B'],100), 'score':np.random.randint(40,95,100)})\ndf\n\n\n\n\n\n\n\n\ntype\nscore\n\n\n\n\n0\nB\n45\n\n\n1\nA\n40\n\n\n2\nB\n79\n\n\n3\nB\n46\n\n\n4\nB\n57\n\n\n...\n...\n...\n\n\n95\nB\n69\n\n\n96\nA\n71\n\n\n97\nA\n93\n\n\n98\nA\n63\n\n\n99\nA\n82\n\n\n\n\n100 rows × 2 columns\n\n\n\n(9) type==’A’의 평균score를 구하는 코드를 작성하라.\n(10) type==’A’의 평균score보다 같거나 큰 값을 가지는 행을 출력하라.\n\n\n\n(1) 플레이어A는 (가위,가위) 중 하나를 선택할 수 있고 플레이어B는 (가위,바위) 중 하나를 선택할 수 있다. 각 플레이어는 각 패 중 하나를 랜덤으로 선택하는 액션을 한다고 가정하자. 아래에 해당하는 확률을 시뮬레이션을 이용하여 추정하라.\n\n플레이어A가 승리할 확률:\n플레이어B가 승리할 확률:\n플레이어A와 플레이어B가 비길 확률:\n\nhint: 50% 확률로 b가 승리하고 50% 확률로 비긴다.\n(2) 문제 (1)과 같이 아래의 상황을 가정하자.\n\n\n\n\n플레이어A\n플레이어B\n\n\n\n\n각 플레이어가 낼 수 있는 패 (candidate)\n(가위,가위)\n(가위,바위)\n\n\n각 패를 선택할 확률 (prob)\n(0.5,0.5)\n(0.5,0.5)\n\n\n\n각 플레이어는 아래와 같은 규칙으로 가위바위보 결과에 따른 보상점수를 적립한다고 하자. - 승리: 보상점수 2점 적립 - 무승부: 보상점수 1점 적립 - 패배: 보상점수 0점 적립\n100번째 대결까지 시뮬레이션을 시행하고 플레이어B가 가위를 낼 경우 얻은 보상점수의 총합과 바위를 낼 경우 얻은 보상점수의 총합을 각각 구하라. 플레이어B는 가위를 내는것이 유리한가? 바위를 내는것이 유리한가?\nhint: 플레이어B는 바위를 내는 것이 유리하다.\nhint: 플레이어B가 100번중에 49번 가위를 내고 51번 바위를 낸다면 플레이어B가 적립할 보상점수는 각각 아래와 같다. - 가위를 내었을 경우: 49 * 1 = 49점 - 바위를 내었을 경우: 51 * 2 = 102점 - 총 보상점수 = 49점 + 102점 = 151점\n(3) (2)에서 얻은 데이터를 학습하여 플레이어B가 “가위” 혹은 “바위” 를 선택할 확률을 매시점 조금씩 조정한다고 가정하자. 구체적으로는 현재시점까지 얻은 보상점수의 비율로 확률을 결정한다. 예를들어 플레이어B가 100회의 대결동안 누적한 보상점수의 총합이 아래와 같다고 하자.\n\n가위를 내었을 경우 보상점수 총합 = 50점\n바위를 내었을 경우 보상점수 총합 = 100점\n\n그렇다면 플레이어B는 각각 (50/150,100/150) 의 확률로 (가위,바위) 중 하나를 선택한다. 101번째 대결에 플레이어B가 가위를 내서 비겼다면 이후에는 (51/151,100/151) 의 확률로 (가위,바위) 중 하나를 선택한다. 102번째 대결에 플레이어B가 바위를 내서 이겼다면 이후에는 각각 (51/153,102/153) 의 확률로 (가위,바위) 중 하나를 선택한다. 이러한 상황을 요약하여 표로 정리하면 아래와 같다.\n\n\n\n\n\n\n\n\n\n시점\n플레이어B가 가위를 냈을 경우 얻은 점수 총합\n플레이어B가 바위를 냈을 경우 얻은 점수 총합\nt+1시점에서 플레이어B가 (가위,바위)를 낼 확률\n\n\n\n\nt=100\n50\n100\n(50/150, 100/150)\n\n\nt=101\n51\n100\n(51/151, 100/151)\n\n\nt=102\n51\n102\n(51/153, 102/153)\n\n\n\n이러한 방식으로 500회까지 게임을 진행하며 확률을 수정하였을 경우 501번째 대결에서 플레이어B가 (가위,바위)를 낼 확률은 각각 얼마인가?\nhint: 시간이 지날수록 플레이어B는 (가위,바위)중 바위를 내는 쪽이 유리하다는 것을 알게 될 것이다.\n\n앞으로 아래와 같은 용어를 사용한다. - (정의) 어떠한 플레이어가 양손 중 하나를 선택하는 확률을 데이터를 바탕으로 매 순간 업데이트 한다면 그 플레이어는 “학습모드 상태이다”고 표현한다. - (정의) 반대로 어떠한 플레이어가 양손 중 하나를 항상 동일한 확률로 낸다면 그 플레이어는 “학습모드 상태가 아니다”라고 표현한다.\n\n(4) 새로운 두명의 플레이어C와 플레이어D를 만들어라. 두 플레이어는 모두 동일하게 (가위,바위) 중 하나를 선택할 수 있다. 두 명의 플레이어는 100번째 대결까지는 두 가지 패중 하나를 랜덤하게 선택하고 101번째 대결부터 500번째 대결까지는 문제(3)의 플레이어B와 같은 방식으로 확률을 업데이트 하여 두 가지 패를 서로 다른 확률로 낸다고 하자. 즉 100번째 대결까지는 두 플레이어가 모두 학습모드 상태가 아니고 101번째부터 500번째 대결까지는 두 플레이어가 모두 학습모드 상태이다. 500번째 대결까지의 학습이 끝났을 경우 플레이어 C와 플레이어D가 각 패를 낼 확률은 각각 얼마인가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어C가 (가위,바위)를 낼 확률\n플레이어D가 (가위,바위)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n대결 데이터를 학습하여 수정한 확률\n대결 데이터를 학습하여 수정한 확률\n양쪽 플레이어 모두 학습모드임\n\n\n\nhint: 시간이 지날수록 두 플레이어 모두 바위를 내는 쪽이 유리하다는 것을 알게 될 것이다.\n(5) 새로운 플레이어 E와 F를 생각하자. 플레이어E와 플레이어F는 각각 (가위,바위) 그리고 (가위,보) 중 하나를 선택할 수 있다고 가정하자. 시뮬레이션 대결결과를 이용하여 아래의 확률을 근사적으로 추정하라.\n\n플레이어E가 승리할 확률:\n플레이어F가 승리할 확률:\n플레이어E와 플레이어F가 비길 확률:\n\nhint: 플레이어E가 가위를 낸다면 최소한 지지는 않기 때문에 플레이어E가 좀 더 유리한 패를 가지고 있다. 따라서 플레이어E의 결과가 더 좋을 것이다.\n(6) (5)와 동일한 두 명의 플레이어E, F를 생각하자. 두 플레이어는 100회까지는 랜덤으로 자신의 패를 선택한다. 그리고 101회부터 500회까지는 플레이어F만 데이터로 부터 학습을 하여 수정된 확률을 사용한다. 500번의 대결이 끝나고 플레이어F가 (가위,보)를 선택하는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\n\nhint: 플레이어F는 보를 내는 것이 낫다고 생각할 것이다. (가위를 내면 지거나 비기지만 보를 내면 지거나 이긴다.)\n(7) (6)번의 플레이어E와 플레이어F가 500회~1000회까지 추가로 게임을 한다. 이번에는 플레이어E만 데이터로부터 학습한다. 1000회까지 대결을 끝낸 이후 플레이어E가 (가위,바위)를 내는 확률은 어떻게 업데이트 되었는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\nt=500시점에 업데이트된 확률\n플레이어E는 학습모드 / 플레이어F는 학습모드아님\n\n\n\nhint: 플레이어F는 보를 내도록 학습되어 있다. 따라서 플레이어E가 바위를 내면 지고 가위를 내면 이길것이다. 따라서 플레이어E는 가위가 유리하다고 생각할 것이다.\n(8) (7)번의 플레이어E와 플레이어F가 1000회~30000회까지 추가로 게임을 한다. 이번에는 플레이어F만 데이터로부터 학습한다. 30000회까지 대결을 끝낸 이후 플레이어F가 (가위,보)를 내는 확률은 어떻게 업데이트 되었는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\nt=500시점에 업데이트된 확률\n플레이어E는 학습모드 / 플레이어F는 학습모드아님\n\n\nt &lt;= 30000\nt=1000시점에 업데이트된 확률\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\n\nhint: 플레이어F는 원래 보가 유리하다고 생각하여 보를 자주 내도록 학습되었다. 하지만 플레이어E가 그러한 플레이어F의 성향을 파악하고 가위를 주로 내도록 학습하였다. 플레이어F는 그러한 플레이어E의 성향을 다시 파악하여 이번에는 가위을 자주 내는 것이 유리하다고 생각할 것이다.\n(9) 플레이어E와 플레이어F의 대결기록을 초기화 한다. 이번에는 플레이어F가 항상 (3/4)의 확률로 가위를 (1/4)의 확률로 보를 낸다고 가정한다. 플레이어E는 100번의 대결까지는 랜덤으로 (가위,바위)중 하나를 내고 101번째 대결부터 1000번째 대결까지는 대결 데이터를 학습하여 수정한 확률을 사용한다고 하자. 1000번째 대결이후에 플레이어E가 (가위,바위)를 내는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(3/4, 1/4)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\n(3/4, 1/4)\n플레이어E는 학습모드 / 플레이어F는 학습모드 아님\n\n\n\n(10) 플레이어E와 플레이어F의 대결기록을 초기화 한다. 이번에는 플레이어F가 항상 (2/3)의 확률로 가위를 (1/3)의 확률로 보를 낸다고 가정한다. 플레이어E는 100번의 대결까지는 랜덤으로 (가위,바위)중 하나를 내고 101번째 대결부터 1000번째 대결까지는 대결 데이터를 학습하여 수정한 확률을 사용한다고 하자. 1000번째 대결이후에 플레이어E가 (가위,바위)를 내는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(2/3, 1/3)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\n(2/3, 1/3)\n플레이어E는 학습모드 / 플레이어F는 학습모드 아님\n\n\n\n\n\n\n- 아래의 클래스를 참고하여 문제1,2을 풀어라. (5월25일 강의노트에 소개된 클래스를 약간 정리한 것) - 참고하지 않아도 감점은 없음\n\nclass RPS:\n    def __init__(self,candidate):\n        self.candidate = candidate\n        self.actions = list() \n        self.rewards = list()\n        self.prob = [0.5,0.5]\n\n    def __eq__(self,other): # 연산 == 를 재정의 \n        return self.actions[-1] == other.actions[-1] \n        #note: 둘의 액션이 같으면 무승부 \n    \n    def __gt__(self,other): # 연산 &gt; 를 재정의 \n        pair = self.actions[-1], other.actions[-1]\n        return pair == ('가위','보') or pair == ('바위','가위') or pair == ('보','바위') \n        #note: 가위&gt;보, 바위&gt;가위, 보&gt;가위 \n    \n    def __mul__(self,other):\n        # step1: 각자의 패를 선택 \n        self.choose()\n        other.choose()\n        \n        # step2: 승패 판단 + upate reward\n        if self == other: # 무승부일경우 \n            self.rewards.append(1)\n            other.rewards.append(1)\n        elif self &gt; other: # self의 승리 \n            self.rewards.append(2)\n            other.rewards.append(0)\n        else: # other의 승리 \n            self.rewards.append(0)\n            other.rewards.append(2)\n        \n        # step3: update data\n        self.update_data()\n        other.update_data()\n    \n    def update_data(self):\n        self.data = pd.DataFrame({'actions':self.actions, 'rewards':self.rewards})\n    \n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt; \n        데이터: &lt;br/&gt;\n        {}\n        \"\"\"        \n        return html_str.format(self.candidate,self.data._repr_html_())\n    \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate,p=self.prob))\n\n- 사용예시\n\na=RPS(['가위','가위'])\nb=RPS(['가위','보'])\n\n\nfor i in range(5):\n    a*b\n\n\na\n\n\n        낼 수 있는 패: ['가위', '가위']  \n        데이터: \n        \n\n\n\n\n\n\nactions\nrewards\n\n\n\n\n0\n가위\n2\n\n\n1\n가위\n2\n\n\n2\n가위\n1\n\n\n3\n가위\n2\n\n\n4\n가위\n2\n\n\n\n\n\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '보']  \n        데이터: \n        \n\n\n\n\n\n\nactions\nrewards\n\n\n\n\n0\n보\n0\n\n\n1\n보\n0\n\n\n2\n가위\n1\n\n\n3\n보\n0\n\n\n4\n보\n0"
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022-06-13-final.html#imports",
    "href": "posts/1_IP2022/04_시험/2022-06-13-final.html#imports",
    "title": "[2022 EXAM] 2022 final exam",
    "section": "",
    "text": "아래코드를 이용하여 numpy, matplotlib, pandas를 import하라.\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd\nfrom IPython.display import HTML"
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022-06-13-final.html#기본문제-50점",
    "href": "posts/1_IP2022/04_시험/2022-06-13-final.html#기본문제-50점",
    "title": "[2022 EXAM] 2022 final exam",
    "section": "",
    "text": "(1) 도함수를 구하는 함수 derivate를 선언하라. 이 함수를 이용하여 \\(f(x)=x^2\\)의 그래프와 \\(f'(x)=2x\\)의 그래프를 \\(x \\in (-1,1)\\)의 범위에서 그려라.\n\ndef f(x):\n    return x**2\n\n\ndef derivate(f): \n    def df(x): \n        h=0.00000000000001\n        return (f(x+h)-f(x))/h \n    return df\n\n\nx = np.linspace(-1,1,100)\nplt.plot(x, f(x))  ## f(x)=x**2\nplt.plot(x, derivate(f)(x)) ## f'(x)=2*x\n\n\n\n\n(2) 적당한 클래스 정의하여 인스턴스 a를 만들고 print(a)의 출력결과가 본인의 학번이 나오도록 하라.\n## 코드예시\nclass Klass:\n    ???\n    ???\na=Klass()\nprint(a)\n## 출력결과\n2022-43052\n\nclass Klass:\n    def __str__(self):\n        return('12345678')\n\n\na = Klass()\n\n\nprint(a)\n\n12345678\n\n\n(3) for문이 실행될때마다 [묵,찌,빠] 중에 하나를 내며 빠를 누적 3회 낼경우 for문이 멈추는 이터레이터를 생성하라.\n(나의풀이)\n\nclass Klass: # 빠를 누적 3회 낼 경우 for문이 멈추는 이터레이터를 만들자.\n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n        self.n = 0\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        if action == '빠':\n            self.n += 1\n            print(action,self.n)\n            if self.n == 3:\n                print('빠가 누적3회 나와서 for문을 멈춥니다.')\n                raise StopIteration\n            else:\n                return action\n        else:\n            return action\n\n\na = Klass()\n\n\nfor i in a:\n    print(i)\n\n찌\n찌\n묵\n찌\n찌\n묵\n빠 1\n빠\n찌\n빠 2\n빠\n빠 3\n빠가 누적3회 나와서 for문을 멈춥니다.\n\n\n(모범답안)\n\nclass Klass: \n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n        self.dic = {'묵':0,'찌':0,'빠':0}\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        self.dic[action] += 1\n        if self.dic['빠'] == 3:\n            print('빠가 3번 누적되어 for문을 멈춥니다.')\n            raise StopIteration\n        else:\n            return action\n\n\na = Klass()\nfor i in a:\n    print(i)\n\n묵\n빠\n찌\n찌\n빠\n빠가 3번 누적되어 for문을 멈춥니다.\n\n\n(4)-(6)\nclass GS25: \n    n=0 \n    total_number_of_guests = 0 \n    def __init__(self):\n        self.number_of_guests = 0 \n(4) 위의 클래스를 수정하여 아래와 같이 GS25에서 새로운 인스턴스가 생성될때마다\nGS25의 점포수가 ?개로 늘었습니다.\n라는 메시지가 출력되도록 하라.\n(5) 함수 come를 인스턴스 메소드로 정의하라. 이 메소드가 실행될때마다 각 점포의 손님 인스턴스 변수 number_of_guests와 클래스변수 total_number_of_guests를 1씩 증가시키고 아래의 메시지를 출력하라.\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 n명입니다. \n현재 GS25 점포를 방문한 손님수는 m명입니다. \n(6) 새로운 클래스메서드 show를 만들고 아래와 같은 메시지를 출력하도록 하라.\nGS25의 점포수: ??\nGS25를 방문한 총 손님수: ??\n(사용예시) (4)-(6)을 모두 적용한 경우 사용예시는 아래와 같다.\n\nclass GS25:\n    n = 0\n    total_numer_of_guests = 0\n    def __init__(self):\n        self.number_of_guests = 0\n        GS25.n += 1\n        print('GS25의 점포수가 {}개로 늘었습니다.'.format(GS25.n))\n    def come(self):\n        GS25.total_number_of_guests += 1\n        self.number_of_guests += 1\n        print('새로운 손님이 오셨습니다.')\n        print('GS25를 방문한 총 손님 수는 {}명입니다.'.format(GS25.total_number_of_guests))\n        print('현재 GS25 점포를 방문한 손님수는 {}명입니다.'.format(self.number_of_guests))\n    @classmethod\n    def show(cls):\n        print('GS25의 점포수:{}'.format(cls.n))\n        print('GS를 방문한 총 손님 수: {}'.format(cls.total_number_of_guests))\n\n\na = GS25()\n\nGS25의 점포수가 5개로 늘었습니다.\n\n\n\na=GS25() ## (4)의 사용예시\n\nGS25의 점포수가 1개로 늘었습니다.\n\n\n\nb=GS25() ## (4)의 사용예시\n\nGS25의 점포수가 2개로 늘었습니다.\n\n\n\na.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 1명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다. \n\n\n\na.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 2명입니다.\n현재 GS25 점포를 방문한 손님수는 2명입니다. \n\n\n\nb.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 3명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다. \n\n\n\nGS25.show() ## (6)의 사용예시\n\nGS25의 점포수: 2\nGS25를 방문한 총 손님수: 3\n\n\n(풀이시작)\n\nclass GS25: \n    n=0 \n    total_number_of_guests = 0 \n    def __init__(self):\n        self.number_of_guests = 0\n        GS25.n += 1\n        print('GS25의 점포수가 {}개로 늘었습니다.'.format(GS25.n))\n    def come(self):\n        self.number_of_guests += 1\n        GS25.total_number_of_guests += 1\n        print('새로운 손님이 오셨습니다!')\n        print('GS25를 방문한 총 손님수는 {}명입니다.'.format(GS25.total_number_of_guests))\n        print('현재 GS25 점포를 방문한 손님수는 {}명입니다.'.format(self.number_of_guests))\n    @classmethod\n    def show(cls):\n        print('GS25의 점포수: {}'.format(cls.n))\n        print('GS25를 방문한 총 손님수: {}'.format(cls.total_number_of_guests))\n\n\na = GS25()\n\nGS25의 점포수가 1개로 늘었습니다.\n\n\n\nb = GS25()\n\nGS25의 점포수가 2개로 늘었습니다.\n\n\n\na.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 1명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다.\n\n\n\na.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 2명입니다.\n현재 GS25 점포를 방문한 손님수는 2명입니다.\n\n\n\nb.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 3명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다.\n\n\n\nGS25.show()\n\nGS25의 점포수: 2\nGS25를 방문한 총 손님수: 3\n\n\n(7) __eq__는 연산 == 를 재정의하는 메소드이다. 클래스 RPS_BASE를 상속하여 새로운 클래스 RPS5를 만들라. 연산 ==를 재정의하여 RPS5의 두 인스턴스의 action이 같은 경우 true를 리턴하는 기능을 구현하라.\n\nclass RPS_BASE:\n    def __init__(self):\n        self.action = np.random.choice(['가위','바위','보'])\n\nhint: Appendix를 참고할 것\nhint: RPS5의 선언부분은 아래와 같은 형태를 가지고 있다.\nclass RPS5(???):\n    def __eq__(self,other):\n        return ??????\nhint: RPS5클래스의 사용예시는 아래와 같다.\n\na=RPS5()\na.action\n\n'바위'\n\n\n\nb=RPS5()\nb.action\n\n'보'\n\n\n\na==b\n\nFalse\n\n\n(풀이시작)\n(8) __gt__는 연산 &gt; 를 재정의하는 메소드이다. 클래스 RPS_BASE를 상속하여 새로운 클래스 RPS6를 만들라. 연산 &gt;를 재정의하여 RPS6의 두 인스턴스 a,b의 action이 각각 (‘가위’,‘보’), (‘바위’,‘가위’), (‘보’,‘바위’) 인 경우 true를 리턴하는 기능을 구현하라.\nhint: Appendix를 참고할 것\nhint: RPS6클래스의 사용예시는 아래와 같다.\n\na=RPS6()\na.action\n\n'바위'\n\n\n\nb=RPS6()\nb.action\n\n'보'\n\n\n\na&gt;b, a&lt;b\n\n(False, True)\n\n\n(9)-(10)\n아래와 같은 데이터프레임을 선언하고 물음에 답하라.\n\nnp.random.seed(43052)\ndf=pd.DataFrame({'type':np.random.choice(['A','B'],100), 'score':np.random.randint(40,95,100)})\ndf\n\n\n\n\n\n\n\n\ntype\nscore\n\n\n\n\n0\nB\n45\n\n\n1\nA\n40\n\n\n2\nB\n79\n\n\n3\nB\n46\n\n\n4\nB\n57\n\n\n...\n...\n...\n\n\n95\nB\n69\n\n\n96\nA\n71\n\n\n97\nA\n93\n\n\n98\nA\n63\n\n\n99\nA\n82\n\n\n\n\n100 rows × 2 columns\n\n\n\n(9) type==’A’의 평균score를 구하는 코드를 작성하라.\n(10) type==’A’의 평균score보다 같거나 큰 값을 가지는 행을 출력하라."
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022-06-13-final.html#가위-바위-보-하나빼기-150점",
    "href": "posts/1_IP2022/04_시험/2022-06-13-final.html#가위-바위-보-하나빼기-150점",
    "title": "[2022 EXAM] 2022 final exam",
    "section": "",
    "text": "(1) 플레이어A는 (가위,가위) 중 하나를 선택할 수 있고 플레이어B는 (가위,바위) 중 하나를 선택할 수 있다. 각 플레이어는 각 패 중 하나를 랜덤으로 선택하는 액션을 한다고 가정하자. 아래에 해당하는 확률을 시뮬레이션을 이용하여 추정하라.\n\n플레이어A가 승리할 확률:\n플레이어B가 승리할 확률:\n플레이어A와 플레이어B가 비길 확률:\n\nhint: 50% 확률로 b가 승리하고 50% 확률로 비긴다.\n(2) 문제 (1)과 같이 아래의 상황을 가정하자.\n\n\n\n\n플레이어A\n플레이어B\n\n\n\n\n각 플레이어가 낼 수 있는 패 (candidate)\n(가위,가위)\n(가위,바위)\n\n\n각 패를 선택할 확률 (prob)\n(0.5,0.5)\n(0.5,0.5)\n\n\n\n각 플레이어는 아래와 같은 규칙으로 가위바위보 결과에 따른 보상점수를 적립한다고 하자. - 승리: 보상점수 2점 적립 - 무승부: 보상점수 1점 적립 - 패배: 보상점수 0점 적립\n100번째 대결까지 시뮬레이션을 시행하고 플레이어B가 가위를 낼 경우 얻은 보상점수의 총합과 바위를 낼 경우 얻은 보상점수의 총합을 각각 구하라. 플레이어B는 가위를 내는것이 유리한가? 바위를 내는것이 유리한가?\nhint: 플레이어B는 바위를 내는 것이 유리하다.\nhint: 플레이어B가 100번중에 49번 가위를 내고 51번 바위를 낸다면 플레이어B가 적립할 보상점수는 각각 아래와 같다. - 가위를 내었을 경우: 49 * 1 = 49점 - 바위를 내었을 경우: 51 * 2 = 102점 - 총 보상점수 = 49점 + 102점 = 151점\n(3) (2)에서 얻은 데이터를 학습하여 플레이어B가 “가위” 혹은 “바위” 를 선택할 확률을 매시점 조금씩 조정한다고 가정하자. 구체적으로는 현재시점까지 얻은 보상점수의 비율로 확률을 결정한다. 예를들어 플레이어B가 100회의 대결동안 누적한 보상점수의 총합이 아래와 같다고 하자.\n\n가위를 내었을 경우 보상점수 총합 = 50점\n바위를 내었을 경우 보상점수 총합 = 100점\n\n그렇다면 플레이어B는 각각 (50/150,100/150) 의 확률로 (가위,바위) 중 하나를 선택한다. 101번째 대결에 플레이어B가 가위를 내서 비겼다면 이후에는 (51/151,100/151) 의 확률로 (가위,바위) 중 하나를 선택한다. 102번째 대결에 플레이어B가 바위를 내서 이겼다면 이후에는 각각 (51/153,102/153) 의 확률로 (가위,바위) 중 하나를 선택한다. 이러한 상황을 요약하여 표로 정리하면 아래와 같다.\n\n\n\n\n\n\n\n\n\n시점\n플레이어B가 가위를 냈을 경우 얻은 점수 총합\n플레이어B가 바위를 냈을 경우 얻은 점수 총합\nt+1시점에서 플레이어B가 (가위,바위)를 낼 확률\n\n\n\n\nt=100\n50\n100\n(50/150, 100/150)\n\n\nt=101\n51\n100\n(51/151, 100/151)\n\n\nt=102\n51\n102\n(51/153, 102/153)\n\n\n\n이러한 방식으로 500회까지 게임을 진행하며 확률을 수정하였을 경우 501번째 대결에서 플레이어B가 (가위,바위)를 낼 확률은 각각 얼마인가?\nhint: 시간이 지날수록 플레이어B는 (가위,바위)중 바위를 내는 쪽이 유리하다는 것을 알게 될 것이다.\n\n앞으로 아래와 같은 용어를 사용한다. - (정의) 어떠한 플레이어가 양손 중 하나를 선택하는 확률을 데이터를 바탕으로 매 순간 업데이트 한다면 그 플레이어는 “학습모드 상태이다”고 표현한다. - (정의) 반대로 어떠한 플레이어가 양손 중 하나를 항상 동일한 확률로 낸다면 그 플레이어는 “학습모드 상태가 아니다”라고 표현한다.\n\n(4) 새로운 두명의 플레이어C와 플레이어D를 만들어라. 두 플레이어는 모두 동일하게 (가위,바위) 중 하나를 선택할 수 있다. 두 명의 플레이어는 100번째 대결까지는 두 가지 패중 하나를 랜덤하게 선택하고 101번째 대결부터 500번째 대결까지는 문제(3)의 플레이어B와 같은 방식으로 확률을 업데이트 하여 두 가지 패를 서로 다른 확률로 낸다고 하자. 즉 100번째 대결까지는 두 플레이어가 모두 학습모드 상태가 아니고 101번째부터 500번째 대결까지는 두 플레이어가 모두 학습모드 상태이다. 500번째 대결까지의 학습이 끝났을 경우 플레이어 C와 플레이어D가 각 패를 낼 확률은 각각 얼마인가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어C가 (가위,바위)를 낼 확률\n플레이어D가 (가위,바위)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n대결 데이터를 학습하여 수정한 확률\n대결 데이터를 학습하여 수정한 확률\n양쪽 플레이어 모두 학습모드임\n\n\n\nhint: 시간이 지날수록 두 플레이어 모두 바위를 내는 쪽이 유리하다는 것을 알게 될 것이다.\n(5) 새로운 플레이어 E와 F를 생각하자. 플레이어E와 플레이어F는 각각 (가위,바위) 그리고 (가위,보) 중 하나를 선택할 수 있다고 가정하자. 시뮬레이션 대결결과를 이용하여 아래의 확률을 근사적으로 추정하라.\n\n플레이어E가 승리할 확률:\n플레이어F가 승리할 확률:\n플레이어E와 플레이어F가 비길 확률:\n\nhint: 플레이어E가 가위를 낸다면 최소한 지지는 않기 때문에 플레이어E가 좀 더 유리한 패를 가지고 있다. 따라서 플레이어E의 결과가 더 좋을 것이다.\n(6) (5)와 동일한 두 명의 플레이어E, F를 생각하자. 두 플레이어는 100회까지는 랜덤으로 자신의 패를 선택한다. 그리고 101회부터 500회까지는 플레이어F만 데이터로 부터 학습을 하여 수정된 확률을 사용한다. 500번의 대결이 끝나고 플레이어F가 (가위,보)를 선택하는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\n\nhint: 플레이어F는 보를 내는 것이 낫다고 생각할 것이다. (가위를 내면 지거나 비기지만 보를 내면 지거나 이긴다.)\n(7) (6)번의 플레이어E와 플레이어F가 500회~1000회까지 추가로 게임을 한다. 이번에는 플레이어E만 데이터로부터 학습한다. 1000회까지 대결을 끝낸 이후 플레이어E가 (가위,바위)를 내는 확률은 어떻게 업데이트 되었는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\nt=500시점에 업데이트된 확률\n플레이어E는 학습모드 / 플레이어F는 학습모드아님\n\n\n\nhint: 플레이어F는 보를 내도록 학습되어 있다. 따라서 플레이어E가 바위를 내면 지고 가위를 내면 이길것이다. 따라서 플레이어E는 가위가 유리하다고 생각할 것이다.\n(8) (7)번의 플레이어E와 플레이어F가 1000회~30000회까지 추가로 게임을 한다. 이번에는 플레이어F만 데이터로부터 학습한다. 30000회까지 대결을 끝낸 이후 플레이어F가 (가위,보)를 내는 확률은 어떻게 업데이트 되었는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\nt=500시점에 업데이트된 확률\n플레이어E는 학습모드 / 플레이어F는 학습모드아님\n\n\nt &lt;= 30000\nt=1000시점에 업데이트된 확률\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\n\nhint: 플레이어F는 원래 보가 유리하다고 생각하여 보를 자주 내도록 학습되었다. 하지만 플레이어E가 그러한 플레이어F의 성향을 파악하고 가위를 주로 내도록 학습하였다. 플레이어F는 그러한 플레이어E의 성향을 다시 파악하여 이번에는 가위을 자주 내는 것이 유리하다고 생각할 것이다.\n(9) 플레이어E와 플레이어F의 대결기록을 초기화 한다. 이번에는 플레이어F가 항상 (3/4)의 확률로 가위를 (1/4)의 확률로 보를 낸다고 가정한다. 플레이어E는 100번의 대결까지는 랜덤으로 (가위,바위)중 하나를 내고 101번째 대결부터 1000번째 대결까지는 대결 데이터를 학습하여 수정한 확률을 사용한다고 하자. 1000번째 대결이후에 플레이어E가 (가위,바위)를 내는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(3/4, 1/4)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\n(3/4, 1/4)\n플레이어E는 학습모드 / 플레이어F는 학습모드 아님\n\n\n\n(10) 플레이어E와 플레이어F의 대결기록을 초기화 한다. 이번에는 플레이어F가 항상 (2/3)의 확률로 가위를 (1/3)의 확률로 보를 낸다고 가정한다. 플레이어E는 100번의 대결까지는 랜덤으로 (가위,바위)중 하나를 내고 101번째 대결부터 1000번째 대결까지는 대결 데이터를 학습하여 수정한 확률을 사용한다고 하자. 1000번째 대결이후에 플레이어E가 (가위,바위)를 내는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(2/3, 1/3)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\n(2/3, 1/3)\n플레이어E는 학습모드 / 플레이어F는 학습모드 아님"
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022-06-13-final.html#appendix",
    "href": "posts/1_IP2022/04_시험/2022-06-13-final.html#appendix",
    "title": "[2022 EXAM] 2022 final exam",
    "section": "",
    "text": "- 아래의 클래스를 참고하여 문제1,2을 풀어라. (5월25일 강의노트에 소개된 클래스를 약간 정리한 것) - 참고하지 않아도 감점은 없음\n\nclass RPS:\n    def __init__(self,candidate):\n        self.candidate = candidate\n        self.actions = list() \n        self.rewards = list()\n        self.prob = [0.5,0.5]\n\n    def __eq__(self,other): # 연산 == 를 재정의 \n        return self.actions[-1] == other.actions[-1] \n        #note: 둘의 액션이 같으면 무승부 \n    \n    def __gt__(self,other): # 연산 &gt; 를 재정의 \n        pair = self.actions[-1], other.actions[-1]\n        return pair == ('가위','보') or pair == ('바위','가위') or pair == ('보','바위') \n        #note: 가위&gt;보, 바위&gt;가위, 보&gt;가위 \n    \n    def __mul__(self,other):\n        # step1: 각자의 패를 선택 \n        self.choose()\n        other.choose()\n        \n        # step2: 승패 판단 + upate reward\n        if self == other: # 무승부일경우 \n            self.rewards.append(1)\n            other.rewards.append(1)\n        elif self &gt; other: # self의 승리 \n            self.rewards.append(2)\n            other.rewards.append(0)\n        else: # other의 승리 \n            self.rewards.append(0)\n            other.rewards.append(2)\n        \n        # step3: update data\n        self.update_data()\n        other.update_data()\n    \n    def update_data(self):\n        self.data = pd.DataFrame({'actions':self.actions, 'rewards':self.rewards})\n    \n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt; \n        데이터: &lt;br/&gt;\n        {}\n        \"\"\"        \n        return html_str.format(self.candidate,self.data._repr_html_())\n    \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate,p=self.prob))\n\n- 사용예시\n\na=RPS(['가위','가위'])\nb=RPS(['가위','보'])\n\n\nfor i in range(5):\n    a*b\n\n\na\n\n\n        낼 수 있는 패: ['가위', '가위']  \n        데이터: \n        \n\n\n\n\n\n\nactions\nrewards\n\n\n\n\n0\n가위\n2\n\n\n1\n가위\n2\n\n\n2\n가위\n1\n\n\n3\n가위\n2\n\n\n4\n가위\n2\n\n\n\n\n\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '보']  \n        데이터: \n        \n\n\n\n\n\n\nactions\nrewards\n\n\n\n\n0\n보\n0\n\n\n1\n보\n0\n\n\n2\n가위\n1\n\n\n3\n보\n0\n\n\n4\n보\n0"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class01.html",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class01.html",
    "title": "[IP2022] class 01단계",
    "section": "",
    "text": "클래스 선언 및 사용 예시\n\n\n\n1. 이미지 자료 불러오기 (PIL 이용)\n2. 클래스 성능 정리\n3. 연습문제\n\n\n\n- 예제1\n\n# 이미지 출력을 위한 패키지 불러오기\nimport requests\nfrom PIL import Image\n\n\nurl= 'https://stat.jbnu.ac.kr/sites/stat/images/intro_about_02.jpg'\n\n\nImage.open(Image.io.BytesIO(requests.get(url).content))\n\n\n\n\n- 예제2\n\nurl1 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\nurl2 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop2.png?raw=true' \n\n\nImage.open(Image.io.BytesIO(requests.get(url1).content))\n\n\n\n\n\nImage.open(Image.io.BytesIO(requests.get(url2).content))\n\n\n\n\n\n\n\nclass STOOOP:\n    title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n                \n\n\n규칙1 : 메소드(=class 안에서 정의된 함수)의 첫번째 인자는 무조건 self\n규칙2 : 메소드에서 class 안에 정의된 변수들 (title, url, end)을 사용하려면 self.변수이름 과 같은 형식으로 쓴다.\n\n즉, self.title, self.url, self.end 와 같은 방식으로 써야한다.\n\n(참고) : 규칙2에서 가끔 self 자리에 STOOOP.title, STOOOP.url, STOOOP.end 와 같이 클래스의 이름을 쓰기도 한다.\n\n\n\n\n\n\n\nschool = STOOOP()\n\n\nschool.stop()\n\n학교폭력!\n멈춰~~~~\n\n\n\n\n\n\n\n\n\nkospi = STOOOP()\n\n\nkospi.title = 'KOSPI 하락'\n\n\nkospi.stop()\n\nKOSPI 하락\n멈춰~~~~\n\n\n\n\n\n\n\n\n\n\n\n\nschool = STOOOP()\nkospi = STOOOP()\n\n\n함수의 사용법과 비슷하다.\n클래스 이름을 쓰고, 콘텐츠를 구체화하는 과정에서 필요한 입력1, 입력2를 ()에 넣는다. 이때는 STOOOP(입력1, 입력2) 와 같이 생성\n위의 예시는 따로 입력이 없으므로 비워둔 상태이다. 즉, STOOOP() 와 같은 식으로 생성\n\n\n\n\n\nschool.title # 출력\n\n'학교폭력!'\n\n\n\nkospi.title # 출력\n\n'학교폭력!'\n\n\n\nkospi.title = '코스피하락' # 변경\n\n\nkospi.title\n\n'코스피하락'\n\n\n\n\n\n\nschool.stop()\n\n학교폭력!\n멈춰~~~~\n\n\n\n\n\n\nkospi.stop()\n\n코스피하락\n멈춰~~~~\n\n\n\n\n\n\n\n\n\n\n\n\n- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 True이다.\n- 클래스에는 show()라는 메소드가 있다. show() 기능은 a의 값을 print하는 기능을 한다.\n\nclass Klass1:\n    a = True # 초기값\n    def show(self):\n        print(self.a)\n\n\nex1 = Klass1()\n\n\nex1.a # 초기값\n\nTrue\n\n\n\nex1.show()\n\nTrue\n\n\n\n\n\n- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 1이다.\n- 클래스에는 up()이라는 메소드가 있다. up()의 기능은 a의 값을 1증가시키는 기능을 한다.\n\nclass Klass2:\n    a = 1 # 초깃값\n    def up(self):\n        self.a = self.a + 1\n\n\nex2 = Klass2()\nex2.a\n\n1\n\n\n\nex2.up()\nex2.a\n\n2\n\n\n\nex2.up()\nex2.a\n\n3\n\n\n\nex2.up()\nex2.a\n\n4\n\n\n\n\n\n- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 \\(0\\) 이다.\n- 클래스에는 up(), down(), show() 라는 메소드가 있다. 각각은 a의 값을 1증가, a값을 1감소, a의 값을 print하는 기능을 한다.\n\nclass Klass3:\n    a = 0\n    def up(self):\n        self.a  = self.a + 1\n    def down(self):\n        self.a = self.a - 1\n    def show(self):\n        print(self.a)\n\n\nex3 = Klass3()\n\n\nex3.show()\n\n0\n\n\n\nex3.up()\nex3.show()\n\n1\n\n\n\nex3.up()\nex3.up()\nex3.show()\n\n3\n\n\n\nex3.down()\nex3.show()\n\n2\n\n\n\n\n\n- 클래스 내에는 변수 url이 있음. url의 초기값은 다음과 같다. https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true\n- 클래스에는 show() 라는 메소드(클래스 안에 정의된 함수)를 가지는데, 메소드는 아래와 같은 기능을 한다. - 기능1: url의 그림을 출력 - 기능2: ‘당신은 이 그림을 \\(n\\) 번 보았습니다.’ 출력. (여기에서 \\(n\\)은 그림을 본 횟수)\n\nclass Klass4:\n    n = 1 # 초기값\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 그림을 {}번 보았습니다.'.format(self.n))\n        self.n = self.n + 1\n\n\nex4 = Klass4()\nex4.show()\n\n\n\n\n당신은 이 그림을 1번 보았습니다.\n\n\n\nex4.show()\n\n\n\n\n당신은 이 그림을 2번 보았습니다.\n\n\n\n# url 변환 (학교 폭력 이미지 말고, SNL 이미지로 출력되게 바꿔보자.)\nex4_1 = Klass4()\nex4_1.url = url2 # SNL image link\n\n\nex4_1.show()\n\n\n\n\n당신은 이 그림을 1번 보았습니다.\n\n\n\n\n\n\n- 클래스를 선언하라. [‘가위’, ‘바위’, ‘보’] 중 하나를 골라서 내는 메소드를 정의하라.\n\n# hint\nimport numpy as np\nnp.random.choice(['가위', '바위', '보'])\n\n'가위'\n\n\n\nclass Klass5:\n    def game(self):\n        print(np.random.choice(['가위','바위','보']))\n\n\nex5 = Klass5()\n\n\nex5.game()\n\n보"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class01.html#contents",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class01.html#contents",
    "title": "[IP2022] class 01단계",
    "section": "",
    "text": "1. 이미지 자료 불러오기 (PIL 이용)\n2. 클래스 성능 정리\n3. 연습문제"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class01.html#이미지-자료-불러오기",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class01.html#이미지-자료-불러오기",
    "title": "[IP2022] class 01단계",
    "section": "",
    "text": "- 예제1\n\n# 이미지 출력을 위한 패키지 불러오기\nimport requests\nfrom PIL import Image\n\n\nurl= 'https://stat.jbnu.ac.kr/sites/stat/images/intro_about_02.jpg'\n\n\nImage.open(Image.io.BytesIO(requests.get(url).content))\n\n\n\n\n- 예제2\n\nurl1 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\nurl2 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop2.png?raw=true' \n\n\nImage.open(Image.io.BytesIO(requests.get(url1).content))\n\n\n\n\n\nImage.open(Image.io.BytesIO(requests.get(url2).content))\n\n\n\n\n\n\n\nclass STOOOP:\n    title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n                \n\n\n규칙1 : 메소드(=class 안에서 정의된 함수)의 첫번째 인자는 무조건 self\n규칙2 : 메소드에서 class 안에 정의된 변수들 (title, url, end)을 사용하려면 self.변수이름 과 같은 형식으로 쓴다.\n\n즉, self.title, self.url, self.end 와 같은 방식으로 써야한다.\n\n(참고) : 규칙2에서 가끔 self 자리에 STOOOP.title, STOOOP.url, STOOOP.end 와 같이 클래스의 이름을 쓰기도 한다.\n\n\n\n\n\n\n\nschool = STOOOP()\n\n\nschool.stop()\n\n학교폭력!\n멈춰~~~~\n\n\n\n\n\n\n\n\n\nkospi = STOOOP()\n\n\nkospi.title = 'KOSPI 하락'\n\n\nkospi.stop()\n\nKOSPI 하락\n멈춰~~~~\n\n\n\n\n\n\n\n\n\n\n\n\nschool = STOOOP()\nkospi = STOOOP()\n\n\n함수의 사용법과 비슷하다.\n클래스 이름을 쓰고, 콘텐츠를 구체화하는 과정에서 필요한 입력1, 입력2를 ()에 넣는다. 이때는 STOOOP(입력1, 입력2) 와 같이 생성\n위의 예시는 따로 입력이 없으므로 비워둔 상태이다. 즉, STOOOP() 와 같은 식으로 생성\n\n\n\n\n\nschool.title # 출력\n\n'학교폭력!'\n\n\n\nkospi.title # 출력\n\n'학교폭력!'\n\n\n\nkospi.title = '코스피하락' # 변경\n\n\nkospi.title\n\n'코스피하락'\n\n\n\n\n\n\nschool.stop()\n\n학교폭력!\n멈춰~~~~\n\n\n\n\n\n\nkospi.stop()\n\n코스피하락\n멈춰~~~~"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class01.html#연습문제",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class01.html#연습문제",
    "title": "[IP2022] class 01단계",
    "section": "",
    "text": "- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 True이다.\n- 클래스에는 show()라는 메소드가 있다. show() 기능은 a의 값을 print하는 기능을 한다.\n\nclass Klass1:\n    a = True # 초기값\n    def show(self):\n        print(self.a)\n\n\nex1 = Klass1()\n\n\nex1.a # 초기값\n\nTrue\n\n\n\nex1.show()\n\nTrue\n\n\n\n\n\n- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 1이다.\n- 클래스에는 up()이라는 메소드가 있다. up()의 기능은 a의 값을 1증가시키는 기능을 한다.\n\nclass Klass2:\n    a = 1 # 초깃값\n    def up(self):\n        self.a = self.a + 1\n\n\nex2 = Klass2()\nex2.a\n\n1\n\n\n\nex2.up()\nex2.a\n\n2\n\n\n\nex2.up()\nex2.a\n\n3\n\n\n\nex2.up()\nex2.a\n\n4\n\n\n\n\n\n- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 \\(0\\) 이다.\n- 클래스에는 up(), down(), show() 라는 메소드가 있다. 각각은 a의 값을 1증가, a값을 1감소, a의 값을 print하는 기능을 한다.\n\nclass Klass3:\n    a = 0\n    def up(self):\n        self.a  = self.a + 1\n    def down(self):\n        self.a = self.a - 1\n    def show(self):\n        print(self.a)\n\n\nex3 = Klass3()\n\n\nex3.show()\n\n0\n\n\n\nex3.up()\nex3.show()\n\n1\n\n\n\nex3.up()\nex3.up()\nex3.show()\n\n3\n\n\n\nex3.down()\nex3.show()\n\n2\n\n\n\n\n\n- 클래스 내에는 변수 url이 있음. url의 초기값은 다음과 같다. https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true\n- 클래스에는 show() 라는 메소드(클래스 안에 정의된 함수)를 가지는데, 메소드는 아래와 같은 기능을 한다. - 기능1: url의 그림을 출력 - 기능2: ‘당신은 이 그림을 \\(n\\) 번 보았습니다.’ 출력. (여기에서 \\(n\\)은 그림을 본 횟수)\n\nclass Klass4:\n    n = 1 # 초기값\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 그림을 {}번 보았습니다.'.format(self.n))\n        self.n = self.n + 1\n\n\nex4 = Klass4()\nex4.show()\n\n\n\n\n당신은 이 그림을 1번 보았습니다.\n\n\n\nex4.show()\n\n\n\n\n당신은 이 그림을 2번 보았습니다.\n\n\n\n# url 변환 (학교 폭력 이미지 말고, SNL 이미지로 출력되게 바꿔보자.)\nex4_1 = Klass4()\nex4_1.url = url2 # SNL image link\n\n\nex4_1.show()\n\n\n\n\n당신은 이 그림을 1번 보았습니다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class01.html#homework",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class01.html#homework",
    "title": "[IP2022] class 01단계",
    "section": "",
    "text": "- 클래스를 선언하라. [‘가위’, ‘바위’, ‘보’] 중 하나를 골라서 내는 메소드를 정의하라.\n\n# hint\nimport numpy as np\nnp.random.choice(['가위', '바위', '보'])\n\n'가위'\n\n\n\nclass Klass5:\n    def game(self):\n        print(np.random.choice(['가위','바위','보']))\n\n\nex5 = Klass5()\n\n\nex5.game()\n\n보"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class05.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class05.html",
    "title": "[IP2022] class 05단계",
    "section": "",
    "text": "특정 자료형에 한정하여 print 이외에 파이썬 내부기능을 재정의해보자.\n\n- 지난시간까지 배운 것: RPC자료형에 한정해서 print() 등의 기능을 조작할 수 있었다. (재정의 할 수 있었다.)\n- 이번시간에 배울 것: 특정 자료형에 한정하여 print 이외에 파이썬 내부기능을 조작하여 보자. (재정의하여 보자.)\n\nimport numpy as np\n\n\n\n- 아래의 연산구조를 관찰하자.\n\na = 1\nb = 2\n\n\na?? # a는 int class에서 만들어진 인스턴스다.\n\n\nType:        int\nString form: 1\nDocstring:  \nint([x]) -&gt; integer\nint(x, base=10) -&gt; integer\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n&gt;&gt;&gt; int('0b100', base=0)\n4\n\n\n\n\na + b\n\n3\n\n\n\na라는 인스턴스와 b라는 인스턴스를 +라는 기호가 연결하고 있다.\n\n- 이번에는 아래의 연산구조를 관찰하자.\n\na = [1,2]\nb = [3,4]\na+b\n\n[1, 2, 3, 4]\n\n\n\na라는 인스턴스와 b라는 인스턴스를 +라는 기호가 연결하고 있다.\n\n- 동작이 다른 이유?\n\n클래스를 배우기 이전: int자료형의 +는 “정수의 덧셈”을 의미하고 list자료형의 +는 “자료의 추가”를 의미한다.\n클래스를 배운 이후: 아마 클래스는 + 라는 연산을 정의하는 숨겨진 메소드가 있을 것이다. (print가 그랬듯이) 그런데 int 클래스에서는 그 메소드를 “정수의 덧셈”이 되도록 정의하였고, list클래스에서는 그 메소드를 “자료의 추가”를 의마하도록 정의하였을 것이다.\n\n- 아래의 결과를 관찰\n\na = 1\nb = 2\n\n\nset(dir(a)) & {'__add__'}\n\n{'__add__'}\n\n\n\na.__add__(b)\n\n3\n\n\n\nb.__add__(a)\n\n3\n\n\n\na = [1,2]\nb = [3,4]\n\n\na.__add__(b)\n\n[1, 2, 3, 4]\n\n\n\nb.__add__(a)\n\n[3, 4, 1, 2]\n\n\n- a+b는 사실 내부적으로 a.__add(b)의 축약구문이다. 따라서 만약 a.__add__(b)의 기능을 바꾸면 (재정의 하면) a+b의 기능도 바뀔 것이다.\n\n\n- 학생예제\n\nclass Student: # student class를 만들어보자. (student 자료형인것.)\n    def __init__(self, age = 20.0, semester = 0):\n        self.age = age\n        self.semester = semester\n        print('입학을 축하합니다. 당신의 나이는 {}이고 현재 학기는 {}학기입니다.'.format(self.age, self.semester))\n    def __add__(self, val):\n        # val == 0: 휴학\n        # val == 1: 등록\n        if val == 0:\n            self.age = self.age + 0.5\n        elif val == 1:\n            self.age = self.age + 0.5\n            self.semester = self.semester + 1\n    def _repr_html_(self):\n        html_str = \"\"\"\n        나이: {} &lt;br/&gt;\n        학기: {} &lt;br/&gt;\n        \"\"\"\n        return html_str.format(self.age, self.semester)\n\n\niu = Student()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기입니다.\n\n\n\niu\n\n\n        나이: 20.0 \n        학기: 0 \n        \n\n\n\niu + 1 ## 1학년 1학기 등록\niu\n\n\n        나이: 20.5 \n        학기: 1 \n        \n\n\n\niu + 0 ## 휴학함\niu\n\n\n        나이: 21.0 \n        학기: 1 \n        \n\n\n\niu.__add__(1)\n\n\niu\n\n\n        나이: 21.5 \n        학기: 2 \n        \n\n\n- 연산을 연속으로 하고 싶다.\n\niu + 1 + 0 + 0 + 0 + 0\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n- 에러의 이유?\n(되는코드)\n\n(1+1)+1 # 1+1+1은 이렇게 볼 수 있다.\n\n3\n\n\n\n_a = (1+1)\ntype(_a)\n\nint\n\n\n\n_a+1 # 이 연산은 int 인스턴스 + int인스턴스\n\n3\n\n\n(안되는코드)\n\niu + 1 + 1\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n\n_a = iu + 1\ntype(_a)\n\nNoneType\n\n\n\n_a + 1\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n- 에러를 해결하는 방법: iu + 1의 결과로 Student클래스가 리턴되면 된다.\n\nclass Student: # student class를 만들어보자. (student 자료형인것.)\n    def __init__(self, age = 20.0, semester = 0):\n        self.age = age\n        self.semester = semester\n        print('입학을 축하합니다. 당신의 나이는 {}이고 현재 학기는 {}학기입니다.'.format(self.age, self.semester))\n    def __add__(self, val):\n        # val == 0: 휴학\n        # val == 1: 등록\n        if val == 0:\n            self.age = self.age + 0.5\n        elif val == 1:\n            self.age = self.age + 0.5\n            self.semester = self.semester + 1\n        return self\n    def _repr_html_(self):\n        html_str = \"\"\"\n        나이: {} &lt;br/&gt;\n        학기: {} &lt;br/&gt;\n        \"\"\"\n        return html_str.format(self.age, self.semester)\n\n\niu = Student()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기입니다.\n\n\n\niu+1  # __add__의 return에 Student 클래스의 인스턴스가 리턴되면서 자동으로 _repr_html_() 실행\n\n\n        나이: 20.5 \n        학기: 1 \n        \n\n\n\niu + 1 + 0 + 0 + 0 + 0\n\n\n        나이: 23.0 \n        학기: 2 \n        \n\n\n\n\n\n\na = 1\nb = 0\na*b\n\n0\n\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n        self.results = list()\n    def __mul__(self, other):\n        self.choose()\n        other.choose()\n        if self.actions[-1] == '가위' and other.actions[-1]=='가위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '가위' and other.actions[-1]=='바위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '가위' and other.actions[-1]=='보':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='가위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='바위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '바위' and other.actions[-1]=='보':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='가위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='바위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '보' and other.actions[-1]=='보':\n            self.results.append(0)\n            other.results.append(0)\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        액션: {} &lt;br/&gt;\n        승패: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions, self.results)\n\n\na = RPC()\nb = RPC()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\na*b\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: ['보'] \n        승패: [-1]\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: ['가위'] \n        승패: [1]\n        \n\n\n\nfor i in range(50000):\n    a*b\n\n\n#a\n\n\n#b\n\n\nsum(a.results), sum(b.results)\n\n(175, -175)\n\n\n\nsum(a.results)/50000\n\n0.0035\n\n\n\nsum(b.results)/50000\n\n-0.0035\n\n\n\n\n\n\nRPC클래스에서 Player a와 Player b를 만들어라. - Player a는 [‘가위’,‘보’] 중에 하나를 낼 수 있다. - 그리고 Player b는 [‘가위’,‘바위’] 중에 하나를 낼 수 있다. - 두 Player는 가지고 있는 패를 (같은 확률로) 랜덤으로 낸다. (즉, Player a가 가위만 내거나 보만 내는 경우는 없다.)\n\n누가 더 유리한가? 이유를 스스로 생각해보라.\n\n\n비슷하지 않을까?\n\n\n50000번을 시뮬레이션을 해보고 결과를 분석해보라.\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n        self.results = list()\n    def __mul__(self, other):\n        self.choose()\n        other.choose()\n        if self.actions[-1] == '가위' and other.actions[-1]=='가위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '가위' and other.actions[-1]=='바위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '가위' and other.actions[-1]=='보':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='가위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='바위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '바위' and other.actions[-1]=='보':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='가위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='바위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '보' and other.actions[-1]=='보':\n            self.results.append(0)\n            other.results.append(0)\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        액션: {} &lt;br/&gt;\n        승패: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions, self.results)\n\n\nplayer_a = RPC(['가위', '보'])\nplayer_b = RPC(['가위', '바위'])\n\n\nplayer_a\n\n\n        낼 수 있는 패: ['가위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\nplayer_b\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        액션: [] \n        승패: []\n        \n\n\n\nplayer_a*player_b\n\n\nplayer_a\n\n\n        낼 수 있는 패: ['가위', '보'] \n        액션: ['보'] \n        승패: [1]\n        \n\n\n\nplayer_b\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        액션: ['바위'] \n        승패: [-1]\n        \n\n\n\nfor i in range(50000):\n    player_a*player_b\n\n\nsum(player_a.results), sum(player_b.results)\n\n(-12279, 12279)\n\n\n\nsum(player_a.results)/50000, sum(player_b.results)/50000\n\n(-0.24558, 0.24558)"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class05.html#motive",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class05.html#motive",
    "title": "[IP2022] class 05단계",
    "section": "",
    "text": "- 아래의 연산구조를 관찰하자.\n\na = 1\nb = 2\n\n\na?? # a는 int class에서 만들어진 인스턴스다.\n\n\nType:        int\nString form: 1\nDocstring:  \nint([x]) -&gt; integer\nint(x, base=10) -&gt; integer\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n&gt;&gt;&gt; int('0b100', base=0)\n4\n\n\n\n\na + b\n\n3\n\n\n\na라는 인스턴스와 b라는 인스턴스를 +라는 기호가 연결하고 있다.\n\n- 이번에는 아래의 연산구조를 관찰하자.\n\na = [1,2]\nb = [3,4]\na+b\n\n[1, 2, 3, 4]\n\n\n\na라는 인스턴스와 b라는 인스턴스를 +라는 기호가 연결하고 있다.\n\n- 동작이 다른 이유?\n\n클래스를 배우기 이전: int자료형의 +는 “정수의 덧셈”을 의미하고 list자료형의 +는 “자료의 추가”를 의미한다.\n클래스를 배운 이후: 아마 클래스는 + 라는 연산을 정의하는 숨겨진 메소드가 있을 것이다. (print가 그랬듯이) 그런데 int 클래스에서는 그 메소드를 “정수의 덧셈”이 되도록 정의하였고, list클래스에서는 그 메소드를 “자료의 추가”를 의마하도록 정의하였을 것이다.\n\n- 아래의 결과를 관찰\n\na = 1\nb = 2\n\n\nset(dir(a)) & {'__add__'}\n\n{'__add__'}\n\n\n\na.__add__(b)\n\n3\n\n\n\nb.__add__(a)\n\n3\n\n\n\na = [1,2]\nb = [3,4]\n\n\na.__add__(b)\n\n[1, 2, 3, 4]\n\n\n\nb.__add__(a)\n\n[3, 4, 1, 2]\n\n\n- a+b는 사실 내부적으로 a.__add(b)의 축약구문이다. 따라서 만약 a.__add__(b)의 기능을 바꾸면 (재정의 하면) a+b의 기능도 바뀔 것이다.\n\n\n- 학생예제\n\nclass Student: # student class를 만들어보자. (student 자료형인것.)\n    def __init__(self, age = 20.0, semester = 0):\n        self.age = age\n        self.semester = semester\n        print('입학을 축하합니다. 당신의 나이는 {}이고 현재 학기는 {}학기입니다.'.format(self.age, self.semester))\n    def __add__(self, val):\n        # val == 0: 휴학\n        # val == 1: 등록\n        if val == 0:\n            self.age = self.age + 0.5\n        elif val == 1:\n            self.age = self.age + 0.5\n            self.semester = self.semester + 1\n    def _repr_html_(self):\n        html_str = \"\"\"\n        나이: {} &lt;br/&gt;\n        학기: {} &lt;br/&gt;\n        \"\"\"\n        return html_str.format(self.age, self.semester)\n\n\niu = Student()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기입니다.\n\n\n\niu\n\n\n        나이: 20.0 \n        학기: 0 \n        \n\n\n\niu + 1 ## 1학년 1학기 등록\niu\n\n\n        나이: 20.5 \n        학기: 1 \n        \n\n\n\niu + 0 ## 휴학함\niu\n\n\n        나이: 21.0 \n        학기: 1 \n        \n\n\n\niu.__add__(1)\n\n\niu\n\n\n        나이: 21.5 \n        학기: 2 \n        \n\n\n- 연산을 연속으로 하고 싶다.\n\niu + 1 + 0 + 0 + 0 + 0\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n- 에러의 이유?\n(되는코드)\n\n(1+1)+1 # 1+1+1은 이렇게 볼 수 있다.\n\n3\n\n\n\n_a = (1+1)\ntype(_a)\n\nint\n\n\n\n_a+1 # 이 연산은 int 인스턴스 + int인스턴스\n\n3\n\n\n(안되는코드)\n\niu + 1 + 1\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n\n_a = iu + 1\ntype(_a)\n\nNoneType\n\n\n\n_a + 1\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n- 에러를 해결하는 방법: iu + 1의 결과로 Student클래스가 리턴되면 된다.\n\nclass Student: # student class를 만들어보자. (student 자료형인것.)\n    def __init__(self, age = 20.0, semester = 0):\n        self.age = age\n        self.semester = semester\n        print('입학을 축하합니다. 당신의 나이는 {}이고 현재 학기는 {}학기입니다.'.format(self.age, self.semester))\n    def __add__(self, val):\n        # val == 0: 휴학\n        # val == 1: 등록\n        if val == 0:\n            self.age = self.age + 0.5\n        elif val == 1:\n            self.age = self.age + 0.5\n            self.semester = self.semester + 1\n        return self\n    def _repr_html_(self):\n        html_str = \"\"\"\n        나이: {} &lt;br/&gt;\n        학기: {} &lt;br/&gt;\n        \"\"\"\n        return html_str.format(self.age, self.semester)\n\n\niu = Student()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기입니다.\n\n\n\niu+1  # __add__의 return에 Student 클래스의 인스턴스가 리턴되면서 자동으로 _repr_html_() 실행\n\n\n        나이: 20.5 \n        학기: 1 \n        \n\n\n\niu + 1 + 0 + 0 + 0 + 0\n\n\n        나이: 23.0 \n        학기: 2 \n        \n\n\n\n\n\n\na = 1\nb = 0\na*b\n\n0\n\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n        self.results = list()\n    def __mul__(self, other):\n        self.choose()\n        other.choose()\n        if self.actions[-1] == '가위' and other.actions[-1]=='가위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '가위' and other.actions[-1]=='바위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '가위' and other.actions[-1]=='보':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='가위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='바위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '바위' and other.actions[-1]=='보':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='가위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='바위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '보' and other.actions[-1]=='보':\n            self.results.append(0)\n            other.results.append(0)\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        액션: {} &lt;br/&gt;\n        승패: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions, self.results)\n\n\na = RPC()\nb = RPC()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\na*b\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: ['보'] \n        승패: [-1]\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: ['가위'] \n        승패: [1]\n        \n\n\n\nfor i in range(50000):\n    a*b\n\n\n#a\n\n\n#b\n\n\nsum(a.results), sum(b.results)\n\n(175, -175)\n\n\n\nsum(a.results)/50000\n\n0.0035\n\n\n\nsum(b.results)/50000\n\n-0.0035"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class05.html#숙제",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class05.html#숙제",
    "title": "[IP2022] class 05단계",
    "section": "",
    "text": "RPC클래스에서 Player a와 Player b를 만들어라. - Player a는 [‘가위’,‘보’] 중에 하나를 낼 수 있다. - 그리고 Player b는 [‘가위’,‘바위’] 중에 하나를 낼 수 있다. - 두 Player는 가지고 있는 패를 (같은 확률로) 랜덤으로 낸다. (즉, Player a가 가위만 내거나 보만 내는 경우는 없다.)\n\n누가 더 유리한가? 이유를 스스로 생각해보라.\n\n\n비슷하지 않을까?\n\n\n50000번을 시뮬레이션을 해보고 결과를 분석해보라.\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n        self.results = list()\n    def __mul__(self, other):\n        self.choose()\n        other.choose()\n        if self.actions[-1] == '가위' and other.actions[-1]=='가위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '가위' and other.actions[-1]=='바위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '가위' and other.actions[-1]=='보':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='가위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='바위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '바위' and other.actions[-1]=='보':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='가위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='바위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '보' and other.actions[-1]=='보':\n            self.results.append(0)\n            other.results.append(0)\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        액션: {} &lt;br/&gt;\n        승패: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions, self.results)\n\n\nplayer_a = RPC(['가위', '보'])\nplayer_b = RPC(['가위', '바위'])\n\n\nplayer_a\n\n\n        낼 수 있는 패: ['가위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\nplayer_b\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        액션: [] \n        승패: []\n        \n\n\n\nplayer_a*player_b\n\n\nplayer_a\n\n\n        낼 수 있는 패: ['가위', '보'] \n        액션: ['보'] \n        승패: [1]\n        \n\n\n\nplayer_b\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        액션: ['바위'] \n        승패: [-1]\n        \n\n\n\nfor i in range(50000):\n    player_a*player_b\n\n\nsum(player_a.results), sum(player_b.results)\n\n(-12279, 12279)\n\n\n\nsum(player_a.results)/50000, sum(player_b.results)/50000\n\n(-0.24558, 0.24558)"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class06.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class06.html",
    "title": "[IP2022] class 06단계",
    "section": "",
    "text": "상속, 사용자정의 자료형\n\n\n\n- 아래와 같은 클래스를 만들자.\n\n이름, 직급, 연봉에 대한 정보가 있다.\n연봉을 올려주는 메소드가 존재함.\n\n\nclass Employee:\n    def __init__(self, name,position=None, pay=0):\n        self.name = name\n        self.position = position\n        self.pay = pay\n    def _repr_html_(self):\n        html_str = \"\"\"\n        이름: {} &lt;br/&gt;\n        직급: {} &lt;br/&gt;\n        연봉: {} &lt;br/&gt;\n        \"\"\".format(self.name, self.position, self.pay)\n        return html_str\n    def giveraise(self, pct):\n        self.pay = self.pay * (1+pct)\n\n- 확인\n\niu = Employee('iu', position = 'staff', pay = 5000)\nhynn = Employee('hynn', position = 'staff', pay = 4000)\nhd = Employee('hodong', position = 'mgr', pay = 8000)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5000 \n        \n\n\n\niu.giveraise(0.1)\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn.giveraise(0.2)\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4800.0 \n        \n\n\n- 회사의 모든 직원의 연봉을 \\(10\\%\\)씩 올려보자.\n\niu = Employee('iu', position = 'staff', pay = 5000)\nhynn = Employee('hynn', position = 'staff', pay = 4000)\nhd = Employee('hodong', position = 'mgr', pay = 8000)\n\n\nfor i in [iu, hynn, hd]:\n    i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 8800.0 \n        \n\n\n- 매니저직은 일반직원들의 상승분에서 \\(5\\%\\)의 보너스가 추가되어 상승한다고 가정하고 모든 직원의 연봉을 \\(10\\%\\)씩 올리는 코드를 구현해보자.\n\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Employee('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu, hynn, hd]:\n    if i.position == 'mgr':\n        i.giveraise(0.1 + 0.05)\n    else:\n        i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.0 \n        \n\n\n\n\n\n\nclass Manager:\n    def __init__(self, name, position=None, pay=0):\n        self.name = name\n        self.position = position\n        self.pay = pay\n    def _repr_html_(self):\n        html_str = \"\"\"\n        이름: {} &lt;br/&gt;\n        직급: {} &lt;br/&gt;\n        연봉: {} &lt;br/&gt;\n        \"\"\".format(self.name, self.position, self.pay)\n        return html_str\n    def giveraise(self,pct):\n        self.pay = self.pay * (1+pct+0.05)\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Manager('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu,hynn,hd]:\n    i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.000000000002 \n        \n\n\n\n\n\n\nclass Manager(Employee):\n    def giveraise(self,pct):\n        self.pay = self.pay * (1+pct+0.05)\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Manager('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu,hynn,hd]:\n    i.giveraise(0.1) \n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.000000000002 \n        \n\n\n- 요약: 이미 만들어진 클래스에서 대부분의 기능은 그대로 쓰지만 일부기능만 변경 혹은 추가하고 싶다면 클래스를 상속하면 된다!\n\n\n\n\nref: http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791165213190\n\n- list와 비슷한데 멤버들의 빈도가 계산되는 메소드를 포함하는 새로운 나만의 list를 만들고 싶다.\n\nlst = ['a','b','a','c','b','a','d']\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n- 아래와 같은 딕셔너리를 만들고 싶다.\n\nfreq = {'a':3, 'b':2, 'c':1, 'd':1} \nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n\nlst.frequency()를 입력하면 위의 기능이 수행되도록 변형된 list를 쓰고 싶다.\n\n- 구현\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = {'a':0, 'b':0, 'c':0, 'd':0}\nfreq\n\n{'a': 0, 'b': 0, 'c': 0, 'd': 0}\n\n\n\nfor item in lst:\n    freq[item] = freq[item] + 1\n\n\nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = dict()\nfreq\n\n{}\n\n\n\nfor item in lst:\n    freq[item] = freq[item] + 1\n\nKeyError: 'a'\n\n\n에러이유? freq['a']를 호출할 수 없다. \\(\\to\\) freq.get('a',0) 이용\n\nfreq['a']\n\nKeyError: 'a'\n\n\n\nfreq.get?\n\n\nSignature: freq.get(key, default=None, /)\nDocstring: Return the value for key if key is in the dictionary, else default.\nType:      builtin_function_or_method\n\n\n\n\nkey에 대응하는 값이 있으면 그 값을 리턴하고 없으면 default를 리턴\n\n\nfreq.get('a') # freq['a']에 해당하는 자료가 없어도 에러가 나지 않음\n\n\nfreq.get('a',0) # freq['a']에 해당하는 자료가 없어도 에러가 나지 않음 + freq['a']에 해당하는 자료가 없으면 0을 리턴\n\n0\n\n\n\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = dict()\nfreq\n\n{}\n\n\n\nfor item in lst:\n    freq[item] = freq.get(item,0) + 1\n\n\nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n- 이것을 내가 정의하는 새로은 list의 메소드로 넣고 싶다.\n\nclass L(list):\n    def frequency(self):\n        freq = dict()\n        for item in self:\n            freq[item] = freq.get(item,0) + 1\n        return freq\n\n\nlst = L([1,1,1,2,2,3])\n\n\nlst # 원래 list에 있는 repr 기능을 상속받아서 이루어지는 결과\n\n[1, 1, 1, 2, 2, 3]\n\n\n\n_lst = L([4,5,6])\nlst + _lst  # L자료형끼리의 덧셈\n\n[1, 1, 1, 2, 2, 3, 4, 5, 6]\n\n\n\nlst + [4,5,6] # lst + [4,5,6] # L자료형과 list자료형의 덧셈도 가능\n\n[1, 1, 1, 2, 2, 3, 4, 5, 6]\n\n\n\nL자료형의 덧셈은 list의 덧셈과 완전히 같음\n\n\nlst.append(10) # append 함수도 그대로 쓸 수 있음.\n\n\nlst\n\n[1, 1, 1, 2, 2, 3, 10]\n\n\n- 기존 리스트에서 추가로 frequency() 메소드가 존재함.\n\nlst.frequency()\n\n{1: 3, 2: 2, 3: 1, 10: 1}\n\n\n\n\n\n\n\n- 사용자정의 자료형이 어떤 경우에는 유용할 수 있다.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nyear = ['2016','2017','2017','2017',2017,2018,2018,2019,2019] \nvalue = np.random.randn(9)\n\n\ndf = pd.DataFrame({'year':year, 'value':value})\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n-0.140139\n\n\n1\n2017\n1.412758\n\n\n2\n2017\n-0.065478\n\n\n3\n2017\n0.107847\n\n\n4\n2017\n0.824112\n\n\n5\n2018\n0.061573\n\n\n6\n2018\n-0.463060\n\n\n7\n2019\n-0.808921\n\n\n8\n2019\n0.389417\n\n\n\n\n\n\n\n\nplt.plot(df.year, df.value)\n\nTypeError: 'value' must be an instance of str or bytes, not a int\n\n\n\n\n\n에러의 이유: df.year에 str,int가 동시에 있음.\n\nnp.array(df.year)\n\narray(['2016', '2017', '2017', '2017', 2017, 2018, 2018, 2019, 2019],\n      dtype=object)\n\n\n자료형을 바꿔주면 해결할 수 있다.\n\nnp.array(df.year, dtype=np.float64)\n#np.array(df.year).astype(np.float64)\n#df.year.astype(np.float64)\n\narray([2016., 2017., 2017., 2017., 2017., 2018., 2018., 2019., 2019.])\n\n\n\nplt.plot(df.year.astype(np.float64),df.value,'.')\n\n\n\n\n\n\n\n\nyear = ['2016','2017','2017','2017년','2017년',2018,2018,2019,2019] \nvalue = np.random.randn(9)\n\n\ndf= pd.DataFrame({'year':year,'value':value})\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n0.127739\n\n\n1\n2017\n1.437921\n\n\n2\n2017\n-1.137349\n\n\n3\n2017년\n-0.178713\n\n\n4\n2017년\n-0.276401\n\n\n5\n2018\n2.467760\n\n\n6\n2018\n-1.068202\n\n\n7\n2019\n-0.313908\n\n\n8\n2019\n1.049837\n\n\n\n\n\n\n\n\nnp.array(df.year,dtype=np.float64) # 타입을 일괄적으로 바꾸기 어렵다. \n\nValueError: could not convert string to float: '2017년'\n\n\n\nL(df.year).frequency()\n\n{'2016': 1, '2017': 2, '2017년': 2, 2018: 2, 2019: 2}\n\n\n\n’2016’와 같은 형태, ’2017년’와 같은 형태, 숫자형이 혼합 \\(\\to\\) 맞춤형 변환이 필요함\n\n\n'2017년'.replace('년','')\n\n'2017'\n\n\n\ndef f(a): ## 사실 데이터의 구조를 모르면 이런 함수를 짤 수 없음 --&gt; 자료의 구조를 확인해준다는 의미에서 freq가 있다면 편리하다. \n    if type(a) is str:\n        if '년' in a:\n            return int(a.replace('년',''))\n        else:\n            return int(a)\n    else:\n        return a\n\n\n[f(a) for a in df.year]\n\n[2016, 2017, 2017, 2017, 2017, 2018, 2018, 2019, 2019]\n\n\n\ndf.year = [f(a) for a in df.year]\n\n\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n0.127739\n\n\n1\n2017\n1.437921\n\n\n2\n2017\n-1.137349\n\n\n3\n2017\n-0.178713\n\n\n4\n2017\n-0.276401\n\n\n5\n2018\n2.467760\n\n\n6\n2018\n-1.068202\n\n\n7\n2019\n-0.313908\n\n\n8\n2019\n1.049837\n\n\n\n\n\n\n\n\nplt.plot(df.year, df.value,'.')"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class06.html#인사관리-예제",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class06.html#인사관리-예제",
    "title": "[IP2022] class 06단계",
    "section": "",
    "text": "- 아래와 같은 클래스를 만들자.\n\n이름, 직급, 연봉에 대한 정보가 있다.\n연봉을 올려주는 메소드가 존재함.\n\n\nclass Employee:\n    def __init__(self, name,position=None, pay=0):\n        self.name = name\n        self.position = position\n        self.pay = pay\n    def _repr_html_(self):\n        html_str = \"\"\"\n        이름: {} &lt;br/&gt;\n        직급: {} &lt;br/&gt;\n        연봉: {} &lt;br/&gt;\n        \"\"\".format(self.name, self.position, self.pay)\n        return html_str\n    def giveraise(self, pct):\n        self.pay = self.pay * (1+pct)\n\n- 확인\n\niu = Employee('iu', position = 'staff', pay = 5000)\nhynn = Employee('hynn', position = 'staff', pay = 4000)\nhd = Employee('hodong', position = 'mgr', pay = 8000)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5000 \n        \n\n\n\niu.giveraise(0.1)\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn.giveraise(0.2)\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4800.0 \n        \n\n\n- 회사의 모든 직원의 연봉을 \\(10\\%\\)씩 올려보자.\n\niu = Employee('iu', position = 'staff', pay = 5000)\nhynn = Employee('hynn', position = 'staff', pay = 4000)\nhd = Employee('hodong', position = 'mgr', pay = 8000)\n\n\nfor i in [iu, hynn, hd]:\n    i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 8800.0 \n        \n\n\n- 매니저직은 일반직원들의 상승분에서 \\(5\\%\\)의 보너스가 추가되어 상승한다고 가정하고 모든 직원의 연봉을 \\(10\\%\\)씩 올리는 코드를 구현해보자.\n\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Employee('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu, hynn, hd]:\n    if i.position == 'mgr':\n        i.giveraise(0.1 + 0.05)\n    else:\n        i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.0 \n        \n\n\n\n\n\n\nclass Manager:\n    def __init__(self, name, position=None, pay=0):\n        self.name = name\n        self.position = position\n        self.pay = pay\n    def _repr_html_(self):\n        html_str = \"\"\"\n        이름: {} &lt;br/&gt;\n        직급: {} &lt;br/&gt;\n        연봉: {} &lt;br/&gt;\n        \"\"\".format(self.name, self.position, self.pay)\n        return html_str\n    def giveraise(self,pct):\n        self.pay = self.pay * (1+pct+0.05)\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Manager('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu,hynn,hd]:\n    i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.000000000002 \n        \n\n\n\n\n\n\nclass Manager(Employee):\n    def giveraise(self,pct):\n        self.pay = self.pay * (1+pct+0.05)\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Manager('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu,hynn,hd]:\n    i.giveraise(0.1) \n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.000000000002 \n        \n\n\n- 요약: 이미 만들어진 클래스에서 대부분의 기능은 그대로 쓰지만 일부기능만 변경 혹은 추가하고 싶다면 클래스를 상속하면 된다!\n\n\n\n\nref: http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791165213190\n\n- list와 비슷한데 멤버들의 빈도가 계산되는 메소드를 포함하는 새로운 나만의 list를 만들고 싶다.\n\nlst = ['a','b','a','c','b','a','d']\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n- 아래와 같은 딕셔너리를 만들고 싶다.\n\nfreq = {'a':3, 'b':2, 'c':1, 'd':1} \nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n\nlst.frequency()를 입력하면 위의 기능이 수행되도록 변형된 list를 쓰고 싶다.\n\n- 구현\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = {'a':0, 'b':0, 'c':0, 'd':0}\nfreq\n\n{'a': 0, 'b': 0, 'c': 0, 'd': 0}\n\n\n\nfor item in lst:\n    freq[item] = freq[item] + 1\n\n\nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = dict()\nfreq\n\n{}\n\n\n\nfor item in lst:\n    freq[item] = freq[item] + 1\n\nKeyError: 'a'\n\n\n에러이유? freq['a']를 호출할 수 없다. \\(\\to\\) freq.get('a',0) 이용\n\nfreq['a']\n\nKeyError: 'a'\n\n\n\nfreq.get?\n\n\nSignature: freq.get(key, default=None, /)\nDocstring: Return the value for key if key is in the dictionary, else default.\nType:      builtin_function_or_method\n\n\n\n\nkey에 대응하는 값이 있으면 그 값을 리턴하고 없으면 default를 리턴\n\n\nfreq.get('a') # freq['a']에 해당하는 자료가 없어도 에러가 나지 않음\n\n\nfreq.get('a',0) # freq['a']에 해당하는 자료가 없어도 에러가 나지 않음 + freq['a']에 해당하는 자료가 없으면 0을 리턴\n\n0\n\n\n\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = dict()\nfreq\n\n{}\n\n\n\nfor item in lst:\n    freq[item] = freq.get(item,0) + 1\n\n\nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n- 이것을 내가 정의하는 새로은 list의 메소드로 넣고 싶다.\n\nclass L(list):\n    def frequency(self):\n        freq = dict()\n        for item in self:\n            freq[item] = freq.get(item,0) + 1\n        return freq\n\n\nlst = L([1,1,1,2,2,3])\n\n\nlst # 원래 list에 있는 repr 기능을 상속받아서 이루어지는 결과\n\n[1, 1, 1, 2, 2, 3]\n\n\n\n_lst = L([4,5,6])\nlst + _lst  # L자료형끼리의 덧셈\n\n[1, 1, 1, 2, 2, 3, 4, 5, 6]\n\n\n\nlst + [4,5,6] # lst + [4,5,6] # L자료형과 list자료형의 덧셈도 가능\n\n[1, 1, 1, 2, 2, 3, 4, 5, 6]\n\n\n\nL자료형의 덧셈은 list의 덧셈과 완전히 같음\n\n\nlst.append(10) # append 함수도 그대로 쓸 수 있음.\n\n\nlst\n\n[1, 1, 1, 2, 2, 3, 10]\n\n\n- 기존 리스트에서 추가로 frequency() 메소드가 존재함.\n\nlst.frequency()\n\n{1: 3, 2: 2, 3: 1, 10: 1}"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class06.html#appendix-사용자정의-자료형의-유용함",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class06.html#appendix-사용자정의-자료형의-유용함",
    "title": "[IP2022] class 06단계",
    "section": "",
    "text": "- 사용자정의 자료형이 어떤 경우에는 유용할 수 있다.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nyear = ['2016','2017','2017','2017',2017,2018,2018,2019,2019] \nvalue = np.random.randn(9)\n\n\ndf = pd.DataFrame({'year':year, 'value':value})\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n-0.140139\n\n\n1\n2017\n1.412758\n\n\n2\n2017\n-0.065478\n\n\n3\n2017\n0.107847\n\n\n4\n2017\n0.824112\n\n\n5\n2018\n0.061573\n\n\n6\n2018\n-0.463060\n\n\n7\n2019\n-0.808921\n\n\n8\n2019\n0.389417\n\n\n\n\n\n\n\n\nplt.plot(df.year, df.value)\n\nTypeError: 'value' must be an instance of str or bytes, not a int\n\n\n\n\n\n에러의 이유: df.year에 str,int가 동시에 있음.\n\nnp.array(df.year)\n\narray(['2016', '2017', '2017', '2017', 2017, 2018, 2018, 2019, 2019],\n      dtype=object)\n\n\n자료형을 바꿔주면 해결할 수 있다.\n\nnp.array(df.year, dtype=np.float64)\n#np.array(df.year).astype(np.float64)\n#df.year.astype(np.float64)\n\narray([2016., 2017., 2017., 2017., 2017., 2018., 2018., 2019., 2019.])\n\n\n\nplt.plot(df.year.astype(np.float64),df.value,'.')\n\n\n\n\n\n\n\n\nyear = ['2016','2017','2017','2017년','2017년',2018,2018,2019,2019] \nvalue = np.random.randn(9)\n\n\ndf= pd.DataFrame({'year':year,'value':value})\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n0.127739\n\n\n1\n2017\n1.437921\n\n\n2\n2017\n-1.137349\n\n\n3\n2017년\n-0.178713\n\n\n4\n2017년\n-0.276401\n\n\n5\n2018\n2.467760\n\n\n6\n2018\n-1.068202\n\n\n7\n2019\n-0.313908\n\n\n8\n2019\n1.049837\n\n\n\n\n\n\n\n\nnp.array(df.year,dtype=np.float64) # 타입을 일괄적으로 바꾸기 어렵다. \n\nValueError: could not convert string to float: '2017년'\n\n\n\nL(df.year).frequency()\n\n{'2016': 1, '2017': 2, '2017년': 2, 2018: 2, 2019: 2}\n\n\n\n’2016’와 같은 형태, ’2017년’와 같은 형태, 숫자형이 혼합 \\(\\to\\) 맞춤형 변환이 필요함\n\n\n'2017년'.replace('년','')\n\n'2017'\n\n\n\ndef f(a): ## 사실 데이터의 구조를 모르면 이런 함수를 짤 수 없음 --&gt; 자료의 구조를 확인해준다는 의미에서 freq가 있다면 편리하다. \n    if type(a) is str:\n        if '년' in a:\n            return int(a.replace('년',''))\n        else:\n            return int(a)\n    else:\n        return a\n\n\n[f(a) for a in df.year]\n\n[2016, 2017, 2017, 2017, 2017, 2018, 2018, 2019, 2019]\n\n\n\ndf.year = [f(a) for a in df.year]\n\n\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n0.127739\n\n\n1\n2017\n1.437921\n\n\n2\n2017\n-1.137349\n\n\n3\n2017\n-0.178713\n\n\n4\n2017\n-0.276401\n\n\n5\n2018\n2.467760\n\n\n6\n2018\n-1.068202\n\n\n7\n2019\n-0.313908\n\n\n8\n2019\n1.049837\n\n\n\n\n\n\n\n\nplt.plot(df.year, df.value,'.')"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class04.html",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class04.html",
    "title": "[IP2022] class 04단계",
    "section": "",
    "text": "프린트 가로채기 __str__, __repr__ (파이썬의 비밀2,3)\n\n\n\n\n\nmotivating example\n__str__, 파이썬의 비밀2\n__repr__, 파이썬의 비밀3\n주피터 노트북의 비밀 (_repr_html_), __repr__와 __str__의 우선적용 순위\n\n\n\n\n\n\nimport numpy as np\n\n\n\n\n\n\n\n\n\n# class1 hw's review\nclass RPC:\n    def throw(self):\n        print(np.random.choice(['가위','바위','보']))\n\n\na = RPC()\n\n\na.throw()\n\n가위\n\n\n\n\n\n[가위, 바위, 보] 말고 [가위, 보] 혹은 [바위, 보] 처럼 정해진 케이스가 아닌 입력으로 받고 싶을 수도 있다.\n\nclass RPC:\n    def throw(self, candidate):\n        print(np.random.choice(candidate))\n\n\na = RPC()\n\n\n# throw(a, ['가위','바위','보'])\na.throw(['가위','바위','보'])\n\n보\n\n\n\na.throw(['가위', '보']) # 보, 가위만.\n\n가위\n\n\n\n\n\n\nclass RPC:\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def throw(self):\n        print(np.random.choice(self.candidate))\n\n\na = RPC() # __init__ 는 암묵적으로 실행\n\n\na.throw()\n\n보\n\n\n\n\n\n위의 코드 3줄과 동일한 코드이며, 풀어써보면 다음과 같다.\n\nclass RPC2:\n    pass\n\n\nb = RPC2() # 아무것도 없음..\n\n\ndef initt(b, candidate = ['가위','바위','보']):\n    b.candidate = candidate\n\n\ninitt(b)\n\n\n# 던져서 화면에 보여주는 과정까지 추가\ndef throw(b):\n    print(np.random.choice(b.candidate))\n\n\nthrow(b)\n\n보\n\n\n\n\n\n풀어쓴 코드를 조합해보면?\n\nclass RPC2:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n    def throw(self):\n        print(np.random.choice(self.candidate))\n\n\nb = RPC2()\n\n\nb.candidate\n\n['가위', '바위', '보']\n\n\n\nb.throw()\n\n가위\n\n\n\n\n\n생각해보니까 throw는 choose + show의 결합인 것 같다.\n\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def choose(self):\n        self.actions = np.random.choice(self.candidate)\n    def show(self):\n        print(self.actions)\n\n\na = RPC()  ## 시점2\n\n\na.actions ## 시점3 (지금은 정의되지 않음, choose를 해야함)\n\nAttributeError: 'RPC' object has no attribute 'actions'\n\n\n\na.choose() # 뭔가 선택했겠지?    ## 시점4\n\n\na.actions # 바위를 선택했구만     ## 시점5 \n\n'바위'\n\n\n\na.show()   ## 시점6\n\n바위\n\n\n\n\n\n위와 같은 코드입니다.\n\nclass _RPC:  ## 시점1 \n    pass  # &lt;-- 이렇게하면 아무 기능이 없는 비어있는 클래스가 정의된다.\n\n\n_a  = _RPC()  ## 시점2\n\ndef _init(_a, candidate = ['가위','바위','보']):\n    _a.candidate = candidate\n    \n_init(_a)\n\n\n_a.actions ## 시점3\n\nAttributeError: '_RPC' object has no attribute 'actions'\n\n\n\n# choose 선언      ## 시점4\ndef _choose(_a):\n    _a.actions = np.random.choice(_a.candidate)\n_choose(_a)\n\n\n_a.actions  ## 시점5\n\n'바위'\n\n\n\n# show 선언    ## 시점6\ndef _show(_a):\n    print(_a.actions)\n_show(_a)\n\n바위\n\n\n\n\n\n\n또 다른 인스턴스 b를 만들자. b는 가위만 낼 수 있다.\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def choose(self):\n        self.actions = np.random.choice(self.candidate)\n    def show(self):\n        print(self.actions)\n        \n\nb = RPC()\n\n\nb.candidate\n\n['가위', '바위', '보']\n\n\n\n아무것도 없으면 b의 candidate이 가위, 가위, 보로 들어감\n\n\nb = RPC(['가위']) # 가위만 포함된 리스트 전달\n\n\nb.candidate\n\n['가위']\n\n\n\nb.choose()\nb.show()\n\n가위\n\n\n- a, b의 선택들을 모아서 기록하고 싶다.\n\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n        self.actions = list() ## 추가\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate)) ## 추가\n    def show(self):\n        print(self.actions)\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nnp.random.seed(123)\nfor i in range(5):\n    a.choose()\n    a.show()\n\n['보']\n['보', '바위']\n['보', '바위', '보']\n['보', '바위', '보', '보']\n['보', '바위', '보', '보', '가위']\n\n\n\nshow() 지난 히스토리까지 다 나오니까 보기 좀 불편하댜\n\n\nnp.random.seed(123)\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n        self.actions = list() ## 추가\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate)) ## 추가\n    def show(self):\n        print(self.actions[-1]) ### 추추가\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n바위\n보\n보\n가위\n\n\n\na.actions\n\n['보', '바위', '보', '보', '가위']\n\n\n\nfor i in range(5):\n    b.choose()\n    b.show()\n\n가위\n가위\n가위\n가위\n가위\n\n\n\nb.actions\n\n['가위', '가위', '가위', '가위', '가위']\n\n\n\na.candidate, a.actions # (낼 수 있는 패, 내가 낸 패)\n\n(['가위', '바위', '보'], ['보', '바위', '보', '보', '가위'])\n\n\n\nb.candidate, b.actions # (낼 수 있는 패, 내가 낸 패)\n\n(['가위'], ['가위', '가위', '가위', '가위', '가위'])\n\n\n- info라는 함수를 만들어서 a의 오브젝트가 가지고 있는 정보를 모두 보도록 하자.\n(예비학습) 문자열 \\n 이 포함된다면?\n\n'클래스\\n어렵네..'\n\n'클래스\\n어렵네..'\n\n\n\nprint('클래스\\n어렵네..')\n\n클래스\n어렵네..\n\n\n예비학습 끝\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])      \n    def info(self):\n        print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n바위\n가위\n보\n가위\n바위\n\n\n\nfor i in range(5):\n    b.choose()\n    b.show()\n\n가위\n가위\n가위\n가위\n가위\n\n\n\na.info()\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위', '가위', '보', '가위', '바위']\n\n\n\nb.info()\n\n낼 수 있는 패: ['가위']\n기록: ['가위', '가위', '가위', '가위', '가위']\n\n\n- 만들고보니까 info와 print의 기능이 거의 비슷함 \\(\\to\\) print(a)를 하면 a.info()와 동일한 효과를 내도록 만들 수 있을까?\n- 말도 안되는 소리같다. 왜? - 안될것 같은 이유1: print는 파이썬 내장기능, 내장기능을 우리가 맘대로 커스터마이징해서 쓰기는 어려울 것 같다. - 안될 것 같은 이유2: 이유1이 해결된다 해도 문제다. 그럼 지금까지 우리가 사용했던 수 많은 print()의 결과는 어떻게 되는가?\n결론은 가능하다\n- 그런데 a의 자료형(RPC 자료형)에 해당하는 오브젝트에 한정하여 print를 수정하는 방법이 가능하다면? (그럼 다른 오브젝트들은 수정된 print에 영향을 받지 않음)\n\n\n\n\n- 관찰1: 현재 print(a)의 결과는 아래와 같다.\n\nprint(a)\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n\na는 RPC클래스에서 만든 오브젝트이며 a가 저장된 메모리 주소는 0x7faaa7500850라는 의미\n\n- 관찰2: a에는 __str__ 이 있다.\n\ndir(a)\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n 'actions',\n 'candidate',\n 'choose',\n 'info',\n 'show']\n\n\n\nset(dir(a)) & {'__str__'}\n\n{'__str__'}\n\n\n이것을 함수처럼 사용하니까 아래와 같다.\n\na.__str__\n\n&lt;method-wrapper '__str__' of RPC object at 0x7faaa7500850&gt;\n\n\n\na.__str__() # 클래스 안에 있는 메소드, 문자열 리턴\n\n'&lt;__main__.RPC object at 0x7faaa7500850&gt;'\n\n\n\nprint(a.__str__()) # 이거 print(a)를 실행한 결과와 같다?\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n\nprint(a)\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n- 생각: 만약에 내가 a.__str__() 라는 함수를 재정의 하여 리턴값을 ’너는 해킹당했다’로 바꾸게 되면 print(a)해서 나오는 결과는 어떻게 될까? (약간 해커같죠)\n(예비학습) 함수 덮어씌우기\n\ndef f():\n    print('asdf')\n\n\nf()\n\nasdf\n\n\n\ndef f():\n    print('guebin hahaha')\n\n\nf()\n\nguebin hahaha\n\n\n이런식으로 함수가 이미 정의되어 있더라도, 내가 나중에 덮어씌우면 그 함수의 기능을 다시 정의한다.\n(해킹시작)\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return '너는 해킹당했다'\n    def info(self):\n        print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\nprint(a)\n\n너는 해킹당했다\n\n\n- __str__ 의 리턴값을 info에서 타이핑했던 문자열로 재정의한다면?\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    # def info(self):\n    #     print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n    def __str__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.choose()\na.show()\n\n바위\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위']\n\n\n\na.choose()\na.show()\n\n가위\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위', '가위']\n\n\n\n\n- print(a) 와 print(a.__str__()) 는 같은 문법이다.\n- 참고로 a.__str__() 와 str(a) 도 같은 방법이다.\n\nstr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: ['바위', '가위']\"\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: ['바위', '가위']\"\n\n\n- 지금까지 우리가 썼던 기능을 확인!\n(예제1)\n\na = [1,2,3]\n\n\nprint(a)\n\n[1, 2, 3]\n\n\n\na.__str__()\n\n'[1, 2, 3]'\n\n\n\nstr(a)\n\n'[1, 2, 3]'\n\n\n(예제2)\n\na = {1,2,3}\nprint(a)\n\n{1, 2, 3}\n\n\n\nstr(a)\n\n'{1, 2, 3}'\n\n\n\na.__str__()\n\n'{1, 2, 3}'\n\n\n(예제3)\n\na = np.array(1)\na.shape\n\n()\n\n\n\ntype(a.shape)\n\ntuple\n\n\n\nprint(a.shape)\n\n()\n\n\n\na.shape.__str__()\n\n'()'\n\n\n\nstr(a.shape)\n\n'()'\n\n\n(예제4)\n\na = range(10)\nprint(a)\n\nrange(0, 10)\n\n\n\na.__str__()\n\n'range(0, 10)'\n\n\n(예제5)\n\na = np.arange(100).reshape(10,10)\nprint(a)\n\n[[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]\n [20 21 22 23 24 25 26 27 28 29]\n [30 31 32 33 34 35 36 37 38 39]\n [40 41 42 43 44 45 46 47 48 49]\n [50 51 52 53 54 55 56 57 58 59]\n [60 61 62 63 64 65 66 67 68 69]\n [70 71 72 73 74 75 76 77 78 79]\n [80 81 82 83 84 85 86 87 88 89]\n [90 91 92 93 94 95 96 97 98 99]]\n\n\n\na.__str__()\n\n'[[ 0  1  2  3  4  5  6  7  8  9]\\n [10 11 12 13 14 15 16 17 18 19]\\n [20 21 22 23 24 25 26 27 28 29]\\n [30 31 32 33 34 35 36 37 38 39]\\n [40 41 42 43 44 45 46 47 48 49]\\n [50 51 52 53 54 55 56 57 58 59]\\n [60 61 62 63 64 65 66 67 68 69]\\n [70 71 72 73 74 75 76 77 78 79]\\n [80 81 82 83 84 85 86 87 88 89]\\n [90 91 92 93 94 95 96 97 98 99]]'\n\n\n\n\n\n\n- 생각해보니까 print를 써서 우리가 원하는 정보를 확인하는건 아니였음\n\na = [1,2,3]\n\n\na\n\n[1, 2, 3]\n\n\n\nprint(a) # print(a.__str__()) + enter ==&gt; a + enter\n\n[1, 2, 3]\n\n\n-`` a + 엔터를 하면 print(a) + 엔터를 하는 것과 같은 효과인가?\n(반례)\n\na = np.array([1,2,3,4]).reshape(2,2)\n\n\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nprint(a)\n\n[[1 2]\n [3 4]]\n\n\n- a + 엔터 는 print(a) + 엔터 가 다른 경우도 있다. \\(\\to\\) 추측: 서로 다른 숨겨진 기능이 있다! \\(\\to\\) 결론: 그 기능은 __repr__ 에 저장되어 있음.\n\n__repr__ 추가 전\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\na\n\n&lt;__main__.RPC at 0x7faaa6d821c0&gt;\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\n__repr__ 추가 후\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\na # print(a.__repr__())\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n- 그럼 우리가 지금까지 했던 것?\n\na = np.array([1,2,3])\n\n\na\n\narray([1, 2, 3])\n\n\n\nprint(a)\n\n[1 2 3]\n\n\n\na.__repr__()\n\n'array([1, 2, 3])'\n\n\n\na.__str__()\n\n'[1 2 3]'\n\n\n\n\n- 대화형콘솔에서 오브젝트이름 + 엔터를 쳐서 나오는 출력은 __repr__의 결과와 연관이 있다.\n\na = np.array(range(10000)).reshape(100,100)\na\n\narray([[   0,    1,    2, ...,   97,   98,   99],\n       [ 100,  101,  102, ...,  197,  198,  199],\n       [ 200,  201,  202, ...,  297,  298,  299],\n       ...,\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])\n\n\n\na.__repr__()\n\n'array([[   0,    1,    2, ...,   97,   98,   99],\\n       [ 100,  101,  102, ...,  197,  198,  199],\\n       [ 200,  201,  202, ...,  297,  298,  299],\\n       ...,\\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])'\n\n\n- 참고로 a.__repr__()은 repr(a)와 같다.\n\nrepr(a)\n\n'array([[   0,    1,    2, ...,   97,   98,   99],\\n       [ 100,  101,  102, ...,  197,  198,  199],\\n       [ 200,  201,  202, ...,  297,  298,  299],\\n       ...,\\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])'\n\n\n\n\n\n- 요즘에는 IDE 발전에 따라서 오브젝트 + 엔터 칠 때 나오는 출력의 형태도 다양해지고 있음.\n\nimport pandas as pd\n\n\ndf = pd.DataFrame({'a':[1,2,3],\n                   'b':[2,3,4]})\n\n\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\n예쁘게 나온다.\n\n- 위의 결과는 print(df.__repr__())의 결과와 조금 다르게 나온다?\n\nprint(df.__repr__())\n\n   a  b\n0  1  2\n1  2  3\n2  3  4\n\n\n- print(df.__repr__())는 예전 검은화면에서 코딩할 때 나오는 출력임\nPython 3.10.2 | packaged by conda-forge | (main, Feb  1 2022, 19:28:35) [GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt; &gt;&gt; import pandas as pd \n&gt;&gt;&gt; df = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})&gt;&gt;&gt; df\n   a  b\n0  1  2\n1  2  3\n2  3  4\n&gt;&gt;&gt;\n- 주피터에서는 ‘오브젝트이름 + 엔터’ 치면 HTML(df.__repr_html())이 실행되고 repr_html_()이 정의되어 있지 않으면 print(df.__rept__())이 실행된다.\n\ndf._repr_html_()\n\n'&lt;div&gt;\\n&lt;style scoped&gt;\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n&lt;/style&gt;\\n&lt;table border=\"1\" class=\"dataframe\"&gt;\\n  &lt;thead&gt;\\n    &lt;tr style=\"text-align: right;\"&gt;\\n      &lt;th&gt;&lt;/th&gt;\\n      &lt;th&gt;a&lt;/th&gt;\\n      &lt;th&gt;b&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;0&lt;/th&gt;\\n      &lt;td&gt;1&lt;/td&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;1&lt;/th&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;2&lt;/th&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n      &lt;td&gt;4&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;'\n\n\n\nhtml 코드!\n\n\nfrom IPython.core.display import HTML\n\n\nHTML(df._repr_html_())\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n- 물론 df._repr_html_()함수가 내부적으로 있어도 html이 지원되지 않는 환경이라면 print(df.__repr__())이 내부적으로 수행된다.\n\n\n\n\n(예제1)\n- 아래의 예제를 관찰하자.\n\nclass RPS:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPS()\na\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__repr__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\nrepr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n- 여기까지는 상식수준의 결과임. 이제 아래를 관찰하라.\n\nprint(a) # print(a.__repr__())\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\nstr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\n__str__()은 건드린적이 없는데?\n\n\na.__repr__??\n\n\nSignature: a.__repr__()\nDocstring: Return repr(self).\nSource:   \n    def __repr__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\nFile:      ~/Dropbox/Quarto-Blog/posts/Python/&lt;ipython-input-296-bcd76efb6380&gt;\nType:      method\n\n\n\n\na.__str__??\n\n\nSignature:      a.__str__()\nCall signature: a.__str__(*args, **kwargs)\nType:           method-wrapper\nString form:    &lt;method-wrapper '__str__' of RPS object at 0x7faaa47aae20&gt;\nDocstring:      Return str(self).\n\n\n\n\n__str__()은 건드린 적이 없는데 \\(\\to\\) 건드린적은 없는데 기능이 바뀌어있음.\n\n(예제2)\n- 아래의 예제를 관찰하자.\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\na.__repr__()\n\n'&lt;__main__.RPC object at 0x7f8f38ca22e0&gt;'\n\n\n\na.__str__??\n\n\nSignature: a.__str__()\nDocstring: Return str(self).\nSource:   \n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\nFile:      ~/Dropbox/Quarto-Blog/posts/Python/&lt;ipython-input-3-2e46ee18321f&gt;\nType:      method\n\n\n\n\na.__repr__??\n\n\nSignature:      a.__repr__()\nCall signature: a.__repr__(*args, **kwargs)\nType:           method-wrapper\nString form:    &lt;method-wrapper '__repr__' of RPC object at 0x7f8f38ca22e0&gt;\nDocstring:      Return repr(self).\n\n\n\n2번째 예제에서는 건드린 애만 바뀌었는데 첫번째 예제에서는 건드리지 않은 애들까지 기능이 바뀌었다.\n(예제3)\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return '너는 해킹당했다. 하하하'\n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\na\n\n너는 해킹당했다. 하하하\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n- __str__ 와 __repr__을 건드리지 않고 출력결과를 바꾸고 싶다면?\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        기록: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\nstr(a)\n\n'&lt;__main__.RPC object at 0x7f8f38bb7730&gt;'\n\n\n\nrepr(a)\n\n'&lt;__main__.RPC object at 0x7f8f38bb7730&gt;'\n\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: []\n        \n\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n바위\n가위\n바위\n보\n\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: ['보', '바위', '가위', '바위', '보']\n        \n\n\n\n\n\n아래의 클래스를 수정하여\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt; \n        기록: {}\n        \"\"\"\n        return html_str.format(self.candidate,self.actions)\n클래스에서 생성된 인스턴스의 출력결과가 아래와 같도록 하라.\n학번: 202143052 \n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['가위', '가위', '보', '보', '바위']\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        학번: {} &lt;br/&gt;\n        낼 수 있는 패: {} &lt;br/&gt; \n        기록: {}\n        \"\"\"\n        return html_str.format(202143052,self.candidate,self.actions)\n\n\na = RPS()\n\n\na\n\n\n        학번: 202143052 \n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: []\n        \n\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n가위\n바위\n바위\n가위\n\n\n\na\n\n\n        학번: 202143052 \n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: ['보', '가위', '바위', '바위', '가위']"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class04.html#contents",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class04.html#contents",
    "title": "[IP2022] class 04단계",
    "section": "",
    "text": "motivating example\n__str__, 파이썬의 비밀2\n__repr__, 파이썬의 비밀3\n주피터 노트북의 비밀 (_repr_html_), __repr__와 __str__의 우선적용 순위"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class04.html#imports",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class04.html#imports",
    "title": "[IP2022] class 04단계",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class04.html#motivating-example",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class04.html#motivating-example",
    "title": "[IP2022] class 04단계",
    "section": "",
    "text": "# class1 hw's review\nclass RPC:\n    def throw(self):\n        print(np.random.choice(['가위','바위','보']))\n\n\na = RPC()\n\n\na.throw()\n\n가위\n\n\n\n\n\n[가위, 바위, 보] 말고 [가위, 보] 혹은 [바위, 보] 처럼 정해진 케이스가 아닌 입력으로 받고 싶을 수도 있다.\n\nclass RPC:\n    def throw(self, candidate):\n        print(np.random.choice(candidate))\n\n\na = RPC()\n\n\n# throw(a, ['가위','바위','보'])\na.throw(['가위','바위','보'])\n\n보\n\n\n\na.throw(['가위', '보']) # 보, 가위만.\n\n가위\n\n\n\n\n\n\nclass RPC:\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def throw(self):\n        print(np.random.choice(self.candidate))\n\n\na = RPC() # __init__ 는 암묵적으로 실행\n\n\na.throw()\n\n보\n\n\n\n\n\n위의 코드 3줄과 동일한 코드이며, 풀어써보면 다음과 같다.\n\nclass RPC2:\n    pass\n\n\nb = RPC2() # 아무것도 없음..\n\n\ndef initt(b, candidate = ['가위','바위','보']):\n    b.candidate = candidate\n\n\ninitt(b)\n\n\n# 던져서 화면에 보여주는 과정까지 추가\ndef throw(b):\n    print(np.random.choice(b.candidate))\n\n\nthrow(b)\n\n보\n\n\n\n\n\n풀어쓴 코드를 조합해보면?\n\nclass RPC2:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n    def throw(self):\n        print(np.random.choice(self.candidate))\n\n\nb = RPC2()\n\n\nb.candidate\n\n['가위', '바위', '보']\n\n\n\nb.throw()\n\n가위\n\n\n\n\n\n생각해보니까 throw는 choose + show의 결합인 것 같다.\n\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def choose(self):\n        self.actions = np.random.choice(self.candidate)\n    def show(self):\n        print(self.actions)\n\n\na = RPC()  ## 시점2\n\n\na.actions ## 시점3 (지금은 정의되지 않음, choose를 해야함)\n\nAttributeError: 'RPC' object has no attribute 'actions'\n\n\n\na.choose() # 뭔가 선택했겠지?    ## 시점4\n\n\na.actions # 바위를 선택했구만     ## 시점5 \n\n'바위'\n\n\n\na.show()   ## 시점6\n\n바위\n\n\n\n\n\n위와 같은 코드입니다.\n\nclass _RPC:  ## 시점1 \n    pass  # &lt;-- 이렇게하면 아무 기능이 없는 비어있는 클래스가 정의된다.\n\n\n_a  = _RPC()  ## 시점2\n\ndef _init(_a, candidate = ['가위','바위','보']):\n    _a.candidate = candidate\n    \n_init(_a)\n\n\n_a.actions ## 시점3\n\nAttributeError: '_RPC' object has no attribute 'actions'\n\n\n\n# choose 선언      ## 시점4\ndef _choose(_a):\n    _a.actions = np.random.choice(_a.candidate)\n_choose(_a)\n\n\n_a.actions  ## 시점5\n\n'바위'\n\n\n\n# show 선언    ## 시점6\ndef _show(_a):\n    print(_a.actions)\n_show(_a)\n\n바위\n\n\n\n\n\n\n또 다른 인스턴스 b를 만들자. b는 가위만 낼 수 있다.\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def choose(self):\n        self.actions = np.random.choice(self.candidate)\n    def show(self):\n        print(self.actions)\n        \n\nb = RPC()\n\n\nb.candidate\n\n['가위', '바위', '보']\n\n\n\n아무것도 없으면 b의 candidate이 가위, 가위, 보로 들어감\n\n\nb = RPC(['가위']) # 가위만 포함된 리스트 전달\n\n\nb.candidate\n\n['가위']\n\n\n\nb.choose()\nb.show()\n\n가위\n\n\n- a, b의 선택들을 모아서 기록하고 싶다.\n\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n        self.actions = list() ## 추가\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate)) ## 추가\n    def show(self):\n        print(self.actions)\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nnp.random.seed(123)\nfor i in range(5):\n    a.choose()\n    a.show()\n\n['보']\n['보', '바위']\n['보', '바위', '보']\n['보', '바위', '보', '보']\n['보', '바위', '보', '보', '가위']\n\n\n\nshow() 지난 히스토리까지 다 나오니까 보기 좀 불편하댜\n\n\nnp.random.seed(123)\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n        self.actions = list() ## 추가\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate)) ## 추가\n    def show(self):\n        print(self.actions[-1]) ### 추추가\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n바위\n보\n보\n가위\n\n\n\na.actions\n\n['보', '바위', '보', '보', '가위']\n\n\n\nfor i in range(5):\n    b.choose()\n    b.show()\n\n가위\n가위\n가위\n가위\n가위\n\n\n\nb.actions\n\n['가위', '가위', '가위', '가위', '가위']\n\n\n\na.candidate, a.actions # (낼 수 있는 패, 내가 낸 패)\n\n(['가위', '바위', '보'], ['보', '바위', '보', '보', '가위'])\n\n\n\nb.candidate, b.actions # (낼 수 있는 패, 내가 낸 패)\n\n(['가위'], ['가위', '가위', '가위', '가위', '가위'])\n\n\n- info라는 함수를 만들어서 a의 오브젝트가 가지고 있는 정보를 모두 보도록 하자.\n(예비학습) 문자열 \\n 이 포함된다면?\n\n'클래스\\n어렵네..'\n\n'클래스\\n어렵네..'\n\n\n\nprint('클래스\\n어렵네..')\n\n클래스\n어렵네..\n\n\n예비학습 끝\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])      \n    def info(self):\n        print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n바위\n가위\n보\n가위\n바위\n\n\n\nfor i in range(5):\n    b.choose()\n    b.show()\n\n가위\n가위\n가위\n가위\n가위\n\n\n\na.info()\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위', '가위', '보', '가위', '바위']\n\n\n\nb.info()\n\n낼 수 있는 패: ['가위']\n기록: ['가위', '가위', '가위', '가위', '가위']\n\n\n- 만들고보니까 info와 print의 기능이 거의 비슷함 \\(\\to\\) print(a)를 하면 a.info()와 동일한 효과를 내도록 만들 수 있을까?\n- 말도 안되는 소리같다. 왜? - 안될것 같은 이유1: print는 파이썬 내장기능, 내장기능을 우리가 맘대로 커스터마이징해서 쓰기는 어려울 것 같다. - 안될 것 같은 이유2: 이유1이 해결된다 해도 문제다. 그럼 지금까지 우리가 사용했던 수 많은 print()의 결과는 어떻게 되는가?\n결론은 가능하다\n- 그런데 a의 자료형(RPC 자료형)에 해당하는 오브젝트에 한정하여 print를 수정하는 방법이 가능하다면? (그럼 다른 오브젝트들은 수정된 print에 영향을 받지 않음)"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class04.html#str__",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class04.html#str__",
    "title": "[IP2022] class 04단계",
    "section": "",
    "text": "- 관찰1: 현재 print(a)의 결과는 아래와 같다.\n\nprint(a)\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n\na는 RPC클래스에서 만든 오브젝트이며 a가 저장된 메모리 주소는 0x7faaa7500850라는 의미\n\n- 관찰2: a에는 __str__ 이 있다.\n\ndir(a)\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n 'actions',\n 'candidate',\n 'choose',\n 'info',\n 'show']\n\n\n\nset(dir(a)) & {'__str__'}\n\n{'__str__'}\n\n\n이것을 함수처럼 사용하니까 아래와 같다.\n\na.__str__\n\n&lt;method-wrapper '__str__' of RPC object at 0x7faaa7500850&gt;\n\n\n\na.__str__() # 클래스 안에 있는 메소드, 문자열 리턴\n\n'&lt;__main__.RPC object at 0x7faaa7500850&gt;'\n\n\n\nprint(a.__str__()) # 이거 print(a)를 실행한 결과와 같다?\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n\nprint(a)\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n- 생각: 만약에 내가 a.__str__() 라는 함수를 재정의 하여 리턴값을 ’너는 해킹당했다’로 바꾸게 되면 print(a)해서 나오는 결과는 어떻게 될까? (약간 해커같죠)\n(예비학습) 함수 덮어씌우기\n\ndef f():\n    print('asdf')\n\n\nf()\n\nasdf\n\n\n\ndef f():\n    print('guebin hahaha')\n\n\nf()\n\nguebin hahaha\n\n\n이런식으로 함수가 이미 정의되어 있더라도, 내가 나중에 덮어씌우면 그 함수의 기능을 다시 정의한다.\n(해킹시작)\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return '너는 해킹당했다'\n    def info(self):\n        print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\nprint(a)\n\n너는 해킹당했다\n\n\n- __str__ 의 리턴값을 info에서 타이핑했던 문자열로 재정의한다면?\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    # def info(self):\n    #     print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n    def __str__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.choose()\na.show()\n\n바위\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위']\n\n\n\na.choose()\na.show()\n\n가위\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위', '가위']\n\n\n\n\n- print(a) 와 print(a.__str__()) 는 같은 문법이다.\n- 참고로 a.__str__() 와 str(a) 도 같은 방법이다.\n\nstr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: ['바위', '가위']\"\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: ['바위', '가위']\"\n\n\n- 지금까지 우리가 썼던 기능을 확인!\n(예제1)\n\na = [1,2,3]\n\n\nprint(a)\n\n[1, 2, 3]\n\n\n\na.__str__()\n\n'[1, 2, 3]'\n\n\n\nstr(a)\n\n'[1, 2, 3]'\n\n\n(예제2)\n\na = {1,2,3}\nprint(a)\n\n{1, 2, 3}\n\n\n\nstr(a)\n\n'{1, 2, 3}'\n\n\n\na.__str__()\n\n'{1, 2, 3}'\n\n\n(예제3)\n\na = np.array(1)\na.shape\n\n()\n\n\n\ntype(a.shape)\n\ntuple\n\n\n\nprint(a.shape)\n\n()\n\n\n\na.shape.__str__()\n\n'()'\n\n\n\nstr(a.shape)\n\n'()'\n\n\n(예제4)\n\na = range(10)\nprint(a)\n\nrange(0, 10)\n\n\n\na.__str__()\n\n'range(0, 10)'\n\n\n(예제5)\n\na = np.arange(100).reshape(10,10)\nprint(a)\n\n[[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]\n [20 21 22 23 24 25 26 27 28 29]\n [30 31 32 33 34 35 36 37 38 39]\n [40 41 42 43 44 45 46 47 48 49]\n [50 51 52 53 54 55 56 57 58 59]\n [60 61 62 63 64 65 66 67 68 69]\n [70 71 72 73 74 75 76 77 78 79]\n [80 81 82 83 84 85 86 87 88 89]\n [90 91 92 93 94 95 96 97 98 99]]\n\n\n\na.__str__()\n\n'[[ 0  1  2  3  4  5  6  7  8  9]\\n [10 11 12 13 14 15 16 17 18 19]\\n [20 21 22 23 24 25 26 27 28 29]\\n [30 31 32 33 34 35 36 37 38 39]\\n [40 41 42 43 44 45 46 47 48 49]\\n [50 51 52 53 54 55 56 57 58 59]\\n [60 61 62 63 64 65 66 67 68 69]\\n [70 71 72 73 74 75 76 77 78 79]\\n [80 81 82 83 84 85 86 87 88 89]\\n [90 91 92 93 94 95 96 97 98 99]]'"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class04.html#repr__",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class04.html#repr__",
    "title": "[IP2022] class 04단계",
    "section": "",
    "text": "- 생각해보니까 print를 써서 우리가 원하는 정보를 확인하는건 아니였음\n\na = [1,2,3]\n\n\na\n\n[1, 2, 3]\n\n\n\nprint(a) # print(a.__str__()) + enter ==&gt; a + enter\n\n[1, 2, 3]\n\n\n-`` a + 엔터를 하면 print(a) + 엔터를 하는 것과 같은 효과인가?\n(반례)\n\na = np.array([1,2,3,4]).reshape(2,2)\n\n\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nprint(a)\n\n[[1 2]\n [3 4]]\n\n\n- a + 엔터 는 print(a) + 엔터 가 다른 경우도 있다. \\(\\to\\) 추측: 서로 다른 숨겨진 기능이 있다! \\(\\to\\) 결론: 그 기능은 __repr__ 에 저장되어 있음.\n\n__repr__ 추가 전\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\na\n\n&lt;__main__.RPC at 0x7faaa6d821c0&gt;\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\n__repr__ 추가 후\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\na # print(a.__repr__())\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n- 그럼 우리가 지금까지 했던 것?\n\na = np.array([1,2,3])\n\n\na\n\narray([1, 2, 3])\n\n\n\nprint(a)\n\n[1 2 3]\n\n\n\na.__repr__()\n\n'array([1, 2, 3])'\n\n\n\na.__str__()\n\n'[1 2 3]'\n\n\n\n\n- 대화형콘솔에서 오브젝트이름 + 엔터를 쳐서 나오는 출력은 __repr__의 결과와 연관이 있다.\n\na = np.array(range(10000)).reshape(100,100)\na\n\narray([[   0,    1,    2, ...,   97,   98,   99],\n       [ 100,  101,  102, ...,  197,  198,  199],\n       [ 200,  201,  202, ...,  297,  298,  299],\n       ...,\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])\n\n\n\na.__repr__()\n\n'array([[   0,    1,    2, ...,   97,   98,   99],\\n       [ 100,  101,  102, ...,  197,  198,  199],\\n       [ 200,  201,  202, ...,  297,  298,  299],\\n       ...,\\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])'\n\n\n- 참고로 a.__repr__()은 repr(a)와 같다.\n\nrepr(a)\n\n'array([[   0,    1,    2, ...,   97,   98,   99],\\n       [ 100,  101,  102, ...,  197,  198,  199],\\n       [ 200,  201,  202, ...,  297,  298,  299],\\n       ...,\\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])'\n\n\n\n\n\n- 요즘에는 IDE 발전에 따라서 오브젝트 + 엔터 칠 때 나오는 출력의 형태도 다양해지고 있음.\n\nimport pandas as pd\n\n\ndf = pd.DataFrame({'a':[1,2,3],\n                   'b':[2,3,4]})\n\n\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\n예쁘게 나온다.\n\n- 위의 결과는 print(df.__repr__())의 결과와 조금 다르게 나온다?\n\nprint(df.__repr__())\n\n   a  b\n0  1  2\n1  2  3\n2  3  4\n\n\n- print(df.__repr__())는 예전 검은화면에서 코딩할 때 나오는 출력임\nPython 3.10.2 | packaged by conda-forge | (main, Feb  1 2022, 19:28:35) [GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt; &gt;&gt; import pandas as pd \n&gt;&gt;&gt; df = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})&gt;&gt;&gt; df\n   a  b\n0  1  2\n1  2  3\n2  3  4\n&gt;&gt;&gt;\n- 주피터에서는 ‘오브젝트이름 + 엔터’ 치면 HTML(df.__repr_html())이 실행되고 repr_html_()이 정의되어 있지 않으면 print(df.__rept__())이 실행된다.\n\ndf._repr_html_()\n\n'&lt;div&gt;\\n&lt;style scoped&gt;\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n&lt;/style&gt;\\n&lt;table border=\"1\" class=\"dataframe\"&gt;\\n  &lt;thead&gt;\\n    &lt;tr style=\"text-align: right;\"&gt;\\n      &lt;th&gt;&lt;/th&gt;\\n      &lt;th&gt;a&lt;/th&gt;\\n      &lt;th&gt;b&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;0&lt;/th&gt;\\n      &lt;td&gt;1&lt;/td&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;1&lt;/th&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;2&lt;/th&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n      &lt;td&gt;4&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;'\n\n\n\nhtml 코드!\n\n\nfrom IPython.core.display import HTML\n\n\nHTML(df._repr_html_())\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n- 물론 df._repr_html_()함수가 내부적으로 있어도 html이 지원되지 않는 환경이라면 print(df.__repr__())이 내부적으로 수행된다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class04.html#repr__와-__str__의-우선적용-순위",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class04.html#repr__와-__str__의-우선적용-순위",
    "title": "[IP2022] class 04단계",
    "section": "",
    "text": "(예제1)\n- 아래의 예제를 관찰하자.\n\nclass RPS:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPS()\na\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__repr__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\nrepr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n- 여기까지는 상식수준의 결과임. 이제 아래를 관찰하라.\n\nprint(a) # print(a.__repr__())\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\nstr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\n__str__()은 건드린적이 없는데?\n\n\na.__repr__??\n\n\nSignature: a.__repr__()\nDocstring: Return repr(self).\nSource:   \n    def __repr__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\nFile:      ~/Dropbox/Quarto-Blog/posts/Python/&lt;ipython-input-296-bcd76efb6380&gt;\nType:      method\n\n\n\n\na.__str__??\n\n\nSignature:      a.__str__()\nCall signature: a.__str__(*args, **kwargs)\nType:           method-wrapper\nString form:    &lt;method-wrapper '__str__' of RPS object at 0x7faaa47aae20&gt;\nDocstring:      Return str(self).\n\n\n\n\n__str__()은 건드린 적이 없는데 \\(\\to\\) 건드린적은 없는데 기능이 바뀌어있음.\n\n(예제2)\n- 아래의 예제를 관찰하자.\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\na.__repr__()\n\n'&lt;__main__.RPC object at 0x7f8f38ca22e0&gt;'\n\n\n\na.__str__??\n\n\nSignature: a.__str__()\nDocstring: Return str(self).\nSource:   \n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\nFile:      ~/Dropbox/Quarto-Blog/posts/Python/&lt;ipython-input-3-2e46ee18321f&gt;\nType:      method\n\n\n\n\na.__repr__??\n\n\nSignature:      a.__repr__()\nCall signature: a.__repr__(*args, **kwargs)\nType:           method-wrapper\nString form:    &lt;method-wrapper '__repr__' of RPC object at 0x7f8f38ca22e0&gt;\nDocstring:      Return repr(self).\n\n\n\n2번째 예제에서는 건드린 애만 바뀌었는데 첫번째 예제에서는 건드리지 않은 애들까지 기능이 바뀌었다.\n(예제3)\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return '너는 해킹당했다. 하하하'\n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\na\n\n너는 해킹당했다. 하하하\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n- __str__ 와 __repr__을 건드리지 않고 출력결과를 바꾸고 싶다면?\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        기록: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\nstr(a)\n\n'&lt;__main__.RPC object at 0x7f8f38bb7730&gt;'\n\n\n\nrepr(a)\n\n'&lt;__main__.RPC object at 0x7f8f38bb7730&gt;'\n\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: []\n        \n\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n바위\n가위\n바위\n보\n\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: ['보', '바위', '가위', '바위', '보']"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class04.html#숙제",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class04.html#숙제",
    "title": "[IP2022] class 04단계",
    "section": "",
    "text": "아래의 클래스를 수정하여\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt; \n        기록: {}\n        \"\"\"\n        return html_str.format(self.candidate,self.actions)\n클래스에서 생성된 인스턴스의 출력결과가 아래와 같도록 하라.\n학번: 202143052 \n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['가위', '가위', '보', '보', '바위']\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        학번: {} &lt;br/&gt;\n        낼 수 있는 패: {} &lt;br/&gt; \n        기록: {}\n        \"\"\"\n        return html_str.format(202143052,self.candidate,self.actions)\n\n\na = RPS()\n\n\na\n\n\n        학번: 202143052 \n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: []\n        \n\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n가위\n바위\n바위\n가위\n\n\n\na\n\n\n        학번: 202143052 \n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: ['보', '가위', '바위', '바위', '가위']"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class08.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class08.html",
    "title": "[IP2022] class 08단계",
    "section": "",
    "text": "for문 복습, iterable object\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\n\n- 아래와 같은 예제들을 관찰하여 for문을 복습하자.\n(예제1)\n\nfor i in [1,2,3,4]:\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제2)\n\nfor i in (1,2,3,4):\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제3)\n\nfor i in '1234':\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제4)\n\na=5\nfor i in a:\n    print(i)\n\nTypeError: 'int' object is not iterable\n\n\n\n5라고 출력되어야 하지 않나?\n\n- 의문1:\nfor i in ???:\n    print(i)\n에서 ???자리에 올 수 있는 것이 무엇일까?\n(예제5)\n상황1\n\nlst = [[1,2,3,4],[3,4,5,6]]\nlst\n\n[[1, 2, 3, 4], [3, 4, 5, 6]]\n\n\n\nfor l in lst:\n    print(l)\n\n[1, 2, 3, 4]\n[3, 4, 5, 6]\n\n\n상황2\n\ndf = pd.DataFrame(lst)\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nfor i in df:\n    print(i)\n\n0\n1\n2\n3\n\n\n칼럼이름들이 나오는 것 같음 \\(\\to\\) 확인해보자.\n\ndf.columns = pd.Index(['X'+str(i) for i in range(1,5)])\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nfor i in df:\n    print(i)\n\nX1\nX2\nX3\nX4\n\n\n- 의문2: for의 출력결과는 어떻게 예측할 수 있을까?\n\n\n\n- 의문1의 해결: 아래의 ??? 자리에 올 수 있는 것은 dir() 하여 __iter__ 가 있는 object이다.\nfor i in ???:\n    print(i)\n- 확인\n\na = [1,2,3] # list\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na = 1,2,3 # tuple\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na = '123' # string\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na=3\nset(dir(a)) & {'__iter__'}\n\nset()\n\n\niterable 하지 않다라는 것은dir()을 쳤을 때 __iter__ 라는 메소드가 없다는 것을 의미\n\n예상대로 예제1~예제4에서는 int클래스의 instance만 __iter__가 없다.\nfor문 뒤 ??? 자리에 올 수 있는 것은 iterable object만 올 수 있다.\n\n- __iter__의 역할: iterable object를 iterator로 만들 수 있다.\n\nlst = [1,2,3]\nlst\n\n[1, 2, 3]\n\n\n\nlst[1] # 충실한 리스트\n\n2\n\n\n\nltor = iter(lst) # 아래와 같은 표현 (a.__str__() = str(a)가 같은 것처럼)\n#ltor = lst.__iter__() # list iterator\nltor\n\n&lt;list_iterator at 0x7f2dfc4c51f0&gt;\n\n\n\nltor[1] # 더이상 리스트가 아니다.\n\nTypeError: 'list_iterator' object is not subscriptable\n\n\n\nltor?\n\n\nType:        list_iterator\nString form: &lt;list_iterator object at 0x7f2dfc4c51f0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n- iterator가 되면 무엇이 좋은가? \\(\\to\\) 숨겨진 기능 __next__가 열린다.\n\nlst\n\n[1, 2, 3]\n\n\n\nset(dir(lst)) & {'__next__'}, set(dir(ltor)) & {'__next__'}\n\n(set(), {'__next__'})\n\n\n\nlst에는 __next__ 가 없지만 ltor에는 있다.\n\n- 그래서 __next__의 기능은? \\(\\to\\) 원소를 차례대로 꺼내준다. + 더 이상 꺼낼 원소가 없으면 Stopiteration Error 발생시킨다.\n\nlst\n\n[1, 2, 3]\n\n\n\nltor.__next__()\n\n1\n\n\n\nltor.__next__()\n\n2\n\n\n\nltor.__next__()\n\n3\n\n\n\nltor.__next__()\n\nStopIteration: \n\n\n- for문의 동작원리\nfor i in lst:\n    print(i)\n\nlst.__iter__() 혹은 iter(lst)를 이용하여 lst를 iterator로 만든다. (iterable object를 iterator object로 만든다.)\niterator에서 .__next__() 함수를 호출하고 결과를 i에 저장한 뒤에 for문 블락안에 있는 내용 (들여쓰기 된 내용)을 실행한다. \\(\\to\\) 반복\nStopIteration 에러가 발생하면 for문을 멈춘다.\n\n- 아래의 ??? 자리에 올 수 있는 것이 iterable object 가 아니라 iterator 자체여도 for문이 돌아갈까? (당연히 돌아가야 할 것 같음)\nfor i in ???:\n    print(i)\n\nfor i in [1,2,3]: # iterable object\n    print(i)\n\n1\n2\n3\n\n\n\n당연히 가능!\n\n- a가 iterator일때 iter(a)의 출력결과가 a와 같도록 조정한다면 for문의 동작원리 (1)-(3)을 수행하지 않아도 좋다. \\(\\to\\) 실제로 이렇게 동작한다.\n- 요약\n\niterable object는 숨겨진 기능으로 __iter__를 가진다.\niterator object는 숨겨진 기능으로 __iter__와 __next__를 가진다. (즉 iterator는 그 자체로 iterable object가 된다!)\n\n\nlst = [1,2,3]\nltor = iter(lst)\n\n\nset(dir(lst)) & {'__iter__','__next__'}\n\n{'__iter__'}\n\n\n\nset(dir(ltor)) & {'__iter__', '__next__'}\n\n{'__iter__', '__next__'}\n\n\n- 의문2의 해결: for의 출력결과는 어떻게 예측할 수 있을까? iterator를 만들어서 .__next__()의 출력값을 확인하면 알 수 있다.\n\nfor i in df:\n    print(i)\n\nX1\nX2\nX3\nX4\n\n\n\ndftor = iter(df)\ndftor?\n\n\nType:        map\nString form: &lt;map object at 0x7f2dfc4c85b0&gt;\nDocstring:  \nmap(func, *iterables) --&gt; map object\nMake an iterator that computes the function using arguments from\neach of the iterables.  Stops when the shortest iterable is exhausted.\n\n\n\n\ndftor.__next__()\n\n'X1'\n\n\n\ndftor.__next__()\n\n'X2'\n\n\n\ndftor.__next__()\n\n'X3'\n\n\n\ndftor.__next__()\n\n'X4'\n\n\n\ndftor.__next__()\n\nStopIteration: \n\n\n\n\n\n- 파이썬에서 for문을 처음 배울 때: range(5)를 써라!\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\nrange(5)가 도대체 무엇이길래? ## iterator 아니면 iterable object 일건데..\n\n\nrange(5)\n\nrange(0, 5)\n\n\n\nrepr(range(5))\n\n'range(0, 5)'\n\n\n- range(5)의 정체는 그냥 iterable object이다.\n\nset(dir(range(5))) & {'__iter__', '__next__'}\n\n{'__iter__'}\n\n\n__next__ 는 갖고있지 않은데 __iter__만 갖고있으니까 range(5)는 iterable object\n- 그래서 언제든지 iterator로 바꿀 수 있다.\n\nrtor = iter(range(5))\n\n\nrtor?\n\n\nType:        range_iterator\nString form: &lt;range_iterator object at 0x7f2dfc4c56c0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\nset(dir(rtor)) & {'__iter__','__next__'}\n\n{'__iter__', '__next__'}\n\n\n- for문에서 range(5)가 행동하는 방법?\n\nrtor.__next__()\n\n0\n\n\n\nrtor.__next__()\n\n1\n\n\n\nrtor.__next__()\n\n2\n\n\n\nrtor.__next__()\n\n3\n\n\n\nrtor.__next__()\n\n4\n\n\n\nrtor.__next__()\n\nStopIteration: \n\n\n\n\n\n- 이터레이터의 개념을 알면 for문에 대한 이해도가 대폭 상승한다.\n\nfor i in zip([1,2,3],'abc'):\n    print(i)\n\n(1, 'a')\n(2, 'b')\n(3, 'c')\n\n\n\nzip은 뭐지???\n\n\nzip([1,2,3],'abc')\n\n&lt;zip at 0x7f2dfc4e8340&gt;\n\n\n- 어차피 for i in ????: 의 ???? 자리는 iterable object(iterator)의 자리이다.\n\nset(dir(zip([1,2,3],'abc'))) & {'__iter__','__next__'}\n\n{'__iter__', '__next__'}\n\n\n\n__next__() 함수가 있음 \\(\\to\\) zip([1,2,3],'abc')는 그자체로 iterator 였다!\n\n\nz = zip([1,2,3],'abc')\n\n\nz.__next__()\n\n(1, 'a')\n\n\n\nz.__next__()\n\n(2, 'b')\n\n\n\nz.__next__()\n\n(3, 'c')\n\n\n\nz.__next__()\n\nStopIteration: \n\n\n\n\n\n- 내가 이터레이터를 만들어보자.\n\nclass Klass: # 찌를 내는 순간 for문이 멈추도록 하는 이터레이터를 만들자.\n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        if action == '찌':\n            print('찌가 나와서 for문을 멈춥니다.')\n            raise StopIteration\n        else:\n            return action\n\n\na = Klass() # 클래스로부터 인스턴스 만들기\n\n\na?\n\n\nType:        Klass\nString form: &lt;__main__.Klass object at 0x7f2dfc373af0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\nset(dir(a)) & {'__iter__', '__next__'} # a는 이터레이터!\n\n{'__iter__', '__next__'}\n\n\n\na.__next__()\n\n'빠'\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n찌가 나와서 for문을 멈춥니다.\n\n\nStopIteration: \n\n\n\nfor i in a:\n    print(i)\n\n빠\n묵\n묵\n빠\n찌가 나와서 for문을 멈춥니다.\n\n\n\n\n\n\n파이썬의 비밀1: 자료형은 클래스의 이름이다.\n파이썬의 비밀2: 클래스에는 __str__ 처럼 숨겨진 매서드가 존재한다. 이를 이용하여 파이썬 내부의 기능을 가로챌 수 있다.\n파이썬의 비밀3: 주피터노트북(대화형 콘솔)에서는 “오브젝트이름 + 엔터”를 쳐서 나오는 출력은 __repr__로 가로챌 수 있다. (주피터의 비밀)\n파이썬의 비밀4: 함수와 클래스는 숨겨진 메소드에 __call__을 가진 오브젝트일 뿐이다.\n파이썬의 비밀5: for문의 비밀(iterable object, iterator, StopIteration Error)"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class08.html#import",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class08.html#import",
    "title": "[IP2022] class 08단계",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class08.html#for문의-복습",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class08.html#for문의-복습",
    "title": "[IP2022] class 08단계",
    "section": "",
    "text": "- 아래와 같은 예제들을 관찰하여 for문을 복습하자.\n(예제1)\n\nfor i in [1,2,3,4]:\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제2)\n\nfor i in (1,2,3,4):\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제3)\n\nfor i in '1234':\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제4)\n\na=5\nfor i in a:\n    print(i)\n\nTypeError: 'int' object is not iterable\n\n\n\n5라고 출력되어야 하지 않나?\n\n- 의문1:\nfor i in ???:\n    print(i)\n에서 ???자리에 올 수 있는 것이 무엇일까?\n(예제5)\n상황1\n\nlst = [[1,2,3,4],[3,4,5,6]]\nlst\n\n[[1, 2, 3, 4], [3, 4, 5, 6]]\n\n\n\nfor l in lst:\n    print(l)\n\n[1, 2, 3, 4]\n[3, 4, 5, 6]\n\n\n상황2\n\ndf = pd.DataFrame(lst)\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nfor i in df:\n    print(i)\n\n0\n1\n2\n3\n\n\n칼럼이름들이 나오는 것 같음 \\(\\to\\) 확인해보자.\n\ndf.columns = pd.Index(['X'+str(i) for i in range(1,5)])\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nfor i in df:\n    print(i)\n\nX1\nX2\nX3\nX4\n\n\n- 의문2: for의 출력결과는 어떻게 예측할 수 있을까?"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class08.html#for문의-동작원리",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class08.html#for문의-동작원리",
    "title": "[IP2022] class 08단계",
    "section": "",
    "text": "- 의문1의 해결: 아래의 ??? 자리에 올 수 있는 것은 dir() 하여 __iter__ 가 있는 object이다.\nfor i in ???:\n    print(i)\n- 확인\n\na = [1,2,3] # list\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na = 1,2,3 # tuple\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na = '123' # string\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na=3\nset(dir(a)) & {'__iter__'}\n\nset()\n\n\niterable 하지 않다라는 것은dir()을 쳤을 때 __iter__ 라는 메소드가 없다는 것을 의미\n\n예상대로 예제1~예제4에서는 int클래스의 instance만 __iter__가 없다.\nfor문 뒤 ??? 자리에 올 수 있는 것은 iterable object만 올 수 있다.\n\n- __iter__의 역할: iterable object를 iterator로 만들 수 있다.\n\nlst = [1,2,3]\nlst\n\n[1, 2, 3]\n\n\n\nlst[1] # 충실한 리스트\n\n2\n\n\n\nltor = iter(lst) # 아래와 같은 표현 (a.__str__() = str(a)가 같은 것처럼)\n#ltor = lst.__iter__() # list iterator\nltor\n\n&lt;list_iterator at 0x7f2dfc4c51f0&gt;\n\n\n\nltor[1] # 더이상 리스트가 아니다.\n\nTypeError: 'list_iterator' object is not subscriptable\n\n\n\nltor?\n\n\nType:        list_iterator\nString form: &lt;list_iterator object at 0x7f2dfc4c51f0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n- iterator가 되면 무엇이 좋은가? \\(\\to\\) 숨겨진 기능 __next__가 열린다.\n\nlst\n\n[1, 2, 3]\n\n\n\nset(dir(lst)) & {'__next__'}, set(dir(ltor)) & {'__next__'}\n\n(set(), {'__next__'})\n\n\n\nlst에는 __next__ 가 없지만 ltor에는 있다.\n\n- 그래서 __next__의 기능은? \\(\\to\\) 원소를 차례대로 꺼내준다. + 더 이상 꺼낼 원소가 없으면 Stopiteration Error 발생시킨다.\n\nlst\n\n[1, 2, 3]\n\n\n\nltor.__next__()\n\n1\n\n\n\nltor.__next__()\n\n2\n\n\n\nltor.__next__()\n\n3\n\n\n\nltor.__next__()\n\nStopIteration: \n\n\n- for문의 동작원리\nfor i in lst:\n    print(i)\n\nlst.__iter__() 혹은 iter(lst)를 이용하여 lst를 iterator로 만든다. (iterable object를 iterator object로 만든다.)\niterator에서 .__next__() 함수를 호출하고 결과를 i에 저장한 뒤에 for문 블락안에 있는 내용 (들여쓰기 된 내용)을 실행한다. \\(\\to\\) 반복\nStopIteration 에러가 발생하면 for문을 멈춘다.\n\n- 아래의 ??? 자리에 올 수 있는 것이 iterable object 가 아니라 iterator 자체여도 for문이 돌아갈까? (당연히 돌아가야 할 것 같음)\nfor i in ???:\n    print(i)\n\nfor i in [1,2,3]: # iterable object\n    print(i)\n\n1\n2\n3\n\n\n\n당연히 가능!\n\n- a가 iterator일때 iter(a)의 출력결과가 a와 같도록 조정한다면 for문의 동작원리 (1)-(3)을 수행하지 않아도 좋다. \\(\\to\\) 실제로 이렇게 동작한다.\n- 요약\n\niterable object는 숨겨진 기능으로 __iter__를 가진다.\niterator object는 숨겨진 기능으로 __iter__와 __next__를 가진다. (즉 iterator는 그 자체로 iterable object가 된다!)\n\n\nlst = [1,2,3]\nltor = iter(lst)\n\n\nset(dir(lst)) & {'__iter__','__next__'}\n\n{'__iter__'}\n\n\n\nset(dir(ltor)) & {'__iter__', '__next__'}\n\n{'__iter__', '__next__'}\n\n\n- 의문2의 해결: for의 출력결과는 어떻게 예측할 수 있을까? iterator를 만들어서 .__next__()의 출력값을 확인하면 알 수 있다.\n\nfor i in df:\n    print(i)\n\nX1\nX2\nX3\nX4\n\n\n\ndftor = iter(df)\ndftor?\n\n\nType:        map\nString form: &lt;map object at 0x7f2dfc4c85b0&gt;\nDocstring:  \nmap(func, *iterables) --&gt; map object\nMake an iterator that computes the function using arguments from\neach of the iterables.  Stops when the shortest iterable is exhausted.\n\n\n\n\ndftor.__next__()\n\n'X1'\n\n\n\ndftor.__next__()\n\n'X2'\n\n\n\ndftor.__next__()\n\n'X3'\n\n\n\ndftor.__next__()\n\n'X4'\n\n\n\ndftor.__next__()\n\nStopIteration:"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class08.html#range",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class08.html#range",
    "title": "[IP2022] class 08단계",
    "section": "",
    "text": "- 파이썬에서 for문을 처음 배울 때: range(5)를 써라!\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\nrange(5)가 도대체 무엇이길래? ## iterator 아니면 iterable object 일건데..\n\n\nrange(5)\n\nrange(0, 5)\n\n\n\nrepr(range(5))\n\n'range(0, 5)'\n\n\n- range(5)의 정체는 그냥 iterable object이다.\n\nset(dir(range(5))) & {'__iter__', '__next__'}\n\n{'__iter__'}\n\n\n__next__ 는 갖고있지 않은데 __iter__만 갖고있으니까 range(5)는 iterable object\n- 그래서 언제든지 iterator로 바꿀 수 있다.\n\nrtor = iter(range(5))\n\n\nrtor?\n\n\nType:        range_iterator\nString form: &lt;range_iterator object at 0x7f2dfc4c56c0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\nset(dir(rtor)) & {'__iter__','__next__'}\n\n{'__iter__', '__next__'}\n\n\n- for문에서 range(5)가 행동하는 방법?\n\nrtor.__next__()\n\n0\n\n\n\nrtor.__next__()\n\n1\n\n\n\nrtor.__next__()\n\n2\n\n\n\nrtor.__next__()\n\n3\n\n\n\nrtor.__next__()\n\n4\n\n\n\nrtor.__next__()\n\nStopIteration:"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class08.html#zip",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class08.html#zip",
    "title": "[IP2022] class 08단계",
    "section": "",
    "text": "- 이터레이터의 개념을 알면 for문에 대한 이해도가 대폭 상승한다.\n\nfor i in zip([1,2,3],'abc'):\n    print(i)\n\n(1, 'a')\n(2, 'b')\n(3, 'c')\n\n\n\nzip은 뭐지???\n\n\nzip([1,2,3],'abc')\n\n&lt;zip at 0x7f2dfc4e8340&gt;\n\n\n- 어차피 for i in ????: 의 ???? 자리는 iterable object(iterator)의 자리이다.\n\nset(dir(zip([1,2,3],'abc'))) & {'__iter__','__next__'}\n\n{'__iter__', '__next__'}\n\n\n\n__next__() 함수가 있음 \\(\\to\\) zip([1,2,3],'abc')는 그자체로 iterator 였다!\n\n\nz = zip([1,2,3],'abc')\n\n\nz.__next__()\n\n(1, 'a')\n\n\n\nz.__next__()\n\n(2, 'b')\n\n\n\nz.__next__()\n\n(3, 'c')\n\n\n\nz.__next__()\n\nStopIteration:"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class08.html#사용자정의-이터레이터",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class08.html#사용자정의-이터레이터",
    "title": "[IP2022] class 08단계",
    "section": "",
    "text": "- 내가 이터레이터를 만들어보자.\n\nclass Klass: # 찌를 내는 순간 for문이 멈추도록 하는 이터레이터를 만들자.\n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        if action == '찌':\n            print('찌가 나와서 for문을 멈춥니다.')\n            raise StopIteration\n        else:\n            return action\n\n\na = Klass() # 클래스로부터 인스턴스 만들기\n\n\na?\n\n\nType:        Klass\nString form: &lt;__main__.Klass object at 0x7f2dfc373af0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\nset(dir(a)) & {'__iter__', '__next__'} # a는 이터레이터!\n\n{'__iter__', '__next__'}\n\n\n\na.__next__()\n\n'빠'\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n찌가 나와서 for문을 멈춥니다.\n\n\nStopIteration: \n\n\n\nfor i in a:\n    print(i)\n\n빠\n묵\n묵\n빠\n찌가 나와서 for문을 멈춥니다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class08.html#파이썬의-비밀-15",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class08.html#파이썬의-비밀-15",
    "title": "[IP2022] class 08단계",
    "section": "",
    "text": "파이썬의 비밀1: 자료형은 클래스의 이름이다.\n파이썬의 비밀2: 클래스에는 __str__ 처럼 숨겨진 매서드가 존재한다. 이를 이용하여 파이썬 내부의 기능을 가로챌 수 있다.\n파이썬의 비밀3: 주피터노트북(대화형 콘솔)에서는 “오브젝트이름 + 엔터”를 쳐서 나오는 출력은 __repr__로 가로챌 수 있다. (주피터의 비밀)\n파이썬의 비밀4: 함수와 클래스는 숨겨진 메소드에 __call__을 가진 오브젝트일 뿐이다.\n파이썬의 비밀5: for문의 비밀(iterable object, iterator, StopIteration Error)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "",
    "text": "모르고 살았다면 더 좋았을 내용"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-비상식적인-append",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-비상식적인-append",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제1: 비상식적인 append",
    "text": "예제1: 비상식적인 append\n\n포인트: 이상한 일의 관찰\n\n- 원소의 추가: + 이용\n\na=[1,2,3]\nb=a\na=a+[4]\n\n\na\n\n[1, 2, 3, 4]\n\n\n\nb\n\n[1, 2, 3]\n\n\n- 원소의 추가 .append 이용\n\na=[1,2,3]\nb=a\na.append(4) # a=a+[4]\n\n\na\n\n[1, 2, 3, 4]\n\n\n\nb\n\n[1, 2, 3, 4]"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#append의-동작원리-틀린상상",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#append의-동작원리-틀린상상",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "append의 동작원리: 틀린상상",
    "text": "append의 동작원리: 틀린상상\n- 상자로서의 변수: 변수가 데이터를 저장하는 일종의 상자와 같다. &lt;– 아주 흔한 오해 (Fluent Python)\n\n흔히 비유하는 ‘상자로서의 변수’ 개념이 실제로는 객체지향적 언어에서 참조변수를 이해하는 데 방해가 된다.\n\n- “상자로서의 변수” 관점에서 아래의 코드를 해석하자. (일단 아래의 해석들이 틀린해석이라는 사실을 명심할 것)\na=[1,2,3]\nb=a\na.append(4)\na,b라는 변수들은 메모리에 어떻게 저장이 되어있을까?\n상상력을 조금 발휘하면 아래와 같이 여길 수 있다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\na=[1,2,3]\n\n메모리주소1에 존재하는 방을 a라고 하고, 그 방에 [1,2,3]을 넣는다.\n\n\n아래를 실행하였을 경우\n\nb=a\n\n메모리주소2에 존재하는 방을 b라고 하고, 그 방에 a를 넣어야하는데, a는 [1,2,3]이니까 [1,2,3]을 넣는다.\n\n\n아래를 실행하면\n\na.append(4)\n\n방 a로가서 [1,2,3]을 [1,2,3,4]로 바꾼다.\n그리고 방 b에는 아무것도 하지 않는다.\n\n- R에서는 맞는 비유인데, 파이썬은 적절하지 않은 비유이다.\n\n틀린이유\n\nid(a)\n\n139765704687936\n\n\n\nid(b)\n\n139765704687936"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#append의-동작원리-올바른-상상",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#append의-동작원리-올바른-상상",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "append의 동작원리: 올바른 상상",
    "text": "append의 동작원리: 올바른 상상\n\n파이썬에서의 변수는 자바에서의 참조변수와 같으므로 변수는 객체에 붙은 레이블이라고 생각하는 것이 좋다.\n\n- 파이썬에서는 아래가 더 적절한 비유이다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\na=[1,2,3]\n\n메모리주소 139753545242336에서 [1,2,3]을 생성\n방 139753545242336의 방문에 a라는 포스트잇을 붙인다.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방중에서 a라는 포스트잇이 붙은 방을 찾아가면 된다.\n\n\n아래를 실행하였을 경우\n\nb=a\n\na라는 포스트잇이 지칭하는 객체를 가져옴. 그리고 그 객체에 b라는 포스트잇을 붙인다.\n쉽게말하면 b라는 포스트잇을 방 139753545242336의 방문에 붙인다는 이야기.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방중에서 a라는 포스트잇이 붙어 있거나 b라는 포스트잇이 붙어있는 방을 찾아가면 된다.\n\n\n아래를 실행하면\n\na.append(4)\n\na라는 포스트잇이 붙어있는 방으로 가서, 그 내용물에 append함수를 적용하여 4를 추가하라. 즉 내용물 [1,2,3]을 [1,2,3,4]로 바꾸라.\n같은방(139753545242336)에 a,b라는 포스트잇이 모두 붙어있음. 따라서 b라는 포스트잇이 붙은 방을 찾아가서 내용물을 열어보면 [1,2,3,4]가 나온다."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-같은-value-다른-id",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-같은-value-다른-id",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제1: 같은 value, 다른 id",
    "text": "예제1: 같은 value, 다른 id\n\n포인트: (1) 포스트잇 개념의 확실한 이해 (2) 할당문을 새로운 시각으로 해석하는 연습 (3) “생성-&gt;할당”과 “참조/에일리어싱”의 구분\n\n\na=[1,2,3] # 우변: 생성된 오브젝트, 좌변: 이름 \nb=a # 우변: 가져온 오브젝트, 좌변: 별명 --&gt; 참조, 에일리어싱(별칭부여)이라고 한다\na.append(4) # a라는 오브젝트를 직접변경\nc=[1,2,3,4] # 우변: 생성된 오브젝트, 좌변: 이름\n\n여기에서 a,b,c는 모두 같은 value를 가진다.\n\na\n\n[1, 2, 3, 4]\n\n\n\nb\n\n[1, 2, 3, 4]\n\n\n\nc\n\n[1, 2, 3, 4]\n\n\n하지만 그 id까지 같은 것은 아니다.\n\nid(a), id(b), id(c)\n\n(140237129249664, 140237129249664, 140237128836544)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제2",
    "text": "예제2\n\n선행지식: “생성-&gt;할당” 과 “참조/에일리어싱”의 구분\n\n\n포인트: 재할당의 이해!!\n\n(관찰)\n\na=[1,2,3] # 생성-&gt;할당\nb=a # 참조/에일리어싱 \na=a+[4] # 생성-&gt;재할당 \nprint('a=',a)\nprint('b=',b)\n\na= [1, 2, 3, 4]\nb= [1, 2, 3]\n\n\n(해설)\n\nid(a),id(b)\n\n(140237129283584, 140237129350912)\n\n\n\n포인트: [1,2,3]+[4] 가 실행되는 순간 새로운 오브젝트가 만들어지고 그 오브젝트를 a라는 이름으로 다시 할당되었음. (재할당)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제1",
    "text": "예제1\n\n선행지식: “생성-&gt;할당,재할당” 과 “참조/에일리어싱”의 구분\n\n\n포인트: 인터닝을 위한 떡밥예제\n\n\na=1+2021\nid(a)\n\n139753546122608\n\n\n\nb=2023-1\nid(b)\n\n139753545299280\n\n\n\nid(2022)\n\n139753545299472\n\n\n\n당연한결과임."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-이제-다-이해했다고-생각했는데..",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-이제-다-이해했다고-생각했는데..",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제2: 이제 다 이해했다고 생각했는데..",
    "text": "예제2: 이제 다 이해했다고 생각했는데..\n\n선행지식: “생성-&gt;할당,재할당” 과 “참조/에일리어싱”의 구분\n\n\n포인트: 인터닝의 이해\n\n\na=1+2 \nid(a)\n\n7394720\n\n\n\nb=4-1\nid(b)\n\n7394720\n\n\n\nid(a)와 id(b)가 왜 똑같지..?\n\n(해설) 파이썬의 경우 효율성을 위해서 -5~256까지의 정수를 미리 저장해둠.\n\nid(3)\n\n7394720\n\n\n\n3은 언제나 7394720에 지박령마냥 밖혀있음"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-1",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제1",
    "text": "예제1\n(관찰) 아래의 예제를 살펴보자. 참조를 제대로 이해했다면 아래의 예제는 자연스럽게 이해가능할 것임.\n\nl1 = [3, [66,55,44]]\nl2 = l1 \nprint('시점1')\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1[0]=4 \nprint('시점2')\nprint('l1=',l1)\nprint('l2=',l2)\n\nl2.append(5)\nprint('시점3')\nprint('l1=',l1)\nprint('l2=',l2)\n\n시점1\nl1= [3, [66, 55, 44]]\nl2= [3, [66, 55, 44]]\n시점2\nl1= [4, [66, 55, 44]]\nl2= [4, [66, 55, 44]]\n시점3\nl1= [4, [66, 55, 44], 5]\nl2= [4, [66, 55, 44], 5]\n\n\n(해설)\n\nl1 = [3, [66,55,44]]\nl2 = l1 \n\n\nid(l1),id(l2)\n\n(140571068242688, 140571068242688)\n\n\n이해는 되지만 우리가 원한건 이런게 아니야"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-r과-같이-를-쓰고-싶다면",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-r과-같이-를-쓰고-싶다면",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제2: R과 같이 = 를 쓰고 싶다면?",
    "text": "예제2: R과 같이 = 를 쓰고 싶다면?\n\n선행지식: “생성-&gt;할당,재할당” 과 “참조/에일리어싱”의 구분\n\n\n포인트: 복사의 사용, 얕은복사의 떡밥\n\n(관찰)\n\nl1 = [3, [66,55,44]]\nl2 = l1.copy()\nprint('시점1')\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1[0]=4 \nprint('시점2')\nprint('l1=',l1)\nprint('l2=',l2)\n\nl2.append(5)\nprint('시점3')\nprint('l1=',l1)\nprint('l2=',l2)\n\n시점1\nl1= [3, [66, 55, 44]]\nl2= [3, [66, 55, 44]]\n시점2\nl1= [4, [66, 55, 44]]\nl2= [3, [66, 55, 44]]\n시점3\nl1= [4, [66, 55, 44]]\nl2= [3, [66, 55, 44], 5]\n\n\n(해설)\n\nl1 = [3, [66,55,44]]\nl2 = l1.copy()\n\n\nid(l1),id(l2) ## 드디어 주소가 달라졌다.\n\n(140571068242688, 140571068242240)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3-이제-다-이해했다고-생각했는데..",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3-이제-다-이해했다고-생각했는데..",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제3: 이제 다 이해했다고 생각했는데..",
    "text": "예제3: 이제 다 이해했다고 생각했는데..\n\n선행지식: “생성-&gt;할당,재할당” 과 “참조/에일리어싱”의 구분, 복사의 사용\n\n\n포인트: 얕은복사를 이해하지 못할때 생기는 개념충돌\n\n(관찰)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\nl1[1].append(33)\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44, 33]]\n\n\n(의문)\n\nid(l1),id(l2)\n\n(140571077644352, 140571068253376)\n\n\n\nl1이랑 l2의 주소도 다르게 나오는데 왜 또 참조한것마냥 l1과 l2가 같이 바뀌고 있지?\n\n나는 진정한 복사(=깊은복사)를 하고싶다"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-2",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-2",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제1",
    "text": "예제1\n\n선행지식: 이전까지 모든것\n\n\n포인트: 0차원 자료형의 메모리 구조 이해, 1차원 자료형의 메모리 구조를 위한 떡밥\n\n(관찰+해설)\n\na=2222\nb=2222\n\n\nid(a),id(b)\n\n(139753545300880, 139753545301808)\n\n\n메모리 상황\n\n2222라는 오브젝트가 어떤공간(139753545300880)에 생성되고 그 공간에 a라는 라벨이 붙음\n2222라는 오브젝트가 어떤공간(139753545301808)에 생성되고 그 공간에 b라는 라벨이 붙음\n\n즉 -5~256 이외의 2개의 메모리 공간을 추가적으로 사용"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-1",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제2",
    "text": "예제2\n\n선행지식: 이전까지 모든것, 0차원 자료형의 메모리저장상태 이해\n\n\n포인트: (1) 1차원 자료형의 메모리 구조 이해 (2) 가변형객체라는 표현의 의미\n\n(관찰)\n\na=[1,2,2222]\nb=[1,2,2222]\na.append(4)\nprint('a=',a)\nprint('b=',b)\n\na= [1, 2, 2222, 4]\nb= [1, 2, 2222]\n\n\n(해설)\n\na=[1,2,2222]\nb=[1,2,2222]\n\n\nid(a), [id(a[0]),id(a[1]),id(a[2])] # a=[1,2,2222]\n\n(140527746917824, [7585472, 7585504, 140528016796752])\n\n\n\nid(b), [id(b[0]),id(b[1]),id(b[2])] # b=[1,2,2222] \n\n(140527746917568, [7585472, 7585504, 140528016796144])\n\n\n\na.append(4)\n\n\na\n\n[1, 2, 2222, 4]\n\n\n\nb\n\n[1, 2, 2222]\n\n\n\nid(a)\n\n140527746917824\n\n\n메모리상황\n\n-5~256까지의 숫자는 미리 메모리에 저장되어 있다. 이중에서 1은 7394656, 2는 7394688에 저장되어있음.\n2222가 공간 139753178093776에서 만들어진다.\n어떠한 리스트오브젝트가 공간 139753182327904에서 만들어지고 원소로 [1,2,2222]를 가진다. 이 공간에 a라는 포스트잇을 붙인다.\n2222가 공간 139753178095568에서 만들어진다.\n어떠한 리스트오브젝트가 공간 139753173818656에서 만들어지고 원소로 [1,2,2222]를 가진다. 이 공간에 b라는 포스트잇을 붙인다.\na라는 포스트잇이 붙은 공간으로 이동하여 원소에 4를 추가시킨다.\n\n즉 -5~256이외에 4개의 메모리 공간을 추가사용 (a,b,a의 2222,b의 2222)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제3",
    "text": "예제3\n\n선행지식: 이전까지 모든 것\n\n\n포인트: l2=l1 와 l2=l1.copy() 의 차이점\n\n(관찰)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\nl1[0] = 7777\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1= [7777, [66, 55, 44]]\nl2= [3, [66, 55, 44]]\n\n\n(해설)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139753183437040, [7394720, 139753183707216])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139753182311120, [7394720, 139753183707216])\n\n\n메모리상황\n\n-5~256까지의 숫자가 메모리에 저장되어 있다.\n저장된 숫자중 66,55,44를 묶어서 리스트로 구성하고 이 리스트를 공간 139753183707216에 저장.\n숫자 3과 공간 139753183707216에 저장된 리스트 [66,55,44]를 하나로 묶어서 새로운 리스트를 구성하고 이를 공간 139753183437040에 저장. 공간 139753183437040에 l1이라는 포스트잇 생성.\n공간 139753182311120에 l1의 원소들을 모아서 새로운 리스트를 구성함. 공간 139753182311120에 l2라는 포스트잇 생성. 그런데 따져보니까 내부구성은 똑같아?\n\n\nl1[0] = 7777\nl1,l2\n\n([7777, [66, 55, 44]], [3, [66, 55, 44]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139753183437040, [139753178092080, 139753183707216])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139753182311120, [7394720, 139753183707216])\n\n\n\nl1[0]은 원래 공간 7394720와 binding 되어 있었음.\n\n그런데 7777이라는 새로운 오브젝트가 공간 139753178092080에 생성되고 l1[0]이 공간 139753178092080와 다시 binding 됨."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제4",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제4",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제4",
    "text": "예제4\n\n선행지식: 이전까지 모든것, .copy()의 동작원리\n\n\n포인트: .copy()의 동작원리 재학습\n\n(관찰)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\nl1.append(7777)\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1= [3, [66, 55, 44], 7777]\nl2= [3, [66, 55, 44]]\n\n\n(해설)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\nl1.append(7777)\n\n\nl1,l2\n\n([3, [66, 55, 44], 7777], [3, [66, 55, 44]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1]), id(l1[2])]\n\n(139753183257056, [7394720, 139753184484240, 139753180268560])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139753183216656, [7394720, 139753184484240])"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제5-우리를-힘들게-했던-그-예제.",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제5-우리를-힘들게-했던-그-예제.",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제5: 우리를 힘들게 했던 그 예제.",
    "text": "예제5: 우리를 힘들게 했던 그 예제.\n\n선행지식: 이전까지 모든것, .copy()의 동작원리\n\n\n포인트: (1) .copy()의 한계, (2) 얕은복사라는 명칭의 유래\n\n(관찰)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\nl1[1].append(7777)\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1= [3, [66, 55, 44, 7777]]\nl2= [3, [66, 55, 44, 7777]]\n\n\n(해설-시점1)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\n\n\nl1,l2\n\n([3, [66, 55, 44]], [3, [66, 55, 44]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139753181411920, [7394720, 139753181409920])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139753181409440, [7394720, 139753181409920])\n\n\n(해설-시점2)\n\nl1[1].append(7777)\n\n\nl1,l2\n\n([3, [66, 55, 44, 7777]], [3, [66, 55, 44, 7777]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139753181411920, [7394720, 139753181409920])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139753181409440, [7394720, 139753181409920])\n\n\n해설: 사실 시점1에서 메모리 주소상황을 잘 이해했다면 신기한 일이 아니다. .copy()는 l1과 l2의 주소만 다르게 만들 뿐 내용물인 l1[0],l1[1]는 동일하니까."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제6-신임교수최규빈이영미",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제6-신임교수최규빈이영미",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제6: 신임교수=[‘최규빈’,‘이영미’]",
    "text": "예제6: 신임교수=[‘최규빈’,‘이영미’]\n\n선행지식: 이전까지의 모든것\n\n\n포인트: 이전까지의 모든것 점검\n\n- 최규빈, 이영미는 신임교수임\n\n신임교수 = ['최규빈','이영미']\n\n\nid(신임교수), id('최규빈'), id('이영미')\n\n(139753182527808, 139753171447312, 139753171447408)\n\n\n- 신임교수를 누군가는 막내들이라고 부르기도 함.\n\n막내들 = 신임교수 \n\n\nid(막내들), id(신임교수)\n\n(139753182527808, 139753182527808)\n\n\n“막내들”이라는 단어와 “신임교수”라는 단어는 사실 같은 말임\n- 새로운 교수 “박혜원”이 뽑혔음.\n\n신임교수.append(\"박혜원\")\n\n\n신임교수, 막내들\n\n(['최규빈', '이영미', '박혜원'], ['최규빈', '이영미', '박혜원'])\n\n\n- 전북대 통계학과에서 R특강팀을 구성하여 방학중 R교육을 실시하고자함. 특강팀은 우선 신임교수들로 구성.\n\nR특강팀 = 신임교수.copy()\nR특강팀 \n\n['최규빈', '이영미', '박혜원']\n\n\n- R특강팀에 최혜미교수님 추가. (그렇지만 최혜미교수님이 막내는 아니야.. // 참조와 shallow copy의 차이점)\n\nR특강팀.append(\"최혜미\") \n\n\nR특강팀, 신임교수, 막내들\n\n(['최규빈', '이영미', '박혜원', '최혜미'], ['최규빈', '이영미', '박혜원'], ['최규빈', '이영미', '박혜원'])\n\n\n- R특강팀에서 양성준 교수를 추가하여 파이썬 특강팀을 구성 (R특강팀의 구분을 위해서 중첩리스트 구조로 만들자)\n\n파이썬특강팀 = [R특강팀, \"양성준\"]\n파이썬특강팀\n\n[['최규빈', '이영미', '박혜원', '최혜미'], '양성준']\n\n\n- 이영미교수는 다른 일이 많아서 R특강 팀에서 제외됨. (그럼 자연히 파이썬에서도 제외됨!!)\n\nR특강팀.remove(\"이영미\")\n\n\nR특강팀, 파이썬특강팀\n\n(['최규빈', '박혜원', '최혜미'], [['최규빈', '박혜원', '최혜미'], '양성준'])\n\n\n하지만 이영미교수는 여전히 신임교수이면서 막내들임\n\n신임교수, 막내들\n\n(['최규빈', '이영미', '박혜원'], ['최규빈', '이영미', '박혜원'])\n\n\n- 새로운 교수로 “손흥민”이 임용됨.\n\n막내들.append(\"손흥민\")\n\n\n막내들, 신임교수\n\n(['최규빈', '이영미', '박혜원', '손흥민'], ['최규빈', '이영미', '박혜원', '손흥민'])\n\n\n- 그렇다고 해서 손흥민 교수가 바로 R이나 파이썬 특강팀에 자동소속되는건 아님\n\nR특강팀, 파이썬특강팀\n\n(['최규빈', '박혜원', '최혜미'], [['최규빈', '박혜원', '최혜미'], '양성준'])"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-motivation-example",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-motivation-example",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제1: Motivation example",
    "text": "예제1: Motivation example\n- 아래의 상황을 다시 생각해보자.\n\n파이썬특강팀 = [\"양성준\",[\"최규빈\",\"이영미\",\"최혜미\"]]\nADSP특강팀 = 파이썬특강팀.copy()\n파이썬특강팀[-1].remove(\"이영미\")\n\n\n파이썬특강팀, ADSP특강팀\n\n(['양성준', ['최규빈', '최혜미']], ['양성준', ['최규빈', '최혜미']])\n\n\n이슈: 이영미교수가 파이썬특강에서 제외되면서 ADSP특강팀에서도 제외되었음. 그런데 사실 이영미교수가 파이썬특강팀에서만 제외되길 원한 것이지 ADSP특강팀에서 제외되길 원한게 아닐수도 있음.\n해결: Deep copy의 사용\n\nimport copy\n\n\n파이썬특강팀 = [\"양성준\",[\"최규빈\",\"이영미\",\"최혜미\"]]\nADSP특강팀 = copy.deepcopy(파이썬특강팀)\n파이썬특강팀[-1].remove(\"이영미\")\n\n\n파이썬특강팀, ADSP특강팀\n\n(['양성준', ['최규빈', '최혜미']], ['양성준', ['최규빈', '이영미', '최혜미']])"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-2",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-2",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제2",
    "text": "예제2\n\n선행지식: 이전까지 모든것, 얕은복사\n\n\n포인트: (1) 깊은복사 (2) 복사의 레벨을 이해 (3) 얕은복사 = 1단계 깊은복사\n\n- deepcopy\n\nl1 = [3,[66,[55,44]]] \nl2 = copy.deepcopy(l1)\n\n\nl2[1][1].append(33)\n\n\nl1,l2\n\n([3, [66, [55, 44]]], [3, [66, [55, 44, 33]]])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 140137133270656\nl2: 140137132727232\n\n\n\n레벨1: l1,l2 의 메모리 주소가 다름을 확인\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 140137133270656 [7585536, 140137133267712]\nl2: 140137132727232 [7585536, 140137133267456]\n\n\n\n레벨2: l1안에 있는 [66,[55,44]]와 l2안에 있는 [66,[55,44]]의 메모리 주소가 다름도 확인.\n\n\nprint('level 3')\nprint('l1:', id(l1), [id(l1[0]),[id(l1[1][0]),id(l1[1][1])]])\nprint('l2:', id(l2), [id(l2[0]),[id(l2[1][0]),id(l2[1][1])]])\n\nlevel 3\nl1: 140137133270656 [7585536, [7587552, 140137133704320]]\nl2: 140137132727232 [7585536, [7587552, 140137137410624]]\n\n\n\n레벨3: l1안의 [66,[55,44]] 안의 [55,44]와 l2안의 [66,[55,44]] 안의 [55,44]의 메모리 주소까지도 다름을 확인.\n\n- 비교를 위한 shallow copy\n\nl1 = [3,[66,[55,44]]] \nl2 = l1.copy()\n\n\nl2[1][1].append(33)\n\n\nl1,l2\n\n([3, [66, [55, 44, 33]]], [3, [66, [55, 44, 33]]])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 140137133470528\nl2: 140137137411136\n\n\n\n레벨1: l1,l2 의 메모리 주소가 다름을 확인\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 140137133470528 [7585536, 140137133703424]\nl2: 140137137411136 [7585536, 140137133703424]\n\n\n\n레벨2: l1안에 있는 [66,[55,44]]와 l2안에 있는 [66,[55,44]]의 메모리 주소는 같음!!\n\n\nprint('level 3')\nprint('l1:', id(l1), [id(l1[0]),[id(l1[1][0]),id(l1[1][1])]])\nprint('l2:', id(l2), [id(l2[0]),[id(l2[1][0]),id(l2[1][1])]])\n\nlevel 3\nl1: 140137133470528 [7585536, [7587552, 140137137410880]]\nl2: 140137137411136 [7585536, [7587552, 140137137410880]]\n\n\n\n레벨3: l1안의 [66,[55,44]] 안의 [55,44]와 l2안의 [66,[55,44]] 안의 [55,44]의 메모리 주소도 같음!!\n\n- 비교를 위한 참조\n\nl1 = [3,[66,[55,44]]] \nl2 = l1\n\n\nl2[1][1].append(33)\n\n\nl1,l2\n\n([3, [66, [55, 44, 33]]], [3, [66, [55, 44, 33]]])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 140137133223232\nl2: 140137133223232\n\n\n\n레벨1: l1,l2 여기서부터 메모리 주소가 같다.\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 140137133223232 [7585536, 140137133698560]\nl2: 140137133223232 [7585536, 140137133698560]\n\n\n\nprint('level 3')\nprint('l1:', id(l1), [id(l1[0]),[id(l1[1][0]),id(l1[1][1])]])\nprint('l2:', id(l2), [id(l2[0]),[id(l2[1][0]),id(l2[1][1])]])\n\nlevel 3\nl1: 140137133223232 [7585536, [7587552, 140137133438144]]\nl2: 140137133223232 [7585536, [7587552, 140137133438144]]\n\n\n\nNote: 문헌에 따라서 shallow copy를 level1 deep copy라고 부르기도 한다."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-3",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-3",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제1",
    "text": "예제1\n\n선행지식: 이전까지 모든것\n\n\n포인트: 얕은복사의 한계점 이해\n\n- 아래의 코드결과를 예측하라. 결과가 나오는 이유를 설명하라.\n\nl1= [3,[66,55,44]]\nl2= l1.copy() \nl1[-1].append(33)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44, 33]]\n\n\n\n포인트: shallow copy (=level 1 deep copy) 이므로 l1안의 [66,55,44]와 l2안의 [66,55,44]는 같은 메모리 주소를 가짐"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-3",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-3",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제2",
    "text": "예제2\n\n선행지식: 이전까지 모든것\n\n\n포인트: 재할당의 활용하여 얕은복사의 한계점 극복\n\n- 아래의 코드결과를 예측하라. 결과가 나오는 이유를 설명하라.\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33] \n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44]]\n\n\n\n포인트: l1[-1]+[33]가 실행되는 순간 새로운 오브젝트가 생성되고 이 새로운 오브젝트가 l1의 마지막 원소에 새롭게 할당된다."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3-1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3-1",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제3",
    "text": "예제3\n\n선행지식: 이전까지 모든것\n\n\n포인트: 재할당의 활용하여 얕은복사의 한계점 극복, 예제4를 위한 떡밥\n\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33] \nl1[-1].remove(33)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44]]\nl2= [3, [66, 55, 44]]\n\n\n\n포인트: 이 상황에서 l1안의 [66,55,44]와 l2안의 [66,55,44]는 서로 다른 메모리 주소를 가진다."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제4-1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제4-1",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제4",
    "text": "예제4\n\n선행지식: 이전까지 모든것\n\n\n포인트: 재할당으로 인해 메모리주소가 틀어짐을 이용한 트릭예제, 예제5의 떡밥예제\n\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33]\nl1[-1].remove(33) \nl1[-1].append(33)\n\n(잘못된 상상) 아래의 코드와 결과가 같을거야!!\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \n# l1[-1] = l1[-1]+[33] \n# l1[-1].remove(33)\nl1[-1].append(33)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44, 33]]\n\n\n(하지만 현실은)\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33] \nl1[-1].remove(33)\nl1[-1].append(33)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44]]"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제5",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제5",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제5",
    "text": "예제5\n\n선행지식: 이전까지 모든것\n\n\n포인트: +=는 재할당이 아니다.\n\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] += [33] # l1[-1] = l1[-1]+[33] \nl1[-1].remove(33)\nl1[-1].append(33)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44, 33]]\n\n\n+= 연산자의 올바른 이해\n\n??? 예제4랑 예제5는 같은코드가 아니었음!!! a += [1] 는 새로운 오브젝트를 만드는게 아니고, 기존의 오브젝트를 변형하는 스타일의 코드였음! (마치 append 메소드처럼)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-motivation-example-1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-motivation-example-1",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "예제1: Motivation example",
    "text": "예제1: Motivation example\n\n선행지식: 이전까지 모든것\n\n\n포인트: +=는 재할당이 아니다.\n\n- 우리는 이제 아래의 내용은 마스터함\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] += [33] # l1[-1].append(33)이랑 같은거..\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44, 33]]\n\n\n- 아래의 결과를 한번 예측해볼까?\n\nl1=[3,(66,55,44)]\nl2=l1.copy()\nl2[1] += (33,)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, (66, 55, 44)]\nl2= [3, (66, 55, 44, 33)]"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#해설",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#해설",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "해설",
    "text": "해설\n(시점1)\n\nl1=[3,(66,55,44)]\nl2=l1.copy()\n\n\nl1,l2\n\n([3, (66, 55, 44)], [3, (66, 55, 44)])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 140006812656640\nl2: 140006812645888\n\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 140006812656640 [7585536, 140006812590400]\nl2: 140006812645888 [7585536, 140006812590400]\n\n\n(시점2)\n\nl2[1] += (33,)\n\n\nl1,l2\n\n([3, (66, 55, 44)], [3, (66, 55, 44, 33)])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 140006812656640\nl2: 140006812645888\n\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 140006812656640 [7585536, 140006812590400]\nl2: 140006812645888 [7585536, 140006813422272]\n\n\n주소 140006812590400:(66,55,44)에 있는 값을 바꾸고 싶지만 불변형이라 못바꿈 \\(\\to\\) 그냥 새로 만들자. 그래서 그걸 140006813422272에 저장하자."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#차원의-실체",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#차원의-실체",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "2차원의 실체",
    "text": "2차원의 실체\n- 2차원 array a,b를 선언하자.\n\na = np.array([[11,22,33,44]]).reshape(2,2)\nb = np.array([[11,22,33,44,55,66]]).reshape(2,3)\nc = np.array([11,22,33,44]).reshape(4,1)\nd = np.array([11,22,33,44]) # 1d\n\n- a,b,c,d 속성비교\n\na.shape, b.shape, c.shape, d.shape ## 차원 \n\n((2, 2), (2, 3), (4, 1), (4,))\n\n\n\na.strides, b.strides, c.strides, d.strides ## 차원이랑 관련이 있어보임.. + 8의 배수 \n\n((16, 8), (24, 8), (8, 8), (8,))\n\n\n- ((16, 8), (24, 8), (8, 8), (8,)) 와 같은 저 숫자들이 도데체 무엇을 의미하는거야?!\n\n사전지식: 컴퓨터는 하나의 숫자를 저장하는데 메모리를 8칸 쓴다.\n가정: 만약에 컴퓨터가 1차원으로만 숫자를 저장한다면??\nstrides의 의미: (다음 행으로 가기위해서 JUMP해야하는 메모리 공간수, 다음 열로 가기위해서 JUMP해야하는 메모리 공간수)\n\n- 통찰: strides의 존재로 인해서 유추할 수 있는 것은 a,b,c,d 는 모두 1차원으로 저장되어있다는 사실이다. (중첩된 리스트꼴이 아니라)\n- 그렇다면.. shallow copy = deep copy?!\n\nA1=[[1,2],[3,4]]\nA2=A1.copy()\nB1=np.array([[1,2],[3,4]])\nB2=B1.copy()\n\n\nB1\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nA2[0][0]=11\nB2[0][0]=11\n\n\nA1,A2\n\n([[11, 2], [3, 4]], [[11, 2], [3, 4]])\n\n\n\nB1,B2\n\n(array([[1, 2],\n        [3, 4]]),\n array([[11,  2],\n        [ 3,  4]]))\n\n\n- 해방: 넘파이를 쓰면 copy.deepcopy()를 쓰지 않아도 된다.\n- 용어정리: (필요할까..?)\n\nnumpy 한정 .copy() 는 copy모듈의 deep copy와 동등한 효과를 준다. 하지만 실제로는 shallow copy 이다. 공식문서에는 “Note that np.copy is a shallow copy and will not copy object elements within arrays.” 라고 명시되어 있음.\n일부 블로그에서 deep copy라고 주장하기도 함. 블로그1, 블로그2, 블로그3 // 블로그2의 경우 참조와 shallow copy도 구분못함..\n이따가 view라는 개념도 나올텐데 .copy()를 deep copy라고 주장하는 블로거들 대부분 .view()를 shallow copy 혹은 참조라고 주장한다. 하지만 copy와 view를 설명하는 공식문서에서는 view가 shallow copy라는 말을 찾아볼 수 없음.\n\n- 정리 (넘파이한정)\n\nnparray.copy(): 실제로는 shallow copy, 그런데 느낌은 deep copy\nnparray.view(): 실제로는 shallow copy 보다 더 얕은 단계의 카피, 그런데 느낌은 shallow copy"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#참조",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#참조",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "참조",
    "text": "참조\n- a를 선언, b는 a의 참조\n\na=np.array([[1,2],[3,4]])\nb=a ## 참조 \n\n\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n\na.shape\n\n(2, 2)\n\n\n\nb.shape\n\n(2, 2)\n\n\n- a의 shape을 바꾸어보자 \\(\\to\\) b도 같이 바뀐다\n\na.shape = (4,)\n\n\na\n\narray([1, 2, 3, 4])\n\n\n\nb\n\narray([1, 2, 3, 4])\n\n\n\nid(a),id(b)\n\n(139680327738544, 139680327738544)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#view",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#view",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "view",
    "text": "view\n- a를 선언, b는 a의 view\n\na=np.array([[1,2],[3,4]]) \nb=a.view() ## 어떤 블로그등에서는 shallow copy라고 부르기도 한다. 그렇게 공부하지 마세여..\n\n\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n\na.shape\n\n(2, 2)\n\n\n\nb.shape\n\n(2, 2)\n\n\n\na.shape= (4,1)\n\n\na\n\narray([[1],\n       [2],\n       [3],\n       [4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nid(a), id(b)\n\n(139679960161232, 139679932937872)\n\n\n- 그런데..\n\na[0]=100\n\n\na\n\narray([[100],\n       [  2],\n       [  3],\n       [  4]])\n\n\n\nb\n\narray([[100,   2],\n       [  3,   4]])\n\n\n- 출생의 비밀\n\nb\n\narray([[100,   2],\n       [  3,   4]])\n\n\n\nb.base\n\narray([[100],\n       [  2],\n       [  3],\n       [  4]])\n\n\n\n? 이거 바뀐 a아니야?\n\n\nid(b.base), id(a)\n\n(139679960161232, 139679960161232)\n\n\n- View\n\nb가 a의 뷰라는 의미는, b가 a를 소스로하여 만들어진 오브젝트란 의미이다.\n따라서 이때 b.base는 a가 된다.\nb는 자체적으로 데이터를 가지고 있지 않으며 a와 공유한다.\n\nnote1 원본 ndarray의 일 경우는 .base가 None으로 나온다.\n\na.base\n\nnote2 b.base의 shpae과 b의 shape은 아무 관련없다.\n\nb.shape\n\n(2, 2)\n\n\n\nb.base.shape # a.shape과 같음\n\n(4, 1)\n\n\n- numpy에서 view를 사용하는 예시 (transpose)\n\nX = np.random.normal(size=[100,2])\nid((X.T).base), id(X)\n\n(139679932937584, 139679932937584)\n\n\n\nX.T 는 X의 view 이다.\n\n\nX.T @ X ## 실제로 X.T를 메모리공간에 새로 만들어 숫자를 저장하지않고 X.T @ X를 계산할 수 있음 (R과차이점) \n\narray([[124.15127928,  -0.45772606],\n       [ -0.45772606,  79.17005817]])"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#copy",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#copy",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "copy",
    "text": "copy\n- a를 선언, b는 a의 copy\n\na=np.array([[1,2],[3,4]])\nb=a.copy() # 껍데기를 새로 생성 (strides, shape) + 데이터도 a와 독립적으로 새로 생성하여 따로 메모리에 저장함. \n\n\nid(a),id(b)\n\n(139680327737776, 139679932938832)\n\n\n- a의 shape을 바꿔도 b에는 적용되지 않음\n\na.shape = (4,1)\na\n\narray([[1],\n       [2],\n       [3],\n       [4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n- 그리고 a[0]의 값을 바꿔도 b에는 적용되지 않음.\n\na[0]=100\n\n\na\n\narray([[100],\n       [  2],\n       [  3],\n       [  4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n- b의 출생을 조사해보니..\n\na.base,b.base\n\n(None, None)\n\n\n출생의 비밀은 없었다. 둘다 원본.\n- .view() 는 껍데기만 새로생성 // .copy() 는 껍데기와 데이터를 모두 새로 생성\n\nAppendix: .copy의 한계(?)\n(관찰)\n\na=np.array([1,[1,2]],dtype='O')\nb=a.copy()\nprint('시점1')\nprint('a=',a)\nprint('b=',b)\n\na[0]=222\nprint('시점2')\nprint('a=',a)\nprint('b=',b)\n\na[1][0]=333\nprint('시점2')\nprint('a=',a)\nprint('b=',b)\n\n시점1\na= [1 list([1, 2])]\nb= [1 list([1, 2])]\n시점2\na= [222 list([1, 2])]\nb= [1 list([1, 2])]\n시점2\na= [222 list([333, 2])]\nb= [1 list([333, 2])]\n\n\n\n왜 또 시점2에서는 a와 b가 같이 움직여?\n\n해결책: 더 깊은 복사\n\nimport copy\n\n\na=np.array([1,[1,2]],dtype='O')\nb=copy.deepcopy(a)\nprint('시점1')\nprint('a=',a)\nprint('b=',b)\n\na[0]=222\nprint('시점2')\nprint('a=',a)\nprint('b=',b)\n\na[1][0]=333\nprint('시점2')\nprint('a=',a)\nprint('b=',b)\n\n시점1\na= [1 list([1, 2])]\nb= [1 list([1, 2])]\n시점2\na= [222 list([1, 2])]\nb= [1 list([1, 2])]\n시점2\na= [222 list([333, 2])]\nb= [1 list([1, 2])]\n\n\n- 중간요약\n\n사실 b=a.copy()는 에서 .copy()는 사실 온전한 deep-copy가 아니다.\n그래서 a의 데이터가 중첩구조를 가지는 경우는 온전한 deep-copy가 수행되지 않는다.\n그런데 일반적으로 넘파이를 이용할때 자주 사용하는 데이터 구조인 행렬, 텐서등은 데이터가 중첩구조를 가지지 않는다. (1차원 array로만 저장되어 있음)\n따라서 행렬, 텐서에 한정하면 .copy()는 온전한 deep-copy라고 이해해도 무방하다. &lt;– 이것만 기억해!"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#요약",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#요약",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "요약",
    "text": "요약\n아래를 구분할 수 있으면 잘 이해한 것!!\narr = np.array(...) # arr -- [arr.shape, arr.strides, arr.base, ... ] \narr2 = arr \narr2 = arr.view()\narr2 = arr.copy()\narr2 = copy.deepcopy(arr)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#별명-뷰-카피",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#별명-뷰-카피",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "별명, 뷰, 카피",
    "text": "별명, 뷰, 카피\n- test 함수 작성\n\ndef test(a,b): \n    if id(a) == id(b): \n        print(\"별명\")\n    elif id(a) == id(b.base) or id(a.base)==id(b): \n        print(\"뷰\")\n    elif (id(a.base)!=id(None) and id(b.base)!=id(None)) and id(a.base) == id(b.base):\n        print(\"공통의 base를 가짐\")\n    else: \n        print(\"카피, 혹은 아무 관련없는 오브젝트\") \n\n- 잘 동작하나?\n(테스트1)\n\na=np.array([1,2,3,4])\nb=a\n\n\ntest(a,b)\n\n별명\n\n\n(테스트2)\n\na=np.array([1,2,3,4])\nb=a.view()\n\n\ntest(a,b)\n\n뷰\n\n\n(테스트3)\n\na=np.array([1,2,3,4])\nb=a.view()\nc=a.view()\n\n\ntest(b,c)\n\n공통의 base를 가짐\n\n\n\ntest(a,b)\n\n뷰\n\n\n\ntest(a,c)\n\n뷰\n\n\n(테스트4)\n\na=np.array([1,2,3,4])\nb=a.copy()\n\n\ntest(a,b)\n\n카피, 혹은 아무 관련없는 오브젝트"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#결론",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#결론",
    "title": "[IP2023] 13wk-1: 깊은복사와 얕은복사",
    "section": "결론",
    "text": "결론\n- 참조, 뷰, 카피의 개념을 잘 알고 있고 때에 따라 메모리를 아끼면서 이들을 적절하게 사용하고 싶을것 같음. 하지만 이건 불가능한 소망임.\n- 우리가 사용했던 어떠한 것들이 뷰가 나올지 카피가 나올지 잘 모른다. (그래서 원리를 이해해도 대응할 방법이 사실없음)\n\n예시1\n\na=np.array([1,2,3,4])\nb=a[:3]\n\n\na\n\narray([1, 2, 3, 4])\n\n\n\nb\n\narray([1, 2, 3])\n\n\n\ntest(a,b)\n\n뷰\n\n\n\nc=a[[0,1,2]]\nc\n\narray([1, 2, 3])\n\n\n\ntest(a,c)\n\n카피, 혹은 아무 관련없는 오브젝트\n\n\n\n\n예시2\n\na=np.array([[1,2],[3,4]])\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nb=a.flatten() # 플래튼은 펼치는건데..\nc=a.ravel() # 라벨도 펼치라는 뜻인데..\nd=a.reshape(-1) # 이것도 뜻은 펼치는건데..\n# 걍 다 똑같은거아냐?\n\n\ntest(a,b)\n\n카피, 혹은 아무 관련없는 오브젝트\n\n\n\ntest(a,c)\n\n뷰\n\n\n\ntest(a,d)\n\n뷰\n\n\n\ntest(c,d)\n\n공통의 base를 가짐\n\n\n\ntest(b,c)\n\n카피, 혹은 아무 관련없는 오브젝트\n\n\n- 심지어 copy인줄 알았던것이 사실 view라서 원치않는 side effect이 생길수 있음. \\(\\to\\) 그냥 방어적 프로그래밍이 최선인듯"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html",
    "title": "[Fourier] 푸리에변환 코드실습 (블로그)",
    "section": "",
    "text": "ref: https://towardsdatascience.com/fourier-transform-the-practical-python-implementation-acdd32f1b96a"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#introduction",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#introduction",
    "title": "[Fourier] 푸리에변환 코드실습 (블로그)",
    "section": "Introduction",
    "text": "Introduction\nFourier Transform (FT) relates the time domain of a signal to its frequency domain, where the frequency domain contains the information about the sinusoids (amplitude, frequency, phase) that construct the signal. Since FT is a continuous transform, the Discrete Fourier Transform (DFT) becomes the applicable transform in the digital world that holds the information of signals in the discrete format as a set of samples, where the sampling theorem is the strict rule of discretizing and the signal. The DFT of a signal (xn) with N number of samples is given by the following equation [2]:\n\\(X_k = \\sum_{n=0}^{N-1} x_n \\cdot e^{i2\\pi kn/N}=\\sum_{n=0}^{N-1}\\cdot [\\cos(2\\pi k/N) - i\\sin(2\\pi kn/N)]\\)\n\n\\(N\\) : Number of Samples\n\\(n\\): Current Sample\n\\(k\\): Current frequency where \\(k\\in [0,N-1]\\)\n\\(xn\\): The sine value at sample \\(n\\)\n\\(Xk\\): The DFT which includes information on both amplitude and phase\n\nThe output of the DFT (Xk) is an array of complex numbers that hold the information of frequency components [2]. Applying DFT on signals using the mathematical equation directly demands a heavy computation complexity. Luckily, a Fast Fourier Transform (FFT) was developed [3] to provide a faster implementation of the DFT. The FFT takes advantage of the symmetry nature of the output of the DFT. We will not further discuss how FFT works as it’s like the standard practical application of DFT. But if you want more details, refer to [3]."
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#lets-get-started",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#lets-get-started",
    "title": "[Fourier] 푸리에변환 코드실습 (블로그)",
    "section": "Let’s get started",
    "text": "Let’s get started\n\n# Import the required packages\nimport numpy as np\nfrom scipy.fft import fft, rfft\nfrom scipy.fft import fftfreq, rfftfreq\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nThe signal we will generate using the previous class contains three sinusoids (1, 10, 20) Hz with amplitudes of (3, 1, 0.5), respectively. The sampling rate will be 200 and the duration of the signal is 2 seconds.\n\n# Import the required package\nimport numpy as np\n\n# Building a class Signal for better use.\nclass Signal:\n    def __init__(self, amplitude=1, frequency=10, duration=1, sampling_rate=100.0, phase=0):\n        self.amplitude = amplitude\n        self.frequency = frequency\n        self.duration = duration\n        self.sampling_rate = sampling_rate\n        self.phase = phase\n        self.time_step = 1.0/self.sampling_rate\n        self.time_axis = np.arange(0, self.duration, self.time_step)\n  \n  # Generate sine wave\n    def sine(self):\n        return self.amplitude*np.sin(2*np.pi*self.frequency*self.time_axis+self.phase)\n  \n  # Generate cosine wave\n    def cosine(self):\n        return self.amplitude*np.cos(2*np.pi*self.frequency*self.time_axis+self.phase)\n\n\n# Generate the three signals using Signal class and its method sine()\nsignal_1hz = Signal(amplitude=3, frequency=1, sampling_rate=200, duration=2)\nsine_1hz = signal_1hz.sine()\nsignal_20hz = Signal(amplitude=1, frequency=20, sampling_rate=200, duration=2)\nsine_20hz = signal_20hz.sine()\nsignal_10hz = Signal(amplitude=0.5, frequency=10, sampling_rate=200, duration=2)\nsine_10hz = signal_10hz.sine()\n\n# Sum the three signals to output the signal we want to analyze\nsignal = sine_1hz + sine_20hz + sine_10hz\n\n# Plot the signal\nplt.plot(signal_1hz.time_axis, signal, 'b')\nplt.xlabel('Time [sec]')\nplt.ylabel('Amplitude')\nplt.title('Sum of three signals')\nplt.show()\n\n\n\n\nThe Fourier Transform of this signal can be calculated using (fft) from the scipy package as follows [4]:"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#fft",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#fft",
    "title": "[Fourier] 푸리에변환 코드실습 (블로그)",
    "section": "FFT",
    "text": "FFT\n\n# Apply the FFT on the signal\nfourier = fft(signal)\n\n# Plot the result (the spectrum |Xk|)\nplt.plot(np.abs(fourier))\nplt.title('The output of the FFT of the signal')\nplt.show()\n\n\n\n\nThe figure above should represent the frequency spectrum of the signal. Notice that the x-axis is the number of samples (instead of the frequency components) and the y-axis should represent the amplitudes of the sinusoids. To get the actual amplitudes of the spectrum, we have to normalize the output of (fft) by N/2 the number of samples.\n\n# Calculate N/2 to normalize the FFT output\nN = len(signal)\nnormalize = N/2\n\n# Plot the normalized FFT (|Xk|)/(N/2)\nplt.plot(np.abs(fourier)/normalize)\nplt.ylabel('Amplitude')\nplt.xlabel('Samples')\nplt.title('Normalized FFT Spectrum')\nplt.show()\n\n\n\n\nTo get the frequency components (x-axis), you can use (fftfreq) from the scipy package. This method needs the number of samples (N) and the sampling rate as input arguments. And it returns a frequency axis with N frequency components [5].\n\n# Get the frequency components of the spectrum\nsampling_rate = 200.0 # It's used as a sample spacing\nfrequency_axis = fftfreq(N, d=1.0/sampling_rate)\nnorm_amplitude = np.abs(fourier)/normalize\n# Plot the results\nplt.plot(frequency_axis, norm_amplitude)\nplt.xlabel('Frequency[Hz]')\nplt.ylabel('Amplitude')\nplt.title('Spectrum')\nplt.show()\n\n\n\n\nThe spectrum with the actual amplitudes and frequency axis\nTo understand what happened in the last code, let’s plot only the frequency axis:\n\n# Plot the frequency axis for more explanation\nplt.plot(frequency_axis)\nplt.ylabel('Frequency[Hz]')\nplt.title('Frequency Axis')\nplt.show()\n\n\n\n\nNotice that the frequency array starts at zero. Then, it begins to increase with (d) step by step to reach its maximum (100Hz). After that, it starts from the negative maximum frequency (-100Hz) to increase back again with (d) step by step. The maximum frequency that can hold information from the signal (100Hz) is half of the sampling rate and this is true according to the Sampling Theorem [2].\nDue to the symmetry of the spectrum for the real-value signals, we only focus on the first half of the spectrum [2]. The Scipy package provides methods to deal with the Fourier transform of the real-value signals, where it takes advantage symmetry nature of the spectrum. Such methods are (rfft [6], rfftfreq [7]). These methods are the same as (fft, fftfreq), respectively. By comparing the time execution between (fft) and (rfft) methods on the same signal, you’ll find that (rfft) is a little bit faster. When dealing with real-value signals, which is most likely the case, using (rfft) is the best choice.\n\n# Calculate the time execution of (fft)\nprint('Execution time of fft function:')\n%timeit fft(signal)\n# Calculate the time execution of (rfft)\nprint('\\nExecution time of rfft function:')\n%timeit rfft(signal)\n\nExecution time of fft function:\n4.08 µs ± 11.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\nExecution time of rfft function:\n4 µs ± 5.39 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nTo conclude our discussion about scaling the amplitudes and generating the frequency axis of the spectrum for real-values signal data that have a symmetry nature in their frequency domain, the code below represents the final form of the spectrum (the actual amplitudes on the right frequencies).\n\n# Plot the actual spectrum of the signal\nplt.plot(rfftfreq(N, d=1/sampling_rate), 2*np.abs(rfft(signal))/N)\nplt.title('Spectrum')\nplt.xlabel('Frequency[Hz]')\nplt.ylabel('Amplitude')\nplt.show()"
  },
  {
    "objectID": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html",
    "href": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html",
    "title": "[수리통계학] 추정 for JY",
    "section": "",
    "text": "- 비편향추정량(UB)란 \\(\\theta\\)의 추정량 중\n\\[\\forall \\theta\\in \\Theta:~ E(\\hat{\\theta})=\\theta\\]\n를 만족하는 추정량 \\(\\hat{\\theta}\\)을 의미한다.\n- (예시) 아래와 같은 상황을 가정하자.\n\\[X_n \\overset{iid}{\\sim} N(\\theta,1)\\]\n여기에서\n\n\\(\\hat{\\theta}_1=0\\) 은 \\(\\theta=0\\) 일 경우에는 \\(E(\\hat{\\theta})=\\theta\\) 를 만족하지만 그 외의 경우에는 \\(E(\\hat{\\theta})\\neq\\theta\\) 이므로 UB가 아니다.\n\\(\\hat{\\theta}_2=X_1\\) 은 UB이다.\n\\(\\hat{\\theta}_3=\\frac{X_1+X_2}{2}\\) 역시 UB이다.\n\\(\\hat{\\theta}_4=X_1+X_2-X_3\\) 역시 UB이다.\n\\(\\hat{\\theta}_5=-99X_1+100X_2\\) 역시 UB이다.\n\\(\\hat{\\theta}_6=\\frac{X_1+0}{2}\\) 은 1과 동일한 이유로 UB가 아니다.\n\\(\\hat{\\theta}_7=\\bar{X}\\)는 UB이다.\n\\(\\hat{\\theta}_8=w_1X_1+\\dots+w_nX_n\\) ,where \\(\\sum_{i=1}^{n}w_i=1\\) 형태의 estimator는 모두 UB이다.\n\n- 최소분산비편향추정량(MVUE)란 \\(\\theta\\)에 대한 비편향추정량을 모아놓은 집합 \\(\\hat{\\Theta}_{UB}\\) 에서 최소분산을 가지는 추정량을 의미한다. MVUE를 구하는 방법은 아래와 같다.\n\n\\(\\theta\\)에 대한 모든 비편향추정량을 구한다. 즉 집합 \\(\\hat{\\Theta}_{UB}\\)를 구한다. 그리고 \\(\\forall \\hat{\\theta} \\in \\hat{\\Theta}_{UB}\\) 에 대하여 \\(V(\\hat{\\theta})\\) 를 구한 뒤 \\(V(\\hat{\\theta})\\)가 가장 작은 \\(\\hat{\\theta}\\)를 선택한다.\n\n예를들어 위의 예제에서 \\(V(\\hat{\\theta}_2)=1\\) 이고 \\(V(\\hat{\\theta}_3)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}\\) 이므로 \\(\\hat{\\theta}_3\\) 이 더 좋은 추정량이라 볼 수 있다.\n- (의문) 왜 비편향추정량만 모아서 그중에서 최소분산을 구할까?\n\n\\(\\hat{\\theta}_1\\)와 같은 추정량은 \\(V(\\hat{\\theta}_1)=0\\) 이므로 그냥 최소분산을 만족한다. 따라서 이러한 추정량은 제외해야지 게임이 성립함.\n\n- 불만: 아래의 방법으로 구하는건 거의 불가능하지 않나?\n\n\\(\\theta\\)에 대한 모든 비편향추정량을 구한다. 즉 집합 \\(\\hat{\\Theta}_{UB}\\)를 구한다. 그리고 \\(\\forall \\hat{\\theta} \\in \\hat{\\Theta}_{UB}\\) 에 대하여 \\(V(\\hat{\\theta})\\) 를 구한 뒤 \\(V(\\hat{\\theta})\\)가 가장 작은 \\(\\hat{\\theta}\\)를 선택한다.\n\n- 이론: 크래머라오 하한값(편의상 \\(L^\\star\\)이라고 하자)이라고 있는데, 이는 \\({\\Theta}_{UB}\\)에 존재하는 모든 추정량에 대한 분산의 하한값을 제공한다.1 즉 아래가 성립한다.\n\n\\(L^\\star\\) is Cramer-Rao lower bound \\(\\Rightarrow\\) \\(\\forall \\hat{\\theta} \\in {\\Theta}_{UB}:~ V(\\hat{\\theta}) \\geq L^\\star\\)\n\n역은 성립하지 않음을 주의하자. 즉 아래를 만족하는 \\(L\\)이 존재할 수 있다.\n\n\\(V(\\hat{\\theta}) \\geq L &gt; L^\\star\\) for some \\(\\hat{\\theta} \\in \\Theta_{UB}\\)\n\n- 위의 이론을 이용하면 아래의 논리전개를 펼 수 있다.\n\n\\(L^\\star\\)를 구한다.\n왠지 MVUE가 될 것 같은 \\(\\hat{\\theta}\\)을 하나 찍고 그것의 분산 \\(V(\\hat{\\theta})\\)를 구한다.\n만약에 \\(V(\\hat{\\theta})=L^\\star\\)를 만족하면 그 \\(\\hat{\\theta}\\)이 MVUE라고 주장할 수 있다.\n\n- 위의 논리전개에 대한 불만 [@ p.212]\n\n\\(V(\\hat{\\theta})=L^\\star\\) 이길 기도해야함.\n\\(\\forall \\hat{\\theta} \\in {\\Theta}_{UB}:~ V(\\hat{\\theta}) \\geq L &gt; L^\\star\\) 와 같은 \\(L\\)이 존재하는 경우는 쓸 수 없음.\n\n- 또 다른 방법: 완비충분통계량을 이용함\n\n\n아래와 같은 상황을 가정하자.\n\\[ X_1,\\dots,X_n \\overset{iid}{\\sim} P_{\\theta}\\]\n- 충분통계량(SS)의 느낌: “이 값만 기억하면 \\(\\theta\\)를 추정하는데 무난할듯”\n- 예시1: \\(X_1 \\sim N(\\theta,1)\\)\n\n\\(X_1\\)은 \\(\\theta_1\\) 의 SS. (하나밖에 없으니 그거라도 기억해야지)\n즉 \\(\\hat{\\theta}=X_1\\)은 \\(\\theta\\)의 SS\n\n- 예시2: \\(X_1,X_2 \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)는 \\(\\theta\\)의 SS (둘다 기억하면 당연히 \\(\\theta\\)를 추정함에 있어서 충분함)\n그렇지만 좀 더 생각해보면 굳이 값 두개를 기억하기보다 \\(\\frac{1}{2}(X_1+X_2)\\)의 값만 기억해도 왠지 충분할것 같음. 따라서 \\(\\hat{\\theta} = \\frac{1}{2}(X_1+X_2)\\) 역시 \\(\\theta\\)의 SS 일듯\n그런데 좀 더 생각해보니까 \\(X_1+X_2\\)의 값만 기억해도 \\(\\frac{1}{2}(X_1+X_2)\\)를 나중에 만들 수 있음 (1/2만 곱하면 되니까) 따라서 \\(X_1+X_2\\)만 기억해도 왠지 충분할 것 같음. 따라서 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- 예시3: \\(X_1,\\dots,X_n \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2,\\dots,X_n)\\)은 \\(\\theta\\)의 SS.\n하지만 \\(n\\)개의 숫자를 기억할 필요 없이 \\(\\sum_{i=1}^{n} X_i\\) 하나의 숫자만 기억해도 왠지 충분할듯. 그래서 \\(\\hat{\\theta} = \\sum_{i=1}^{n} X_i\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- SS에 대한 직관1\n\n기억할 숫자가 적을수록 유리 -&gt; MSS의 개념\n충분통계량의 1:1은 충분통계량 (\\(\\frac{1}{2}(X_1+X_2)\\)을 기억하면 충분한 상황이라면, \\(X_1+X_2\\)를 기억해도 충분하니까..)\n\n- 예시4: \\(X_1,X_2 \\sim {\\cal B}er(\\theta)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)은 \\(\\theta\\)의 SS.\n그리고 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\) SS 일듯.\n두개보다 한개가 유리하니까 둘다 SS이면 \\((X_1,X_2)\\)보다 \\(X_1+X_2\\)가 더 좋은 SS.\n\\(X_1\\)은 SS가 아닐듯. \\(p\\)를 추정함에 있어서 \\(X_1\\)만 가지고서는 충분하지 않아보임\n\\(X_2\\)도 SS가 아닐듯.\n\n왠지 충분할 것 같은 느낌의 정의\n아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n- 일반적으로\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=\\theta(1-\\theta)\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=\\theta^2\\)\n\n와 같은 확률들은 \\(\\theta\\)가 unknown일 때 하나의 숫자로 정할 수 없다. 예를들어 \\(\\theta=0\\) 이라면 아래와 같을 것이고\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=0\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=0\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=0\\)\n\n\\(\\theta=1/2\\) 이라면 아래와 같을 것이다.\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=1/4\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=1/4\\)\n\n즉 \\(X_1,X_2\\)의 결합확률분포는 \\(\\theta\\)가 변함에 따라 같이 변화한다. 이를 이용해 우리는 \\(X_1,X_2\\)의 결합확률분포에서 관찰한 샘플들을 이용하여 \\(\\theta\\)의 값을 역으로 추론한다.\n- 만약에 어떠한 “특수한 정보를 알고 있을 경우” \\(X_1,X_2\\)의 결합확률분포를 완벽하게 기술할 수 있을 때를 가정해보자.\n- 경우1: \\(\\theta\\)를 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(\\theta=1/2\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=0,X_2=1 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=1 | \\theta=1/2)=1/4\\)\n\n- 경우2: \\(X_1,X_2\\)의 realization을 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(X_1=0,X_2=1\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | X_1=0,X_2=0)=0\\)\n\\(P(X_1=0,X_2=1| X_1=0,X_2=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1,X_2=0)=1\\)\n\\(P(X_1=1,X_2=1| X_1=1,X_2=1)=0\\)\n\n- 경우3: \\((X_1+X_2)(\\omega)\\)의 realization을 알고 있을 경우. 이때도 매우 특이하게 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 있다.\ncase1: \\(X_1+X_2=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=0)=1\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=0)=0\\)\n\ncase2: \\(X_1+X_2=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=1)=0\\)\n\ncase3: \\(X_1+X_2=2\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=2)=1\\)\n\n- 경우4: \\(X_1\\)의 realization만 알고 있을 경우. 이때는 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 없다.\ncase1: \\(X_1=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=0)=1-\\theta\\)\n\\(P(X_1=0,X_2=1| X_1=0)=\\theta\\)\n\\(P(X_1=1,X_2=0| X_1=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1=0)=0\\)\n\ncase2: \\(X_1=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1)=1-\\theta\\)\n\\(P(X_1=1,X_2=1| X_1=1)=\\theta\\)\n\n- 종합해보면 경우1,경우2,경우3은 경우4와 구분되는 어떠한 공통점을 가지고 있다 볼 수 있다. 특징은 결합확률분포가 \\(\\theta\\)에 대한 함수로 표현되지 않는다는 것이다. 하나씩 살펴보면\n\n경우1: 당연히 \\(\\theta\\)를 줬으니까 \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우2: \\(X_1,X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우3: \\(X_1+X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n\n이렇게보면 경우1과 경우2,3은 또 다시 구분된다. 경우1은 \\(\\theta\\)에 대한 완전한 정보를 준 상황이므로 당연히 조인트는 \\(\\theta\\)에 의존하지 않는다. 경우2-3은 \\(\\theta\\)를 주지 않았음에도 조인트가 \\(\\theta\\)에 의존하지 않는 매우 특별해보이는 상황이다. 따라서 이를 통해서 유추하면\n\n경우2에서는 \\((X_1,X_2)\\) 가 경우3에서는 \\(X_1+X_2\\)가 \\(\\theta\\)에 대한 완전한 정보를 대신하고 있는것 아닐까?\n\n라는 생각이 든다. 정리하면\n\n경우2: \\((X_1,X_2)\\)을 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n경우3: \\(X_1+X_2\\)를 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n\n라고 해석할 수 있는데 이를 수식화 하면 아래와 같다.\n- 대충정의: 어떠한 통계량 \\(S\\)의 값을 줬을때, \\((X_1,X_2\\dots,X_n)\\)의 조인트가 \\(\\theta\\)에 의존하지 않으면 그 통계량 \\(S\\)를 \\(\\theta\\)의 충분통계량이라고 한다.\n- 충분통계량 구하는 방법\n\n지수족일때 구하는 방식이 있음! &lt;– 외우세여\n분해정리를 쓰는 경우. &lt;– 거의 안쓰는거같은데..\n1-2로도 잘 모르겠으면 충분통계량일듯한 애를 잡아와서 정의에 넣고 노가다로 때려맞춤. (문제가 디스크릿할때만 쓸것)\n\n\n\n\n- 충분통계량에 대한 realization을 알려주면 \\(\\theta\\)의 값을 그냥 알려주는 효과임. 그래서 충분통계량은 좋은 것임\n- 그런데 충분통계량에도 급이 있음. 아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n이 경우\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n이지만 1은 두개의 숫자를 기억해야하고 2는 하나의 숫자만 기억하면 되니까 2가 더 좋음\n- 예비개념: 상태1과 상태2가 있다고 하자. 상태1에서 상태2로 가는 변화는 쉽지만, 상태2에서 상태1로 가는 변화는 어렵다고 할때, 상태1이 더 좋은 상태이다.\n\n두가지 상태 “500원을 가지고 있음”, “1000원을 가지고 있음” 을 고려하자. 1000원을 500원을 만드는 것은 쉽지만 500원을 1000원으로 만들기는 어렵다. 따라서 1000원이 더 좋은 상태이다.\n\n- 충분통계량의 급을 어떻게 구분할까? 아래의 상황에서\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n1을 이용하면 2를 만들 수 있지만, 2를 이용해서 1을 만들 수는 없음. 즉 \\(1\\to 2\\) 인 변환(=함수)는 가능하지만 \\(2\\to 1\\)로 만드는 변환(=함수)는 가능하지 않음. 예비개념을 잘 이해했다면 2가 더 좋은 상태라고 볼 수 있다.\n- 이를 확장하자. 어떠한 충분 통계량 \\(S^\\star\\)가 있다고 가정하자. 다른 모든 충분통계량 \\(S_1,S_2,S_3 \\dots\\)에서 \\(S^\\star\\)로 만드는 변환은 존재하는데 (함수는 존재하는데) 그 반대는 \\(S^\\star\\)의 전단사인 충분통계량만 가능하다고 하자. 그렇다면 \\(S^\\star\\)는 가장 좋은 충분통계량이라고 하며, 가장 적은 숫자만 기억하면 되는 충분통계량이라 볼 수 있다. 이러한 충분통계량을 MSS 라고 하자.\n\n\n\n- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, @Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다.\n\n\n\n- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, @Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#충분통계량",
    "href": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#충분통계량",
    "title": "[수리통계학] 추정 for JY",
    "section": "",
    "text": "아래와 같은 상황을 가정하자.\n\\[ X_1,\\dots,X_n \\overset{iid}{\\sim} P_{\\theta}\\]\n- 충분통계량(SS)의 느낌: “이 값만 기억하면 \\(\\theta\\)를 추정하는데 무난할듯”\n- 예시1: \\(X_1 \\sim N(\\theta,1)\\)\n\n\\(X_1\\)은 \\(\\theta_1\\) 의 SS. (하나밖에 없으니 그거라도 기억해야지)\n즉 \\(\\hat{\\theta}=X_1\\)은 \\(\\theta\\)의 SS\n\n- 예시2: \\(X_1,X_2 \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)는 \\(\\theta\\)의 SS (둘다 기억하면 당연히 \\(\\theta\\)를 추정함에 있어서 충분함)\n그렇지만 좀 더 생각해보면 굳이 값 두개를 기억하기보다 \\(\\frac{1}{2}(X_1+X_2)\\)의 값만 기억해도 왠지 충분할것 같음. 따라서 \\(\\hat{\\theta} = \\frac{1}{2}(X_1+X_2)\\) 역시 \\(\\theta\\)의 SS 일듯\n그런데 좀 더 생각해보니까 \\(X_1+X_2\\)의 값만 기억해도 \\(\\frac{1}{2}(X_1+X_2)\\)를 나중에 만들 수 있음 (1/2만 곱하면 되니까) 따라서 \\(X_1+X_2\\)만 기억해도 왠지 충분할 것 같음. 따라서 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- 예시3: \\(X_1,\\dots,X_n \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2,\\dots,X_n)\\)은 \\(\\theta\\)의 SS.\n하지만 \\(n\\)개의 숫자를 기억할 필요 없이 \\(\\sum_{i=1}^{n} X_i\\) 하나의 숫자만 기억해도 왠지 충분할듯. 그래서 \\(\\hat{\\theta} = \\sum_{i=1}^{n} X_i\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- SS에 대한 직관1\n\n기억할 숫자가 적을수록 유리 -&gt; MSS의 개념\n충분통계량의 1:1은 충분통계량 (\\(\\frac{1}{2}(X_1+X_2)\\)을 기억하면 충분한 상황이라면, \\(X_1+X_2\\)를 기억해도 충분하니까..)\n\n- 예시4: \\(X_1,X_2 \\sim {\\cal B}er(\\theta)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)은 \\(\\theta\\)의 SS.\n그리고 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\) SS 일듯.\n두개보다 한개가 유리하니까 둘다 SS이면 \\((X_1,X_2)\\)보다 \\(X_1+X_2\\)가 더 좋은 SS.\n\\(X_1\\)은 SS가 아닐듯. \\(p\\)를 추정함에 있어서 \\(X_1\\)만 가지고서는 충분하지 않아보임\n\\(X_2\\)도 SS가 아닐듯.\n\n왠지 충분할 것 같은 느낌의 정의\n아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n- 일반적으로\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=\\theta(1-\\theta)\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=\\theta^2\\)\n\n와 같은 확률들은 \\(\\theta\\)가 unknown일 때 하나의 숫자로 정할 수 없다. 예를들어 \\(\\theta=0\\) 이라면 아래와 같을 것이고\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=0\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=0\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=0\\)\n\n\\(\\theta=1/2\\) 이라면 아래와 같을 것이다.\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=1/4\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=1/4\\)\n\n즉 \\(X_1,X_2\\)의 결합확률분포는 \\(\\theta\\)가 변함에 따라 같이 변화한다. 이를 이용해 우리는 \\(X_1,X_2\\)의 결합확률분포에서 관찰한 샘플들을 이용하여 \\(\\theta\\)의 값을 역으로 추론한다.\n- 만약에 어떠한 “특수한 정보를 알고 있을 경우” \\(X_1,X_2\\)의 결합확률분포를 완벽하게 기술할 수 있을 때를 가정해보자.\n- 경우1: \\(\\theta\\)를 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(\\theta=1/2\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=0,X_2=1 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=1 | \\theta=1/2)=1/4\\)\n\n- 경우2: \\(X_1,X_2\\)의 realization을 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(X_1=0,X_2=1\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | X_1=0,X_2=0)=0\\)\n\\(P(X_1=0,X_2=1| X_1=0,X_2=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1,X_2=0)=1\\)\n\\(P(X_1=1,X_2=1| X_1=1,X_2=1)=0\\)\n\n- 경우3: \\((X_1+X_2)(\\omega)\\)의 realization을 알고 있을 경우. 이때도 매우 특이하게 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 있다.\ncase1: \\(X_1+X_2=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=0)=1\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=0)=0\\)\n\ncase2: \\(X_1+X_2=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=1)=0\\)\n\ncase3: \\(X_1+X_2=2\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=2)=1\\)\n\n- 경우4: \\(X_1\\)의 realization만 알고 있을 경우. 이때는 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 없다.\ncase1: \\(X_1=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=0)=1-\\theta\\)\n\\(P(X_1=0,X_2=1| X_1=0)=\\theta\\)\n\\(P(X_1=1,X_2=0| X_1=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1=0)=0\\)\n\ncase2: \\(X_1=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1)=1-\\theta\\)\n\\(P(X_1=1,X_2=1| X_1=1)=\\theta\\)\n\n- 종합해보면 경우1,경우2,경우3은 경우4와 구분되는 어떠한 공통점을 가지고 있다 볼 수 있다. 특징은 결합확률분포가 \\(\\theta\\)에 대한 함수로 표현되지 않는다는 것이다. 하나씩 살펴보면\n\n경우1: 당연히 \\(\\theta\\)를 줬으니까 \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우2: \\(X_1,X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우3: \\(X_1+X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n\n이렇게보면 경우1과 경우2,3은 또 다시 구분된다. 경우1은 \\(\\theta\\)에 대한 완전한 정보를 준 상황이므로 당연히 조인트는 \\(\\theta\\)에 의존하지 않는다. 경우2-3은 \\(\\theta\\)를 주지 않았음에도 조인트가 \\(\\theta\\)에 의존하지 않는 매우 특별해보이는 상황이다. 따라서 이를 통해서 유추하면\n\n경우2에서는 \\((X_1,X_2)\\) 가 경우3에서는 \\(X_1+X_2\\)가 \\(\\theta\\)에 대한 완전한 정보를 대신하고 있는것 아닐까?\n\n라는 생각이 든다. 정리하면\n\n경우2: \\((X_1,X_2)\\)을 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n경우3: \\(X_1+X_2\\)를 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n\n라고 해석할 수 있는데 이를 수식화 하면 아래와 같다.\n- 대충정의: 어떠한 통계량 \\(S\\)의 값을 줬을때, \\((X_1,X_2\\dots,X_n)\\)의 조인트가 \\(\\theta\\)에 의존하지 않으면 그 통계량 \\(S\\)를 \\(\\theta\\)의 충분통계량이라고 한다.\n- 충분통계량 구하는 방법\n\n지수족일때 구하는 방식이 있음! &lt;– 외우세여\n분해정리를 쓰는 경우. &lt;– 거의 안쓰는거같은데..\n1-2로도 잘 모르겠으면 충분통계량일듯한 애를 잡아와서 정의에 넣고 노가다로 때려맞춤. (문제가 디스크릿할때만 쓸것)"
  },
  {
    "objectID": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#최소충분통계량",
    "href": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#최소충분통계량",
    "title": "[수리통계학] 추정 for JY",
    "section": "",
    "text": "- 충분통계량에 대한 realization을 알려주면 \\(\\theta\\)의 값을 그냥 알려주는 효과임. 그래서 충분통계량은 좋은 것임\n- 그런데 충분통계량에도 급이 있음. 아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n이 경우\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n이지만 1은 두개의 숫자를 기억해야하고 2는 하나의 숫자만 기억하면 되니까 2가 더 좋음\n- 예비개념: 상태1과 상태2가 있다고 하자. 상태1에서 상태2로 가는 변화는 쉽지만, 상태2에서 상태1로 가는 변화는 어렵다고 할때, 상태1이 더 좋은 상태이다.\n\n두가지 상태 “500원을 가지고 있음”, “1000원을 가지고 있음” 을 고려하자. 1000원을 500원을 만드는 것은 쉽지만 500원을 1000원으로 만들기는 어렵다. 따라서 1000원이 더 좋은 상태이다.\n\n- 충분통계량의 급을 어떻게 구분할까? 아래의 상황에서\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n1을 이용하면 2를 만들 수 있지만, 2를 이용해서 1을 만들 수는 없음. 즉 \\(1\\to 2\\) 인 변환(=함수)는 가능하지만 \\(2\\to 1\\)로 만드는 변환(=함수)는 가능하지 않음. 예비개념을 잘 이해했다면 2가 더 좋은 상태라고 볼 수 있다.\n- 이를 확장하자. 어떠한 충분 통계량 \\(S^\\star\\)가 있다고 가정하자. 다른 모든 충분통계량 \\(S_1,S_2,S_3 \\dots\\)에서 \\(S^\\star\\)로 만드는 변환은 존재하는데 (함수는 존재하는데) 그 반대는 \\(S^\\star\\)의 전단사인 충분통계량만 가능하다고 하자. 그렇다면 \\(S^\\star\\)는 가장 좋은 충분통계량이라고 하며, 가장 적은 숫자만 기억하면 되는 충분통계량이라 볼 수 있다. 이러한 충분통계량을 MSS 라고 하자."
  },
  {
    "objectID": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#라오블랙웰",
    "href": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#라오블랙웰",
    "title": "[수리통계학] 추정 for JY",
    "section": "",
    "text": "- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, @Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#레만쉐페정리",
    "href": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#레만쉐페정리",
    "title": "[수리통계학] 추정 for JY",
    "section": "",
    "text": "- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, @Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html",
    "title": "3wk-1 그래프2",
    "section": "",
    "text": "import matplotlib\n\n\n%matplotlib inline\n# matplotlib.use(\"TKAgg\")    # 그래픽 백엔드로 Tk를 사용하고자 한다면, 이 코드를 사용하시기 바랍니다. (밖에 창이뜰 것)\n\n\n# pyplot\n\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 4, 9, 5, 3])\nplt.show()\n\n\n\n\n데이터 몇 개로 plot 함수를 호출한 다음, show 함수를 호출해주면 간단히 그래프를 그려볼 수 있습니다!\nplot 함수에 단일 배열의 데이터가 주어진다면, 수직 축의 좌표로서 이를 사용하게 되며, 각 데이터의 배열상 색인(인덱스)을 수평 좌표로서 사용합니다. 두 개의 배열을 넣어줄 수도 있습니다: 그러면, 하나는 x 축에 대한것이며, 다른 하나는 y 축에 대한것이 됩니다:\n- object oriented API를 이용하여 그려보자.\n\n지금까지는 뭘할지만 썼다면, object oriented를 이용하면 이 함수를 “누가” 실행해야하는지 앞에 누가 가 들어간다.1\n\n\n# 1. 도화지(Figure: fig)를 깔고 그래프를 그릴 구역(Axes: ax)를 정의한다.\nfig, ax = plt.subplots()\n\n# 2. ax위에 그래프를 그린다.\nax.plot([1, 2, 4, 9, 5, 3]) ## ax에 일을 시킨다.\n\n# 3. 그래프를 화면에 출력.\nplt.show()\n\n\n\n\n\npyplot과 동일한 형태의 그래프가 그려집니다.\n\nfig, ax를 선언하느라 한 줄을 더 입력해야 한다는 불편함이 있지만 ax 객체가 있어 그래프를 제어하기 더 쉬워집니다.\n많은 경우 fig, ax = plt.subplots() 대신 ax = plt.subplot()으로 해도 됩니다.\n그러나 fig 대상 명령(예. savefig)을 사용해야 할 때도 있고, 두 가지를 따로 외우려면 혼동이 되니 한 가지로 통일하는 것이 좋습니다.\n\n\n# pyplot\n\nplt.plot([-3, -2, 5, 0], [1, 6, 4, 3])\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\nax.plot([-3, -2, 5, 0], [1, 6, 4, 3])\nplt.show()\n\n\n\n\n이번에는 수학적인 함수를 그려보겠습니다. NumPy의 linespace 함수를 사용하여 -2 ~ 2 범위에 속하는 500개의 부동소수로 구성된 x 배열을 생성합니다. 그 다음 x의 각 값의 거듭제곱된 값을 포함하는 y 배열을 생성합니다 (NumPy에 대하여 좀 더 알고 싶다면, NumPy 튜토리얼을 참고하시기 바랍니다).\n\nx = np.linspace(-2, 2, 401) # -2~2까지 400등분\nx\n\narray([-2.  , -1.99, -1.98, -1.97, -1.96, -1.95, -1.94, -1.93, -1.92,\n       -1.91, -1.9 , -1.89, -1.88, -1.87, -1.86, -1.85, -1.84, -1.83,\n       -1.82, -1.81, -1.8 , -1.79, -1.78, -1.77, -1.76, -1.75, -1.74,\n       -1.73, -1.72, -1.71, -1.7 , -1.69, -1.68, -1.67, -1.66, -1.65,\n       -1.64, -1.63, -1.62, -1.61, -1.6 , -1.59, -1.58, -1.57, -1.56,\n       -1.55, -1.54, -1.53, -1.52, -1.51, -1.5 , -1.49, -1.48, -1.47,\n       -1.46, -1.45, -1.44, -1.43, -1.42, -1.41, -1.4 , -1.39, -1.38,\n       -1.37, -1.36, -1.35, -1.34, -1.33, -1.32, -1.31, -1.3 , -1.29,\n       -1.28, -1.27, -1.26, -1.25, -1.24, -1.23, -1.22, -1.21, -1.2 ,\n       -1.19, -1.18, -1.17, -1.16, -1.15, -1.14, -1.13, -1.12, -1.11,\n       -1.1 , -1.09, -1.08, -1.07, -1.06, -1.05, -1.04, -1.03, -1.02,\n       -1.01, -1.  , -0.99, -0.98, -0.97, -0.96, -0.95, -0.94, -0.93,\n       -0.92, -0.91, -0.9 , -0.89, -0.88, -0.87, -0.86, -0.85, -0.84,\n       -0.83, -0.82, -0.81, -0.8 , -0.79, -0.78, -0.77, -0.76, -0.75,\n       -0.74, -0.73, -0.72, -0.71, -0.7 , -0.69, -0.68, -0.67, -0.66,\n       -0.65, -0.64, -0.63, -0.62, -0.61, -0.6 , -0.59, -0.58, -0.57,\n       -0.56, -0.55, -0.54, -0.53, -0.52, -0.51, -0.5 , -0.49, -0.48,\n       -0.47, -0.46, -0.45, -0.44, -0.43, -0.42, -0.41, -0.4 , -0.39,\n       -0.38, -0.37, -0.36, -0.35, -0.34, -0.33, -0.32, -0.31, -0.3 ,\n       -0.29, -0.28, -0.27, -0.26, -0.25, -0.24, -0.23, -0.22, -0.21,\n       -0.2 , -0.19, -0.18, -0.17, -0.16, -0.15, -0.14, -0.13, -0.12,\n       -0.11, -0.1 , -0.09, -0.08, -0.07, -0.06, -0.05, -0.04, -0.03,\n       -0.02, -0.01,  0.  ,  0.01,  0.02,  0.03,  0.04,  0.05,  0.06,\n        0.07,  0.08,  0.09,  0.1 ,  0.11,  0.12,  0.13,  0.14,  0.15,\n        0.16,  0.17,  0.18,  0.19,  0.2 ,  0.21,  0.22,  0.23,  0.24,\n        0.25,  0.26,  0.27,  0.28,  0.29,  0.3 ,  0.31,  0.32,  0.33,\n        0.34,  0.35,  0.36,  0.37,  0.38,  0.39,  0.4 ,  0.41,  0.42,\n        0.43,  0.44,  0.45,  0.46,  0.47,  0.48,  0.49,  0.5 ,  0.51,\n        0.52,  0.53,  0.54,  0.55,  0.56,  0.57,  0.58,  0.59,  0.6 ,\n        0.61,  0.62,  0.63,  0.64,  0.65,  0.66,  0.67,  0.68,  0.69,\n        0.7 ,  0.71,  0.72,  0.73,  0.74,  0.75,  0.76,  0.77,  0.78,\n        0.79,  0.8 ,  0.81,  0.82,  0.83,  0.84,  0.85,  0.86,  0.87,\n        0.88,  0.89,  0.9 ,  0.91,  0.92,  0.93,  0.94,  0.95,  0.96,\n        0.97,  0.98,  0.99,  1.  ,  1.01,  1.02,  1.03,  1.04,  1.05,\n        1.06,  1.07,  1.08,  1.09,  1.1 ,  1.11,  1.12,  1.13,  1.14,\n        1.15,  1.16,  1.17,  1.18,  1.19,  1.2 ,  1.21,  1.22,  1.23,\n        1.24,  1.25,  1.26,  1.27,  1.28,  1.29,  1.3 ,  1.31,  1.32,\n        1.33,  1.34,  1.35,  1.36,  1.37,  1.38,  1.39,  1.4 ,  1.41,\n        1.42,  1.43,  1.44,  1.45,  1.46,  1.47,  1.48,  1.49,  1.5 ,\n        1.51,  1.52,  1.53,  1.54,  1.55,  1.56,  1.57,  1.58,  1.59,\n        1.6 ,  1.61,  1.62,  1.63,  1.64,  1.65,  1.66,  1.67,  1.68,\n        1.69,  1.7 ,  1.71,  1.72,  1.73,  1.74,  1.75,  1.76,  1.77,\n        1.78,  1.79,  1.8 ,  1.81,  1.82,  1.83,  1.84,  1.85,  1.86,\n        1.87,  1.88,  1.89,  1.9 ,  1.91,  1.92,  1.93,  1.94,  1.95,\n        1.96,  1.97,  1.98,  1.99,  2.  ])\n\n\n\n# pyplot\n\nimport numpy as np\nx = np.linspace(-2, 2, 500)\ny = x**2\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\n\nax.plot(x, y)\n\nplt.show()\n\n\n\n\n그래프가 약간은 삭막해 보입니다. 타이틀과 x 및 y축에 대한 라벨, 그리고 모눈자를 추가적으로 그려보겠습니다.\n\n# pyplot\n\nplt.plot(x, y)\nplt.title(\"Square function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y = x**2\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\nobject-oriented API는 축 이름과 같은 설정 명령어가 pyplot과 다소 다릅니다.\n\n대체로 축 이름(label), 범위(limits) 등을 지정하는 명령어는 set_대상(), 거꾸로 그래프에서 설정값을 가져오는 명령어는 get_대상()으로 통일되어 있습니다.\n\n개인적으로 pyplot의 명령어 체계보다 object-oriented API의 체계를 선호합니다.\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\n\nax.plot(x, y)\nax.set_title(\"Square function\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y = x**2\")\nax.grid(True)\n\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#tools-matplotlib",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#tools-matplotlib",
    "title": "3wk-1 그래프2",
    "section": "",
    "text": "import matplotlib\n\n\n%matplotlib inline\n# matplotlib.use(\"TKAgg\")    # 그래픽 백엔드로 Tk를 사용하고자 한다면, 이 코드를 사용하시기 바랍니다. (밖에 창이뜰 것)\n\n\n# pyplot\n\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 4, 9, 5, 3])\nplt.show()\n\n\n\n\n데이터 몇 개로 plot 함수를 호출한 다음, show 함수를 호출해주면 간단히 그래프를 그려볼 수 있습니다!\nplot 함수에 단일 배열의 데이터가 주어진다면, 수직 축의 좌표로서 이를 사용하게 되며, 각 데이터의 배열상 색인(인덱스)을 수평 좌표로서 사용합니다. 두 개의 배열을 넣어줄 수도 있습니다: 그러면, 하나는 x 축에 대한것이며, 다른 하나는 y 축에 대한것이 됩니다:\n- object oriented API를 이용하여 그려보자.\n\n지금까지는 뭘할지만 썼다면, object oriented를 이용하면 이 함수를 “누가” 실행해야하는지 앞에 누가 가 들어간다.1\n\n\n# 1. 도화지(Figure: fig)를 깔고 그래프를 그릴 구역(Axes: ax)를 정의한다.\nfig, ax = plt.subplots()\n\n# 2. ax위에 그래프를 그린다.\nax.plot([1, 2, 4, 9, 5, 3]) ## ax에 일을 시킨다.\n\n# 3. 그래프를 화면에 출력.\nplt.show()\n\n\n\n\n\npyplot과 동일한 형태의 그래프가 그려집니다.\n\nfig, ax를 선언하느라 한 줄을 더 입력해야 한다는 불편함이 있지만 ax 객체가 있어 그래프를 제어하기 더 쉬워집니다.\n많은 경우 fig, ax = plt.subplots() 대신 ax = plt.subplot()으로 해도 됩니다.\n그러나 fig 대상 명령(예. savefig)을 사용해야 할 때도 있고, 두 가지를 따로 외우려면 혼동이 되니 한 가지로 통일하는 것이 좋습니다.\n\n\n# pyplot\n\nplt.plot([-3, -2, 5, 0], [1, 6, 4, 3])\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\nax.plot([-3, -2, 5, 0], [1, 6, 4, 3])\nplt.show()\n\n\n\n\n이번에는 수학적인 함수를 그려보겠습니다. NumPy의 linespace 함수를 사용하여 -2 ~ 2 범위에 속하는 500개의 부동소수로 구성된 x 배열을 생성합니다. 그 다음 x의 각 값의 거듭제곱된 값을 포함하는 y 배열을 생성합니다 (NumPy에 대하여 좀 더 알고 싶다면, NumPy 튜토리얼을 참고하시기 바랍니다).\n\nx = np.linspace(-2, 2, 401) # -2~2까지 400등분\nx\n\narray([-2.  , -1.99, -1.98, -1.97, -1.96, -1.95, -1.94, -1.93, -1.92,\n       -1.91, -1.9 , -1.89, -1.88, -1.87, -1.86, -1.85, -1.84, -1.83,\n       -1.82, -1.81, -1.8 , -1.79, -1.78, -1.77, -1.76, -1.75, -1.74,\n       -1.73, -1.72, -1.71, -1.7 , -1.69, -1.68, -1.67, -1.66, -1.65,\n       -1.64, -1.63, -1.62, -1.61, -1.6 , -1.59, -1.58, -1.57, -1.56,\n       -1.55, -1.54, -1.53, -1.52, -1.51, -1.5 , -1.49, -1.48, -1.47,\n       -1.46, -1.45, -1.44, -1.43, -1.42, -1.41, -1.4 , -1.39, -1.38,\n       -1.37, -1.36, -1.35, -1.34, -1.33, -1.32, -1.31, -1.3 , -1.29,\n       -1.28, -1.27, -1.26, -1.25, -1.24, -1.23, -1.22, -1.21, -1.2 ,\n       -1.19, -1.18, -1.17, -1.16, -1.15, -1.14, -1.13, -1.12, -1.11,\n       -1.1 , -1.09, -1.08, -1.07, -1.06, -1.05, -1.04, -1.03, -1.02,\n       -1.01, -1.  , -0.99, -0.98, -0.97, -0.96, -0.95, -0.94, -0.93,\n       -0.92, -0.91, -0.9 , -0.89, -0.88, -0.87, -0.86, -0.85, -0.84,\n       -0.83, -0.82, -0.81, -0.8 , -0.79, -0.78, -0.77, -0.76, -0.75,\n       -0.74, -0.73, -0.72, -0.71, -0.7 , -0.69, -0.68, -0.67, -0.66,\n       -0.65, -0.64, -0.63, -0.62, -0.61, -0.6 , -0.59, -0.58, -0.57,\n       -0.56, -0.55, -0.54, -0.53, -0.52, -0.51, -0.5 , -0.49, -0.48,\n       -0.47, -0.46, -0.45, -0.44, -0.43, -0.42, -0.41, -0.4 , -0.39,\n       -0.38, -0.37, -0.36, -0.35, -0.34, -0.33, -0.32, -0.31, -0.3 ,\n       -0.29, -0.28, -0.27, -0.26, -0.25, -0.24, -0.23, -0.22, -0.21,\n       -0.2 , -0.19, -0.18, -0.17, -0.16, -0.15, -0.14, -0.13, -0.12,\n       -0.11, -0.1 , -0.09, -0.08, -0.07, -0.06, -0.05, -0.04, -0.03,\n       -0.02, -0.01,  0.  ,  0.01,  0.02,  0.03,  0.04,  0.05,  0.06,\n        0.07,  0.08,  0.09,  0.1 ,  0.11,  0.12,  0.13,  0.14,  0.15,\n        0.16,  0.17,  0.18,  0.19,  0.2 ,  0.21,  0.22,  0.23,  0.24,\n        0.25,  0.26,  0.27,  0.28,  0.29,  0.3 ,  0.31,  0.32,  0.33,\n        0.34,  0.35,  0.36,  0.37,  0.38,  0.39,  0.4 ,  0.41,  0.42,\n        0.43,  0.44,  0.45,  0.46,  0.47,  0.48,  0.49,  0.5 ,  0.51,\n        0.52,  0.53,  0.54,  0.55,  0.56,  0.57,  0.58,  0.59,  0.6 ,\n        0.61,  0.62,  0.63,  0.64,  0.65,  0.66,  0.67,  0.68,  0.69,\n        0.7 ,  0.71,  0.72,  0.73,  0.74,  0.75,  0.76,  0.77,  0.78,\n        0.79,  0.8 ,  0.81,  0.82,  0.83,  0.84,  0.85,  0.86,  0.87,\n        0.88,  0.89,  0.9 ,  0.91,  0.92,  0.93,  0.94,  0.95,  0.96,\n        0.97,  0.98,  0.99,  1.  ,  1.01,  1.02,  1.03,  1.04,  1.05,\n        1.06,  1.07,  1.08,  1.09,  1.1 ,  1.11,  1.12,  1.13,  1.14,\n        1.15,  1.16,  1.17,  1.18,  1.19,  1.2 ,  1.21,  1.22,  1.23,\n        1.24,  1.25,  1.26,  1.27,  1.28,  1.29,  1.3 ,  1.31,  1.32,\n        1.33,  1.34,  1.35,  1.36,  1.37,  1.38,  1.39,  1.4 ,  1.41,\n        1.42,  1.43,  1.44,  1.45,  1.46,  1.47,  1.48,  1.49,  1.5 ,\n        1.51,  1.52,  1.53,  1.54,  1.55,  1.56,  1.57,  1.58,  1.59,\n        1.6 ,  1.61,  1.62,  1.63,  1.64,  1.65,  1.66,  1.67,  1.68,\n        1.69,  1.7 ,  1.71,  1.72,  1.73,  1.74,  1.75,  1.76,  1.77,\n        1.78,  1.79,  1.8 ,  1.81,  1.82,  1.83,  1.84,  1.85,  1.86,\n        1.87,  1.88,  1.89,  1.9 ,  1.91,  1.92,  1.93,  1.94,  1.95,\n        1.96,  1.97,  1.98,  1.99,  2.  ])\n\n\n\n# pyplot\n\nimport numpy as np\nx = np.linspace(-2, 2, 500)\ny = x**2\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\n\nax.plot(x, y)\n\nplt.show()\n\n\n\n\n그래프가 약간은 삭막해 보입니다. 타이틀과 x 및 y축에 대한 라벨, 그리고 모눈자를 추가적으로 그려보겠습니다.\n\n# pyplot\n\nplt.plot(x, y)\nplt.title(\"Square function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y = x**2\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\nobject-oriented API는 축 이름과 같은 설정 명령어가 pyplot과 다소 다릅니다.\n\n대체로 축 이름(label), 범위(limits) 등을 지정하는 명령어는 set_대상(), 거꾸로 그래프에서 설정값을 가져오는 명령어는 get_대상()으로 통일되어 있습니다.\n\n개인적으로 pyplot의 명령어 체계보다 object-oriented API의 체계를 선호합니다.\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\n\nax.plot(x, y)\nax.set_title(\"Square function\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y = x**2\")\nax.grid(True)\n\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#한글오류",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#한글오류",
    "title": "3wk-1 그래프2",
    "section": "한글오류",
    "text": "한글오류\n\nimport matplotlib as mpl\nplt.rc('font', family='NanumGothic')\nmpl.rcParams['axes.unicode_minus'] = False\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\n\nax.plot(x, y)\nax.set_title(\"2차함수\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y = $x^2$\")\nax.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n그래프 title에 수식쓰는 법\n\n\n\n따옴표 내부에 달러기호 안에 수식을 쓰면 된다."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#선의-스타일과-색상",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#선의-스타일과-색상",
    "title": "3wk-1 그래프2",
    "section": "선의 스타일과 색상",
    "text": "선의 스타일과 색상\n\n# pyplot\n\nplt.plot([0, 100, 100, 0, 0, 100, 50, 0, 100], [0, 0, 100, 100, 0, 100, 130, 100, 0])\nplt.axis([-10, 110, -10, 140])\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\n\nax.plot([0, 100, 100, 0, 0, 100, 50, 0, 100], [0, 0, 100, 100, 0, 100, 130, 100, 0])\nax.set_xlim(-10, 110)\nax.set_ylim(-10, 140)\n\n# 그래프의 범위는 pyplot과 같이 ax.axis([-10, 110, -10, 140]) 으로 지정할 수 있습니다.\n# 하지만 위와 같이 set_xlim, set_ylim을 사용해서 명시하는 것이 더 체계적으로 느껴집니다.\n\nplt.show()\n\n\n\n\n세번째 파라미터를 지정하면 선의 스타일과 색상을 바꿀 수 있습니다. 예를들어 “g–”는 “초록색 파선”을 의미합니다.\n\n# pyplot\n\nplt.plot([0, 100, 100, 0, 0], [0, 0, 100, 100, 0], \"r-\", [0, 100, 50, 0, 100], [0, 100, 130, 100, 0], \"g--\")\nplt.axis([-10, 110, -10, 140])\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\n\nax.plot([0,100,100,0,0], [0,0,100,100,0], \"r-\", [0,100, 50,0,100],[0,100,130,100,0],\"g--\")\nax.set_xlim(-10, 110)\nax.set_ylim(-10, 140)\n\nplt.show()\n\n\n\n\n또는 show를 호출하기 전 plot을 여러번 호출해도 가능합니다.\n\n# pyplot\n\nplt.plot([0, 100, 100, 0, 0], [0, 0, 100, 100, 0], \"r--\")\nplt.plot([0, 100, 50, 0, 100], [0, 100, 130, 100, 0], \"g--\")\nplt.axis([-10, 110, -10, 140])\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\n\nax.plot([0, 100, 100, 0, 0], [0, 0, 100, 100, 0], \"r-\", [0, 100, 50, 0, 100], [0, 100, 130, 100, 0], \"g--\")\nax.set_xlim(-10, 110)\nax.set_ylim(-10, 140)\n\nplt.show()\n\n\n\n\n또는 show를 호출하기 전 plot을 여러번 호출해도 가능합니다.\n\nfigs, axes = plt.subplots(1,2)\naxes[0].plot([0,100,100,0,0],[0,0,100,100,0],'--r')\naxes[1].plot([0,100,100,0,0],[0,0,100,100,0],'bv')\n\n\n\n\n선 대신에 간단한 점을 그려보는 것도 가능합니다. 아래는 초록색 파선, 빨강 점선, 파랑 삼각형의 예를 보여줍니다. 공식 문서에서 사용 가능한 스타일 및 색상의 모든 옵션을 확인해 볼 수 있습니다.\n\n# pyplot\n\nx = np.linspace(-1.4, 1.4, 30)\nplt.plot(x, x, 'g--', x, x**2, 'r:', x, x**3, 'b^')\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\n\nx = np.linspace(-1.4, 1.4, 30)\n\nax.plot(x, x, 'g--')\nax.plot(x, x**2, 'r:')\nax.plot(x, x**3, 'b^')\n\n# 여러 그래프를 ax.plot(x, x, 'g--', x, x**2, 'r:', x, x**3, 'b^')과 같이 한 줄에 그릴 수도 있습니다.\n# 그러나 이와 같이 따로 떼서 그리면 혼동을 방지할 수 있습니다.\n# 이는 pyplot도 마찬가지입니다.\n\nplt.show()\n\n\n\n\nplot 함수는 Line2D객체로 구성된 리스트를 반환합니다 (각 객체가 각 선에 대응됩니다). 이 선들에 대한 추가적인 속성을 설정할 수도 있습니다. 가령 선의 두께, 스타일, 투명도 같은것의 설정이 가능합니다. 공식 문서에서 설정 가능한 모든 속성을 확인해볼 수 있습니다.\n\n# pyplot\n\nx = np.linspace(-1.4, 1.4, 30)\nline1, line2, line3 = plt.plot(x, x, 'g--', x, x**2, 'r:', x, x**3, 'b^')\nline1.set_linewidth(3.0) # line1의 선의두께\nline1.set_dash_capstyle(\"round\") # line1의 선끝을 각지게할건지 둥글게 할건지?\nline3.set_alpha(0.2) # line3에 투명도\nplt.show()\n\n\n\n\n\n# object oriented API\n\nx = np.linspace(-1.4, 1.4, 30)\n\nfig, ax = plt.subplots()\n\n# plot을 나누어 그리면 어디에 어떤 설정이 적용되었는지 알아보기 편합니다.\n# linewidth, alpha와 같은 line style도 plot() 안에 넣으면 혼동을 방지할 수 있습니다.\nline1 = ax.plot(x, x, 'g--', linewidth=3, dash_capstyle='round')\nline2 = ax.plot(x, x**2, 'r:')\nline3 = ax.plot(x, x**3, 'b^', alpha=0.2)\n\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#그림-저장",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#그림-저장",
    "title": "3wk-1 그래프2",
    "section": "그림 저장",
    "text": "그림 저장\n그래프를 그림파일로 저장하는 방법은 간단합니다. 단순히 파일이름을 지정하여 savefig 함수를 호출해 주기만 하면 됩니다. 가능한 이미지 포맷은 사용하는 그래픽 백엔드에 따라서 지원 여부가 결정됩니다.\n\n# savefig()는 pyplot과 object oriented API 모두 동일합니다.\n\nx = np.linspace(-1.4, 1.4, 30)\nplt.plot(x, x**2)\nplt.savefig(\"my_square_function.png\", transparent=True)"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#부분-그래프-subplot",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#부분-그래프-subplot",
    "title": "3wk-1 그래프2",
    "section": "부분 그래프 (subplot)",
    "text": "부분 그래프 (subplot)\nmatplotlib는 하나의 그림(figure)에 여러개의 부분 그래프를 포함할 수 있습니다. 이 부분 그래프는 격자 형식으로 관리됩니다. subplot 함수를 호출하여 부분 그래프를 생성할 수 있습니다. 이 때 격자의 행/열의 수 및 그래프를 그리고자 하는 부분 그래프의 색인을 파라미터로서 지정해줄 수 있습니다 (색인은 1부터 시작하며, 좌-&gt;우, 상단-&gt;하단의 방향입니다). * pyplot은 현재 활성화된 부분 그래프를 계속해서 추적합니다 (plt.gca()를 호출하여 해당 부분 그래프의 참조를 얻을 수 있습니다). 따라서, plot 함수를 호출할 때 활성화된 부분 그래프에 그림이 그려지게 됩니다.\n\n이제현 주 :\n* object oriented API 방식에서는 그래프를 그리기 전에 먼저 틀을 잡아둡니다. 그래프를 그릴 때 사전에 정의된 영역 중 어디에 그래프를 그릴지 지정하는 방식입니다.\n* pyplot의 plt.gca()가 바로 object oriented API의 axes입니다.\n\n\n# pyplot\n\nx = np.linspace(-1.4, 1.4, 30)\n\n# subplot(2,2,1)은 subplot(221)로 축약할 수 있습니다.\nplt.subplot(2, 2, 1)  # 2 행 2 열 크기의 격자 중 첫 번째 부분 그래프 = 좌측 상단\nplt.plot(x, x)\nplt.subplot(2, 2, 2)  # 2 행 2 열 크기의 격자 중 두 번째 부분 그래프 = 우측 상단\nplt.plot(x, x**2)\nplt.subplot(2, 2, 3)  # 2 행 2 열 크기의 격자 중 세 번째 부분 그래프 = 좌측 하단\nplt.plot(x, x**3)\nplt.subplot(2, 2, 4)  # 2 행 2 열 크기의 격자 중 네 번째 부분 그래프 = 우측 하단\nplt.plot(x, x**4)\nplt.show()\n\n\n\n\n\n# object oriented API\n\nx = np.linspace(-1.4, 1.4, 30)\n\nfig, ax = plt.subplots(2, 2) # 순서대로 row의 갯수, col의 갯수입니다. nrows=2, cols=2로 지정할 수도 있습니다.\n\n# plot위치는 ax[row, col] 또는 ax[row][col]로 지정합니다.\nax[0, 0].plot(x, x)      # 2 행 2 열 크기의 격자 중 첫 번째 부분 그래프 = 좌측 상단\nax[0, 1].plot(x, x**2)   # 2 행 2 열 크기의 격자 중 두 번째 부분 그래프 = 우측 상단\nax[1, 0].plot(x, x**3)   # 2 행 2 열 크기의 격자 중 세 번째 부분 그래프 = 좌측 하단\nax[1, 1].plot(x, x**4)   # 2 행 2 열 크기의 격자 중 네 번째 부분 그래프 = 우측 하단\n\nplt.show()\n\n\n\n\n격자의 여러 영역으로 확장된 부분 그래프를 생성하는 것도 쉽습니다.\n\n# pyplot\n\nplt.subplot(2, 2, 1)  # 2 행 2 열 크기의 격자 중 첫 번째 부분 그래프 = 좌측 상단\nplt.plot(x, x)\nplt.subplot(2, 2, 2)  # 2 행 2 열 크기의 격자 중 두 번째 부분 그래프 = 우측 상단\nplt.plot(x, x**2)\nplt.subplot(2, 1, 2)  # 2행 *1* 열의 두 번째 부분 그래프 = 하단\n                      # 2행 1열 크기의 그래프가 두 개 그려질 수 있지만,\n                      # 상단 부분은 이미 두 개의 부분 그래프가 차지하였다.\n                      # 따라서, 두 번째 부분 그래프로 지정함\nplt.plot(x, x**3)\nplt.show()\n\n\n\n\n\n# object oriented API\n\ngrid = plt.GridSpec(2, 2)  # 2행 2열 크기의 격?자를 준비합니다.\n\nax1 = plt.subplot(grid[0, 0])  # 2행 2열 크기의 격자 중 첫 번째 부분 그래프 = 좌측 상단\nax2 = plt.subplot(grid[0, 1])  # 2행 2열 크기의 격자 중 두 번째 부분 그래프 = 우측 상단\nax3 = plt.subplot(grid[1, 0:]) # 2행 *1*열의 두 번째 부분 그래프 = 하단\n                               # 범위를 [1, 0:]으로 설정하여 2행 전체를 지정함.\n\nax1.plot(x, x)\nax2.plot(x, x**2)\nax3.plot(x, x**3)\n\nplt.show()\n\n\n\n\n보다 복잡한 부분 그래프의 위치 선정이 필요하다면, subplot2grid를 대신 사용할 수 있습니다. 격자의 행과 열의 번호 및 격자에서 해당 부분 그래프를 그릴 위치를 지정해줄 수 있습니다 (좌측상단 = (0,0). 또한 몇 개의 행/열로 확장되어야 하는지도 추가적으로 지정할 수 있습니다. 아래는 그에 대한 예를 보여줍니다:\n\n# pyplot\n\nplt.subplot2grid((3,3), (0, 0), rowspan=2, colspan=2)\nplt.plot(x, x**2)\nplt.subplot2grid((3,3), (0, 2))\nplt.plot(x, x**3)\nplt.subplot2grid((3,3), (1, 2), rowspan=2)\nplt.plot(x, x**4)\nplt.subplot2grid((3,3), (2, 0), colspan=2)\nplt.plot(x, x**5)\nplt.show()\n\n\n\n\n\n# object oriented API\n\ngridsize = (3, 3)     # 2행 2열 크기의 격자를 준비합니다.\nax1 = plt.subplot2grid(gridsize, (0,0), rowspan=2, colspan=2)\nax2 = plt.subplot2grid(gridsize, (0,2))\nax3 = plt.subplot2grid(gridsize, (1,2), rowspan=2)\nax4 = plt.subplot2grid(gridsize, (2,0), colspan=2)\n\nax1.plot(x, x**2)\nax2.plot(x, x**3)\nax3.plot(x, x**4)\nax4.plot(x, x**5)\n\nplt.show()\n\n\n\n\n보다 유연한 부분그래프 위치선정이 필요하다면, GridSpec 문서를 확인해 보시길 바랍니다."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#여러개의-그림-figure",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#여러개의-그림-figure",
    "title": "3wk-1 그래프2",
    "section": "여러개의 그림 (figure)",
    "text": "여러개의 그림 (figure)\n여러개의 그림을 그리는것도 가능합니다. 각 그림은 하나 이상의 부분 그래프를 가질 수 있습니다. 기본적으로는 matplotlib이 자동으로 figure(1)을 생성합니다. 그림간 전환을 할 때, pyplot은 현재 활성화된 그림을 계속해서 추적합니다 (이에대한 참조는 plt.gcf()의 호출로 알 수 있습니다). 또한 활성화된 그림의 활성화된 부분 그래프가 현재 그래프가 그려질 부분 그래프가 됩니다.\n\n이제현 주 : * object oriented API에서는 실행 순이 아니라 객체를 중심으로 명령을 실행합니다. * 다른 그림을 그리다가 앞서 그림을 추가할 때 pyplot에서 plt.figure() 명령으로 위 그림을 호출하는 대신 object oriented API는 목표 Axes를 지정하여 추가합니다.\n\n\n# pyplot\n\nx = np.linspace(-1.4, 1.4, 30)\n\nplt.figure(1)\nplt.subplot(211)\nplt.plot(x, x**2)\nplt.title(\"Square and Cube\")\nplt.subplot(212)\nplt.plot(x, x**3)\n\nplt.figure(2, figsize=(10, 5))\nplt.subplot(121)\nplt.plot(x, x**4)\nplt.title(\"y = x**4\")\nplt.subplot(122)\nplt.plot(x, x**5)\nplt.title(\"y = x**5\")\n\nplt.figure(1)      # 그림 1로 돌아가며, 활성화된 부분 그래프는 212 (하단)이 됩니다\nplt.plot(x, -x**3, \"r:\")\n\nplt.show()\n\n\n\n\n\n\n\n\n# object oriented API\n\nx = np.linspace(-1.4, 1.4, 30)\n\nfig1, ax1 = plt.subplots(nrows=2, ncols=1)\n\nax1[0].plot(x, x**2)\nax1[0].set_title(\"Square and Cube\")\n\nax1[1].plot(x, x**3)\n\n\nfig2, ax2 = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\nax2[0].plot(x, x**4)\nax2[0].set_title(\"y = x**4\")\n\nax2[1].plot(x, x**5)\nax2[1].set_title(\"y = x**5\")\n\nax1[1].plot(x, -x**3, \"r:\")    # 그림 1로 돌아가며, 활성화된 부분 그래프는 ax1[1] (하단)이 됩니다.\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#pyplot의-상태-머신-암시적-vs-명시적",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#pyplot의-상태-머신-암시적-vs-명시적",
    "title": "3wk-1 그래프2",
    "section": "Pyplot의 상태 머신: 암시적 vs 명시적",
    "text": "Pyplot의 상태 머신: 암시적 vs 명시적\n지금까지 현재의 활성화된 부분 그래프를 추적하는 Pyplot의 상태 머신을 사용했었습니다. plot 함수를 호출할 때마다 pyplot은 단지 현재 활성화된 부분 그래프에 그림을 그립니다. 그리고 plot 함수를 호출 할 때, 그림 및 부분 그래프가 아직 존재하지 않는다면 이들을 만들어내는 마법같은(?) 작업도 일부 수행합니다. 이는 주피터와 같은 대화식의 환경에서 편리합니다.\n그러나 프로그램을 작성하는 것이라면, 명시적인 것이 암시적인것 보다 더 낫습니다. 명시적인 코드는 일반적으로 디버깅과 유지보수가 더 쉽습니다. 이 말에 동의하지 않는다면, Python 젠(Zen)의 두 번째 규칙을 읽어보시기 바랍니다.\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\n아름다움이 추한 것보다 낫다.\n명확함이 함축된 것보다 낫다.\n단순함이 복잡한 것보다 낫다.\n복잡함이 난해한 것보다 낫다.\n단조로움이 중접된 것보다 낫다.\n여유로움이 밀집된 것보다 낫다.\n가독성은 중요하다.\n비록 실용성이 이상을 능가한다 하더라도 규칙을 깨야할 정도로 특별한 경우란 없다.\n알고도 침묵하지 않는 한 오류는 결코 조용히 지나가지 않는다.\n모호함을 마주하고 추측하려는 유혹을 거절하라. 비록 당신이 우둔해서 처음에는 명백해 보이지 않을 수도 있겠지만 문제를 해결할 하나의 - 바람직하고 유일한 - 명백한 방법이 있을 것이다.\n비록 하지않는 것이 지금 하는 것보다 나을 때도 있지만 지금 하는 것이 전혀 안하는 것보다 낫다.\n설명하기 어려운 구현이라면 좋은 아이디어가 아니다. 쉽게 설명할 수 있는 구현이라면 좋은 아이디어일 수 있다. 네임스페이스는 정말 대단한 아이디어다. – 자주 사용하자!\nfrom 출처\n다행히도 Pyplot은 상태 머신을 완전히 무시할 수 있게끔 해 줍니다. 따라서 아름다운 명시적 코드를 작성하는것이 가능하죠. 간단히 subplots 함수를 호출해서 반환되는 figure 객체 및 축의 리스트를 사용하면 됩니다*. 마법은 더 이상 없습니다!\n\n이제현 주: * 여기서 설명하는 부분이 matplotlib의 object oriented API(객체지향 인터페이스)입니다.\n\n아래는 이에 대한 예 입니다:\n\nx = np.linspace(-2, 2, 200)\nfig1, (ax_top, ax_bottom) = plt.subplots(2, 1, sharex=True)\nfig1.set_size_inches(10,5)\nline1, line2 = ax_top.plot(x, np.sin(3*x**2), \"r-\", x, np.cos(5*x**2), \"b-\")\nline3, = ax_bottom.plot(x, np.sin(3*x), \"r-\")\nax_top.grid(True)\n\nfig2, ax = plt.subplots(1, 1)\nax.plot(x, x**2)\nplt.show()\n\n\n\n\n\n\n\n일관성을 위해서 이 튜토리얼의 나머지 부분에서는 pyplot의 상태 머신을 계속해서 사용할 것입니다. 그러나 프로그램에서는 객체지향 인터페이스의 사용을 권장하고 싶습니다."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#pylab-vs-pyplot-vs-matplotlib",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#pylab-vs-pyplot-vs-matplotlib",
    "title": "3wk-1 그래프2",
    "section": "Pylab vs Pyplot vs Matplotlib",
    "text": "Pylab vs Pyplot vs Matplotlib\npylab, pyplot, matplotlib 간의 관계에대한 혼동이 있습니다. 그러나 이들의 관계는 매우 단순합니다: matplotlib은 완전한 라이브러리이며, pylab 및 pyplot을 포함한 모든것을 가지고 있습니다.\nPyplot은 그래프를 그리기위한 다양한 도구를 제공합니다. 여기에는 내부적인 객체지향적인 그래프 그리기 라이브러리에 대한 상태 머신 인터페이스도 포함됩니다.\nPylab은 mkatplotlib.pyplot 및 NumPy를 단일 네임스페이스로 임포트하는 편리성을 위한 모듈입니다. 인터넷에 떠도는 pylab을 사용하는 여러 예제를 보게 될 것입니다. 그러나 이는 더이상 권장되는 사용방법은 아닙니다 (왜냐하면 명시적인 임포트가 암시적인것 보다 더 낫기 때문입니다).\n\n이제현 주 : * Pylab, Pyplot, Object oriented API의 관계는 여기를 참고하십시오"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#텍스트-그리기",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#텍스트-그리기",
    "title": "3wk-1 그래프2",
    "section": "텍스트 그리기",
    "text": "텍스트 그리기\ntext 함수를 호출하여 텍스트를 그래프의 원하는 위치에 추가할 수 있습니다. 출력을 원하는 텍스트와 수평 및 수직 좌표를 지정하고, 추가적으로 몇 가지 속성을 지정해 주기만 하면 됩니다. matplotlib의 모든 텍스트는 TeX 방정식 표현을 포함할 수 있습니다. 더 자세한 내용은 공식 문서를 참조하시기 바랍니다.\n\n# pyplot\n\nx = np.linspace(-1.5, 1.5, 30)\npx = 0.8\npy = px**2\n\nplt.plot(x, x**2, \"b-\", px, py, \"ro\")\n\nplt.text(0, 1.5, \"Square function\\n$y = x^2$\", fontsize=20, color='blue', horizontalalignment=\"center\")\nplt.text(px - 0.08, py, \"Beautiful point\", ha=\"right\", weight=\"heavy\")\nplt.text(px, py, \"x = %0.2f\\ny = %0.2f\"%(px, py), rotation=50, color='gray')\n\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\n\nax.plot(x, x**2, \"b-\")\nax.plot(px, py, \"ro\")\n\nax.text(0, 1.5, \"Square function\\n$y = x^2$\", fontsize=20, color='blue', horizontalalignment=\"center\")\nax.text(px - 0.08, py, \"Beautiful point\", ha=\"right\", weight=\"heavy\")\nax.text(px, py, \"x = %0.2f\\ny = %0.2f\"%(px, py), rotation=50, color='gray')\n\nplt.show()\n\n\n\n\n\n알아둘 것: ha는 horizontalalignment(수평정렬)의 이명 입니다.\n\n더 많은 텍스트 속성을 알고 싶다면, 공식 문서를 참조하시기 바랍니다.\n아래 그래프의 “beautiful point” 같은 텍스트 처럼, 그래프의 요소에 주석을 다는것은 꽤 흔한 일입니다. annotate 함수는 이를 쉽게 할 수 있게 해 줍니다: 관심있는 부분의 위치를 지정하고, 텍스트의 위치를 지정합니다. 그리고 텍스트 및 화살표에 대한 추가적인 속성도 지정해줄 수 있습니다.\n\n# pyplot\n\nplt.plot(x, x**2, px, py, \"ro\")\nplt.annotate(\"Beautiful point\", xy=(px, py), xytext=(px-1.3,py+0.5),\n                           color=\"green\", weight=\"heavy\", fontsize=14,\n                           arrowprops={\"facecolor\": \"lightgreen\"})\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\nax.plot(x, x**2, px, py, \"ro\")\nax.annotate(\"Beautiful point\", xy=(px, py), xytext=(px-1.3,py+0.5),\n                           color=\"green\", weight=\"heavy\", fontsize=14,\n                           arrowprops={\"facecolor\": \"lightgreen\"})\nplt.show()\n\n\n\n\nbbox 속성을 사용하면, 텍스트를 포함하는 사각형을 그려볼 수도 있습니다:\n\n# pyplot\n\nplt.plot(x, x**2, px, py, \"ro\")\n\nbbox_props = dict(boxstyle=\"rarrow,pad=0.3\", ec=\"b\", lw=2, fc=\"lightblue\")\nplt.text(px-0.2, py, \"Beautiful point\", bbox=bbox_props, ha=\"right\")\n\nbbox_props = dict(boxstyle=\"round4,pad=1,rounding_size=0.2\", ec=\"black\", fc=\"#EEEEFF\", lw=5)\nplt.text(0, 1.5, \"Square function\\n$y = x^2$\", fontsize=20, color='black', ha=\"center\", bbox=bbox_props)\n\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\nax.plot(x, x**2)\nax.plot(px, py, \"ro\")\n\nbbox_props = dict(boxstyle=\"rarrow,pad=0.3\", ec=\"b\", lw=2, fc=\"lightblue\")\nax.text(px-0.2, py, \"Beautiful point\", bbox=bbox_props, ha=\"right\")\n\nbbox_props = dict(boxstyle=\"round4,pad=1,rounding_size=0.2\", ec=\"black\", fc=\"#EEEEFF\", lw=5)\nax.text(0, 1.5, \"Square function\\n$y = x^2$\", fontsize=20, color='black', ha=\"center\", bbox=bbox_props)\n\nplt.show()\n\n\n\n\n재미를 위해서 xkcd 스타일의 그래프를 그려보고 싶다면, with plt.xkcd() 섹션 블록을 활용할 수도 있습니다:\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nwith plt.xkcd():\n    plt.plot(x, x**2, px, py, \"ro\")\n\n    bbox_props = dict(boxstyle=\"rarrow,pad=0.3\", ec=\"b\", lw=2, fc=\"lightblue\")\n    plt.text(px-0.2, py, \"Beautiful point\", bbox=bbox_props, ha=\"right\")\n\n    bbox_props = dict(boxstyle=\"round4,pad=1,rounding_size=0.2\", ec=\"black\", fc=\"#EEEEFF\", lw=5)\n    plt.text(0, 1.5, \"Square function\\n$y = x^2$\", fontsize=20, color='black', ha=\"center\", bbox=bbox_props)\n\n    plt.show()\n\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found.\nfindfont: Font family 'xkcd' not found.\nfindfont: Font family 'xkcd Script' not found.\nfindfont: Font family 'Humor Sans' not found.\nfindfont: Font family 'Comic Neue' not found.\nfindfont: Font family 'Comic Sans MS' not found."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#범례-legends",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#범례-legends",
    "title": "3wk-1 그래프2",
    "section": "범례 (Legends)",
    "text": "범례 (Legends)\n범례를 추가하는 가장 간단한 방법은 모든 선에 라벨을 설정해주고, legend 함수를 호출하는 것입니다.\n\n# pyplot\n\nx = np.linspace(-1.4, 1.4, 50)\nplt.plot(x, x**2, \"r--\", label=\"Square function\")\nplt.plot(x, x**3, \"g-\", label=\"Cube function\")\nplt.legend(loc=\"best\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n# object oriented API\n\nx = np.linspace(-1.4, 1.4, 50)\n\nfig, ax = plt.subplots()\n\nax.plot(x, x**2, \"r--\", label=\"Square function\")\nax.plot(x, x**3, \"g-\", label=\"Cube function\")\nax.legend(loc=\"best\")\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#비선형-척도",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#비선형-척도",
    "title": "3wk-1 그래프2",
    "section": "비선형 척도",
    "text": "비선형 척도\nMatplotlib은 로그, 로짓(logit)과 같은 비선형 척도를 지원합니다.\n\n# pyplot\n\nx = np.linspace(0.1, 15, 500)\ny = x**3/np.exp(2*x)\n\nplt.figure(1)\nplt.plot(x, y)\nplt.yscale('linear')\nplt.title('linear')\nplt.grid(True)\n\nplt.figure(2)\nplt.plot(x, y)\nplt.yscale('log')\nplt.title('log')\nplt.grid(True)\n\nplt.figure(3)\nplt.plot(x, y)\nplt.yscale('logit')\nplt.title('logit')\nplt.grid(True)\n\nplt.figure(4)\nplt.plot(x, y - y.mean())\nplt.yscale('symlog', linthresh=0.05)\nplt.title('symlog')\nplt.grid(True)\n\nplt.show()\n\n\n\n\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol.\nFont 'default' does not have a glyph for '\\u2212' [U+2212], substituting with a dummy symbol."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#틱과-틱커-ticks-and-tickers",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#틱과-틱커-ticks-and-tickers",
    "title": "3wk-1 그래프2",
    "section": "틱과 틱커 (Ticks and tickers)",
    "text": "틱과 틱커 (Ticks and tickers)\n각 축에는 “틱(ticks)”이라는 작은 표시가 있습니다. 정확히 말하자면, “틱”은 표시(예. (-1, 0, 1))의 위치”이며, 틱 선은 그 위치에 그려지는 작은 선입니다. 또한 “틱 라벨”은 틱 선 옆에 그려지는 라벨이며, “틱커”는 틱의 위치를 결정하는 객체 입니다. 기본적인 틱커는 ~5 에서 8 틱을 위치시키는데 꽤 잘 작동합니다. 즉, 틱 서로간에 적당한 거리를 표현합니다.\n하지만, 가끔은 좀 더 이를 제어할 필요가 있습니다 (예. 위의 로짓 그래프에서는 너무 많은 틱 라벨이 있습니다). 다행히도 matplotlib은 틱을 완전히 제어하는 방법을 제공합니다. 심지어 보조 눈금(minor tick)을 활성화 할 수도 있습니다.\n\n# pyplot\n\n# 이제현 주: 사실상 object oriented API 입니다.\n\nx = np.linspace(-2, 2, 100)\n\nplt.figure(1, figsize=(15,10))\nplt.subplot(131)\nplt.plot(x, x**3)\nplt.grid(True)\nplt.title(\"Default ticks\")\n\nax = plt.subplot(132)\nplt.plot(x, x**3)\nax.xaxis.set_ticks(np.arange(-2, 2, 1))\nplt.grid(True)\nplt.title(\"Manual ticks on the x-axis\")\n\nax = plt.subplot(133)\nplt.plot(x, x**3)\nplt.minorticks_on()\nax.tick_params(axis='x', which='minor', bottom='off')\nax.xaxis.set_ticks([-2, 0, 1, 2])\nax.yaxis.set_ticks(np.arange(-5, 5, 1))\nax.yaxis.set_ticklabels([\"min\", -4, -3, -2, -1, 0, 1, 2, 3, \"max\"])\nplt.title(\"Manual ticks and tick labels\\n(plus minor ticks) on the y-axis\")\n\n\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n# object oriented API\n\n# 위 pyplot 예제는 사실상 object oriented API 입니다.\n# 여기에서는 같은 기능을 더 단순한 코드로 구현하였습니다\n\nx = np.linspace(-2, 2, 100)\n\nfig, ax = plt.subplots(ncols=3, figsize=(15, 10))\n\nax[0].plot(x, x**3)\nax[0].grid(True)\nax[0].set_title(\"Default ticks\")\n\nax[1].plot(x, x**3)\nax[1].grid(True)\nax[1].set_xticks(np.arange(-2, 2, 1))\nax[1].set_title(\"Manual ticks on the x-axis\")\n\nax[2].plot(x, x**3)\nax[2].grid(True)\nax[2].minorticks_on()\nax[2].set_xticks([-2, 0, 1, 2], minor=False)\nax[2].set_yticks(np.arange(-5, 5, 1))\nax[2].set_yticklabels([\"min\", -4, -3, -2, -1, 0, 1, 2, 3, \"max\"])\nax[2].set_title(\"Manual ticks and tick labels\\n(plus minor ticks) on the y-axis\")\n\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#극좌표계의-투영-polar-projection",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#극좌표계의-투영-polar-projection",
    "title": "3wk-1 그래프2",
    "section": "극좌표계의 투영 (Polar projection)",
    "text": "극좌표계의 투영 (Polar projection)\n극좌표계 그래프를 그리는 것은 매우 간단합니다. 부분 그래프를 생성할 때 projection 속성을 \"polar\"로 설정해 주기만 하면 됩니다.\n\n이제현 주:\n* object oriented API는 일반적으로 plt.subplots()로 Figure와 Axes 객체를 동시에 생성합니다. * plt.subplots()는 projection 속성을 가지고 있지 않습니다. * 따라서 projection을 사용할 때는 plt.figure()로 Figure 객체를 먼저 생성한 후 plt.subplot()이나 plt.add_subplot()으로 Axes 객체를 추가해 주거나, fig.subplots() 안에 subplot_kw=={'polar':True}로 지정해 주어야 합니다.\n\n\n# pyplot\n\nradius = 1\ntheta = np.linspace(0, 2*np.pi*radius, 1000)\n\nplt.subplot(111, projection='polar')\nplt.plot(theta, np.sin(5*theta), \"g-\")\nplt.plot(theta, 0.5*np.cos(20*theta), \"b-\")\nplt.show()\n\n\n\n\n\n# object oriented API\n\nradius = 1\ntheta = np.linspace(0, 2*np.pi*radius, 1000)\n\nfig = plt.figure()\nax = fig.add_subplot(projection='polar')\n\n# 또는, subplot_kw 를 이용해서 polar plot으로 설정합니다.\n# fig, ax = plt.subplots(subplot_kw={'polar':True})\n\nax.plot(theta, np.sin(5*theta), \"g-\")\nax.plot(theta, 0.5*np.cos(20*theta), \"b-\")\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#차원-투영",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#차원-투영",
    "title": "3wk-1 그래프2",
    "section": "3차원 투영",
    "text": "3차원 투영\n3차원 그래프를 그리는것은 꽤 간단합니다. 우선 \"3d\" 투영을 등록하는 Axes3D를 임포트 해줘야 합니다. 그리곤 projection 속성을 \"3d\"로 설정된 부분 그래프 생성합니다. 그러면 Axes3DSubplot 이라는 객체가 반환되는데, 이 객체의 plot_surface 메서드를 호출하면 x, y, z 좌표를 포함한 추가적이나 속성을 지정할 수 있습니다.\n\n# pyplot\n\n# 이제현 주: 사실상 object oriented API 입니다.\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.linspace(-5, 5, 50)\ny = np.linspace(-5, 5, 50)\nX, Y = np.meshgrid(x, y)\nR = np.sqrt(X**2 + Y**2)\nZ = np.sin(R)\n\nfigure = plt.figure(1, figsize = (12, 4))\nsubplot3d = plt.subplot(111, projection='3d')  # 이제현 주: Axes 객체입니다.\nsurface = subplot3d.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, linewidth=0.1)\nplt.show()\n\n\n\n\n동일한 데이터를 출력하는 또 다른 방법은 등고선도(contour plot)를 이용하는 것입니다.\n\n# pyplot\n\nplt.contourf(X, Y, Z, cmap=matplotlib.cm.coolwarm)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n# object oriented API\n\n# 이제현 주: 종종 object oriented API가 pyplot보다 불편할 때가 있습니다.\n#            contour plot의 colorbar는 무엇을 대상으로 할 지를 인자로 전달해야 합니다.\n\nfig, ax = plt.subplots()\ncontour = ax.contourf(X, Y, Z, cmap=matplotlib.cm.coolwarm)\nplt.colorbar(contour)\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#산점도-scatter-plot",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#산점도-scatter-plot",
    "title": "3wk-1 그래프2",
    "section": "산점도 (Scatter plot)",
    "text": "산점도 (Scatter plot)\n단순히 각 점에 대한 x 및 y 좌표를 제공하면 산점도를 그릴 수 있습니다.\n\n# pyplot\n\nfrom numpy.random import rand\nx, y = rand(2, 100)\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfrom numpy.random import rand\nx, y = rand(2, 100)\n\nfig, ax = plt.subplots()\nax.scatter(x, y)\nplt.show()\n\n\n\n\n부수적으로 각 점의 크기를 정할 수도 있습니다.\n\n# pyplot\n\nx, y, scale = rand(3, 100)\nscale = 500 * scale ** 5\nplt.scatter(x, y, s=scale)\nplt.show()\n\n\n\n\n\n# object oriented API\n\nx, y, scale = rand(3, 100)\nscale = 500 * scale ** 5\n\nfig, ax = plt.subplots()\nax.scatter(x, y, s=scale)\nplt.show()\n\n\n\n\n마찬가지로 여러 속성을 설정할 수 있습니다. 가령 테두리 및 모양의 내부 색상, 그리고 투명도와 같은것의 설정이 가능합니다.\n\n# pyplot\n\nfor color in ['red', 'green', 'blue']:\n    n = 100\n    x, y = rand(2, n)\n    scale = 500.0 * rand(n) ** 5\n    plt.scatter(x, y, s=scale, c=color, alpha=0.3, edgecolors='blue')\n\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\n\nfor color in ['red', 'green', 'blue']:\n    n = 100\n    x, y = rand(2, n)\n    scale = 500.0 * rand(n) ** 5\n    ax.scatter(x, y, s=scale, c=color, alpha=0.3, edgecolors='blue')\n\nax.grid(True)\n\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#선",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#선",
    "title": "3wk-1 그래프2",
    "section": "선",
    "text": "선\n지금까지 해온것 처럼 plot 함수를 사용하여 선을 그릴 수 있습니다. 하지만, 가끔은 그래프를 통과하는 무한한 선을 그리는 유틸리티 함수를 만들면 편리합니다 (기울기와 절편으로). 또한 hlines 및 vlines 함수를 사용하면, 아래와 같이 부분 수평 및 수직 선을 그릴 수도 있습니다:\n\n# pyplot\n\nfrom numpy.random import randn\n\ndef plot_line(axis, slope, intercept, **kargs):\n    xmin, xmax = axis.get_xlim()\n    plt.plot([xmin, xmax], [xmin*slope+intercept, xmax*slope+intercept], **kargs)\n\nx = randn(1000)\ny = 0.5*x + 5 + randn(1000)*2\nplt.axis([-2.5, 2.5, -5, 15])\nplt.scatter(x, y, alpha=0.2)\nplt.plot(1, 0, \"ro\")\nplt.vlines(1, -5, 0, color=\"red\")\nplt.hlines(0, -2.5, 1, color=\"red\")\nplot_line(axis=plt.gca(), slope=0.5, intercept=5, color=\"magenta\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfrom numpy.random import randn\n\n# Axis를 인자로 전달하여 함수 연산과 시각화를 수행합니다.\ndef plot_line(axis, slope, intercept, **kargs):\n    xmin, xmax = axis.get_xlim()\n    axis.plot([xmin, xmax], [xmin*slope+intercept, xmax*slope+intercept], **kargs)\n\nx = randn(1000)\ny = 0.5*x + 5 + randn(1000)*2\n\nfig, ax = plt.subplots()\n\nax.set_xlim(-2.5, 2.5)\nax.set_ylim(-5, 15)\nax.scatter(x, y, alpha=0.2)\nax.plot(1, 0, \"ro\")\nax.vlines(1, -5, 0, color=\"red\")\nax.hlines(0, -2.5, 1, color=\"red\")\nplot_line(axis=ax, slope=0.5, intercept=5, color=\"magenta\")\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#히스토그램",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#히스토그램",
    "title": "3wk-1 그래프2",
    "section": "히스토그램",
    "text": "히스토그램\n\n# pyplot\n\ndata = [1, 1.1, 1.8, 2, 2.1, 3.2, 3, 3, 3, 3]\nplt.subplot(211)\nplt.hist(data, bins = 10, rwidth=0.8)\n\nplt.subplot(212)\nplt.hist(data, bins = [1, 1.5, 2, 2.5, 3], rwidth=0.95)\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\n\nplt.show()\n\n\n\n\n\n# object oriented API\n\ndata = [1, 1.1, 1.8, 2, 2.1, 3.2, 3, 3, 3, 3]\n\nfig, ax = plt.subplots(2, 1)\nax[0].hist(data, bins = 10, rwidth=0.8)\n\nax[1].hist(data, bins = [1, 1.5, 2, 2.5, 3], rwidth=0.95)\nax[1].set_xlabel(\"Value\")\nax[1].set_ylabel(\"Frequency\")\n\nplt.show()\n\n\n\n\n\n# pyplot\n\ndata1 = np.random.randn(400)\ndata2 = np.random.randn(500) + 3\ndata3 = np.random.randn(450) + 6\n\nplt.hist(data1, bins=5, color='g', alpha=0.75, label='bar hist') # default histtype='bar'\nplt.hist(data2, color='b', alpha=0.65, histtype='stepfilled', label='stepfilled hist')\nplt.hist(data3, color='r', histtype='step', label='step hist')\n\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n# object oriented API\n\ndata1 = np.random.randn(400)\ndata2 = np.random.randn(500) + 3\ndata3 = np.random.randn(450) + 6\n\nfig, ax = plt.subplots()\nax.hist(data1, bins=5, color='g', alpha=0.75, label='bar hist') # default histtype='bar'\nax.hist(data2, color='b', alpha=0.65, histtype='stepfilled', label='stepfilled hist')\nax.hist(data3, color='r', histtype='step', label='step hist')\n\nax.set_xlabel(\"Value\")\nax.set_ylabel(\"Frequency\")\nax.legend()\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#이미지",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#이미지",
    "title": "3wk-1 그래프2",
    "section": "이미지",
    "text": "이미지\nmatplotlib에서의 이미지 불러오기, 생성하기, 화면에 그리기는 꽤 간단합니다.\n이미지를 불러오려면 matplotlib.image 모듈을 임포트하고, 파일이름을 지정한 imread 함수를 호출해 주면 됩니다. 그러면 이미지 데이터가 NumPy의 배열로서 반환됩니다. 앞서 저장했던 my_square_function.png 이미지에 대하여 이를 수행해 보겠습니다.\n\n이제현 주 : * 이미지 단독 출력은 pyplot과 object oriented API 사이에 별 차이가 없습니다. * Axes를 지정해서 출력하는 것이 다를 뿐입니다. * pyplot과의 중복성이 강하지만 익숙해지는 차원에서 object oriented API를 함께 도시합니다.\n\n\nimport matplotlib.image as mpimg\n\nimg = mpimg.imread('my_square_function.png')\nprint(img.shape, img.dtype)\n\n(480, 640, 4) float32\n\n\n288x432 크기의 이미지를 불러왔습니다. 각 픽셀은 0~1 사이의 32비트 부동소수 값인 4개의 요소(빨강, 초록, 파랑, 투명도)로 구성된 배열로 표현됩니다. 이번에는 imshow함수를 호출해 보겠습니다:\n\n# pyplot\n\nplt.imshow(img)\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\nax.imshow(img)\nplt.show()\n\n\n\n\n허허허… 이미지 출력에 포함된 축을 숨기고 싶다면 아래와 같이 축을 off 시켜줄 수 있습니다:\n\n# pyplot\n\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\nax.imshow(img)\nax.axis('off')\nplt.show()\n\n\n\n\n여러분만의 이미지를 생성하는것도 마찬가지로 간단합니다:\n\n# pyplot\n\nimg = np.arange(100*100).reshape(100, 100)\nprint(img)\nplt.imshow(img)\nplt.show()\n\n[[   0    1    2 ...   97   98   99]\n [ 100  101  102 ...  197  198  199]\n [ 200  201  202 ...  297  298  299]\n ...\n [9700 9701 9702 ... 9797 9798 9799]\n [9800 9801 9802 ... 9897 9898 9899]\n [9900 9901 9902 ... 9997 9998 9999]]\n\n\n\n\n\n\n# object oriented API\n\nimg = np.arange(100*100).reshape(100, 100)\nprint(img)\n\nfig, ax = plt.subplots()\nax.imshow(img)\nplt.show()\n\n[[   0    1    2 ...   97   98   99]\n [ 100  101  102 ...  197  198  199]\n [ 200  201  202 ...  297  298  299]\n ...\n [9700 9701 9702 ... 9797 9798 9799]\n [9800 9801 9802 ... 9897 9898 9899]\n [9900 9901 9902 ... 9997 9998 9999]]\n\n\n\n\n\nRGB 수준을 제공하지 않는다면, imshow 함수는 자동으로 값을 색그래디언트에 매핑합니다. 기본적인 동작에서의 색그래디언트는 파랑(낮은 값) 에서 빨강(높은 값)으로 움직입니다. 하지만 아래와 같이 다른 색상맵을 선택할 수도 있습니다:\n\n# pyplot\n\nplt.imshow(img, cmap=\"hot\")\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\nax.imshow(img, cmap=\"hot\")\nplt.show()\n\n\n\n\nRGB 이미지를 직접적으로 생성하는것 또한 가능합니다:\n\n# pyplot\n\nimg = np.empty((20,30,3))\nimg[:, :10] = [0, 0, 0.6]\nimg[:, 10:20] = [1, 1, 1]\nimg[:, 20:] = [0.6, 0, 0]\nplt.imshow(img, interpolation='bilinear')\nplt.show()\n\n\n\n\n\n# object oriented API\n\nimg = np.empty((20,30,3))\nimg[:, :10] = [0, 0, 0.6]\nimg[:, 10:20] = [1, 1, 1]\nimg[:, 20:] = [0.6, 0, 0]\n\nfig, ax = plt.subplots()\nax.imshow(img, interpolation='bilinear')\nplt.show()\n\n\n\n\nimg 배열이 매우 작기 때문에 (20x30), imshow 함수는 이미지를 figure 크기에 맞도록 늘려버린채 출력합니다. 이러한 늘리기의 기본 동작은 쌍선형 보간법(bilinear interpolation)을 사용하여 추가된 픽셀을 매꿉니다. 테두리가 흐릿한 이유입니다.\n다른 보간법 알고리즘을 선택할 수도 있습니다. 가령 아래와 같이 근접 픽셀을 복사하는 방법이 있습니다:\n\n이제현 주 : * 위 코드의 ax.imshow(img, interpolation='bilinear') 부분은 원문에서 ax.imshow(img)로 되어 있습니다. * matplotlib 2.0 이전에는 interpolation='bilinear'가 기본값이기 때문에 경계선이 흐려지는 문제가 있었습니다. * 그러나 이후 interpolation='nearest'로 기본값이 변경되어 흐려지는 문제가 더 이상 발생하지 않습니다. * 자세한 사항은 이 글을 참고하십시오.\n\n\n# pyplot\n\nplt.imshow(img, interpolation=\"nearest\")\nplt.show()\n\n\n\n\n\n# object oriented API\n\nfig, ax = plt.subplots()\nax.imshow(img, interpolation=\"nearest\")\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#애니메이션",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#애니메이션",
    "title": "3wk-1 그래프2",
    "section": "애니메이션",
    "text": "애니메이션\nmatplotlib은 이미지 생성에 주로 사용되지만, 애니메이션의 출력도 가능합니다. 우선 matplotlib.animation을 임포트 해 줘야 합니다. 그 다음은 (주피터 노트북에서) nbagg를 백엔드로 설정하거나, 아래의 코드를 실행해 주면 됩니다.\n\nimport matplotlib.animation as animation\nmatplotlib.rc('animation', html='jshtml')\n\n다음의 예는 데이터를 생성하는것으로 시작됩니다. 그 다음, 빈 그래프를 생성하고, 애니메이션을 그릴 매 프레임 마다 호출될 갱신(update) 함수를 정의합니다. 마지막으로, FuncAnimation 인스턴스를 생성하여 그래프에 애니메이션을 추가합니다.\nFuncAnimation 생성자는 figure, 갱신 함수, 그 외의 파라미터를 수용합니다. 각 프레임간 20ms의 시간차가 있는 100개의 프레임으로 구성된 애니메이션에 대한 인스턴스를 만들었습니다. 애니메이션의 각 프레임마다 FuncAnimation 는 갱신 함수를 호출하고, 프레임 번호를 num (이 예에서는 0~99의 범위) 으로서 전달해 줍니다. 또한 갱신 함수의 추가적인 두 파라미터는 FuncAnimation 생성시 fargs에 넣어준 값이 됩니다.\n작성한 갱신 함수는 선을 구성하는 데이터를 0 ~ num 데이터로 설정합니다 (따라서 데이터가 점진적으로 그려집니다). 그리고 약간의 재미 요소를 위해서, 각 데이터에 약간의 무작위 수를 추가하여 선이 씰룩씰룩 움직이게끔 해 주었습니다.\n\n# pyplot\n\nx = np.linspace(-1, 1, 100)\ny = np.sin(x**2*25)\ndata = np.array([x, y])\n\nfig = plt.figure()\nline, = plt.plot([], [], \"r-\") # start with an empty plot\nplt.axis([-1.1, 1.1, -1.1, 1.1])\nplt.plot([-0.5, 0.5], [0, 0], \"b-\", [0, 0], [-0.5, 0.5], \"b-\", 0, 0, \"ro\")\nplt.grid(True)\nplt.title(\"Marvelous animation\")\n\n# this function will be called at every iteration\ndef update_line(num, data, line):\n    line.set_data(data[..., :num] + np.random.rand(2, num) / 25)  # we only plot the first `num` data points.\n    return line,\n\nline_ani = animation.FuncAnimation(fig, update_line, frames=100, fargs=(data, line), interval=67)\nplt.close()\nline_ani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n# objected oriented API\n\nx = np.linspace(-1, 1, 100)\ny = np.sin(x**2*25)\ndata = np.array([x, y])\n\nfig, ax = plt.subplots()\n\nline, = ax.plot([], [], \"r-\") # start with an empty plot\nax.set_xlim(-1.1, 1.1)\nax.set_ylim(-1.1, 1.1)\nax.plot([-0.5, 0.5], [0, 0], \"b-\", [0, 0], [-0.5, 0.5], \"b-\", 0, 0, \"ro\")\nax.grid(True)\nax.set_title(\"Marvelous animation\")\n\n# this function will be called at every iteration\ndef update_line(num, data, line):\n    line.set_data(data[..., :num] + np.random.rand(2, num) / 25)  # we only plot the first `num` data points.\n    return line,\n\nline_ani = animation.FuncAnimation(fig, update_line, frames=100, fargs=(data, line), interval=67)\nplt.close()\nline_ani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#애니메이션을-비디오로-저장",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#애니메이션을-비디오로-저장",
    "title": "3wk-1 그래프2",
    "section": "애니메이션을 비디오로 저장",
    "text": "애니메이션을 비디오로 저장\n비디오로 저장하기 위해서 Matplotlib은 써드파티 라이브러리(FFMPEG 또는 ImageMagick에 의존합니다. 다음의 예는 FFMPEG를 사용하기 때문에, 이 라이브러리가 먼저 설치되어 있어야만 합니다. 애니메이션을 GIF로 저장하고 싶다면 ImageMagick이 필요할 것입니다.\n\n# Writer = animation.writers['ffmpeg']\n# writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n# line_ani.save('my_wiggly_animation.mp4', writer=writer)"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-19-graph2.html#mcs로-파이-구하기",
    "href": "posts/7_study/7_ds2023/2023-09-19-graph2.html#mcs로-파이-구하기",
    "title": "3wk-1 그래프2",
    "section": "MCS로 파이 구하기",
    "text": "MCS로 파이 구하기\n\n# pyplot\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# scattering n points over the unit square\nn = 1000\np = np.random.rand(n,2) * 2 - 1\n\n# counting the points inside the unit circle\nidx = np.sqrt(p[:,0]**2+p[:,1]**2) &lt; 1\n\n# estimation of pi\nprint(\"MCS로 수치해석한 pi:\\t{:0.8f}\".format(sum(idx).astype('double')/n*4))\nprint(\"정확한 pi:\\t\\t{:0.8f}\".format(np.pi))\n\nplt.figure(figsize=(5,5))\nplt.plot(p[idx,0],p[idx,1],'g.') # point inside\nplt.plot(p[~idx,0],p[~idx,1],'r.') # point outside\nplt.axis([-1.1,1.1,-1.1,1.1])\nplt.show()\n\nMCS로 수치해석한 pi:  3.07200000\n정확한 pi:     3.14159265\n\n\n\n\n\n\n# objected oriented API\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# scattering n points over the unit square\nn = 1000\np = np.random.rand(n,2) * 2 - 1\n\n# counting the points inside the unit circle\nidx = np.sqrt(p[:,0]**2+p[:,1]**2) &lt; 1\n\n# estimation of pi\nprint(\"MCS로 수치해석한 pi:\\t{:0.8f}\".format(sum(idx).astype('double')/n*4))\nprint(\"정확한 pi:\\t\\t{:0.8f}\".format(np.pi))\n\nfig, ax = plt.subplots(figsize=(5,5))\n\nax.plot(p[idx,0],p[idx,1],'g.') # point inside\nax.plot(p[~idx,0],p[~idx,1],'r.') # point outside\nax.axis([-1.1,1.1,-1.1,1.1])\nplt.show()\n\nMCS로 수치해석한 pi:  3.10000000\n정확한 pi:     3.14159265\n\n\n\n\n\nref: https://clauswilke.com/dataviz/"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "",
    "text": "import this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\n\n파이썬은 명확하고 쉽게 표현하기 위한 언어."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#the-zen-of-python",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#the-zen-of-python",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "",
    "text": "import this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\n\n파이썬은 명확하고 쉽게 표현하기 위한 언어."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#가상환경",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#가상환경",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "가상환경",
    "text": "가상환경\n\n!는 시스템 쉘에서 쓰는 명령어. (컴퓨터 자체한테 일을 시키는 것.)\n\n\n# 현재 환경에 설치된 패키지 리스트 출력\n!pip freeze | grep pandas\n\npandas==1.3.5\n\n\n\nfreeze는 깔려있는 패키지를 동결시킨다. (동결시킨 패키지 목록을 필요에 따라 복붙하면 됨.)\n\n\n# 파이썬 버전 확인\n!python --version\n\nPython 3.8.16\n\n\n\n3번째 버전은 신경안써도 됨. 중간 숫자는 1년에 한번씩 업데이트 된다고 보면 됨.\n\n\n# 사용중인 GPU / 드라이브 버전 / CUDA 버전 등 GPU 상태 확인\n!nvidia-smi\n\nTue Sep 12 17:28:14 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |\n|  0%   33C    P8    31W / 420W |      2MiB / 24576MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n\n# 플랫폼 사양 확인\nimport platform as p\np.platform()\n\n'Linux-6.2.0-26-generic-x86_64-with-glibc2.17'\n\n\n\n# 리눅스 종류 확인\n!cat /etc/issue.net\n\nUbuntu 22.04.2 LTS\n\n\n\n# CPU 정보 확인\n# !cat /proc/cpuinfo\n\n\n# Memory 확인\n!cat /proc/meminfo\n\nMemTotal:       131818196 kB\nMemFree:        15323916 kB\nMemAvailable:   37475372 kB\nBuffers:          407604 kB\nCached:         22186364 kB\nSwapCached:            0 kB\nActive:         11569860 kB\nInactive:       103288084 kB\nActive(anon):       6120 kB\nInactive(anon): 92263696 kB\nActive(file):   11563740 kB\nInactive(file): 11024388 kB\nUnevictable:           0 kB\nMlocked:               0 kB\nSwapTotal:       2097148 kB\nSwapFree:        2097148 kB\nZswap:                 0 kB\nZswapped:              0 kB\nDirty:                64 kB\nWriteback:             0 kB\nAnonPages:      92263116 kB\nMapped:          1037044 kB\nShmem:              5832 kB\nKReclaimable:     810380 kB\nSlab:            1055988 kB\nSReclaimable:     810380 kB\nSUnreclaim:       245608 kB\nKernelStack:       28640 kB\nPageTables:       246096 kB\nSecPageTables:         0 kB\nNFS_Unstable:          0 kB\nBounce:                0 kB\nWritebackTmp:          0 kB\nCommitLimit:    68006244 kB\nCommitted_AS:   116634948 kB\nVmallocTotal:   34359738367 kB\nVmallocUsed:      129356 kB\nVmallocChunk:          0 kB\nPercpu:            29696 kB\nHardwareCorrupted:     0 kB\nAnonHugePages:   6203392 kB\nShmemHugePages:        0 kB\nShmemPmdMapped:        0 kB\nFileHugePages:         0 kB\nFilePmdMapped:         0 kB\nHugePages_Total:       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nHugetlb:               0 kB\nDirectMap4k:      881700 kB\nDirectMap2M:    39938048 kB\nDirectMap1G:    93323264 kB\n\n\n\n# KB -&gt; GB\n!awk '/MemFree/ { printf \"%.3f \\n\", $2/1024/1024 }' /proc/meminfo\n\n14.612 \n\n\n\n# 디스크 사용량 확인\n!df -h\n\nFilesystem      Size  Used Avail Use% Mounted on\ntmpfs            13G  2.0M   13G   1% /run\n/dev/nvme0n1p3  916G  234G  635G  27% /\ntmpfs            63G  1.6M   63G   1% /dev/shm\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\n/dev/nvme0n1p2  512M  6.1M  506M   2% /boot/efi\ntmpfs            13G   80K   13G   1% /run/user/127\ntmpfs            13G   68K   13G   1% /run/user/1000"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#modules",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#modules",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Modules",
    "text": "Modules\n모듈/패키지/프레임워크\n\nimport re\nmy_regex = re.compile(\"[0-9]+\", re.I)\nmy_regex.findall(\"010-1234-5678abd 23af\")\n\n['010', '1234', '5678', '23']\n\n\n\nfrom collections import defaultdict, Counter\nlookup = defaultdict(int)\nmy_counter = Counter()\n\n\nmatch = 30\nfrom re import *\nprint(match)\n\n&lt;function match at 0x7f83e4b4be50&gt;\n\n\n\nsum([x for x in range(1,11)])\n\n55\n\n\n\nsum = 0\nfor i in range(1, 10+1):\n    sum += i\nprint(sum)\n\n55\n\n\n\nsum # 지금 sum에는 55가 저장되어 있음.\n\n55\n\n\n\nsum([x for x in range(1,11)]) # 따라서 함수는 작동하지 X\n\nTypeError: 'int' object is not callable\n\n\n\nimport해서 가져오면 변수이름과 동일할 경우 변수 이름이 덮어씌워진다."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#functions",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#functions",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Functions",
    "text": "Functions\n\ndef double(x):\n  \"\"\"주어진 값을 두배로 만들어서 돌려줍니다.\"\"\"\n  return x*2\n\n\n따옴표 3개쓰는 이유? 여러 줄 쓰기 위해..\n함수 선언은 메모리에만 올라가는 것이고 실행되지는 않는다.\n\n\ndouble?\n\n\nSignature: double(x)\nDocstring: 주어진 값을 두배로 만들어서 돌려줍니다.\nFile:      ~/Dropbox/임지윤/Quarto-Blog/posts/5_study/7_ds2023/&lt;ipython-input-15-7ee74bb62645&gt;\nType:      function\n\n\n\n\ndef apply_to_one(f): # sinx, cosx 이든 어떤 식이 와도 1을 집어넣는 함수.\n  \"\"\"주어진 함수에 1을 적용합니다.\"\"\"\n  return f(1)\n\n\napply_to_one(double)\n\n2\n\n\n\napply_to_one(lambda x: x + 5)\n\n6\n\n\n\nadd_five = lambda x: x + 5     # 이런형태로는 람다를 쓰지마세요.\n\ndef add_five(n):               # 차라리 이렇게 쓰는 게 낫습니다.\n  \"\"\"주어진 숫자에 5를 더합니다.\"\"\"\n  return n + 5\n\n\ndef hello(name=\"고객\"): # default값을 줄 수 있음.\n  print(f\"안녕하세요. {name}님\")\n\n\nhello() # 덮어씌워 줄 게 없으니까 디폴트값.\nhello('개굴') # 개굴을 덮어씌워 줌.\n\n안녕하세요. 고객님\n안녕하세요. 개굴님"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#strings",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#strings",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Strings",
    "text": "Strings\n\nsingle_quoted_string = 'data science'\ndouble_quoted_string = \"data science\"\n\n\ntab_string = \"\\t\"\nlen(tab_string)\n\n1\n\n\n\nnot_tab_string = r\"\\t\"      # raw string, 역슬래시 자체를 써야 하는 경우. 정규표현식.\nlen(not_tab_string)\n\n2"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#exceptions",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#exceptions",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Exceptions",
    "text": "Exceptions\n\ntry:\n  print(0 / 0)\nexcept ZeroDivisionError:\n  print(\"0으로 나눌 수 없습니다.\")\n\n0으로 나눌 수 없습니다."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#lists",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#lists",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Lists",
    "text": "Lists\n\ninteger_list = [1, 2, 3]\nheterogeneous_list = [\"string\", 0.1, True]\nlist_of_lists = [integer_list, heterogeneous_list, []]\n\n\nlist_of_lists\n\n[[1, 2, 3], ['string', 0.1, True], []]\n\n\n\n# integer_list 리스트의 길이(갯수)는?\nprint('integet_list의 길이?:', len(integer_list))\n\n# integer_list 리스트 안 모든 값의 합은?\nsum_ = 0\nfor i in range(len(integer_list)):\n    sum_ += integer_list[i]\nprint('integer_list 안 모든 값의 합은?:', sum_)\n\ninteget_list의 길이?: 3\ninteger_list 안 모든 값의 합은?: 6\n\n\n\nx = [x for x in range(10)]\nx\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nzero = x[0]\nnine = x[-1]\nx[0] = -1\n\n\nx\n\n[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nfirst_three = x[:3]    # 첫 3개 항목\nthree_to_end = x[3:]   # 숫자 3부터 끝까지\none_to_four = x[1:5]    # 숫자 1부터 4까지\nlast_three = x[-3:]     # 마지막 3개 항목\nwithout_first_and_last =  x[1:-1]    # 첫항목과 끝항목 빼고 나머지\ncopy_of_x =  x[:]     # 리스트를 복사\n\n\nfirst_three, three_to_end, one_to_four, last_three, without_first_and_last, copy_of_x\n\n([-1, 1, 2],\n [3, 4, 5, 6, 7, 8, 9],\n [1, 2, 3, 4],\n [7, 8, 9],\n [1, 2, 3, 4, 5, 6, 7, 8],\n [-1, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nevery_third = x[::3]\nfive_to_three = x[5:2:-1]\n\n\nevery_third, five_to_three\n\n([-1, 3, 6, 9], [5, 4, 3])\n\n\n\nprint(1 in [1, 2, 3])\nprint(0 in [1, 2, 3])\n\nTrue\nFalse\n\n\n\nx = [1,2,3]\n\n\nx.append(4)\n# x.append(5)\n# x.append(6)\n\n\nx\n\n[1, 2, 3, 4]\n\n\n\nx.append(5)\nx\n\n[1, 2, 3, 4, 5]\n\n\n\nx.append(6)\nx\n\n[1, 2, 3, 4, 5, 6]\n\n\n\nx.append(4,5,6) # append는 하나만 들어간다.\n\nAttributeError: 'int' object has no attribute 'append'\n\n\n\nx = [1, 2, 3]\nx.append([4, 5, 6]) # 리스트를 한 덩어리로 취급.\nx\n\n[1, 2, 3, [4, 5, 6]]\n\n\n\nx = [1,2,3]\nx.extend([4,5,6])\nx\n\n[1, 2, 3, 4, 5, 6]\n\n\n\nx = [1, 2, 3]\ny = x + [4,5,6]\ny\n\n[1, 2, 3, 4, 5, 6]\n\n\n\nx, y = [1, 2]\n\n\n_, y = [1, 2]\n\n\n_, y, _ = [1, 2, 3]"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#tuples",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#tuples",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Tuples",
    "text": "Tuples\n\nmy_list = [1, 2]\nmy_tuple = (1, 2)\nother_tuple = 3, 4\n\n\nother_tuple\n\n(3, 4)\n\n\n\nx, y = 1, 2\n\n\nx, y = y, x # swap x, y. 다른 언어에서는 지원하지 않음. 주의.\n\n\nprint(x)\nprint(y)\n\n2\n1\n\n\n\nmy_list[0] = 5\nmy_list\n\n[5, 2]\n\n\n\nmy_tuple[0] = 5\nmy_tuple\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n프로그램을 만들 때 tuple을 많이 사용한다. (누군가 맘대로 못바꾸게)"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#dictionaries",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#dictionaries",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Dictionaries",
    "text": "Dictionaries\n\nempty_dict = {}\nempty_dict2 = dict()\ngrades = {'홍길동': 80, \"임꺽정\": 95}\n\n\nhong_grade = grades['홍길동']\n\n\nhong_grade\n\n80\n\n\n\nkim_grade = grades[\"김철수\"]\n\nKeyError: '김철수'\n\n\n\nkim_grade = grades.get('김철수',0)\nkim_grade\n\n0\n\n\n\nkim_grade = grades.get('김철수')\n\n\nkim_grade # 아무것도 출력이 안됨.\n\n\nprint(kim_grade)\n\nNone\n\n\n\ngrades.keys(), grades.values(), grades.items() # 반복문 쓸때\n\n(dict_keys(['홍길동', '임꺽정']),\n dict_values([80, 95]),\n dict_items([('홍길동', 80), ('임꺽정', 95)]))\n\n\n\n18일"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#defaultdict",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#defaultdict",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "defaultdict",
    "text": "defaultdict\n\ndocument = [\"동해물\", \"백두산\", \"하느님\", \"우리나라\", \"만세\", \"백두산\", \"우리나라\", \"우리나라\"]\n\nword_counts = {}\nfor word in document: # 사전 안에 그 단어가 있으면\n    if word in word_counts:\n        word_counts[word] += 1\n    else:\n        word_counts[word] = 1 # step1: \"동해물\" 없으니까 1 / \"백두산\" 없으니까 1 / ...\n\n\nword_counts\n\n{'동해물': 1, '백두산': 2, '하느님': 1, '우리나라': 3, '만세': 1}\n\n\n\n\"forgiveness is better than permission\"\n\nword_counts = {}\nfor word in document:\n    try:\n        word_counts[word] += 1\n    except KeyError:\n        word_counts[word] = 1\n\n\nword_counts\n\n{'동해물': 1, '백두산': 2, '하느님': 1, '우리나라': 3, '만세': 1}\n\n\n\nword_counts = {}\nfor word in document:\n    previous_count = word_counts.get(word, 0)\n    word_counts[word] = previous_count + 1\n\n\nword_counts\n\n{'동해물': 1, '백두산': 2, '하느님': 1, '우리나라': 3, '만세': 1}\n\n\n\nfrom collections import defaultdict\n\nword_counts = defaultdict(int)\nfor word in document:\n    word_counts[word] += 1\n\n\nword_counts\n\ndefaultdict(int, {'동해물': 1, '백두산': 2, '하느님': 1, '우리나라': 3, '만세': 1})"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#counters",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#counters",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Counters",
    "text": "Counters\n\nfrom collections import Counter\nc = Counter([0, 1, 2, 0])\n\n\nword_counts = Counter(document)\n\n\nfor word, count in word_counts.most_common(10):\n    print(word, count)\n\n우리나라 3\n백두산 2\n동해물 1\n하느님 1\n만세 1\n\n\n\nword_counts.most_common(10)\n\n[('우리나라', 3), ('백두산', 2), ('동해물', 1), ('하느님', 1), ('만세', 1)]"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#sets",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#sets",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Sets",
    "text": "Sets\n\nprimes_below_10 = {2, 3, 5, 7}\n\n\ns = set()\ns.add(1)\ns.add(2)\ns.add(2)\ns\n\n{1, 2}\n\n\n\nx = len(s)\ny = 2 in s\nz = 3 in s\n\n\nx, y, z\n\n(2, True, False)\n\n\n\n# list\nstopwords_list = [\"a\", \"an\", \"at\", \"yet\", \"you\"]    # \"a\", \"an\", \"at\", ...., \"yet\", \"you\"\n\"zip\" in stopwords_list\n\nFalse\n\n\n\n# set\nstopwords_set = set(stopwords_list)\n\"zip\" in stopwords_set\n\nFalse\n\n\n\n리스트도 한개씩 돌아가면서 체크를 하기 때문에 구현은 가능하지만 비효율적. 데이터 타입을 set 으로만 바꿔줘도 엄청 빨리 계산됨."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#sorting",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#sorting",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Sorting",
    "text": "Sorting\n\nx = [4, 1, 2, 3]\ny = sorted(x)\nprint(x)\nprint(y)\n\n[4, 1, 2, 3]\n[1, 2, 3, 4]\n\n\n\nprint(x.sort())\n\nNone\n\n\n\nx\n\n[1, 2, 3, 4]\n\n\n\nsorted는 리턴되서 나오는게 정렬된 리스트.\nsort자체는 그 자체를 바꿔버리고 리턴 값이 없다.\n그래서 sort는 할당을 하면 None이 나옴을 주의하자."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#list-comprehensions",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#list-comprehensions",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "List Comprehensions",
    "text": "List Comprehensions\n\neven_numbers = [x for x in range(10) if x % 2 == 0]\nsquares = [x**2 for x in range(10)]\neven_squares = [x**2 for x in range(10) if x % 2 == 0]\n\n\n[x for x in range(10)]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n[x**2 for x in range(10)]\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\n\n[x**2 for x in range(10) if x % 2 == 0]\n\n[0, 4, 16, 36, 64]\n\n\n\nsquare_dict = {x: x**2 for x in range(10)}\nsquare_set = {x**2 for x in [1, -1]}\n\n\n{x: x**2 for x in range(10)}\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81}\n\n\n\n{x**2 for x in [1, -1]}\n\n{1}\n\n\n\nzeros = [0 for _ in even_numbers] # 관심없는 것은 _\nzeros\n\n[0, 0, 0, 0, 0]\n\n\n\npairs = [(x, y)\n         for x in range(10)\n         for y in range(10)]\n\n\nlen(pairs)\n\n100"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#automated-testing-and-assert",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#automated-testing-and-assert",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Automated Testing and assert",
    "text": "Automated Testing and assert\n\nassert 1 + 1 == 2\nassert 1 + 1 == 2, \"1+1은 2여야 하는데 결과가 이와 다릅니다.\""
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#randomness",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#randomness",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Randomness",
    "text": "Randomness\n\nimport random\nrandom.seed(42)\n\n\nfour_uniform_randoms = [random.random() for _ in range(4)]\n\n\nfour_uniform_randoms\n\n[0.6394267984578837,\n 0.025010755222666936,\n 0.27502931836911926,\n 0.22321073814882275]\n\n\n\nrandom.seed(42)\nprint(random.random())\n\n0.6394267984578837\n\n\n\nrandom.seed(42)\nprint(random.random())\n\n0.6394267984578837\n\n\n\nrandom.randrange(10) # range(10)\n\n2\n\n\n\nrandom.randrange(3, 6) # range(3, 6)\n\n3\n\n\n\nlottery_numbers = range(1, 45 + 1)\nwinning_numbers = random.sample(lottery_numbers, 6)\nwinning_numbers\n\n[44, 35, 6, 38, 28, 3]\n\n\n\nrandom.seed(11)\nfour_with_replacement = [random.choice(range(10)) for _ in range(4)]\nprint(four_with_replacement)\n\n[7, 8, 7, 7]"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#zip-and-argument-unpacking",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#zip-and-argument-unpacking",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "zip and Argument Unpacking",
    "text": "zip and Argument Unpacking\n\nlist1 =  ['a', 'b', 'c']\nlist2 = [1, 2, 3]\n\npairs = [pairs for pair in zip(list1, list2)]\n# pairs"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#args-and-kwargs",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#args-and-kwargs",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "args and kwargs",
    "text": "args and kwargs\n\ndef method_multiple_parameters(*args, **kwargs):\n  print(\"unnamed args:\", args)\n  print(\"keyword args:\", kwargs)\n\n\nmethod_multiple_parameters(1, 2, key=\"word\", key2=\"word2\")\n\nunnamed args: (1, 2)\nkeyword args: {'key': 'word', 'key2': 'word2'}"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#type-annotations",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#type-annotations",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Type Annotations",
    "text": "Type Annotations\n\ndef add(a: int, b: int) -&gt; int:\n  return a + b\n\n\nvalues = []\nbest_so_far = None\n\n\nfrom typing import Optional, List\nvalues: List[int] = [] # 정수만 들어가는 리스트\nbest_so_far: Optional[float] = None   # float or None"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-13.html",
    "href": "posts/7_study/6_as2023/2023-09-13.html",
    "title": "2wk-2 웹 기초",
    "section": "",
    "text": "meta: ~을 넘어/ 상위 \\(\\to\\) 메타정보란 정보의 정보\n보이기 위해 필요한 자잘한 정보\ntag\n\n&lt;h&gt;: Heading의 약자\n&lt;p&gt;: paragraph 문단\n&lt;div&gt; : division\n&lt;br&gt; : 줄바꿈\n\n요새는 html은 내용만 적고 스타일은 css로 잡아준다.\n- 마크업 랭귀지?\n예를들어 Markdown도 마크업 랭귀지.\n\n\n\n\n\n\nNote\n\n\n\n마크업 언어(Markup Language)는 문서나 데이터를 구조화하고 표현하는 데 사용되는 특수한 종류의 언어입니다. 주로 텍스트 데이터를 태그와 같은 특별한 표시로 둘러싸서 그 데이터의 의미와 구조를 정의합니다. 마크업 언어를 사용하면 컴퓨터가 데이터를 이해하고 처리할 수 있도록 도와줍니다.\n가장 널리 사용되는 마크업 언어 중 하나는 HTML(HyperText Markup Language)입니다. HTML은 웹 페이지의 구조를 정의하고 웹 브라우저에서 웹 페이지를 표시하는 데 사용됩니다. HTML 문서는 여러 태그로 구성되며, 각 태그는 텍스트, 이미지, 링크 등 다양한 요소를 정의하고 배치합니다.\n또한 XML(Extensible Markup Language)은 데이터 저장 및 교환에 사용되는 다목적 마크업 언어로, 데이터베이스, 설정 파일, 웹 서비스 등 다양한 응용 분야에서 활용됩니다.\n마크다운(Markdown)은 더 간단한 마크업 언어로, 문서 작성을 쉽게 만들어주는데 주로 사용됩니다. 마크다운은 텍스트로 작성되며, 간단한 기호와 형식을 사용하여 문서 구조와 스타일을 정의합니다.\n\n\n- 메모장에 실행코드를 복붙하여 txt -&gt; html로 변경하여 실행 (안쓰는 방법)\n\n\n\n다음과 같은 코드 실행 결과를 메모장에 복붙\n\n\n- 확인사항\n\n\n\n파일탐색기-우상단(점점점)-옵션-보기-해당박스에 체크표시 해제\n\n\n\nctrl+r : 한꺼번에 바꿀때\n\n- 파일 내에서 모두 처리 (보통 쓰는 방법)\n\n\n\n\n\n\n\n웹상에 변경사항이 반영이 안될 때\n\n\n\n마우스 오르쪽버튼 &gt; open in(다음에서 열기) &gt; 탐색기 에서 열면 웹상에서 잘 반영되서 보임.\n\n\n파이썬 \\(\\overset{(1)}{\\Longrightarrow}\\) html코드(index.html) \\(\\overset{(2)}{\\Longrightarrow}\\) 웹브라우저로 가는 두 단계가 있다.\n\n첫번째는 \\n\n두번째는 &lt;br&gt; tag\n\n- 배포는?\nflask, jango, fastai\n\nflask: 한땀한땀\njango: 한번에\nfastai: api를 빨리 만들 수 있는 패키지\n\n\n단축키 정리\n\nctrl+r : 한꺼번에 바꿀때"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-18.html",
    "href": "posts/7_study/6_as2023/2023-09-18.html",
    "title": "3wk-1",
    "section": "",
    "text": "@app.route('/&lt;city_name&gt;')\ndef show_detail(city_name):\n    city_info = {\n        \"jeongeup\" : {\"name\":\"전라북도 정읍시\",\\\n                      \"img_url\": \"https://\"}\n        \"Hampeong\": {\"name\":\"전라북도 함평군\",\\\n                     \"img_url\": \"https://\"}\n        \n    }\n    \n    # return f\"&lt;h2&gt;{city_info}{city_name}[\"name\"]"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-18.html#html",
    "href": "posts/7_study/6_as2023/2023-09-18.html#html",
    "title": "3wk-1",
    "section": "HTML",
    "text": "HTML\n입력하세요~ 페이지\n주소를 만들어준다.\nhttp://127.0.0.1/dan/7\nHTML\n브라우저 상에도 똑같이 보이게!!\n자바스크립트에서 import\njquery\n하나는 버튼클릭했을 때 //"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-08-10-transformer-ts.html",
    "href": "posts/7_study/5_etc/2023-08-10-transformer-ts.html",
    "title": "Time Series Transformere (Stock Price)",
    "section": "",
    "text": "Training a transformer model to forecast time series sequence of stock closing price Using 10 timesteps to forecast 1 forward timestep\n\n\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport time\nimport math\nimport matplotlib.pyplot as plt\n\n\n\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/ctxj/Time-Series-Transformer-Pytorch/main/FB_raw.csv')\ndf.head()\n\n\n\n\n\n\n\n\ndate\nminute\nlabel\nhigh\nlow\nopen\nclose\naverage\nvolume\nnotional\n...\nrsi\nlong\nexit_long\nshort\nexit_short\nlong_pnl\nshort_pnl\ntotal_pnl\nlong_labels\nshort_labels\n\n\n\n\n0\n2019-04-01\n09:30\n09:30 AM\n168.000\n167.52\n167.925\n167.82\n167.775\n9294.0\n1559300.720\n...\nNaN\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n1\n2019-04-01\n09:31\n09:31 AM\n168.190\n167.76\n167.760\n168.19\n167.908\n863.0\n144904.500\n...\n100.000000\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n2\n2019-04-01\n09:32\n09:32 AM\n168.630\n168.36\n168.390\n168.36\n168.490\n2707.0\n456101.680\n...\n100.000000\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n3\n2019-04-01\n09:33\n09:33 AM\n168.135\n167.94\n168.135\n168.04\n168.132\n5503.0\n925229.920\n...\n46.468401\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n4\n2019-04-01\n09:34\n09:34 AM\n168.200\n168.00\n168.045\n168.01\n168.189\n15236.0\n2562520.845\n...\n43.215212\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n\n\n5 rows × 21 columns\n\n\n\n- global variables\n\ninput_window = 10 # number of input steps\noutput_window = 1 # number of prediction steps // 여기서는 1로 고정\nbatch_size = 250\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice = torch.device('cpu')\n\n\nclose = np.array(df['close'])\nlogreturn = np.diff(np.log(close)) # transform closing price to log returns\ncsum_logreturn = logreturn.cumsum() # cumulative sum of log returns\n\n- plot\n\nfig, axes = plt.subplots(2,1)\naxes[0].plot(close, color='red')\naxes[0].set_title('Closing Price')\naxes[0].set_ylabel('Close Price')\naxes[0].set_xlabel('Time Steps')\n\naxes[1].plot(csum_logreturn, color='green')\naxes[1].set_title('Cumulative Sum of Log Returns')\naxes[1].set_xlabel('Time Steps')\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()       \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:x.size(0), :]                          \n\n\n\n\n\nclass TransAm(nn.Module):\n    def __init__(self, feature_size=250, num_layers=1, dropout=0.1):\n        super(TransAm, self).__init__()\n        self.model_type = 'Transformer'\n        \n        self.src_mask = None\n        self.pos_encoder = PositionalEncoding(feature_size)\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n        self.decoder = nn.Linear(feature_size,1)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1    \n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self,src):\n        if self.src_mask is None or self.src_mask.size(0) != len(src):\n            device = src.device\n            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n            self.src_mask = mask\n\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src,self.src_mask)\n        output = self.decoder(output)\n        return output\n\n    def _generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n- Window function\nsplit data into sequence window\n\ndef create_inout_sequences(input_data, tw):\n    inout_seq = []\n    L = len(input_data)\n    for i in range(L-tw):\n        train_seq = input_data[i:i+tw]\n        train_label = input_data[i+output_window:i+tw+output_window]\n        inout_seq.append((train_seq ,train_label))\n    return torch.FloatTensor(inout_seq)\n\nSplit data in training and testing, prepared in windowed sequences and pass through GPU\n\ndef get_data(data, split):\n    \"\"\"Split ratio of training data\"\"\"\n\n    series = data\n    \n    split = round(split*len(series))\n    train_data = series[:split]\n    test_data = series[split:]\n\n    train_data = train_data.cumsum()\n    train_data = 2*train_data # Training data augmentation, increase amplitude for the model to better generalize.(Scaling by 2 is aribitrary)\n                              # Similar to image transformation to allow model to train on wider data sets\n\n    test_data = test_data.cumsum()\n\n    train_sequence = create_inout_sequences(train_data,input_window)\n    train_sequence = train_sequence[:-output_window]\n\n    test_data = create_inout_sequences(test_data,input_window)\n    test_data = test_data[:-output_window]\n\n    return train_sequence.to(device), test_data.to(device)\n\nSplit into training batches\n\ndef get_batch(source, i, batch_size):\n    seq_len = min(batch_size, len(source) - 1 - i)\n    data = source[i:i+seq_len]    \n    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window, 1))\n    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window, 1))\n    return input, target\n\nTraining function\n\ndef train(train_data):\n    model.train() # Turn on the evaluation mode\n    total_loss = 0.\n    start_time = time.time()\n\n    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n        data, targets = get_batch(train_data, i,batch_size)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, targets)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n        optimizer.step()\n\n        total_loss += loss.item()\n        log_interval = int(len(train_data) / batch_size / 5)\n        if batch % log_interval == 0 and batch &gt; 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n                  'lr {:02.10f} | {:5.2f} ms | '\n                  'loss {:5.7f}'.format(\n                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n                    elapsed * 1000 / log_interval,\n                    cur_loss))\n            total_loss = 0\n            start_time = time.time()\n\nEvaluation function for model after training\n\ndef evaluate(eval_model, data_source):\n    eval_model.eval() # Turn on the evaluation mode\n    total_loss = 0.\n    eval_batch_size = 1000\n    with torch.no_grad():\n        for i in range(0, len(data_source) - 1, eval_batch_size):\n            data, targets = get_batch(data_source, i, eval_batch_size)\n            output = eval_model(data)            \n            total_loss += len(data[0])* criterion(output, targets).cpu().item()\n    return total_loss / len(data_source)\n\nFunction to forecast 1 time step from window sequence\n\ndef model_forecast(model, seqence):\n    model.eval() \n    total_loss = 0.\n    test_result = torch.Tensor(0)    \n    truth = torch.Tensor(0)\n\n    seq = np.pad(seqence, (0, 3), mode='constant', constant_values=(0, 0))\n    seq = create_inout_sequences(seq, input_window)\n    seq = seq[:-output_window].to(device)\n\n    seq, _ = get_batch(seq, 0, 1)\n    with torch.no_grad():\n        for i in range(0, output_window):            \n            output = model(seq[-output_window:])                        \n            seq = torch.cat((seq, output[-1:]))\n\n    seq = seq.cpu().view(-1).numpy()\n\n    return seq\n\nFunction to forecast entire sequence\n\ndef forecast_seq(model, sequences):\n    \"\"\"Sequences data has to been windowed and passed through device\"\"\"\n    start_timer = time.time()\n    model.eval() \n    forecast_seq = torch.Tensor(0)    \n    actual = torch.Tensor(0)\n    with torch.no_grad():\n        for i in range(0, len(sequences) - 1):\n            data, target = get_batch(sequences, i, 1)\n            output = model(data)            \n            forecast_seq = torch.cat((forecast_seq, output[-1].view(-1).cpu()), 0)\n            actual = torch.cat((actual, target[-1].view(-1).cpu()), 0)\n    timed = time.time()-start_timer\n    print(f\"{timed} sec\")\n\n    return forecast_seq, actual\n\nPrepare data for training model\n\ntrain_data, val_data = get_data(logreturn, 0.6) # 60% train, 40% test split\nmodel = TransAm().to(device)\n\n\ncriterion = nn.MSELoss() # Loss function\nlr = 0.00005 # learning rate\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n\nepochs =  10 # Number of epochs 150\n\n\nfor epoch in range(1, epochs + 1):\n    epoch_start_time = time.time()\n    train(train_data)\n    \n    if(epoch % epochs == 0): # Valid model after last training epoch\n        val_loss = evaluate(model, val_data)\n        print('-' * 80)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss: {:5.7f}'.format(epoch, (time.time() - epoch_start_time), val_loss))\n        print('-' * 80)\n\n    else:   \n        print('-' * 80)\n        print('| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time)))\n        print('-' * 80)\n\n    scheduler.step() \n\n| epoch   1 |    77/  385 batches | lr 0.0000500000 | 57.22 ms | loss 0.0136349\n| epoch   1 |   154/  385 batches | lr 0.0000500000 | 56.69 ms | loss 0.0101929\n| epoch   1 |   231/  385 batches | lr 0.0000500000 | 56.71 ms | loss 0.0057727\n| epoch   1 |   308/  385 batches | lr 0.0000500000 | 56.94 ms | loss 0.0042714\n| epoch   1 |   385/  385 batches | lr 0.0000500000 | 56.60 ms | loss 0.0135543\n--------------------------------------------------------------------------------\n| end of epoch   1 | time: 21.88s\n--------------------------------------------------------------------------------\n| epoch   2 |    77/  385 batches | lr 0.0000451250 | 57.75 ms | loss 0.0058133\n| epoch   2 |   154/  385 batches | lr 0.0000451250 | 57.89 ms | loss 0.0048406\n| epoch   2 |   231/  385 batches | lr 0.0000451250 | 56.89 ms | loss 0.0023565\n| epoch   2 |   308/  385 batches | lr 0.0000451250 | 57.11 ms | loss 0.0025112\n| epoch   2 |   385/  385 batches | lr 0.0000451250 | 56.61 ms | loss 0.0074962\n--------------------------------------------------------------------------------\n| end of epoch   2 | time: 22.04s\n--------------------------------------------------------------------------------\n| epoch   3 |    77/  385 batches | lr 0.0000428687 | 57.97 ms | loss 0.0040375\n| epoch   3 |   154/  385 batches | lr 0.0000428687 | 57.34 ms | loss 0.0036393\n| epoch   3 |   231/  385 batches | lr 0.0000428687 | 57.57 ms | loss 0.0015818\n| epoch   3 |   308/  385 batches | lr 0.0000428687 | 57.03 ms | loss 0.0035247\n| epoch   3 |   385/  385 batches | lr 0.0000428687 | 56.50 ms | loss 0.0056444\n--------------------------------------------------------------------------------\n| end of epoch   3 | time: 22.05s\n--------------------------------------------------------------------------------\n| epoch   4 |    77/  385 batches | lr 0.0000407253 | 57.57 ms | loss 0.0046868\n| epoch   4 |   154/  385 batches | lr 0.0000407253 | 56.73 ms | loss 0.0026414\n| epoch   4 |   231/  385 batches | lr 0.0000407253 | 56.21 ms | loss 0.0018956\n| epoch   4 |   308/  385 batches | lr 0.0000407253 | 58.26 ms | loss 0.0027017\n| epoch   4 |   385/  385 batches | lr 0.0000407253 | 56.28 ms | loss 0.0050008\n--------------------------------------------------------------------------------\n| end of epoch   4 | time: 21.95s\n--------------------------------------------------------------------------------\n| epoch   5 |    77/  385 batches | lr 0.0000386890 | 57.32 ms | loss 0.0052833\n| epoch   5 |   154/  385 batches | lr 0.0000386890 | 56.76 ms | loss 0.0036348\n| epoch   5 |   231/  385 batches | lr 0.0000386890 | 57.19 ms | loss 0.0021735\n| epoch   5 |   308/  385 batches | lr 0.0000386890 | 57.64 ms | loss 0.0020005\n| epoch   5 |   385/  385 batches | lr 0.0000386890 | 56.69 ms | loss 0.0055897\n--------------------------------------------------------------------------------\n| end of epoch   5 | time: 21.99s\n--------------------------------------------------------------------------------\n| epoch   6 |    77/  385 batches | lr 0.0000367546 | 57.75 ms | loss 0.0070904\n| epoch   6 |   154/  385 batches | lr 0.0000367546 | 56.02 ms | loss 0.0022925\n| epoch   6 |   231/  385 batches | lr 0.0000367546 | 56.53 ms | loss 0.0018821\n| epoch   6 |   308/  385 batches | lr 0.0000367546 | 57.27 ms | loss 0.0022888\n| epoch   6 |   385/  385 batches | lr 0.0000367546 | 56.54 ms | loss 0.0090193\n--------------------------------------------------------------------------------\n| end of epoch   6 | time: 21.88s\n--------------------------------------------------------------------------------\n| epoch   7 |    77/  385 batches | lr 0.0000349169 | 57.88 ms | loss 0.0068115\n| epoch   7 |   154/  385 batches | lr 0.0000349169 | 57.39 ms | loss 0.0021631\n| epoch   7 |   231/  385 batches | lr 0.0000349169 | 58.06 ms | loss 0.0018259\n| epoch   7 |   308/  385 batches | lr 0.0000349169 | 57.22 ms | loss 0.0022266\n| epoch   7 |   385/  385 batches | lr 0.0000349169 | 56.09 ms | loss 0.0118806\n--------------------------------------------------------------------------------\n| end of epoch   7 | time: 22.07s\n--------------------------------------------------------------------------------\n| epoch   8 |    77/  385 batches | lr 0.0000331710 | 58.05 ms | loss 0.0077954\n| epoch   8 |   154/  385 batches | lr 0.0000331710 | 56.74 ms | loss 0.0031891\n| epoch   8 |   231/  385 batches | lr 0.0000331710 | 56.75 ms | loss 0.0040912\n| epoch   8 |   308/  385 batches | lr 0.0000331710 | 56.93 ms | loss 0.0023997\n| epoch   8 |   385/  385 batches | lr 0.0000331710 | 56.25 ms | loss 0.0081000\n--------------------------------------------------------------------------------\n| end of epoch   8 | time: 21.92s\n--------------------------------------------------------------------------------\n| epoch   9 |    77/  385 batches | lr 0.0000315125 | 57.79 ms | loss 0.0066046\n| epoch   9 |   154/  385 batches | lr 0.0000315125 | 56.83 ms | loss 0.0039425\n| epoch   9 |   231/  385 batches | lr 0.0000315125 | 56.75 ms | loss 0.0037167\n| epoch   9 |   308/  385 batches | lr 0.0000315125 | 57.20 ms | loss 0.0019323\n| epoch   9 |   385/  385 batches | lr 0.0000315125 | 57.33 ms | loss 0.0132667\n--------------------------------------------------------------------------------\n| end of epoch   9 | time: 22.02s\n--------------------------------------------------------------------------------\n| epoch  10 |    77/  385 batches | lr 0.0000299368 | 58.24 ms | loss 0.0085330\n| epoch  10 |   154/  385 batches | lr 0.0000299368 | 57.27 ms | loss 0.0032725\n| epoch  10 |   231/  385 batches | lr 0.0000299368 | 70.65 ms | loss 0.0034041\n| epoch  10 |   308/  385 batches | lr 0.0000299368 | 70.60 ms | loss 0.0015592\n| epoch  10 |   385/  385 batches | lr 0.0000299368 | 68.78 ms | loss 0.0146613\n--------------------------------------------------------------------------------\n| end of epoch  10 | time: 38.02s | valid loss: 0.2860207\n--------------------------------------------------------------------------------\n\n\n\nmodel\n\nTransAm(\n  (pos_encoder): PositionalEncoding()\n  (encoder_layer): TransformerEncoderLayer(\n    (self_attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=250, out_features=250, bias=True)\n    )\n    (linear1): Linear(in_features=250, out_features=2048, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linear2): Linear(in_features=2048, out_features=250, bias=True)\n    (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n    (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n    (dropout1): Dropout(p=0.1, inplace=False)\n    (dropout2): Dropout(p=0.1, inplace=False)\n  )\n  (transformer_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=250, out_features=250, bias=True)\n        )\n        (linear1): Linear(in_features=250, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=250, bias=True)\n        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): Linear(in_features=250, out_features=1, bias=True)\n)\n\n\n\ntest_result, truth = forecast_seq(model, val_data)\n\n1600.3889005184174 sec\n\n\n- Plot forecasted sequence vs. actual\n\nplt.plot(truth, color='red', alpha=0.7)\nplt.plot(test_result, color='blue', linewidth=0.7)\nplt.title('Actual vs Forecast')\nplt.legend(['Actual', 'Forecast'])\nplt.xlabel('Time Steps')\nplt.show()\n\n\n\n\n\n에폭 수가 너무 적어서 학습이 잘 안된 듯..\n\n- Test random sequence\n\nr = np.random.randint(100000, 160000)\ntest_forecast = model_forecast(model, csum_logreturn[r: r+10]) # random 10 sequence length\n\nprint(f\"forecast sequence: {test_forecast}\")\nprint(f\"Actual sequence: {csum_logreturn[r: r+11]}\")\n\nforecast sequence: [ 0.45956996  0.45955116  0.45804468  0.45834616  0.46043515  0.45947587\n  0.45996502  0.45966405  0.46049154  0.45876053 -0.08363973]\nActual sequence: [0.45956996 0.45955115 0.45804467 0.45834615 0.46043515 0.45947588\n 0.45996503 0.45966404 0.46049154 0.45876053 0.45747915]\n\n\n\nref: https://github.com/ctxj/Time-Series-Transformer-Pytorch/tree/main"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-08-10-transformer-ts.html#import",
    "href": "posts/7_study/5_etc/2023-08-10-transformer-ts.html#import",
    "title": "Time Series Transformere (Stock Price)",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport time\nimport math\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-08-10-transformer-ts.html#load-data",
    "href": "posts/7_study/5_etc/2023-08-10-transformer-ts.html#load-data",
    "title": "Time Series Transformere (Stock Price)",
    "section": "",
    "text": "df = pd.read_csv('https://raw.githubusercontent.com/ctxj/Time-Series-Transformer-Pytorch/main/FB_raw.csv')\ndf.head()\n\n\n\n\n\n\n\n\ndate\nminute\nlabel\nhigh\nlow\nopen\nclose\naverage\nvolume\nnotional\n...\nrsi\nlong\nexit_long\nshort\nexit_short\nlong_pnl\nshort_pnl\ntotal_pnl\nlong_labels\nshort_labels\n\n\n\n\n0\n2019-04-01\n09:30\n09:30 AM\n168.000\n167.52\n167.925\n167.82\n167.775\n9294.0\n1559300.720\n...\nNaN\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n1\n2019-04-01\n09:31\n09:31 AM\n168.190\n167.76\n167.760\n168.19\n167.908\n863.0\n144904.500\n...\n100.000000\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n2\n2019-04-01\n09:32\n09:32 AM\n168.630\n168.36\n168.390\n168.36\n168.490\n2707.0\n456101.680\n...\n100.000000\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n3\n2019-04-01\n09:33\n09:33 AM\n168.135\n167.94\n168.135\n168.04\n168.132\n5503.0\n925229.920\n...\n46.468401\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n4\n2019-04-01\n09:34\n09:34 AM\n168.200\n168.00\n168.045\n168.01\n168.189\n15236.0\n2562520.845\n...\n43.215212\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n\n\n5 rows × 21 columns\n\n\n\n- global variables\n\ninput_window = 10 # number of input steps\noutput_window = 1 # number of prediction steps // 여기서는 1로 고정\nbatch_size = 250\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice = torch.device('cpu')\n\n\nclose = np.array(df['close'])\nlogreturn = np.diff(np.log(close)) # transform closing price to log returns\ncsum_logreturn = logreturn.cumsum() # cumulative sum of log returns\n\n- plot\n\nfig, axes = plt.subplots(2,1)\naxes[0].plot(close, color='red')\naxes[0].set_title('Closing Price')\naxes[0].set_ylabel('Close Price')\naxes[0].set_xlabel('Time Steps')\n\naxes[1].plot(csum_logreturn, color='green')\naxes[1].set_title('Cumulative Sum of Log Returns')\naxes[1].set_xlabel('Time Steps')\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()       \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:x.size(0), :]                          \n\n\n\n\n\nclass TransAm(nn.Module):\n    def __init__(self, feature_size=250, num_layers=1, dropout=0.1):\n        super(TransAm, self).__init__()\n        self.model_type = 'Transformer'\n        \n        self.src_mask = None\n        self.pos_encoder = PositionalEncoding(feature_size)\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n        self.decoder = nn.Linear(feature_size,1)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1    \n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self,src):\n        if self.src_mask is None or self.src_mask.size(0) != len(src):\n            device = src.device\n            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n            self.src_mask = mask\n\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src,self.src_mask)\n        output = self.decoder(output)\n        return output\n\n    def _generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n- Window function\nsplit data into sequence window\n\ndef create_inout_sequences(input_data, tw):\n    inout_seq = []\n    L = len(input_data)\n    for i in range(L-tw):\n        train_seq = input_data[i:i+tw]\n        train_label = input_data[i+output_window:i+tw+output_window]\n        inout_seq.append((train_seq ,train_label))\n    return torch.FloatTensor(inout_seq)\n\nSplit data in training and testing, prepared in windowed sequences and pass through GPU\n\ndef get_data(data, split):\n    \"\"\"Split ratio of training data\"\"\"\n\n    series = data\n    \n    split = round(split*len(series))\n    train_data = series[:split]\n    test_data = series[split:]\n\n    train_data = train_data.cumsum()\n    train_data = 2*train_data # Training data augmentation, increase amplitude for the model to better generalize.(Scaling by 2 is aribitrary)\n                              # Similar to image transformation to allow model to train on wider data sets\n\n    test_data = test_data.cumsum()\n\n    train_sequence = create_inout_sequences(train_data,input_window)\n    train_sequence = train_sequence[:-output_window]\n\n    test_data = create_inout_sequences(test_data,input_window)\n    test_data = test_data[:-output_window]\n\n    return train_sequence.to(device), test_data.to(device)\n\nSplit into training batches\n\ndef get_batch(source, i, batch_size):\n    seq_len = min(batch_size, len(source) - 1 - i)\n    data = source[i:i+seq_len]    \n    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window, 1))\n    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window, 1))\n    return input, target\n\nTraining function\n\ndef train(train_data):\n    model.train() # Turn on the evaluation mode\n    total_loss = 0.\n    start_time = time.time()\n\n    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n        data, targets = get_batch(train_data, i,batch_size)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, targets)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n        optimizer.step()\n\n        total_loss += loss.item()\n        log_interval = int(len(train_data) / batch_size / 5)\n        if batch % log_interval == 0 and batch &gt; 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n                  'lr {:02.10f} | {:5.2f} ms | '\n                  'loss {:5.7f}'.format(\n                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n                    elapsed * 1000 / log_interval,\n                    cur_loss))\n            total_loss = 0\n            start_time = time.time()\n\nEvaluation function for model after training\n\ndef evaluate(eval_model, data_source):\n    eval_model.eval() # Turn on the evaluation mode\n    total_loss = 0.\n    eval_batch_size = 1000\n    with torch.no_grad():\n        for i in range(0, len(data_source) - 1, eval_batch_size):\n            data, targets = get_batch(data_source, i, eval_batch_size)\n            output = eval_model(data)            \n            total_loss += len(data[0])* criterion(output, targets).cpu().item()\n    return total_loss / len(data_source)\n\nFunction to forecast 1 time step from window sequence\n\ndef model_forecast(model, seqence):\n    model.eval() \n    total_loss = 0.\n    test_result = torch.Tensor(0)    \n    truth = torch.Tensor(0)\n\n    seq = np.pad(seqence, (0, 3), mode='constant', constant_values=(0, 0))\n    seq = create_inout_sequences(seq, input_window)\n    seq = seq[:-output_window].to(device)\n\n    seq, _ = get_batch(seq, 0, 1)\n    with torch.no_grad():\n        for i in range(0, output_window):            \n            output = model(seq[-output_window:])                        \n            seq = torch.cat((seq, output[-1:]))\n\n    seq = seq.cpu().view(-1).numpy()\n\n    return seq\n\nFunction to forecast entire sequence\n\ndef forecast_seq(model, sequences):\n    \"\"\"Sequences data has to been windowed and passed through device\"\"\"\n    start_timer = time.time()\n    model.eval() \n    forecast_seq = torch.Tensor(0)    \n    actual = torch.Tensor(0)\n    with torch.no_grad():\n        for i in range(0, len(sequences) - 1):\n            data, target = get_batch(sequences, i, 1)\n            output = model(data)            \n            forecast_seq = torch.cat((forecast_seq, output[-1].view(-1).cpu()), 0)\n            actual = torch.cat((actual, target[-1].view(-1).cpu()), 0)\n    timed = time.time()-start_timer\n    print(f\"{timed} sec\")\n\n    return forecast_seq, actual\n\nPrepare data for training model\n\ntrain_data, val_data = get_data(logreturn, 0.6) # 60% train, 40% test split\nmodel = TransAm().to(device)\n\n\ncriterion = nn.MSELoss() # Loss function\nlr = 0.00005 # learning rate\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n\nepochs =  10 # Number of epochs 150\n\n\nfor epoch in range(1, epochs + 1):\n    epoch_start_time = time.time()\n    train(train_data)\n    \n    if(epoch % epochs == 0): # Valid model after last training epoch\n        val_loss = evaluate(model, val_data)\n        print('-' * 80)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss: {:5.7f}'.format(epoch, (time.time() - epoch_start_time), val_loss))\n        print('-' * 80)\n\n    else:   \n        print('-' * 80)\n        print('| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time)))\n        print('-' * 80)\n\n    scheduler.step() \n\n| epoch   1 |    77/  385 batches | lr 0.0000500000 | 57.22 ms | loss 0.0136349\n| epoch   1 |   154/  385 batches | lr 0.0000500000 | 56.69 ms | loss 0.0101929\n| epoch   1 |   231/  385 batches | lr 0.0000500000 | 56.71 ms | loss 0.0057727\n| epoch   1 |   308/  385 batches | lr 0.0000500000 | 56.94 ms | loss 0.0042714\n| epoch   1 |   385/  385 batches | lr 0.0000500000 | 56.60 ms | loss 0.0135543\n--------------------------------------------------------------------------------\n| end of epoch   1 | time: 21.88s\n--------------------------------------------------------------------------------\n| epoch   2 |    77/  385 batches | lr 0.0000451250 | 57.75 ms | loss 0.0058133\n| epoch   2 |   154/  385 batches | lr 0.0000451250 | 57.89 ms | loss 0.0048406\n| epoch   2 |   231/  385 batches | lr 0.0000451250 | 56.89 ms | loss 0.0023565\n| epoch   2 |   308/  385 batches | lr 0.0000451250 | 57.11 ms | loss 0.0025112\n| epoch   2 |   385/  385 batches | lr 0.0000451250 | 56.61 ms | loss 0.0074962\n--------------------------------------------------------------------------------\n| end of epoch   2 | time: 22.04s\n--------------------------------------------------------------------------------\n| epoch   3 |    77/  385 batches | lr 0.0000428687 | 57.97 ms | loss 0.0040375\n| epoch   3 |   154/  385 batches | lr 0.0000428687 | 57.34 ms | loss 0.0036393\n| epoch   3 |   231/  385 batches | lr 0.0000428687 | 57.57 ms | loss 0.0015818\n| epoch   3 |   308/  385 batches | lr 0.0000428687 | 57.03 ms | loss 0.0035247\n| epoch   3 |   385/  385 batches | lr 0.0000428687 | 56.50 ms | loss 0.0056444\n--------------------------------------------------------------------------------\n| end of epoch   3 | time: 22.05s\n--------------------------------------------------------------------------------\n| epoch   4 |    77/  385 batches | lr 0.0000407253 | 57.57 ms | loss 0.0046868\n| epoch   4 |   154/  385 batches | lr 0.0000407253 | 56.73 ms | loss 0.0026414\n| epoch   4 |   231/  385 batches | lr 0.0000407253 | 56.21 ms | loss 0.0018956\n| epoch   4 |   308/  385 batches | lr 0.0000407253 | 58.26 ms | loss 0.0027017\n| epoch   4 |   385/  385 batches | lr 0.0000407253 | 56.28 ms | loss 0.0050008\n--------------------------------------------------------------------------------\n| end of epoch   4 | time: 21.95s\n--------------------------------------------------------------------------------\n| epoch   5 |    77/  385 batches | lr 0.0000386890 | 57.32 ms | loss 0.0052833\n| epoch   5 |   154/  385 batches | lr 0.0000386890 | 56.76 ms | loss 0.0036348\n| epoch   5 |   231/  385 batches | lr 0.0000386890 | 57.19 ms | loss 0.0021735\n| epoch   5 |   308/  385 batches | lr 0.0000386890 | 57.64 ms | loss 0.0020005\n| epoch   5 |   385/  385 batches | lr 0.0000386890 | 56.69 ms | loss 0.0055897\n--------------------------------------------------------------------------------\n| end of epoch   5 | time: 21.99s\n--------------------------------------------------------------------------------\n| epoch   6 |    77/  385 batches | lr 0.0000367546 | 57.75 ms | loss 0.0070904\n| epoch   6 |   154/  385 batches | lr 0.0000367546 | 56.02 ms | loss 0.0022925\n| epoch   6 |   231/  385 batches | lr 0.0000367546 | 56.53 ms | loss 0.0018821\n| epoch   6 |   308/  385 batches | lr 0.0000367546 | 57.27 ms | loss 0.0022888\n| epoch   6 |   385/  385 batches | lr 0.0000367546 | 56.54 ms | loss 0.0090193\n--------------------------------------------------------------------------------\n| end of epoch   6 | time: 21.88s\n--------------------------------------------------------------------------------\n| epoch   7 |    77/  385 batches | lr 0.0000349169 | 57.88 ms | loss 0.0068115\n| epoch   7 |   154/  385 batches | lr 0.0000349169 | 57.39 ms | loss 0.0021631\n| epoch   7 |   231/  385 batches | lr 0.0000349169 | 58.06 ms | loss 0.0018259\n| epoch   7 |   308/  385 batches | lr 0.0000349169 | 57.22 ms | loss 0.0022266\n| epoch   7 |   385/  385 batches | lr 0.0000349169 | 56.09 ms | loss 0.0118806\n--------------------------------------------------------------------------------\n| end of epoch   7 | time: 22.07s\n--------------------------------------------------------------------------------\n| epoch   8 |    77/  385 batches | lr 0.0000331710 | 58.05 ms | loss 0.0077954\n| epoch   8 |   154/  385 batches | lr 0.0000331710 | 56.74 ms | loss 0.0031891\n| epoch   8 |   231/  385 batches | lr 0.0000331710 | 56.75 ms | loss 0.0040912\n| epoch   8 |   308/  385 batches | lr 0.0000331710 | 56.93 ms | loss 0.0023997\n| epoch   8 |   385/  385 batches | lr 0.0000331710 | 56.25 ms | loss 0.0081000\n--------------------------------------------------------------------------------\n| end of epoch   8 | time: 21.92s\n--------------------------------------------------------------------------------\n| epoch   9 |    77/  385 batches | lr 0.0000315125 | 57.79 ms | loss 0.0066046\n| epoch   9 |   154/  385 batches | lr 0.0000315125 | 56.83 ms | loss 0.0039425\n| epoch   9 |   231/  385 batches | lr 0.0000315125 | 56.75 ms | loss 0.0037167\n| epoch   9 |   308/  385 batches | lr 0.0000315125 | 57.20 ms | loss 0.0019323\n| epoch   9 |   385/  385 batches | lr 0.0000315125 | 57.33 ms | loss 0.0132667\n--------------------------------------------------------------------------------\n| end of epoch   9 | time: 22.02s\n--------------------------------------------------------------------------------\n| epoch  10 |    77/  385 batches | lr 0.0000299368 | 58.24 ms | loss 0.0085330\n| epoch  10 |   154/  385 batches | lr 0.0000299368 | 57.27 ms | loss 0.0032725\n| epoch  10 |   231/  385 batches | lr 0.0000299368 | 70.65 ms | loss 0.0034041\n| epoch  10 |   308/  385 batches | lr 0.0000299368 | 70.60 ms | loss 0.0015592\n| epoch  10 |   385/  385 batches | lr 0.0000299368 | 68.78 ms | loss 0.0146613\n--------------------------------------------------------------------------------\n| end of epoch  10 | time: 38.02s | valid loss: 0.2860207\n--------------------------------------------------------------------------------\n\n\n\nmodel\n\nTransAm(\n  (pos_encoder): PositionalEncoding()\n  (encoder_layer): TransformerEncoderLayer(\n    (self_attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=250, out_features=250, bias=True)\n    )\n    (linear1): Linear(in_features=250, out_features=2048, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linear2): Linear(in_features=2048, out_features=250, bias=True)\n    (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n    (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n    (dropout1): Dropout(p=0.1, inplace=False)\n    (dropout2): Dropout(p=0.1, inplace=False)\n  )\n  (transformer_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=250, out_features=250, bias=True)\n        )\n        (linear1): Linear(in_features=250, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=250, bias=True)\n        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): Linear(in_features=250, out_features=1, bias=True)\n)\n\n\n\ntest_result, truth = forecast_seq(model, val_data)\n\n1600.3889005184174 sec\n\n\n- Plot forecasted sequence vs. actual\n\nplt.plot(truth, color='red', alpha=0.7)\nplt.plot(test_result, color='blue', linewidth=0.7)\nplt.title('Actual vs Forecast')\nplt.legend(['Actual', 'Forecast'])\nplt.xlabel('Time Steps')\nplt.show()\n\n\n\n\n\n에폭 수가 너무 적어서 학습이 잘 안된 듯..\n\n- Test random sequence\n\nr = np.random.randint(100000, 160000)\ntest_forecast = model_forecast(model, csum_logreturn[r: r+10]) # random 10 sequence length\n\nprint(f\"forecast sequence: {test_forecast}\")\nprint(f\"Actual sequence: {csum_logreturn[r: r+11]}\")\n\nforecast sequence: [ 0.45956996  0.45955116  0.45804468  0.45834616  0.46043515  0.45947587\n  0.45996502  0.45966405  0.46049154  0.45876053 -0.08363973]\nActual sequence: [0.45956996 0.45955115 0.45804467 0.45834615 0.46043515 0.45947588\n 0.45996503 0.45966404 0.46049154 0.45876053 0.45747915]\n\n\n\nref: https://github.com/ctxj/Time-Series-Transformer-Pytorch/tree/main"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-08-09-transformer.html",
    "href": "posts/7_study/5_etc/2023-08-09-transformer.html",
    "title": "Attention is all you need",
    "section": "",
    "text": "ref: https://arxiv.org/abs/1706.03762"
  },
  {
    "objectID": "posts/6_note/2023-09-09_baseline_code.html",
    "href": "posts/6_note/2023-09-09_baseline_code.html",
    "title": "연습장2",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n\n# 데이터 불러오기\ninputs = pd.read_csv('./farm/train_input.csv')\noutputs = pd.read_csv('./farm/train_output.csv')\n\n\ninputs.head()\n\n\n\n\n\n\n\n\nSample_no\n시설ID\n일\n주차\n내부CO2\n내부습도\n내부온도\n지온\n강우감지\n일사량\n외부온도\n외부풍향\n외부풍속\n지습\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n품종\n재배형태\n\n\n\n\n0\n0\nfarm25\n20220323\n30주차\n517.041667\n84.985417\n20.610833\n0.0\nNaN\n1879\n11.166667\n195.0\n0.083333\n0.0\n14\n2.68\n4.42\n88\ntomato09\nNaN\n\n\n1\n0\nfarm25\n20220324\n30주차\n514.416667\n88.291250\n20.695000\n0.0\nNaN\n1411\n12.708333\n142.5\n0.000000\n0.0\n14\n2.78\n5.63\n97\ntomato09\nNaN\n\n\n2\n0\nfarm25\n20220326\n30주차\n471.875000\n83.514583\n20.402500\n0.0\nNaN\n1955\n8.791667\n202.5\n0.000000\n0.0\n14\n2.69\n4.25\n101\ntomato09\nNaN\n\n\n3\n0\nfarm25\n20220327\n30주차\n469.250000\n80.916250\n20.139167\n0.0\nNaN\n2231\n8.041667\n180.0\n0.000000\n0.0\n14\n2.70\n4.25\n99\ntomato09\nNaN\n\n\n4\n0\nfarm25\n20220328\n30주차\n465.750000\n82.026250\n17.653333\n0.0\nNaN\n2284\n9.000000\n97.5\n0.041667\n0.0\n13\n2.66\n4.21\n94\ntomato09\nNaN\n\n\n\n\n\n\n\n\noutputs.head()\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n0\n20220330\n30주차\n208.0\n6.9\n16.67\n\n\n1\n1\n20220330\n30주차\n172.0\n6.8\n17.33\n\n\n2\n2\n20220330\n30주차\n150.0\n9.3\n16.00\n\n\n3\n3\n20220330\n30주차\n121.0\n5.9\n16.20\n\n\n4\n4\n20220406\n31주차\n175.0\n5.8\n17.40\n\n\n\n\n\n\n\n\ninputs.shape, outputs.shape\n\n((10112, 20), (1518, 6))\n\n\n\ninputs.isna().sum().sort_values(ascending=False)\n\n품종            7114\n외부풍향          6993\n지습            5873\n재배형태          2408\n지온            1749\n강우감지          1505\n외부풍속           670\n외부온도           201\n내부온도             0\n일사량              0\n시설ID             0\n내부습도             0\n내부CO2            0\n주차               0\n급액횟수             0\n급액EC(dS/m)       0\n급액pH             0\n급액량(회당)          0\n일                0\nSample_no        0\ndtype: int64\n\n\n\n# nan 제거  -- 베이스라인이므로 간단한 처리를 위해 nan 항목 보간 없이 학습\ninputs = inputs.dropna(axis=1)\n\n\n# 주차 정보 수치 변환\ninputs['주차'] = [int(i.replace('주차', \"\")) for i in inputs['주차']]\n\n\n# scaler\ninput_scaler = MinMaxScaler()\noutput_scaler = MinMaxScaler()\n\n\ninputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n주차\n내부CO2\n내부습도\n내부온도\n일사량\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n\n\n\n\n0\n30\n517.041667\n84.985417\n20.610833\n1879\n14\n2.68\n4.42\n88\n\n\n1\n30\n514.416667\n88.291250\n20.695000\n1411\n14\n2.78\n5.63\n97\n\n\n2\n30\n471.875000\n83.514583\n20.402500\n1955\n14\n2.69\n4.25\n101\n\n\n3\n30\n469.250000\n80.916250\n20.139167\n2231\n14\n2.70\n4.25\n99\n\n\n4\n30\n465.750000\n82.026250\n17.653333\n2284\n13\n2.66\n4.21\n94\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10107\n7\n334.684002\n65.565417\n21.985833\n979\n26\n2.06\n5.80\n81\n\n\n10108\n7\n333.726601\n61.144167\n22.530833\n2515\n28\n2.43\n4.42\n32\n\n\n10109\n7\n344.862883\n72.867917\n20.397917\n1972\n21\n2.71\n5.88\n27\n\n\n10110\n7\n372.708516\n66.672917\n24.401667\n1314\n18\n2.50\n5.39\n82\n\n\n10111\n7\n372.612192\n59.257083\n28.352500\n1310\n16\n2.50\n5.39\n82\n\n\n\n\n10112 rows × 9 columns\n\n\n\n\noutputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n208.0\n6.90\n16.67\n\n\n1\n172.0\n6.80\n17.33\n\n\n2\n150.0\n9.30\n16.00\n\n\n3\n121.0\n5.90\n16.20\n\n\n4\n175.0\n5.80\n17.40\n\n\n...\n...\n...\n...\n\n\n1513\n150.0\n6.95\n2.20\n\n\n1514\n140.0\n10.13\n1.40\n\n\n1515\n200.0\n9.61\n1.40\n\n\n1516\n210.0\n8.47\n2.20\n\n\n1517\n150.0\n9.16\n3.20\n\n\n\n\n1518 rows × 3 columns\n\n\n\n\n# scaling\ninput_sc = input_scaler.fit_transform(inputs.iloc[:,3:].to_numpy())\noutput_sc = output_scaler.fit_transform(outputs.iloc[:,3:].to_numpy())\n\n\nlen(inputs['Sample_no'].unique()) \n\n1518\n\n\n\n# 입력 시계열화\ninput_ts = []\nfor i in outputs['Sample_no']:\n    sample = input_sc[inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    input_ts.append(sample)\ninput_ts = np.concatenate(input_ts, axis=0)\n\n\ninput_ts.shape\n\n(1518, 7, 9)\n\n\n\n# 셋 분리\ntrain_x, val_x, train_y, val_y = train_test_split(input_ts, output_sc, test_size=0.2,\n                                                  shuffle=True, random_state=0)\n\n\ntrain_x.shape, val_x.shape, train_y.shape, val_y.shape\n\n((1214, 7, 9), (304, 7, 9), (1214, 3), (304, 3))\n\n\n\n# 모델 정의\ndef create_model():\n    x = Input(shape=[7, 9])\n    l1 = LSTM(64)(x)\n    out = Dense(3, activation='tanh')(l1)\n    return Model(inputs=x, outputs=out)\n\nmodel = create_model()\nmodel.summary()\ncheckpointer = ModelCheckpoint(monitor='val_loss', filepath='baseline.h5',\n                               verbose=1, save_best_only=True, save_weights_only=True)\n\nmodel.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['mse'])\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 7, 9)]            0         \n                                                                 \n lstm (LSTM)                 (None, 64)                18944     \n                                                                 \n dense (Dense)               (None, 3)                 195       \n                                                                 \n=================================================================\nTotal params: 19,139\nTrainable params: 19,139\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n\n\n\n# 학습\nhist = model.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[checkpointer])\n\nEpoch 1/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0170 - mse: 0.0170 \nEpoch 1: val_loss improved from inf to 0.00995, saving model to baseline.h5\n38/38 [==============================] - 1s 9ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0099 - val_mse: 0.0099\nEpoch 2/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0117 - mse: 0.0117\nEpoch 2: val_loss improved from 0.00995 to 0.00839, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0084 - val_mse: 0.0084\nEpoch 3/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0109 - mse: 0.0109\nEpoch 3: val_loss improved from 0.00839 to 0.00777, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 4/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0106 - mse: 0.0106\nEpoch 4: val_loss did not improve from 0.00777\n38/38 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 5/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0101 - mse: 0.0101\nEpoch 5: val_loss improved from 0.00777 to 0.00757, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0076 - val_mse: 0.0076\nEpoch 6/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0105 - mse: 0.0105\nEpoch 6: val_loss improved from 0.00757 to 0.00751, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0075 - val_mse: 0.0075\nEpoch 7/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0098 - mse: 0.0098\nEpoch 7: val_loss improved from 0.00751 to 0.00738, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0074 - val_mse: 0.0074\nEpoch 8/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 8: val_loss improved from 0.00738 to 0.00709, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0071 - val_mse: 0.0071\nEpoch 9/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 9: val_loss improved from 0.00709 to 0.00702, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 10/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 10: val_loss did not improve from 0.00702\n38/38 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0072 - val_mse: 0.0072\nEpoch 11/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 11: val_loss improved from 0.00702 to 0.00696, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 12/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 12: val_loss improved from 0.00696 to 0.00682, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 13/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 13: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 14/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 14: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 15/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0094 - mse: 0.0094\nEpoch 15: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 16/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 16: val_loss improved from 0.00682 to 0.00674, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 17/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 17: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 18/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 18: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 19/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 19: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 20/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 20: val_loss improved from 0.00674 to 0.00670, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 21/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 21: val_loss improved from 0.00670 to 0.00659, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 22/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 22: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 23/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 23: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 24/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0090 - mse: 0.0090\nEpoch 24: val_loss improved from 0.00659 to 0.00650, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 25/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 25: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 26/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 26: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 27/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 27: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 28/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 28: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 29/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 29: val_loss improved from 0.00650 to 0.00637, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0064 - val_mse: 0.0064\nEpoch 30/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0084 - mse: 0.0084\nEpoch 30: val_loss improved from 0.00637 to 0.00632, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0063 - val_mse: 0.0063\nEpoch 31/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 31: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 32/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0077 - mse: 0.0077\nEpoch 32: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 33/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 33: val_loss improved from 0.00632 to 0.00623, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 34/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0082 - mse: 0.0082\nEpoch 34: val_loss improved from 0.00623 to 0.00614, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0061 - val_mse: 0.0061\nEpoch 35/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0080 - mse: 0.0080\nEpoch 35: val_loss improved from 0.00614 to 0.00604, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 36/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 36: val_loss did not improve from 0.00604\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 37/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 37: val_loss improved from 0.00604 to 0.00601, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 38/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 38: val_loss improved from 0.00601 to 0.00586, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0059 - val_mse: 0.0059\nEpoch 39/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0078 - mse: 0.0078\nEpoch 39: val_loss did not improve from 0.00586\n38/38 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 40/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 40: val_loss improved from 0.00586 to 0.00551, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 41/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 41: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 42/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0068 - mse: 0.0068\nEpoch 42: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 43/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 43: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 44/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0071 - mse: 0.0071\nEpoch 44: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 45/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 45: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 46/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 46: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 47/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 47: val_loss improved from 0.00551 to 0.00509, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 48/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 48: val_loss did not improve from 0.00509\n38/38 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0054 - val_mse: 0.0054\nEpoch 49/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0062 - mse: 0.0062\nEpoch 49: val_loss improved from 0.00509 to 0.00507, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 50/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 50: val_loss did not improve from 0.00507\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0056 - val_mse: 0.0056\n\n\n\n# loss 히스토리 확인\nfig, loss_ax = plt.subplots()\nloss_ax.plot(hist.history['loss'], 'r', label='loss')\nloss_ax.plot(hist.history['val_loss'], 'g', label='val_loss')\nloss_ax.set_xlabel('epoch')\nloss_ax.set_ylabel('loss')\nloss_ax.legend()\nplt.title('Training loss - Validation loss plot')\nplt.show()\n\n\n\n\n\n# 저장된 가중치 불러오기\nmodel.load_weights('baseline.h5')\n\n\n# 테스트셋 전처리 및 추론\ntest_inputs = pd.read_csv('./farm/test_input.csv')\noutput_sample = pd.read_csv('./farm/answer_sample.csv')\n\ntest_inputs = test_inputs[inputs.columns]\ntest_inputs['주차'] = [int(i.replace('주차', \"\")) for i in test_inputs['주차']]\ntest_input_sc = input_scaler.transform(test_inputs.iloc[:,3:].to_numpy())\n\ntest_input_ts = []\nfor i in output_sample['Sample_no']:\n    sample = test_input_sc[test_inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    test_input_ts.append(sample)\ntest_input_ts = np.concatenate(test_input_ts, axis=0)\n\n\ntest_input_ts.shape\n\n(506, 7, 9)\n\n\n\nprediction = model.predict(test_input_ts)\n\nprediction = output_scaler.inverse_transform(prediction)\noutput_sample[['생장길이', '줄기직경', '개화군']] = prediction\n\n16/16 [==============================] - 0s 793us/step\n\n\n\noutput_sample\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n9\n20220413\n32주차\n47.151882\n7.198693\n13.229403\n\n\n1\n12\n20170312\n30주차\n420.956116\n3.321363\n10.489825\n\n\n2\n19\n20170319\n31주차\n589.641235\n4.079537\n7.553223\n\n\n3\n23\n20170326\n32주차\n281.593994\n4.563877\n7.977988\n\n\n4\n27\n20170430\n37주차\n89.870880\n7.789731\n4.768530\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n501\n2015\n20160508\n14주차\n188.253265\n12.274371\n5.467629\n\n\n502\n2016\n20160529\n17주차\n1998.461670\n4.776089\n1.905745\n\n\n503\n2024\n20160828\n7주차\n49.577644\n13.211569\n2.025992\n\n\n504\n2025\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n505\n2026\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n\n\n506 rows × 6 columns\n\n\n\n\n# 제출할 추론 결과 저장\noutput_sample.to_csv('prediction.csv', index=False)\n\n- 텐서보드\n\n# 학습\ncb1 = tf.keras.callbacks.TensorBoard()\nmodel.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[cb1])\n\nEpoch 1/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 2/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 3/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 4/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 5/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 6/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 7/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 8/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 9/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 10/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 11/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 12/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 13/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 14/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 15/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 16/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 17/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0044 - val_mse: 0.0044\nEpoch 18/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 19/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 20/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0050 - val_mse: 0.0050\nEpoch 21/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 22/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 23/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 24/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 25/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 26/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 27/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 28/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 29/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 30/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 31/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 32/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 33/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 34/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 35/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 36/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 37/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 38/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 39/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 40/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 41/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 42/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 43/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 44/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 45/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 46/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0037 - val_mse: 0.0037\nEpoch 47/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 48/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 49/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 50/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n\n\n&lt;keras.callbacks.History at 0x7f2df8e88b80&gt;\n\n\n\n%load_ext tensorboard\n%tensorboard --logdir logs --host \n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\nReusing TensorBoard on port 6006 (pid 109833), started 0:04:21 ago. (Use '!kill 109833' to kill it.)"
  },
  {
    "objectID": "posts/6_note/2023-09-09_baseline_code.html#import",
    "href": "posts/6_note/2023-09-09_baseline_code.html#import",
    "title": "연습장2",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n\n# 데이터 불러오기\ninputs = pd.read_csv('./farm/train_input.csv')\noutputs = pd.read_csv('./farm/train_output.csv')\n\n\ninputs.head()\n\n\n\n\n\n\n\n\nSample_no\n시설ID\n일\n주차\n내부CO2\n내부습도\n내부온도\n지온\n강우감지\n일사량\n외부온도\n외부풍향\n외부풍속\n지습\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n품종\n재배형태\n\n\n\n\n0\n0\nfarm25\n20220323\n30주차\n517.041667\n84.985417\n20.610833\n0.0\nNaN\n1879\n11.166667\n195.0\n0.083333\n0.0\n14\n2.68\n4.42\n88\ntomato09\nNaN\n\n\n1\n0\nfarm25\n20220324\n30주차\n514.416667\n88.291250\n20.695000\n0.0\nNaN\n1411\n12.708333\n142.5\n0.000000\n0.0\n14\n2.78\n5.63\n97\ntomato09\nNaN\n\n\n2\n0\nfarm25\n20220326\n30주차\n471.875000\n83.514583\n20.402500\n0.0\nNaN\n1955\n8.791667\n202.5\n0.000000\n0.0\n14\n2.69\n4.25\n101\ntomato09\nNaN\n\n\n3\n0\nfarm25\n20220327\n30주차\n469.250000\n80.916250\n20.139167\n0.0\nNaN\n2231\n8.041667\n180.0\n0.000000\n0.0\n14\n2.70\n4.25\n99\ntomato09\nNaN\n\n\n4\n0\nfarm25\n20220328\n30주차\n465.750000\n82.026250\n17.653333\n0.0\nNaN\n2284\n9.000000\n97.5\n0.041667\n0.0\n13\n2.66\n4.21\n94\ntomato09\nNaN\n\n\n\n\n\n\n\n\noutputs.head()\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n0\n20220330\n30주차\n208.0\n6.9\n16.67\n\n\n1\n1\n20220330\n30주차\n172.0\n6.8\n17.33\n\n\n2\n2\n20220330\n30주차\n150.0\n9.3\n16.00\n\n\n3\n3\n20220330\n30주차\n121.0\n5.9\n16.20\n\n\n4\n4\n20220406\n31주차\n175.0\n5.8\n17.40\n\n\n\n\n\n\n\n\ninputs.shape, outputs.shape\n\n((10112, 20), (1518, 6))\n\n\n\ninputs.isna().sum().sort_values(ascending=False)\n\n품종            7114\n외부풍향          6993\n지습            5873\n재배형태          2408\n지온            1749\n강우감지          1505\n외부풍속           670\n외부온도           201\n내부온도             0\n일사량              0\n시설ID             0\n내부습도             0\n내부CO2            0\n주차               0\n급액횟수             0\n급액EC(dS/m)       0\n급액pH             0\n급액량(회당)          0\n일                0\nSample_no        0\ndtype: int64\n\n\n\n# nan 제거  -- 베이스라인이므로 간단한 처리를 위해 nan 항목 보간 없이 학습\ninputs = inputs.dropna(axis=1)\n\n\n# 주차 정보 수치 변환\ninputs['주차'] = [int(i.replace('주차', \"\")) for i in inputs['주차']]\n\n\n# scaler\ninput_scaler = MinMaxScaler()\noutput_scaler = MinMaxScaler()\n\n\ninputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n주차\n내부CO2\n내부습도\n내부온도\n일사량\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n\n\n\n\n0\n30\n517.041667\n84.985417\n20.610833\n1879\n14\n2.68\n4.42\n88\n\n\n1\n30\n514.416667\n88.291250\n20.695000\n1411\n14\n2.78\n5.63\n97\n\n\n2\n30\n471.875000\n83.514583\n20.402500\n1955\n14\n2.69\n4.25\n101\n\n\n3\n30\n469.250000\n80.916250\n20.139167\n2231\n14\n2.70\n4.25\n99\n\n\n4\n30\n465.750000\n82.026250\n17.653333\n2284\n13\n2.66\n4.21\n94\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10107\n7\n334.684002\n65.565417\n21.985833\n979\n26\n2.06\n5.80\n81\n\n\n10108\n7\n333.726601\n61.144167\n22.530833\n2515\n28\n2.43\n4.42\n32\n\n\n10109\n7\n344.862883\n72.867917\n20.397917\n1972\n21\n2.71\n5.88\n27\n\n\n10110\n7\n372.708516\n66.672917\n24.401667\n1314\n18\n2.50\n5.39\n82\n\n\n10111\n7\n372.612192\n59.257083\n28.352500\n1310\n16\n2.50\n5.39\n82\n\n\n\n\n10112 rows × 9 columns\n\n\n\n\noutputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n208.0\n6.90\n16.67\n\n\n1\n172.0\n6.80\n17.33\n\n\n2\n150.0\n9.30\n16.00\n\n\n3\n121.0\n5.90\n16.20\n\n\n4\n175.0\n5.80\n17.40\n\n\n...\n...\n...\n...\n\n\n1513\n150.0\n6.95\n2.20\n\n\n1514\n140.0\n10.13\n1.40\n\n\n1515\n200.0\n9.61\n1.40\n\n\n1516\n210.0\n8.47\n2.20\n\n\n1517\n150.0\n9.16\n3.20\n\n\n\n\n1518 rows × 3 columns\n\n\n\n\n# scaling\ninput_sc = input_scaler.fit_transform(inputs.iloc[:,3:].to_numpy())\noutput_sc = output_scaler.fit_transform(outputs.iloc[:,3:].to_numpy())\n\n\nlen(inputs['Sample_no'].unique()) \n\n1518\n\n\n\n# 입력 시계열화\ninput_ts = []\nfor i in outputs['Sample_no']:\n    sample = input_sc[inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    input_ts.append(sample)\ninput_ts = np.concatenate(input_ts, axis=0)\n\n\ninput_ts.shape\n\n(1518, 7, 9)\n\n\n\n# 셋 분리\ntrain_x, val_x, train_y, val_y = train_test_split(input_ts, output_sc, test_size=0.2,\n                                                  shuffle=True, random_state=0)\n\n\ntrain_x.shape, val_x.shape, train_y.shape, val_y.shape\n\n((1214, 7, 9), (304, 7, 9), (1214, 3), (304, 3))\n\n\n\n# 모델 정의\ndef create_model():\n    x = Input(shape=[7, 9])\n    l1 = LSTM(64)(x)\n    out = Dense(3, activation='tanh')(l1)\n    return Model(inputs=x, outputs=out)\n\nmodel = create_model()\nmodel.summary()\ncheckpointer = ModelCheckpoint(monitor='val_loss', filepath='baseline.h5',\n                               verbose=1, save_best_only=True, save_weights_only=True)\n\nmodel.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['mse'])\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 7, 9)]            0         \n                                                                 \n lstm (LSTM)                 (None, 64)                18944     \n                                                                 \n dense (Dense)               (None, 3)                 195       \n                                                                 \n=================================================================\nTotal params: 19,139\nTrainable params: 19,139\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n\n\n\n# 학습\nhist = model.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[checkpointer])\n\nEpoch 1/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0170 - mse: 0.0170 \nEpoch 1: val_loss improved from inf to 0.00995, saving model to baseline.h5\n38/38 [==============================] - 1s 9ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0099 - val_mse: 0.0099\nEpoch 2/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0117 - mse: 0.0117\nEpoch 2: val_loss improved from 0.00995 to 0.00839, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0084 - val_mse: 0.0084\nEpoch 3/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0109 - mse: 0.0109\nEpoch 3: val_loss improved from 0.00839 to 0.00777, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 4/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0106 - mse: 0.0106\nEpoch 4: val_loss did not improve from 0.00777\n38/38 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 5/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0101 - mse: 0.0101\nEpoch 5: val_loss improved from 0.00777 to 0.00757, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0076 - val_mse: 0.0076\nEpoch 6/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0105 - mse: 0.0105\nEpoch 6: val_loss improved from 0.00757 to 0.00751, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0075 - val_mse: 0.0075\nEpoch 7/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0098 - mse: 0.0098\nEpoch 7: val_loss improved from 0.00751 to 0.00738, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0074 - val_mse: 0.0074\nEpoch 8/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 8: val_loss improved from 0.00738 to 0.00709, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0071 - val_mse: 0.0071\nEpoch 9/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 9: val_loss improved from 0.00709 to 0.00702, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 10/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 10: val_loss did not improve from 0.00702\n38/38 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0072 - val_mse: 0.0072\nEpoch 11/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 11: val_loss improved from 0.00702 to 0.00696, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 12/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 12: val_loss improved from 0.00696 to 0.00682, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 13/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 13: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 14/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 14: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 15/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0094 - mse: 0.0094\nEpoch 15: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 16/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 16: val_loss improved from 0.00682 to 0.00674, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 17/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 17: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 18/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 18: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 19/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 19: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 20/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 20: val_loss improved from 0.00674 to 0.00670, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 21/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 21: val_loss improved from 0.00670 to 0.00659, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 22/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 22: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 23/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 23: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 24/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0090 - mse: 0.0090\nEpoch 24: val_loss improved from 0.00659 to 0.00650, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 25/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 25: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 26/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 26: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 27/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 27: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 28/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 28: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 29/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 29: val_loss improved from 0.00650 to 0.00637, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0064 - val_mse: 0.0064\nEpoch 30/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0084 - mse: 0.0084\nEpoch 30: val_loss improved from 0.00637 to 0.00632, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0063 - val_mse: 0.0063\nEpoch 31/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 31: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 32/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0077 - mse: 0.0077\nEpoch 32: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 33/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 33: val_loss improved from 0.00632 to 0.00623, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 34/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0082 - mse: 0.0082\nEpoch 34: val_loss improved from 0.00623 to 0.00614, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0061 - val_mse: 0.0061\nEpoch 35/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0080 - mse: 0.0080\nEpoch 35: val_loss improved from 0.00614 to 0.00604, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 36/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 36: val_loss did not improve from 0.00604\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 37/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 37: val_loss improved from 0.00604 to 0.00601, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 38/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 38: val_loss improved from 0.00601 to 0.00586, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0059 - val_mse: 0.0059\nEpoch 39/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0078 - mse: 0.0078\nEpoch 39: val_loss did not improve from 0.00586\n38/38 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 40/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 40: val_loss improved from 0.00586 to 0.00551, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 41/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 41: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 42/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0068 - mse: 0.0068\nEpoch 42: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 43/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 43: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 44/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0071 - mse: 0.0071\nEpoch 44: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 45/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 45: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 46/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 46: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 47/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 47: val_loss improved from 0.00551 to 0.00509, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 48/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 48: val_loss did not improve from 0.00509\n38/38 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0054 - val_mse: 0.0054\nEpoch 49/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0062 - mse: 0.0062\nEpoch 49: val_loss improved from 0.00509 to 0.00507, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 50/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 50: val_loss did not improve from 0.00507\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0056 - val_mse: 0.0056\n\n\n\n# loss 히스토리 확인\nfig, loss_ax = plt.subplots()\nloss_ax.plot(hist.history['loss'], 'r', label='loss')\nloss_ax.plot(hist.history['val_loss'], 'g', label='val_loss')\nloss_ax.set_xlabel('epoch')\nloss_ax.set_ylabel('loss')\nloss_ax.legend()\nplt.title('Training loss - Validation loss plot')\nplt.show()\n\n\n\n\n\n# 저장된 가중치 불러오기\nmodel.load_weights('baseline.h5')\n\n\n# 테스트셋 전처리 및 추론\ntest_inputs = pd.read_csv('./farm/test_input.csv')\noutput_sample = pd.read_csv('./farm/answer_sample.csv')\n\ntest_inputs = test_inputs[inputs.columns]\ntest_inputs['주차'] = [int(i.replace('주차', \"\")) for i in test_inputs['주차']]\ntest_input_sc = input_scaler.transform(test_inputs.iloc[:,3:].to_numpy())\n\ntest_input_ts = []\nfor i in output_sample['Sample_no']:\n    sample = test_input_sc[test_inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    test_input_ts.append(sample)\ntest_input_ts = np.concatenate(test_input_ts, axis=0)\n\n\ntest_input_ts.shape\n\n(506, 7, 9)\n\n\n\nprediction = model.predict(test_input_ts)\n\nprediction = output_scaler.inverse_transform(prediction)\noutput_sample[['생장길이', '줄기직경', '개화군']] = prediction\n\n16/16 [==============================] - 0s 793us/step\n\n\n\noutput_sample\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n9\n20220413\n32주차\n47.151882\n7.198693\n13.229403\n\n\n1\n12\n20170312\n30주차\n420.956116\n3.321363\n10.489825\n\n\n2\n19\n20170319\n31주차\n589.641235\n4.079537\n7.553223\n\n\n3\n23\n20170326\n32주차\n281.593994\n4.563877\n7.977988\n\n\n4\n27\n20170430\n37주차\n89.870880\n7.789731\n4.768530\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n501\n2015\n20160508\n14주차\n188.253265\n12.274371\n5.467629\n\n\n502\n2016\n20160529\n17주차\n1998.461670\n4.776089\n1.905745\n\n\n503\n2024\n20160828\n7주차\n49.577644\n13.211569\n2.025992\n\n\n504\n2025\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n505\n2026\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n\n\n506 rows × 6 columns\n\n\n\n\n# 제출할 추론 결과 저장\noutput_sample.to_csv('prediction.csv', index=False)\n\n- 텐서보드\n\n# 학습\ncb1 = tf.keras.callbacks.TensorBoard()\nmodel.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[cb1])\n\nEpoch 1/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 2/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 3/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 4/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 5/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 6/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 7/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 8/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 9/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 10/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 11/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 12/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 13/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 14/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 15/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 16/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 17/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0044 - val_mse: 0.0044\nEpoch 18/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 19/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 20/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0050 - val_mse: 0.0050\nEpoch 21/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 22/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 23/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 24/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 25/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 26/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 27/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 28/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 29/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 30/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 31/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 32/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 33/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 34/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 35/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 36/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 37/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 38/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 39/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 40/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 41/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 42/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 43/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 44/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 45/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 46/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0037 - val_mse: 0.0037\nEpoch 47/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 48/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 49/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 50/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n\n\n&lt;keras.callbacks.History at 0x7f2df8e88b80&gt;\n\n\n\n%load_ext tensorboard\n%tensorboard --logdir logs --host \n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\nReusing TensorBoard on port 6006 (pid 109833), started 0:04:21 ago. (Use '!kill 109833' to kill it.)"
  },
  {
    "objectID": "posts/6_note/2023-09-14-autogluon_ts.html",
    "href": "posts/6_note/2023-09-14-autogluon_ts.html",
    "title": "AutoGluon TS",
    "section": "",
    "text": "Autogluon Timeseries는 AWS에서의 연구 및 개발을 기반으로 하며, 오픈 소스 커뮤니티와 공개 소프트웨어로 제공되고 있습니다. 이 라이브러리는 Python 언어로 작성되었으며, 주로 주피터 노트북과 함께 사용되며, 다양한 시계열 데이터에 대한 모델을 빠르게 실험하고 평가하는 데 도움이 됩니다.\n\nautogluon.timeseries 라이브러리를 이용하여 시계열 예측을 하려면 아래와 같은 두개의 클래스를 이용하면 된다.\n1. : TimeSeriesDataFrame\n\n시계열 데이터를 로드하고, 데이터의 전처리, 탐색 및 준비를 수행하는 단계.\n각 시계열은 “item_id,” “timestamp,” 그리고 “target” 열과 같은 기본 열 구조를 따라야 한다.\n\n2. : TimeSeriesPredictor (1-학습/2-예측/3-평가)\n\nfit/predict/leaderboard\n시계열 데이터에 대한 모델을 훈련하고 예측을 하는 단계.\n시계열 모델을 자동으로 피팅(fitting)하고 모델 선택 및 하이퍼파라미터 튜닝을 수행한다.\n다양한 시계열 모델 및 예측 알고리즘을 자동으로 적용하고 최적의 모델을 선택하여 사용자에게 최상의 예측 결과를 제공한다.\n\n위를 요약하면, autogluon.timeseries 라이브러리를 사용하여 시계열 데이터를 저장/관리(TimeSeriesDataFrame)를 하고, 이러한 데이터를 기반으로 모델을 훈련하고 예측을 하는(TimeSeriesPredictor) 자동화된 시계열 예측 작업을 수행할 수 있다."
  },
  {
    "objectID": "posts/6_note/2023-09-14-autogluon_ts.html#autogluon.timeseries",
    "href": "posts/6_note/2023-09-14-autogluon_ts.html#autogluon.timeseries",
    "title": "AutoGluon TS",
    "section": "",
    "text": "Autogluon Timeseries는 AWS에서의 연구 및 개발을 기반으로 하며, 오픈 소스 커뮤니티와 공개 소프트웨어로 제공되고 있습니다. 이 라이브러리는 Python 언어로 작성되었으며, 주로 주피터 노트북과 함께 사용되며, 다양한 시계열 데이터에 대한 모델을 빠르게 실험하고 평가하는 데 도움이 됩니다.\n\nautogluon.timeseries 라이브러리를 이용하여 시계열 예측을 하려면 아래와 같은 두개의 클래스를 이용하면 된다.\n1. : TimeSeriesDataFrame\n\n시계열 데이터를 로드하고, 데이터의 전처리, 탐색 및 준비를 수행하는 단계.\n각 시계열은 “item_id,” “timestamp,” 그리고 “target” 열과 같은 기본 열 구조를 따라야 한다.\n\n2. : TimeSeriesPredictor (1-학습/2-예측/3-평가)\n\nfit/predict/leaderboard\n시계열 데이터에 대한 모델을 훈련하고 예측을 하는 단계.\n시계열 모델을 자동으로 피팅(fitting)하고 모델 선택 및 하이퍼파라미터 튜닝을 수행한다.\n다양한 시계열 모델 및 예측 알고리즘을 자동으로 적용하고 최적의 모델을 선택하여 사용자에게 최상의 예측 결과를 제공한다.\n\n위를 요약하면, autogluon.timeseries 라이브러리를 사용하여 시계열 데이터를 저장/관리(TimeSeriesDataFrame)를 하고, 이러한 데이터를 기반으로 모델을 훈련하고 예측을 하는(TimeSeriesPredictor) 자동화된 시계열 예측 작업을 수행할 수 있다."
  },
  {
    "objectID": "posts/6_note/2023-09-14-autogluon_ts.html#loading-time-series-data-as-a-timeseriesdataframe",
    "href": "posts/6_note/2023-09-14-autogluon_ts.html#loading-time-series-data-as-a-timeseriesdataframe",
    "title": "AutoGluon TS",
    "section": "1. Loading time series data as a TimeSeriesDataFrame",
    "text": "1. Loading time series data as a TimeSeriesDataFrame\n\nimport pandas as pd\nfrom autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n\n\ndf = pd.read_csv(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly_subset/train.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nitem_id\ntimestamp\ntarget\n\n\n\n\n0\nH1\n1750-01-01 00:00:00\n605.0\n\n\n1\nH1\n1750-01-01 01:00:00\n586.0\n\n\n2\nH1\n1750-01-01 02:00:00\n586.0\n\n\n3\nH1\n1750-01-01 03:00:00\n559.0\n\n\n4\nH1\n1750-01-01 04:00:00\n511.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAutoGluon을 사용하려면 시계열 데이터가 long format 이어야 한다.\n\n\n\ntrain_data = TimeSeriesDataFrame.from_data_frame(\n    df,\n    id_column=\"item_id\",\n    timestamp_column=\"timestamp\"\n)\n\n\ntrain_data\n\n\n\n\n\n\n\n\n\ntarget\n\n\nitem_id\ntimestamp\n\n\n\n\n\nH1\n1750-01-01 00:00:00\n605.0\n\n\n1750-01-01 01:00:00\n586.0\n\n\n1750-01-01 02:00:00\n586.0\n\n\n1750-01-01 03:00:00\n559.0\n\n\n1750-01-01 04:00:00\n511.0\n\n\n...\n...\n...\n\n\nH200\n1750-02-09 19:00:00\n24.2\n\n\n1750-02-09 20:00:00\n22.6\n\n\n1750-02-09 21:00:00\n20.8\n\n\n1750-02-09 22:00:00\n19.6\n\n\n1750-02-09 23:00:00\n18.8\n\n\n\n\n148060 rows × 1 columns\n\n\n\n\ntype(train_data)\n\nautogluon.timeseries.dataset.ts_dataframe.TimeSeriesDataFrame\n\n\n- item\nTimeSeriesDataFrame 클래스를 사용하여 여러 개의 시계열 데이터를 저장하며, 이러한 각각의 시계열 데이터를 “item” 이라고 부른다.\n예를들어, 수요예측에서 각 “item”은 각각 다른 제품을 나타낼 수도 있고, 금융 데이터셋에서는 각 “item”이 주식을 나타낼 수 있음.\n- 다변량 예측과의 차이\n“multivariate forecasting”과 다르다는 것을 주의하자. “Multivariate forecasting”은 여러 변수 간의 상호작용을 모델링하고 다른 시계열 간의 관계를 고려하여 예측하는 것을 의미하는 반면 AutoGluon은 각 시계열을 개별적으로 처리하므로 “item” 간의 상호작용을 고려하지 않는다.\n- pd.DataFrame을 상속\n\n# TimeSeriesDataFrame??\n\n\n\n\nTimeSeriesDataFrame은 pandas.DataFrame을 상속하고 있어 pandas.DataFrame의 모든 속성 및 메서드를 사용할 수 있다."
  },
  {
    "objectID": "posts/6_note/2023-09-14-autogluon_ts.html#training-time-series-models-with-timeseriespredictor.fit",
    "href": "posts/6_note/2023-09-14-autogluon_ts.html#training-time-series-models-with-timeseriespredictor.fit",
    "title": "AutoGluon TS",
    "section": "2. Training time series models with TimeSeriesPredictor.fit",
    "text": "2. Training time series models with TimeSeriesPredictor.fit\n시계열 예측을 위해 TimeSeriesPredictor 오브젝트를 생성하자.\n\n- predictor 생성\n\npredictor = TimeSeriesPredictor(\n    prediction_length=48, # 시간별 자료 // 미래의 2일을 예측하고 싶어!\n    path=\"autogluon-m4-hourly\", # 학습된 모델 저장폴더 경로\n    target=\"target\",\n    eval_metric=\"MASE\",\n)\n\n\n\n- 적합\n\npredictor.fit(\n    train_data,\n    presets=\"medium_quality\", # 훈련시간 10분 제한 (medium_quality / fast_training / high_quality / best_quality)\n    time_limit=600,\n)\n\n================ TimeSeriesPredictor ================\nTimeSeriesPredictor.fit() called\nSetting presets to: medium_quality\nFitting with arguments:\n{'enable_ensemble': True,\n 'evaluation_metric': 'MASE',\n 'excluded_model_types': None,\n 'hyperparameter_tune_kwargs': None,\n 'hyperparameters': 'medium_quality',\n 'num_val_windows': 1,\n 'prediction_length': 48,\n 'random_seed': None,\n 'target': 'target',\n 'time_limit': 600,\n 'verbosity': 2}\nProvided training data set with 148060 rows, 200 items (item = single time series). Average time series length is 740.3. Data frequency is 'H'.\n=====================================================\nAutoGluon will save models to autogluon-m4-hourly/\nAutoGluon will gauge predictive performance using evaluation metric: 'MASE'\n    This metric's sign has been flipped to adhere to being 'higher is better'. The reported score can be multiplied by -1 to get the metric value.\n\nProvided dataset contains following columns:\n    target:           'target'\n\nStarting training. Start time is 2023-09-15 00:47:18\nModels that will be trained: ['Naive', 'SeasonalNaive', 'Theta', 'AutoETS', 'RecursiveTabular', 'DeepAR']\nTraining timeseries model Naive. Training for up to 599.88s of the 599.88s of remaining time.\n    -6.6629       = Validation score (-MASE)\n    0.06    s     = Training runtime\n    4.91    s     = Validation (prediction) runtime\nTraining timeseries model SeasonalNaive. Training for up to 594.90s of the 594.90s of remaining time.\n    -1.2169       = Validation score (-MASE)\n    0.05    s     = Training runtime\n    0.10    s     = Validation (prediction) runtime\nTraining timeseries model Theta. Training for up to 594.74s of the 594.74s of remaining time.\n    -2.1425       = Validation score (-MASE)\n    0.06    s     = Training runtime\n    29.74   s     = Validation (prediction) runtime\nTraining timeseries model AutoETS. Training for up to 564.94s of the 564.94s of remaining time.\n    -1.9400       = Validation score (-MASE)\n    0.06    s     = Training runtime\n    31.95   s     = Validation (prediction) runtime\nTraining timeseries model RecursiveTabular. Training for up to 532.92s of the 532.92s of remaining time.\n    -0.8988       = Validation score (-MASE)\n    5.95    s     = Training runtime\n    1.25    s     = Validation (prediction) runtime\nTraining timeseries model DeepAR. Training for up to 525.71s of the 525.71s of remaining time.\n    -1.5221       = Validation score (-MASE)\n    47.08   s     = Training runtime\n    0.77    s     = Validation (prediction) runtime\nFitting simple weighted ensemble.\n    -0.8823       = Validation score (-MASE)\n    2.68    s     = Training runtime\n    34.07   s     = Validation (prediction) runtime\nTraining complete. Models trained: ['Naive', 'SeasonalNaive', 'Theta', 'AutoETS', 'RecursiveTabular', 'DeepAR', 'WeightedEnsemble']\nTotal runtime: 124.87 s\nBest model: WeightedEnsemble\nBest model score: -0.8823\n\n\n&lt;autogluon.timeseries.predictor.TimeSeriesPredictor at 0x7f5246b844c0&gt;\n\n\n- presets\n프리셋이란, 말그대로 미리 정의해둔 설정이다. 즉, AutoGluon에서 사용할 모델과 훈련설정을 미리 정의해둔 설정을 말한다. 이 설정은 모델 선택과 하이퍼파라미터 튜닝을 자동화하여 더 빠르게 원하는 예측 모델을 찾을 수 있도록 도와준다.\n\n옵션: medium_quality, fast_training, high_quality, best_quality 등을 제공\n\n더 높은 품질의 프리셋은 일반적으로 더 정확한 예측 결과를 생성하지만 훈련에 더 많은 시간이 소요될 수 있다.\n\n모델: medium_quality는 간단한 기본모델인 Naive, SeasonalNaive와 통계모델 (AutoETS, Theta), 트리기반 모델(LightGBM), 심층학습모델(DeepAR) 및 이러한 모델을 결합한 가중앙상블이 포함된다.\n\n여기서 medium_quality는 훈련시간 10분 내로 제한하고, 이 모델 중 가장 좋은 예측 성능을 얻으려고 노력한다."
  },
  {
    "objectID": "posts/6_note/2023-09-14-autogluon_ts.html#generating-forecasts-with-timeseriespredictor.predict",
    "href": "posts/6_note/2023-09-14-autogluon_ts.html#generating-forecasts-with-timeseriespredictor.predict",
    "title": "AutoGluon TS",
    "section": "3. Generating forecasts with TimeSeriesPredictor.predict",
    "text": "3. Generating forecasts with TimeSeriesPredictor.predict\n\npredictions = predictor.predict(train_data)\npredictions.head()\n\nGlobal seed set to 123\nModel not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n\n\n\n\n\n\n\n\n\n\nmean\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\n\nitem_id\ntimestamp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH1\n1750-01-30 04:00:00\n624.225400\n592.533253\n603.438425\n611.274422\n617.975393\n624.239736\n630.498044\n637.191945\n645.042920\n655.908431\n\n\n1750-01-30 05:00:00\n558.038120\n515.235197\n530.005515\n540.575718\n549.627181\n558.071079\n566.513349\n575.543314\n586.115937\n600.829335\n\n\n1750-01-30 06:00:00\n515.495213\n464.240430\n481.872256\n494.568026\n505.378544\n515.514013\n525.643002\n536.460744\n549.135748\n566.750336\n\n\n1750-01-30 07:00:00\n481.701016\n423.316455\n443.383962\n457.847898\n470.179675\n481.696121\n493.234340\n505.589204\n520.044493\n540.152060\n\n\n1750-01-30 08:00:00\n459.156714\n394.461819\n416.640338\n432.661205\n446.361193\n459.170291\n471.982521\n485.641357\n501.591228\n523.913884\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# TimeSeriesDataFrame can also be loaded directly from a file\ntest_data = TimeSeriesDataFrame.from_path(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly_subset/test.csv\")\n\nplt.figure(figsize=(20, 3))\n\nitem_id = \"H1\"\ny_past = train_data.loc[item_id][\"target\"]\ny_pred = predictions.loc[item_id]\ny_test = test_data.loc[item_id][\"target\"][-48:]\n\nplt.plot(y_past[-200:], label=\"Past time series values\")\nplt.plot(y_pred[\"mean\"], label=\"Mean forecast\")\nplt.plot(y_test, label=\"Future time series values\")\n\nplt.fill_between(\n    y_pred.index, y_pred[\"0.1\"], y_pred[\"0.9\"], color=\"red\", alpha=0.1, label=f\"10%-90% confidence interval\"\n)\nplt.legend();"
  },
  {
    "objectID": "posts/6_note/2023-09-14-autogluon_ts.html#evaluating-the-performance-of-different-models",
    "href": "posts/6_note/2023-09-14-autogluon_ts.html#evaluating-the-performance-of-different-models",
    "title": "AutoGluon TS",
    "section": "4. Evaluating the performance of different models",
    "text": "4. Evaluating the performance of different models\n- 리더보드 확인\n\n# The test score is computed using the last\n# prediction_length=48 timesteps of each time series in test_data\npredictor.leaderboard(test_data, silent=True)\n\nAdditional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time_marginal\nfit_order\n\n\n\n\n0\nWeightedEnsemble\n-0.850095\n-0.882350\n48.495575\n34.074169\n2.684069\n7\n\n\n1\nRecursiveTabular\n-0.870271\n-0.898770\n0.802261\n1.253653\n5.947196\n5\n\n\n2\nSeasonalNaive\n-1.022854\n-1.216909\n0.107641\n0.095930\n0.053485\n2\n\n\n3\nDeepAR\n-1.586545\n-1.522113\n0.808109\n0.772847\n47.082557\n6\n\n\n4\nAutoETS\n-1.778462\n-1.939952\n46.754016\n31.951739\n0.059222\n4\n\n\n5\nTheta\n-1.905365\n-2.142531\n34.860988\n29.740138\n0.058667\n3\n\n\n6\nNaive\n-6.696079\n-6.662942\n0.103503\n4.909723\n0.061440\n1"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-x6USW3HM9Lm-B19o9qrm19&si=EFy8hdlgDJ-LUFHi"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#a.-데이터",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#a.-데이터",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"titanic/train.csv\")\ntst = TabularDataset(\"titanic/test.csv\")"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#b.-predictor-생성",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230913_064329/\""
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#c.-적합fit",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#c.-적합fit",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr,presets='best_quality') # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230913_064329/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.16\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   675.44 GB / 982.82 GB (68.7%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    36523.72 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.18s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6308   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6364   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.835    = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    0.43s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8373   = Validation score   (accuracy)\n    0.35s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8361   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8608   = Validation score   (accuracy)\n    1.59s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8294   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8328   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.853    = Validation score   (accuracy)\n    2.15s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    0.57s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8462   = Validation score   (accuracy)\n    3.68s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    1.06s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8608   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 19.56s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230913_064329/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fb1783c2160&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.860831       0.034114  1.593431                0.034114           1.593431            1       True          7\n1       WeightedEnsemble_L2   0.860831       0.035236  2.013822                0.001122           0.420391            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.852974       0.067540  2.145609                0.067540           2.145609            1       True         10\n3     NeuralNetTorch_BAG_L1   0.846240       0.073878  3.684342                0.073878           3.684342            1       True         12\n4           LightGBM_BAG_L1   0.840629       0.024179  0.434251                0.024179           0.434251            1       True          4\n5      LightGBMLarge_BAG_L1   0.840629       0.027767  1.059269                0.027767           1.059269            1       True         13\n6            XGBoost_BAG_L1   0.840629       0.035949  0.567940                0.035949           0.567940            1       True         11\n7   RandomForestGini_BAG_L1   0.837262       0.056912  0.350744                0.056912           0.350744            1       True          5\n8   RandomForestEntr_BAG_L1   0.836139       0.054147  0.253396                0.054147           0.253396            1       True          6\n9         LightGBMXT_BAG_L1   0.835017       0.026695  0.332248                0.026695           0.332248            1       True          3\n10    ExtraTreesEntr_BAG_L1   0.832772       0.075741  0.255411                0.075741           0.255411            1       True          9\n11    ExtraTreesGini_BAG_L1   0.829405       0.054623  0.255257                0.054623           0.255257            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.005361  0.003128                0.005361           0.003128            1       True          2\n13    KNeighborsUnif_BAG_L1   0.630752       0.033042  0.002380                0.033042           0.002380            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.860831\n0.034114\n1.593431\n0.034114\n1.593431\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.860831\n0.035236\n2.013822\n0.001122\n0.420391\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.852974\n0.067540\n2.145609\n0.067540\n2.145609\n1\nTrue\n10\n\n\n3\nNeuralNetTorch_BAG_L1\n0.846240\n0.073878\n3.684342\n0.073878\n3.684342\n1\nTrue\n12\n\n\n4\nLightGBM_BAG_L1\n0.840629\n0.024179\n0.434251\n0.024179\n0.434251\n1\nTrue\n4\n\n\n5\nLightGBMLarge_BAG_L1\n0.840629\n0.027767\n1.059269\n0.027767\n1.059269\n1\nTrue\n13\n\n\n6\nXGBoost_BAG_L1\n0.840629\n0.035949\n0.567940\n0.035949\n0.567940\n1\nTrue\n11\n\n\n7\nRandomForestGini_BAG_L1\n0.837262\n0.056912\n0.350744\n0.056912\n0.350744\n1\nTrue\n5\n\n\n8\nRandomForestEntr_BAG_L1\n0.836139\n0.054147\n0.253396\n0.054147\n0.253396\n1\nTrue\n6\n\n\n9\nLightGBMXT_BAG_L1\n0.835017\n0.026695\n0.332248\n0.026695\n0.332248\n1\nTrue\n3\n\n\n10\nExtraTreesEntr_BAG_L1\n0.832772\n0.075741\n0.255411\n0.075741\n0.255411\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.829405\n0.054623\n0.255257\n0.054623\n0.255257\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.005361\n0.003128\n0.005361\n0.003128\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.630752\n0.033042\n0.002380\n0.033042\n0.002380\n1\nTrue\n1\n\n\n\n\n\n\n\n\nCatBoost_BAG_L1"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#d.-예측-predict",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.898989898989899\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"./titanic_sub/autogluon(best_quality)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#submission-result",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#submission-result",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "Submission Result",
    "text": "Submission Result\n\n\n\nSub and Description\nPublic Score\n\n\n\n\nautogulon_sub\n0.75358\n\n\nautogulon(Fsize)_sub\n0.77272\n\n\nautogulon(Fsize,Drop)\n0.78947\n\n\nautogulon(best_quality)\n0.80143"
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-023.html",
    "href": "posts/5_STBDA2023/06wk-023.html",
    "title": "06wk-023: 취업+각종영어점수, Ridge",
    "section": "",
    "text": "오버피팅을 해결하는 방법 중 하나로 Ridge Regression을 소개한다."
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-023.html#a.-정확한-설명",
    "href": "posts/5_STBDA2023/06wk-023.html#a.-정확한-설명",
    "title": "06wk-023: 취업+각종영어점수, Ridge",
    "section": "A. 정확한 설명",
    "text": "A. 정확한 설명\n- SVD를 이용하여 이론적인 계산하면 sklearn.linear_model.LinearRegression()로 적합한 결과보다 sklearn.linear_model.Ridge()로 적합한 결과를 더 좋게 만드는 \\(\\alpha\\)가 항상 존재함을 증명할 수 있음."
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-023.html#b.-직관적-설명-엄밀하지-않은-설명",
    "href": "posts/5_STBDA2023/06wk-023.html#b.-직관적-설명-엄밀하지-않은-설명",
    "title": "06wk-023: 취업+각종영어점수, Ridge",
    "section": "B. 직관적 설명 (엄밀하지 않은 설명)",
    "text": "B. 직관적 설명 (엄밀하지 않은 설명)\n\nStep1: LinearRegression은 왜 망했는가?\n- 토익의 계수는 실제로 \\(\\frac{1}{100}\\)이다. 적딩히\n\ntoeic_coef + … + toeic499_coef \\(\\approx\\) 0.01 이라면\n\n대충 맞는 답이다.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.LinearRegression()\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n#---# \nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 1.0000\ntest_score: 0.1171\n\n\n\ns= pd.Series(predictr.coef_)\ns.index = X.columns\ns[1:].sum()\n\n0.010302732920631844\n\n\n- 그런데 사실 저 0.01이라는 값은 몇개의 계수만 있어도 만들 수 있다. (toeic2와 toeic3에 해당하는 계수)\n\ns['toeic2']+s['toeic3']\n\n0.010142832986793814\n\n\n- 이런논리로 치면 toeic2, toeic3에 해당하는 계수만 있다면 사실 \\(y\\)를 설명하는데 충분했고, 나머지는 불필요한 특징이 된다. (그리고 불필요한 특징은 오버피팅을 유발한다)\n\n\nStep2: Ridge의 아이디어\n- Ridge의 아이디어: toeic2, toeic3 와 같이 몇개의 변수로만 0.01이라는 수를 설명할 수 없도록 “강제”하자. 즉 몇개의 변수로만 0.01이라는 수를 설명할 수 없도록 “패널티”를 주자.\n- 패널티: 유사토익들의 계수값을 제곱한뒤 합치고(=L2-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n이러한 패널티를 줄 경우 결과적으로 0.01의 값이 “동일하게 나누어져서” 나오는 값(=\\(\\frac{1}{100}\\frac{1}{501}\\))이 계수값으로 추정된다. (왜? – 요건 정확하게 이해하는게 사실 지금은 힘듬)\n패널티를 주는정도? \\(\\alpha\\)로 조절함.. \\(\\alpha\\)를 크게 할수록 패널티를 많이줌.\n\n- 잘 적용된 Ridge의 결과를 보면 아래와 같이 계수값이 저장되어 있음.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge(alpha=5e8)\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n#---# \nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.7507\ntest_score: 0.7438\n\n\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns\ns\n\ngpa         0.000001\ntoeic       0.000019\ntoeic0      0.000018\ntoeic1      0.000018\ntoeic2      0.000019\n              ...   \ntoeic495    0.000018\ntoeic496    0.000019\ntoeic497    0.000019\ntoeic498    0.000019\ntoeic499    0.000019\nLength: 502, dtype: float64\n\n\n\ns[1], s[2], s[3]\n\n(1.899380907380025e-05, 1.7776834322452106e-05, 1.821183320256662e-05)\n\n\n\n0.01/501\n\n1.9960079840319362e-05\n\n\n- 결국 Ridge를 사용하면 계수들의 값이 “동일하게 나누어지는” 효과가 나타남\n- 패널티를 주는 정도? \\(\\alpha\\)로 조절함.. \\(\\alpha\\)를 크게 할수록 패널티를 많이줌"
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-023.html#c.-alpha에-따른-계수값-변화",
    "href": "posts/5_STBDA2023/06wk-023.html#c.-alpha에-따른-계수값-변화",
    "title": "06wk-023: 취업+각종영어점수, Ridge",
    "section": "C. \\(\\alpha\\)에 따른 계수값 변화",
    "text": "C. \\(\\alpha\\)에 따른 계수값 변화\n- 여러개의 predictor 학습\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\nalphas = [5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8]\npredictrs = [sklearn.linear_model.Ridge(alpha=alpha) for alpha in alphas]\n## step3 \nfor predictr in predictrs:\n    predictr.fit(X,y)\n## step4 -- pass \n\n- 계수값 시각화\n\npredictrs[0].alpha\n\n500.0\n\n\n\n# toeic에 해당하는 coef 시각화\nplt.plot(predictrs[0].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[0].alpha)) \nplt.plot(predictrs[3].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[3].alpha))\nplt.legend(loc=1)\n\n&lt;matplotlib.legend.Legend at 0x7f2e97e7f970&gt;\n\n\n\n\n\n\nplt.plot(predictrs[3].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[3].alpha))\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2e97d1dee0&gt;\n\n\n\n\n\n\nplt.plot(predictrs[5].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[5].alpha))\nplt.plot(predictrs[-1].coef_[1:],label=r'$\\alpha$={}'.format(predictrs[-1].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f2e97cd7910&gt;\n\n\n\n\n\n- 직관: 마지막 predictor의 계수값을 살펴보자.\n\npredictrs[-1].coef_\n\narray([1.10421248e-06, 1.89938091e-05, 1.77768343e-05, 1.82118332e-05,\n       1.90895673e-05, 1.87128138e-05, 1.90343037e-05, 1.82483251e-05,\n       1.90405022e-05, 1.85802628e-05, 1.90021086e-05, 1.88952130e-05,\n       1.96003229e-05, 1.89154663e-05, 1.86638217e-05, 1.92666606e-05,\n       1.97107043e-05, 1.92214868e-05, 1.92961317e-05, 1.93321368e-05,\n       1.92194541e-05, 1.85663279e-05, 1.86805137e-05, 1.81649873e-05,\n       1.78656367e-05, 1.83171419e-05, 1.94428947e-05, 1.89710925e-05,\n       2.00598946e-05, 1.88384883e-05, 1.98903125e-05, 1.81113551e-05,\n       1.85043847e-05, 1.84424971e-05, 1.91508275e-05, 1.97427867e-05,\n       1.93598061e-05, 1.98264264e-05, 1.89934042e-05, 1.84770850e-05,\n       1.83617634e-05, 1.79346774e-05, 1.84943159e-05, 1.89803006e-05,\n       1.78633749e-05, 1.80073666e-05, 1.85664525e-05, 1.97390143e-05,\n       1.86574281e-05, 1.92233226e-05, 1.91281904e-05, 1.85617627e-05,\n       1.83939489e-05, 1.84309427e-05, 1.88142167e-05, 1.84159665e-05,\n       1.94078579e-05, 1.84515402e-05, 1.88107980e-05, 1.85889903e-05,\n       1.89357356e-05, 1.88750847e-05, 1.92107444e-05, 1.81799279e-05,\n       1.92122152e-05, 1.97863670e-05, 1.89851436e-05, 1.88974919e-05,\n       1.88566578e-05, 1.95841935e-05, 1.86398380e-05, 1.95801159e-05,\n       1.87550098e-05, 1.87392625e-05, 1.87462595e-05, 1.96056001e-05,\n       1.80626630e-05, 1.88237701e-05, 1.83108446e-05, 1.88087164e-05,\n       1.84723703e-05, 1.84767748e-05, 1.89267252e-05, 1.87604297e-05,\n       1.86945591e-05, 1.92924236e-05, 1.77843453e-05, 1.85415541e-05,\n       1.91448999e-05, 1.98281375e-05, 1.97994651e-05, 1.86653004e-05,\n       1.87298830e-05, 1.87474975e-05, 1.90018315e-05, 1.92043808e-05,\n       1.88941675e-05, 1.81646176e-05, 1.91508494e-05, 2.04322537e-05,\n       1.92111546e-05, 1.93061022e-05, 1.92088349e-05, 1.80206353e-05,\n       1.89399818e-05, 1.96895533e-05, 1.94410839e-05, 1.92051217e-05,\n       1.84961416e-05, 1.89785667e-05, 1.92235780e-05, 1.86729143e-05,\n       1.88439733e-05, 1.76776615e-05, 1.87493841e-05, 1.86986837e-05,\n       1.81917859e-05, 1.94657238e-05, 1.82063420e-05, 1.78143049e-05,\n       1.88432683e-05, 1.90674860e-05, 1.86411824e-05, 1.93286721e-05,\n       1.75163829e-05, 1.86852659e-05, 2.02343956e-05, 1.82025623e-05,\n       1.89153395e-05, 1.98862774e-05, 1.94775038e-05, 1.90665531e-05,\n       1.94170642e-05, 1.88227118e-05, 1.88792179e-05, 1.89712787e-05,\n       1.87855482e-05, 1.87895464e-05, 2.00798925e-05, 1.97167119e-05,\n       1.91644137e-05, 1.90990710e-05, 1.85836048e-05, 1.82346595e-05,\n       1.85731253e-05, 1.84871242e-05, 1.90728256e-05, 1.90277156e-05,\n       1.93085319e-05, 1.91719254e-05, 1.80097271e-05, 1.82517485e-05,\n       1.90904218e-05, 1.85232604e-05, 1.88184612e-05, 1.84002976e-05,\n       2.00337440e-05, 1.86478638e-05, 1.93507546e-05, 1.85547358e-05,\n       1.97154574e-05, 1.91189346e-05, 1.93320777e-05, 1.85313268e-05,\n       1.91085306e-05, 1.88406812e-05, 1.87444892e-05, 1.96637559e-05,\n       1.83552699e-05, 1.80759243e-05, 1.94662845e-05, 1.93761303e-05,\n       1.98339288e-05, 1.87139235e-05, 1.91131387e-05, 1.85801855e-05,\n       1.91544816e-05, 1.98413649e-05, 1.84027849e-05, 1.81842651e-05,\n       1.95888229e-05, 1.80738476e-05, 1.92457286e-05, 1.91474170e-05,\n       1.88737956e-05, 1.78029998e-05, 1.97734483e-05, 1.92409710e-05,\n       1.97346045e-05, 1.99425451e-05, 1.89157923e-05, 1.82538525e-05,\n       1.87475300e-05, 1.79663692e-05, 1.94360535e-05, 1.93333725e-05,\n       1.81368431e-05, 1.91860664e-05, 2.03648683e-05, 1.92870391e-05,\n       1.92561212e-05, 1.92408929e-05, 1.77556464e-05, 1.89317813e-05,\n       1.95230859e-05, 1.91845519e-05, 1.88923023e-05, 1.88368476e-05,\n       1.89013580e-05, 1.82113056e-05, 1.86295402e-05, 1.92236940e-05,\n       1.80025543e-05, 1.92322271e-05, 1.80917953e-05, 1.87188051e-05,\n       1.93772655e-05, 1.87894009e-05, 1.86773984e-05, 1.96830961e-05,\n       1.94593808e-05, 1.99377297e-05, 1.85707832e-05, 1.88667594e-05,\n       1.85589760e-05, 1.98498326e-05, 1.88878514e-05, 1.90686529e-05,\n       1.86868639e-05, 1.90576790e-05, 1.95494214e-05, 1.86567117e-05,\n       1.85992014e-05, 1.77199587e-05, 1.82193592e-05, 1.90965903e-05,\n       1.96016869e-05, 1.88116657e-05, 1.81131528e-05, 1.85436209e-05,\n       1.92951259e-05, 1.92495993e-05, 1.84570073e-05, 1.94529446e-05,\n       1.92760629e-05, 1.92236816e-05, 1.85750512e-05, 1.95451343e-05,\n       1.82912208e-05, 1.88851896e-05, 1.86295173e-05, 1.84150640e-05,\n       1.95101106e-05, 1.98423439e-05, 1.88687440e-05, 1.91657943e-05,\n       1.89387389e-05, 1.89907539e-05, 1.90653825e-05, 1.80854343e-05,\n       1.86906336e-05, 1.85793308e-05, 1.84992786e-05, 1.93964742e-05,\n       1.83344151e-05, 1.89611068e-05, 1.91457644e-05, 1.88755070e-05,\n       1.98511526e-05, 1.93068196e-05, 1.93316489e-05, 1.89507435e-05,\n       1.89083004e-05, 1.91358509e-05, 1.87803906e-05, 1.78160168e-05,\n       1.94603877e-05, 2.02569965e-05, 1.87423291e-05, 1.94609617e-05,\n       1.91292677e-05, 1.85958571e-05, 1.88629266e-05, 1.90600256e-05,\n       1.82221314e-05, 1.95093258e-05, 1.89176339e-05, 2.00028045e-05,\n       1.94052035e-05, 1.86744967e-05, 1.89125601e-05, 2.02089363e-05,\n       1.80569192e-05, 2.02141130e-05, 1.93147541e-05, 1.89011113e-05,\n       1.93335891e-05, 1.96767360e-05, 1.90364715e-05, 1.94635849e-05,\n       1.90397143e-05, 1.91973258e-05, 1.85857694e-05, 1.91487106e-05,\n       1.92897509e-05, 1.99589223e-05, 1.89690091e-05, 1.90089893e-05,\n       1.80391078e-05, 1.89867708e-05, 1.91430968e-05, 1.92719424e-05,\n       1.95648244e-05, 1.85975115e-05, 1.92077870e-05, 1.84415844e-05,\n       1.88715614e-05, 1.85970322e-05, 1.93261490e-05, 1.86726361e-05,\n       1.97716032e-05, 1.92749150e-05, 2.00954709e-05, 1.90876286e-05,\n       1.89190693e-05, 1.98831620e-05, 1.91612367e-05, 1.86269524e-05,\n       1.89155394e-05, 1.89824518e-05, 1.98347756e-05, 1.86788886e-05,\n       1.83508292e-05, 1.85069060e-05, 1.86909372e-05, 1.85978543e-05,\n       1.88150510e-05, 1.89755849e-05, 1.90099289e-05, 1.90515657e-05,\n       1.93189513e-05, 1.82151178e-05, 1.78471089e-05, 1.91763316e-05,\n       1.84903926e-05, 1.92863572e-05, 1.90497739e-05, 1.87657428e-05,\n       1.87801680e-05, 1.85137448e-05, 1.91226761e-05, 1.94084785e-05,\n       1.81950620e-05, 1.81823646e-05, 1.87513814e-05, 1.97922951e-05,\n       1.87200102e-05, 1.98409879e-05, 1.85874173e-05, 1.90513332e-05,\n       1.85234477e-05, 1.81902197e-05, 1.76367508e-05, 1.90389194e-05,\n       1.85299355e-05, 1.95358518e-05, 1.81772601e-05, 1.93671350e-05,\n       1.91528856e-05, 1.91322975e-05, 1.85830738e-05, 1.85626882e-05,\n       1.86250726e-05, 1.84514809e-05, 1.86800234e-05, 1.89256964e-05,\n       1.90280385e-05, 1.88870537e-05, 1.86929332e-05, 1.95167742e-05,\n       1.86377119e-05, 1.93693632e-05, 1.94429807e-05, 1.90730542e-05,\n       1.86276638e-05, 1.86225787e-05, 1.87333026e-05, 1.94293224e-05,\n       1.87174307e-05, 1.93106731e-05, 1.91898445e-05, 1.91446507e-05,\n       1.83627209e-05, 1.85185991e-05, 1.90680366e-05, 1.88180597e-05,\n       1.86586581e-05, 1.80051184e-05, 1.83329730e-05, 1.82088945e-05,\n       1.87516598e-05, 1.82744310e-05, 1.90219092e-05, 1.89098591e-05,\n       1.89001214e-05, 1.90959896e-05, 1.77157866e-05, 1.91760361e-05,\n       1.80496598e-05, 1.85629242e-05, 1.93527162e-05, 1.85046434e-05,\n       1.97977476e-05, 1.82757747e-05, 1.92849021e-05, 1.86829990e-05,\n       1.86752898e-05, 1.95540241e-05, 1.92250030e-05, 1.84817730e-05,\n       1.94636774e-05, 1.86057300e-05, 1.90096458e-05, 1.91037821e-05,\n       1.98095086e-05, 1.92558748e-05, 1.94175627e-05, 1.86155519e-05,\n       1.91386204e-05, 1.89659072e-05, 1.89507918e-05, 1.88868989e-05,\n       1.91223138e-05, 1.81488441e-05, 1.95885497e-05, 1.87850789e-05,\n       1.90457546e-05, 1.96549561e-05, 1.86983597e-05, 1.89788151e-05,\n       1.98384237e-05, 1.99479277e-05, 1.91275095e-05, 1.89970341e-05,\n       1.85749782e-05, 1.91683345e-05, 1.91850806e-05, 1.97386011e-05,\n       1.93320833e-05, 1.92560345e-05, 1.85426153e-05, 1.85185853e-05,\n       1.85764448e-05, 1.94279426e-05, 1.97685699e-05, 1.91733090e-05,\n       1.84972022e-05, 1.89924907e-05, 1.83467563e-05, 1.95149016e-05,\n       1.84410610e-05, 1.86536281e-05, 1.88181888e-05, 1.85487807e-05,\n       1.88565643e-05, 1.89056942e-05, 1.95082352e-05, 1.91711709e-05,\n       1.91422027e-05, 1.91363321e-05, 1.89114818e-05, 1.85390554e-05,\n       1.92949067e-05, 1.88019353e-05, 1.85332879e-05, 1.86699430e-05,\n       1.96934870e-05, 2.01293426e-05, 1.81411289e-05, 1.86806981e-05,\n       1.90987154e-05, 1.85866377e-05, 1.96875267e-05, 1.88785203e-05,\n       1.94435510e-05, 1.85812461e-05, 1.97178935e-05, 1.90067232e-05,\n       2.02306858e-05, 1.86213361e-05, 1.94255182e-05, 1.86417320e-05,\n       1.95689564e-05, 1.97728792e-05, 1.94352125e-05, 1.93768903e-05,\n       1.90643113e-05, 1.79709383e-05, 1.90573271e-05, 1.85638225e-05,\n       1.91337229e-05, 1.86437625e-05])\n\n\n\n불필요한 변수가 나올 수 없는 구조가 되어버렸음 (한 두개로 0.01을 만들 수 없음)\n모든 변수는 대략 2e-5(\\(\\approx \\frac{1}{100}\\frac{1}{501}\\))정도 만큼 똑같이 중요하다고 생각된다.\n고급: **살짝 1/(100*501)보다 전체적으로 값이 작아보이는데, 이는 기분탓이 아님 (Ridge 특징)**\n\n\n1/100*1/501\n\n1.9960079840319362e-05\n\n\n\npredictrs[0].coef_[1:].sum(), predictrs[1].coef_[1:].sum(), predictrs[2].coef_[1:].sum() # 점점 작아진다..\n\n(0.010274546089649755, 0.010157633994737944, 0.009948779293135868)\n\n\n\n\n\n\n\n\nLinear Regression시 문제가 되었던 부분\n\n\n\nLinear Regression을 할 때 toeic2 + toeic3 \\(\\approx 0.01\\) 이 되었다. 즉, 두 개의 변수만 가지고도 계수의 합이 \\(0.01\\) 되어 다른 변수들이 필요가 없었다.\nRidge의 경우 구조상 toeic ~ toeic499의 계수에 거의 똑같은 값이 들어있다. 이러한 구조라면 2,3개를 더해서 0.01을 만들 수가 없다.\n결국 필요없는 변수가 만들어지지 않도록 강제가 된다."
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-023.html#d.-alpha에-따른-실험내용-정리",
    "href": "posts/5_STBDA2023/06wk-023.html#d.-alpha에-따른-실험내용-정리",
    "title": "06wk-023: 취업+각종영어점수, Ridge",
    "section": "D. \\(\\alpha\\)에 따른 실험내용 정리",
    "text": "D. \\(\\alpha\\)에 따른 실험내용 정리\n- 예비개념: L2-penalty는 그냥 대충 분산같은것..\n\nx = np.random.randn(5)\nl2_penalty = (x**2).sum()\nl2_penalty, 5*(x.var()+(x.mean()**2))\n\n(7.084638326878741, 7.084638326878739)\n\n\n- \\(\\alpha\\)가 커질수록 생기는 일\n\n크게 느낀것: 계수들의 값이 점점 비슷해짐 –&gt; 계수들의 값들을 모아서 분산을 구하면 작아진다는 의미 –&gt; L2-penalty 가 작아진다는 의미\n미묘하게 느껴지는 점: toeic, 그리고 toeic0~toeic499 까지의 계수총합은 0.01이 되어야 하는데, 그 총합이 미묘하게 작어지는 느낌.\n\n\nfor predictr in predictrs: \n    print(\n        f'alpha={predictr.alpha:.2e}\\t'\n        f'l2_penalty={((predictr.coef_)**2).sum():.6f}\\t'\n        f'sum(toeic_coefs)={((predictr.coef_[1:])).sum():.4f}\\t'\n        f'test_score={predictr.score(XX,yy):.4f}'\n    )\n\nalpha=5.00e+02  l2_penalty=0.046715 sum(toeic_coefs)=0.0103 test_score=0.2026\nalpha=5.00e+03  l2_penalty=0.021683 sum(toeic_coefs)=0.0102 test_score=0.4638\nalpha=5.00e+04  l2_penalty=0.003263 sum(toeic_coefs)=0.0099 test_score=0.6889\nalpha=5.00e+05  l2_penalty=0.000109 sum(toeic_coefs)=0.0099 test_score=0.7407\nalpha=5.00e+06  l2_penalty=0.000002 sum(toeic_coefs)=0.0099 test_score=0.7447\nalpha=5.00e+07  l2_penalty=0.000000 sum(toeic_coefs)=0.0098 test_score=0.7450\nalpha=5.00e+08  l2_penalty=0.000000 sum(toeic_coefs)=0.0095 test_score=0.7438\n\n\n- L2-penalty의 느낌은 대충 아래와 같이 분산으로 이해해도 무방\n\nfor predictr in predictrs: \n    print(\n        f'alpha={predictr.alpha:.2e}\\t'\n        f'var(coefs)={(predictr.coef_).var()*501:.6f}\\t'\n        f'l2_penalty={(predictr.coef_).var()*501 + ((predictr.coef_).mean())**2*501:.6f}\\t'\n        f'sum(toeic_coefs)={((predictr.coef_[1:])).sum():.4f}\\t'\n        f'test_score={predictr.score(XX,yy):.4f}'\n    )\n\nalpha=5.00e+02  var(coefs)=0.046618 l2_penalty=0.046622 sum(toeic_coefs)=0.0103 test_score=0.2026\nalpha=5.00e+03  var(coefs)=0.021638 l2_penalty=0.021640 sum(toeic_coefs)=0.0102 test_score=0.4638\nalpha=5.00e+04  var(coefs)=0.003256 l2_penalty=0.003256 sum(toeic_coefs)=0.0099 test_score=0.6889\nalpha=5.00e+05  var(coefs)=0.000109 l2_penalty=0.000109 sum(toeic_coefs)=0.0099 test_score=0.7407\nalpha=5.00e+06  var(coefs)=0.000001 l2_penalty=0.000002 sum(toeic_coefs)=0.0099 test_score=0.7447\nalpha=5.00e+07  var(coefs)=0.000000 l2_penalty=0.000000 sum(toeic_coefs)=0.0098 test_score=0.7450\nalpha=5.00e+08  var(coefs)=0.000000 l2_penalty=0.000000 sum(toeic_coefs)=0.0095 test_score=0.7438\n\n\n\nL2 penalty와 var(coefs) 거의 비슷하다. (완전 같지는 않음.)"
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-023.html#e.-alpha가-크다고-무조건-좋은건-아니다.",
    "href": "posts/5_STBDA2023/06wk-023.html#e.-alpha가-크다고-무조건-좋은건-아니다.",
    "title": "06wk-023: 취업+각종영어점수, Ridge",
    "section": "E. \\(\\alpha\\)가 크다고 무조건 좋은건 아니다.",
    "text": "E. \\(\\alpha\\)가 크다고 무조건 좋은건 아니다.\n\n## step1 --- toeic, gpa 만 남기고 나머지 변수를 삭제\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2\npredictr = sklearn.linear_model.Ridge(alpha=1e12) # alpha를 매우 큰 값으로.\n## step3 \npredictr.fit(X,y)\n## step4 -- pass \n\nRidge(alpha=1000000000000.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=1000000000000.0)\n\n\n\nprint(f'train_score={predictr.score(X,y):.4f}')\nprint(f'test_score={predictr.score(XX,yy):.4f}')\n\ntrain_score=0.0191\ntest_score=0.0140\n\n\n\npredictr.coef_[1:].sum() # 이 값이 0.01이어야 하는데, 많이 작아짐\n\n0.0001258531920489157\n\n\ntoeic coefficients의 합이 0.01근처로 나오지 않으면, 모형자체가 적합이 잘 안되기 때문에 각각의 계수들의 값이 똑같아질 수는 있지만, 전체적으로 0에 가까워짐. –&gt; 즉, 알파값이 적당해야한다."
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-019.html#a.-모티브",
    "href": "posts/5_STBDA2023/05wk-019.html#a.-모티브",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 모티브",
    "text": "A. 모티브\n- 예제자료 로드\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 모형을 돌려보고 해석한 결과\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n\n토익이 중요해? 아니면 학점이 중요해?\n얼만큼 중요해?\n\n- 모티브: 토익점수를 0-1사이로 맞추고 gpa도 0-1사이로 맞추면 해석이 쉽지 않을까?\n\n(toeic) 0~1 / 995가 1\n(gpa) 0~1 / 4.5가 1"
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-019.html#b.-사용방법",
    "href": "posts/5_STBDA2023/05wk-019.html#b.-사용방법",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. 사용방법",
    "text": "B. 사용방법\n- 스케일러 생성\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n- fit, transform\n\nsclr.fit(df)\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nsclr.transform(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n\nmin(df.toeic) , max(df.toeic)\n\n(65, 935)\n\n\n\nnp.stack([(df.toeic - min(df.toeic)) / (max(df.toeic) - min(df.toeic)),  (df.gpa - min(df.gpa)) / (max(df.gpa) - min(df.gpa))],axis=1)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n- fit_transform\n\nsclr.fit_transform(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])"
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-019.html#c.-잘못된-사용",
    "href": "posts/5_STBDA2023/05wk-019.html#c.-잘못된-사용",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "C. 잘못된 사용",
    "text": "C. 잘못된 사용\n- sclr.fit()와 sclr.fit_transform()은 입력으로 2차원 자료구조 를 기대한다. (그중에서도 은근히 numpy array를 기대함)\n\ntype(df['toeic'])\n\npandas.core.series.Series\n\n\n\nsclr.fit_transform(df['toeic']) # df['toeic']는 1차원 자료구조\n\nValueError: Expected 2D array, got 1D array instead:\narray=[135. 935. 485.  65. 445.  65. 290. 730.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\n\n\nsclr.fit_transform(df[['toeic']]) # df[['toeic']]는 2차원 자료구조\n\narray([[0.08045977],\n       [1.        ],\n       [0.48275862],\n       [0.        ],\n       [0.43678161],\n       [0.        ],\n       [0.25862069],\n       [0.76436782]])\n\n\n\ntype(df[['toeic']])\n\npandas.core.frame.DataFrame"
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-019.html#a.-사용방법",
    "href": "posts/5_STBDA2023/05wk-019.html#a.-사용방법",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 사용방법",
    "text": "A. 사용방법\n- scaler를 오브젝트로 따로 만들지 않고 함수형으로 구현\n\nsklearn.preprocessing.minmax_scale(df)\n\narray([[0.08045977, 0.        ],\n       [1.        , 0.07772319],\n       [0.48275862, 0.55663499],\n       [0.        , 0.28847292],\n       [0.43678161, 1.        ],\n       [0.        , 0.45907256],\n       [0.25862069, 0.06607128],\n       [0.76436782, 0.07275881]])\n\n\n- 이것은 심지어 1차원 자료구조에도 적용가능하다.\n\nsklearn.preprocessing.minmax_scale(df['toeic']) # 1차원도 해줌.\n\narray([0.08045977, 1.        , 0.48275862, 0.        , 0.43678161,\n       0.        , 0.25862069, 0.76436782])\n\n\n- 열별로 스케일링을 하는게 아니라 행별로 스케일링을 하는 것도 가능하다. (여기서는 필요없지만..)\n\nsklearn.preprocessing.minmax_scale(df,axis=1) # axis=1 : 행별 스케일링\n\narray([[1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.]])"
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-019.html#b.-discussions",
    "href": "posts/5_STBDA2023/05wk-019.html#b.-discussions",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. discussions",
    "text": "B. discussions\n- 언뜻 보기에는 MinMaxScaler 보다 minmax_scale이 좋아보이는데, 생각보다 일반적으로 minmax_scale을 사용하지는 않음. 이유는 아래와 같음.\n\n파이썬을 쓰는 사람들이 함수형 접근방식보다 객체지향 접근방식을 선호한다. (이건 제생각)\n학습데이터와 테스트데이터의 스케일링시 동일한 변환을 유지하는 상황에서는 MinMaxScaler 가 유리함.\ninverse_transform 메서드를 같은 부가기능을 제공함."
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-019.html#a.-잘못된-스케일링-방법-비효율의-문제",
    "href": "posts/5_STBDA2023/05wk-019.html#a.-잘못된-스케일링-방법-비효율의-문제",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "A. 잘못된 스케일링 방법 – 비효율의 문제",
    "text": "A. 잘못된 스케일링 방법 – 비효율의 문제\n\nsklearn.preprocessing.minmax_scale(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsklearn.preprocessing.minmax_scale(XX)\n\narray([[0. ],\n       [0.5],\n       [1. ]])\n\n\n- 이 방법은 전략적으로 비효율적인 문제이지, 치팅과 관련된 치명적인 잘못은 아니다.\n\n만약에 어떠한 경우에 이러한 전처리 방식이 오히려 전략적이라고 판단될 경우 사용할수도 있음."
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-019.html#b.-올바른-스케일링-방법",
    "href": "posts/5_STBDA2023/05wk-019.html#b.-올바른-스케일링-방법",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "B. 올바른 스케일링 방법",
    "text": "B. 올바른 스케일링 방법\n- 방법1\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit(X) # \n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nsclr.transform(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsclr.transform(XX)\n\narray([[0.125],\n       [0.375],\n       [0.625]])\n\n\n- 방법2\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit_transform(X)\n\narray([[0.  ],\n       [0.25],\n       [0.5 ],\n       [0.75],\n       [1.  ]])\n\n\n\nsclr.transform(XX)\n\narray([[0.125],\n       [0.375],\n       [0.625]])"
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-019.html#c.-scaled_value-in-01",
    "href": "posts/5_STBDA2023/05wk-019.html#c.-scaled_value-in-01",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "C. scaled_value \\(\\in\\) [0,1]?",
    "text": "C. scaled_value \\(\\in\\) [0,1]?\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n\nsclr = sklearn.preprocessing.MinMaxScaler()\nsclr.fit_transform(X)\nsclr.transform(XX)\n\narray([[0.16666667],\n       [0.5       ],\n       [1.33333333]])\n\n\n\n스케일링된 값이 1보다 클 수도 있다."
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-019.html#d.-아주-잘못된-스케일링-방법-정보누수",
    "href": "posts/5_STBDA2023/05wk-019.html#d.-아주-잘못된-스케일링-방법-정보누수",
    "title": "05wk-019: MinMaxScaler를 이용한 전처리",
    "section": "D. 아주 잘못된 스케일링 방법 – 정보누수",
    "text": "D. 아주 잘못된 스케일링 방법 – 정보누수\n- 주어진 자료가 아래와 같다고 하자.\n\nX = np.array([1.0, 2.0, 3.0, 4.0, 3.5]).reshape(-1,1)\nXX = np.array([1.5, 2.5, 5.0]).reshape(-1,1)\n\n- 데이터를 합친다.. (미쳤어??)\n\nnp.concatenate([X,XX])\n\narray([[1. ],\n       [2. ],\n       [3. ],\n       [4. ],\n       [3.5],\n       [1.5],\n       [2.5],\n       [5. ]])\n\n\n- 합친데이터에서 스케일링\n\nsklearn.preprocessing.minmax_scale(np.concatenate([X,XX]))[:5]\n\narray([[0.   ],\n       [0.25 ],\n       [0.5  ],\n       [0.75 ],\n       [0.625]])\n\n\n- 이러한 전처리 방식을 정보누수 라고 한다. (대회 규정에 따라서 탈락사유에 해당함)"
  },
  {
    "objectID": "posts/5_STBDA2023/10wk-037.html#a.-max_depth1",
    "href": "posts/5_STBDA2023/10wk-037.html#a.-max_depth1",
    "title": "10wk-037: 아이스크림 – 의사결정나무, max_depth",
    "section": "A. max_depth=1",
    "text": "A. max_depth=1\n- step1~4\n\n## step1\nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1) \n## step3 \npredictr.fit(X,y) \n## step4 -- pass \n# predictr.predict(X) \n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n- 결과 시각화\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,predictr.predict(X),'--.',label='Predicted')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f8bdda52f80&gt;\n\n\n\n\n\n- tree 시각화\n\nsklearn.tree.plot_tree(predictr)\n\n[Text(0.5, 0.75, 'x[0] &lt;= 5.05\\nsquared_error = 111.946\\nsamples = 100\\nvalue = 33.973'),\n Text(0.25, 0.25, 'squared_error = 34.94\\nsamples = 45\\nvalue = 24.788'),\n Text(0.75, 0.25, 'squared_error = 49.428\\nsamples = 55\\nvalue = 41.489')]"
  },
  {
    "objectID": "posts/5_STBDA2023/10wk-037.html#b.-max_depth2",
    "href": "posts/5_STBDA2023/10wk-037.html#b.-max_depth2",
    "title": "10wk-037: 아이스크림 – 의사결정나무, max_depth",
    "section": "B. max_depth=2",
    "text": "B. max_depth=2\n- step1~4\n\n## step1\nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=2) \n## step3 \npredictr.fit(X,y) \n## step4 -- pass \n# predictr.predict(X) \n\nDecisionTreeRegressor(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=2)\n\n\n- 결과 시각화\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,predictr.predict(X),'.--',label='Predicted')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f8bdda8f520&gt;\n\n\n\n\n\n- tree 시각화\n\nsklearn.tree.plot_tree(predictr)\n\n[Text(0.5, 0.8333333333333334, 'x[0] &lt;= 5.05\\nsquared_error = 111.946\\nsamples = 100\\nvalue = 33.973'),\n Text(0.25, 0.5, 'x[0] &lt;= 1.75\\nsquared_error = 34.94\\nsamples = 45\\nvalue = 24.788'),\n Text(0.125, 0.16666666666666666, 'squared_error = 15.12\\nsamples = 19\\nvalue = 19.105'),\n Text(0.375, 0.16666666666666666, 'squared_error = 8.587\\nsamples = 26\\nvalue = 28.94'),\n Text(0.75, 0.5, 'x[0] &lt;= 10.7\\nsquared_error = 49.428\\nsamples = 55\\nvalue = 41.489'),\n Text(0.625, 0.16666666666666666, 'squared_error = 19.819\\nsamples = 47\\nvalue = 39.251'),\n Text(0.875, 0.16666666666666666, 'squared_error = 21.051\\nsamples = 8\\nvalue = 54.638')]"
  },
  {
    "objectID": "posts/5_STBDA2023/10wk-037.html#c.-애니메이션",
    "href": "posts/5_STBDA2023/10wk-037.html#c.-애니메이션",
    "title": "10wk-037: 아이스크림 – 의사결정나무, max_depth",
    "section": "C. 애니메이션",
    "text": "C. 애니메이션\n- step1~4\n\n## step1 \nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictrs = [sklearn.tree.DecisionTreeRegressor(max_depth=k) for k in range(1,11)]\n## step3 \nfor k in range(10):\n    predictrs[k].fit(X,y)\n## step4 -- pass\n\n- 애니메이션\n\nfig = plt.figure()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca()\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.5) \n    ax.plot(X,predictrs[frame].predict(X),'.--') \n    ax.set_title(f'max_depth={predictrs[frame].max_depth}')\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames=10\n)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-012.out.html",
    "href": "posts/5_STBDA2023/03wk-012.out.html",
    "title": "03wk-012: 취업, 로지스틱",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-z03THS4jG11HPcozk3ZfVS&si=Ry49nDAOI3PSu0Ja"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-012.out.html#a.-데이터-정리",
    "href": "posts/5_STBDA2023/03wk-012.out.html#a.-데이터-정리",
    "title": "03wk-012: 취업, 로지스틱",
    "section": "A. 데이터 정리",
    "text": "A. 데이터 정리\n\nX = pd.get_dummies(df[['toeic','gpa']])\ny = df[['employment']]\n\n\nX\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n...\n...\n...\n\n\n495\n280\n4.288465\n\n\n496\n310\n2.601212\n\n\n497\n225\n0.042323\n\n\n498\n320\n1.041416\n\n\n499\n375\n3.626883\n\n\n\n\n500 rows × 2 columns"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-012.out.html#b.-predictor-starstarstar",
    "href": "posts/5_STBDA2023/03wk-012.out.html#b.-predictor-starstarstar",
    "title": "03wk-012: 취업, 로지스틱",
    "section": "B. Predictor (\\(\\star\\star\\star\\))",
    "text": "B. Predictor (\\(\\star\\star\\star\\))\n- 여기가 중요함. \\(y\\)가 연속형이 아니라 범주형으로 이루어진 경우는 sklearn.linear_model.LogisticRegression() 이용하여 predictor를 만들 것\n\npredictr = sklearn.linear_model.LogisticRegression()\npredictr\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-012.out.html#c.-학습",
    "href": "posts/5_STBDA2023/03wk-012.out.html#c.-학습",
    "title": "03wk-012: 취업, 로지스틱",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\n/home/jy/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-012.out.html#d.-예측",
    "href": "posts/5_STBDA2023/03wk-012.out.html#d.-예측",
    "title": "03wk-012: 취업, 로지스틱",
    "section": "D. 예측",
    "text": "D. 예측\n\npredictr.predict(X) \n\narray([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n       0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1])\n\n\n\ndf.assign(employmente_hat = predictr.predict(X))\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nemploymente_hat\n\n\n\n\n0\n135\n0.051535\n0\n0\n\n\n1\n935\n0.355496\n0\n0\n\n\n2\n485\n2.228435\n0\n0\n\n\n3\n65\n1.179701\n0\n0\n\n\n4\n445\n3.962356\n1\n1\n\n\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n1\n\n\n496\n310\n2.601212\n1\n0\n\n\n497\n225\n0.042323\n0\n0\n\n\n498\n320\n1.041416\n0\n0\n\n\n499\n375\n3.626883\n1\n1\n\n\n\n\n500 rows × 4 columns\n\n\n\n\n꽤 괜찮게 모형학습이 된 것 같다."
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-012.out.html#e.-평가",
    "href": "posts/5_STBDA2023/03wk-012.out.html#e.-평가",
    "title": "03wk-012: 취업, 로지스틱",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.882\n\n\n\n(predictr.predict(X) == y.employment).mean()\n\n0.882\n\n\n\nplt.plot(df.toeic,df.gpa,'o')\ndf_filtered = df[predictr.predict(X)==1]\nplt.plot(df_filtered.toeic,df_filtered.gpa,'o') \n\n\n\n\n\n이 정도면 합리적임"
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html",
    "href": "posts/5_STBDA2023/01wk-hw1.html",
    "title": "titanic",
    "section": "",
    "text": "- 변수 설명\n“Embarked” 변수는 승객이 타이타닉호에 어느 항구에서 탑승했는지를 나타냅니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#sibsp와-parch-에-대한-피처엔지니어링의-아이디어",
    "href": "posts/5_STBDA2023/01wk-hw1.html#sibsp와-parch-에-대한-피처엔지니어링의-아이디어",
    "title": "titanic",
    "section": "SibSp와 Parch 에 대한 피처엔지니어링의 아이디어",
    "text": "SibSp와 Parch 에 대한 피처엔지니어링의 아이디어\n- 두 변수 SibSp와 Parch를 더한 family_size라는 새로운 변수를 만들어 y를 예측할 수 있다.\n(함께 탑승한) 가족 구성원 수와 생존 여부 사이에는 일부 관련성이 있을 수 있으며, 다음과 같은 방식으로 관련성을 고려할 수 있습니다:\n\n가족 구성원 수와 생존 여부의 관련성: 가족과 함께 여행한 승객은 가족 구성원 간에 서로 도움을 주고 받을 수 있으므로 생존 가능성이 높을 수 있습니다. 특히 부모와 어린 자녀, 형제자매와 함께 여행한 경우에 이러한 경향이 뚜렷할 수 있습니다.\n가족 구성원 수와 구명보트 할당: 가족 구성원 수는 구명보트 할당에 영향을 미칠 수 있습니다. 가족 구성원이 함께 있을 경우, 함께 탑승하려는 경향이 있을 수 있으며, 구명보트의 공간이 한정되어 있기 때문에 가족 구성원 수에 따라 구명보트 할당이 결정될 수 있습니다.\n성별과 연령과의 조합: 가족 구성원 수와 함께 승객의 성별과 연령을 고려하는 경우, 가족 구성원과 함께 여행한 여성과 어린이의 생존 가능성이 높을 수 있습니다.\n데이터 분석과 모델링: 데이터 분석 및 머신 러닝 모델을 사용하여 가족 구성원 수와 생존 여부 간의 관계를 더 자세히 이해할 수 있습니다. 가족 구성원 수를 고려한 변수를 만들거나, 통계 분석을 통해 가족 구성원 수와 생존 여부 간의 상관 관계를 확인할 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#age",
    "href": "posts/5_STBDA2023/01wk-hw1.html#age",
    "title": "titanic",
    "section": "Age",
    "text": "Age\n- Age 변수를 연령대로 나눈 범주형 변수 Age_cate 사용하게 되면 해당 연령대 내에서의 패턴을 감지하기 위해 모델에 더 적합할 수 있다.\nAge변수를 연령대로 범주화한 경우 vs. 그렇지 않은 경우\n1.정보 손실 vs. 성능 개선:\n\n연령대로 나누는 경우: 연령대로 구분하면 연령 정보가 연속적인 값에서 범주형 값으로 변환됩니다. 이렇게 하면 연령 정보의 정확한 값을 잃게 되며, 대신 각 연령대에 대한 패턴과 차이를 분석할 수 있습니다. 이는 특정 연령대에서의 생존율 차이 등을 파악하는 데 유용할 수 있습니다.\n그대로 적용하는 경우: 연령을 그대로 사용하면 연속적인 값으로 정보를 보존합니다. 이는 모델에 연령에 따른 선형적인 관계를 고려할 수 있게 합니다. 모델이 연령 정보의 정확한 값을 활용하여 예측을 수행할 수 있지만, 노이즈와 이상치에 민감할 수 있습니다.\n\n2.모델의 복잡성:\n\n연령대로 나누는 경우: 연령대로 나눈 범주형 변수는 모델의 복잡성을 줄일 수 있으며, 해당 연령대 내에서의 패턴을 감지하기 위해 모델에 더 적합한 경우가 있습니다.\n그대로 적용하는 경우: 연령을 그대로 사용하면 모델이 더 복잡한 관계를 고려할 수 있으며, 연령 정보의 세부적인 영향을 고려할 수 있습니다. 그러나 데이터의 노이즈나 이상치에 민감할 수 있습니다.\n\n3.해석 가능성:\n\n연령대로 나누는 경우: 연령대는 그룹별로 패턴을 파악하기 용이하므로, 모델의 결과를 해석하거나 시각화하기에 유용합니다.\n그대로 적용하는 경우: 연령을 그대로 사용하면 모델 결과가 연령에 따른 선형적인 영향을 나타낼 수 있어 해석이 다소 복잡할 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#fare",
    "href": "posts/5_STBDA2023/01wk-hw1.html#fare",
    "title": "titanic",
    "section": "Fare",
    "text": "Fare\nFare 혹은 Age 등 수치형 변수는 구간화를 통해 범주형 변수로 만들어 y를 예측할 수 있다.\n\n비선형 관계 고려: 일부 모델은 변수와 반응 사이의 비선형 관계를 잘 처리할 수 있습니다. 구간화를 통해 비선형성을 모델링할 수 있습니다.\n이상치 관리: 구간화를 사용하면 이상치를 처리하는 데 도움이 될 수 있습니다. 이상치가 있는 경우 해당 값은 특정 구간에 할당될 수 있으며, 모델은 이상치에 대한 영향을 완화할 수 있습니다.\n해석력 향상: 수치형 변수를 범주형 변수로 변환하면 모델의 해석력을 향상시킬 수 있습니다. 각 구간(버킷)은 특정 범주의 의미를 가지므로 결과를 더 쉽게 해석할 수 있습니다.\n설명변수 추가: 구간화를 통해 범주형 변수를 생성하면 데이터에 추가적인 설명변수를 도입할 수 있습니다. 이는 모델이 변수 간의 상호작용을 고려하고 예측 성능을 향상시킬 수 있습니다.\n다중 공선성 제거: 구간화를 통해 수치형 변수를 범주형으로 변환하면 다중 공선성 문제를 일부 해결할 수 있습니다. 다중 공선성은 모델에서 변수 간의 높은 상관성으로 인해 발생할 수 있는 문제입니다.\n\n구간화를 적용할 때는 구간의 수, 경계값의 선택, 각 구간의 크기 등을 신중하게 고려해야 합니다. 또한 구간화의 효과는 데이터와 모델에 따라 다를 수 있으므로 실험을 통해 최적의 구간화 전략을 찾는 것이 중요합니다.\n- 구간화 기준\n\n등간 구간 (Equal Interval): 변수의 값 범위를 동일한 크기의 구간으로 나눕니다. 예를 들어, 연령을 0-10, 11-20, 21-30, … 등의 동일한 크기의 구간으로 분할합니다. 이 방법은 간단하고 균형잡힌 구간을 생성하지만, 데이터의 분포가 불균형할 때는 부적합할 수 있습니다.\n분위수 구간 (Quantile Interval): 데이터의 분위수(4분위, 5분위 등)를 사용하여 구간을 나눕니다. 분위수는 데이터를 동일한 백분율로 나눈 값입니다. 예를 들어, 중앙값(2분위)을 기준으로 데이터를 나눌 수 있습니다. 이 방법은 데이터 분포의 특성을 반영하며 이상치에 민감하지 않습니다.\n도메인 지식 활용: 분석 대상 데이터와 관련된 도메인 지식을 활용하여 구간을 정할 수 있습니다. 예를 들어, 연령대를 나눌 때 어린이, 청소년, 성인, 노인과 같이 도메인 지식을 활용하여 구간을 설정할 수 있습니다.\n연구 목적에 따른 최적화: 분석 목적에 따라 최적의 구간을 찾는 것이 중요합니다. 예를 들어, 데이터를 구간화할 때 생존 여부를 예측하는 모델을 만든다면 구간을 나눌 때 예측 성능을 고려하여 최적화할 수 있습니다.\n시각적 탐색: 데이터를 히스토그램 또는 상자 그림과 같은 시각화 도구를 사용하여 시각적으로 탐색한 후, 구간을 나누는 데 도움이 될 수 있는 경향성을 확인할 수 있습니다. 구간을 나눌 때 주의해야 할 점은 너무 많은 구간을 만들면 모델이 과적합될 수 있으며, 너무 적은 구간을 만들면 데이터의 패턴을 잃을 수 있습니다. 따라서 적절한 구간 수와 경계값을 선택하는 것이 중요합니다. 실험을 통해 최적의 구간화 전략을 찾고 모델의 성능을 지속적으로 평가하는 것이 좋습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#cabin",
    "href": "posts/5_STBDA2023/01wk-hw1.html#cabin",
    "title": "titanic",
    "section": "Cabin",
    "text": "Cabin\n객실번호는 객실위치와 관련이 있을 수 있다. 예를 들어 탈출하기 좋은 객실위치에 있는 승객이라면 생존확률이 더 높지 않을까?\n\n그런데, Cabin변수에 결측치가 많았던 것 같다.\n\n객실 번호는 탈출하기에 좋은 배의 위치와 관련하여 정보를 제공할 수 있습니다. 타이타닉의 침몰 시, 객실 위치는 생존 가능성에 영향을 미칠 수 있었습니다. 다음은 객실 번호와 탈출 가능성 간의 관련성을 고려할 수 있는 몇 가지 요소입니다:\n\n객실 위치: 객실 번호를 통해 어디에 위치한 객실인지를 파악할 수 있습니다. 일반적으로 상부 갑판에 위치한 객실은 비상 구명 보트에 더 가깝게 위치했을 가능성이 높습니다.\n침몰 지점: 타이타닉 침몰 시 어떤 부분이 먼저 가라앉았는지를 고려해야 합니다. 일부 객실은 침몰 시 빠르게 가라앉았을 수 있으며, 이로 인해 해당 객실에 있던 승객들이 탈출하기 어려웠을 것입니다.\n구명 보트 배치: 객실 위치와 가까운 구명 보트가 어떻게 배치되었는지를 고려해야 합니다. 특정 객실 위치에서 구명 보트가 가까이 있었다면 해당 승객들의 탈출 확률이 높았을 것입니다.\n승객 분포: 특정 객실 번호에 얼마나 많은 승객이 있었는지를 고려할 필요가 있습니다. 만약 특정 객실 번호에 많은 승객이 있었다면 그 객실에서 탈출하기 어려울 수 있었습니다.\n\n객실 번호와 생존 여부 간의 관련성을 분석하고 모델링에 활용할 수 있습니다. 이를 통해 객실 위치가 생존에 미치는 영향을 확인하고 예측할 수 있습니다. 데이터 분석을 통해 객실 위치가 생존 가능성에 어떤 영향을 미칠 수 있는지를 조사하고 모델에 포함하는 것이 중요합니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#age-와-pclass에-대한-피처엔지니어링의-아이디어",
    "href": "posts/5_STBDA2023/01wk-hw1.html#age-와-pclass에-대한-피처엔지니어링의-아이디어",
    "title": "titanic",
    "section": "Age 와 Pclass에 대한 피처엔지니어링의 아이디어",
    "text": "Age 와 Pclass에 대한 피처엔지니어링의 아이디어\n- 두 변수 Age_cate와 Pclass를 곱해서 Age_Pclass 라는 변수를 만들고 이를 활용하여 y를 예측할 수 있다.\nAge와 Pclass를 곱하는 것은 다양한 변수 간의 상호작용을 고려하는 한 가지 방법일 수 있습니다. 이렇게 하면 각 연령 그룹에서의 객실 등급에 따른 영향을 조사할 수 있습니다.예를 들어, 연령대가 어린 승객이 고급 객실 등급에 탔을 때와 연령대가 높은 승객이 저급 객실 등급에 탔을 때와 같이 특정 연령대와 객실 등급 조합이 생존 확률에 미치는 영향을 더 잘 나타낼 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#pclass와-embarked에-대한-피처엔지니어링의-아이디어",
    "href": "posts/5_STBDA2023/01wk-hw1.html#pclass와-embarked에-대한-피처엔지니어링의-아이디어",
    "title": "titanic",
    "section": "Pclass와 Embarked에 대한 피처엔지니어링의 아이디어",
    "text": "Pclass와 Embarked에 대한 피처엔지니어링의 아이디어\n탑승 항구는 승객의 경제적 요인 / 여행 목적/ 문화적 영향을 반영할 수 있고 이는 Pclass에 영향을 미칠 수 있으며 두 변수를 조합한 Em_Pclass라는 변수를 만들어 y를 예측할 수 있다.\n\n경제적 요인: 특정 항구에서 탑승한 승객들 중 높은 등급의 객실을 선택하는 비율이 높을 수 있습니다. 경제적으로 부유한 지역에서 출발한 승객들이 고급 객실을 더 많이 선택할 가능성이 높습니다.\n여행 목적: 승객의 여행 목적에 따라 객실 선택이 달라질 수 있습니다. 휴양지로 가는 승객들은 편안한 환경을 선호하고 고급 객실을 선택할 가능성이 높을 수 있습니다.\n가족 규모: 가족이 함께 여행하는 경우, 가족의 크기와 연령에 따라 객실 등급을 선택하는 경향이 달라질 수 있습니다. 대가족은 더 큰 객실이 필요할 수 있으며, 이로 인해 고급 객실을 선택할 가능성이 높을 수 있습니다.\n객실 가용성: 특정 항구에서 고급 객실의 가용성이 높았을 경우, 해당 항구에서 높은 등급의 객실을 더 많이 선택할 수 있습니다.\n문화적 영향: 승객의 문화적, 사회적 배경은 객실 선택에 영향을 미칠 수 있습니다. 어떤 문화나 국가에서는 특정 등급의 객실을 선호하는 경향이 있을 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-021.html#a.-학부12학년-수준의-설명",
    "href": "posts/5_STBDA2023/05wk-021.html#a.-학부12학년-수준의-설명",
    "title": "05wk-021: 취업+밸런스게임, 오버피팅",
    "section": "A. 학부1~2학년 수준의 설명",
    "text": "A. 학부1~2학년 수준의 설명\n- 과적합(Overfitting): 머신러닝과 통계에서 자주 나타나는 문제로, 모델이 학습데이터에 과도하게 최적화가 되어서 실제로 새로운 데이터나 테스트데이터에서 성능이 저하되는 현상을 말함.\n- 오버피팅의 원인:\n\n불필요한 특징: 불필요한 특징 이 데이터에 포함되어 있다면 오버피팅이 발생할 수 있음."
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-021.html#b.-일반인-수준의-설명",
    "href": "posts/5_STBDA2023/05wk-021.html#b.-일반인-수준의-설명",
    "title": "05wk-021: 취업+밸런스게임, 오버피팅",
    "section": "B. 일반인 수준의 설명",
    "text": "B. 일반인 수준의 설명\n- 시험 공부(1): 공부를 하랬더니 외우고 있음..\n- 시험 공부(2): (시험 하루 전날에) 공부 그만하고 술이나 먹으러 가자.. 더 공부하면 train error만 줄일 뿐이야..\n- 운전: 특정도로에서만 운전연습을 했음. 그래서 그 도로의 구멍, 곡률, 신호등의 위치까지 완벽하게 숙지하였음. 그 결과 그 도로에서는 잘하게 되었지만, 그 도로 이외의 다른도로에서 운전을 한다면 문제가 발생함.\n- 언어: 특정 주제나 특정 상황에 대한 대화만을 반복적으로 연습하여, 그 상황에서는 완벽한 대화가 가능하지만 그 외의 상황에서는 대화를 제대로 이어나갈 수 없음."
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-027.html",
    "href": "posts/5_STBDA2023/07wk-027.html",
    "title": "07wk-027: 아이스크림(이상치) / 회귀분석",
    "section": "",
    "text": "1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-yf6Ht2bBpj-mf50aPboSsu&si=6dYX9WfoPmbbbGsG\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 200\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n200.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n...\n...\n...\n\n\n95\n12.4\n17.508688\n\n\n96\n13.4\n17.105376\n\n\n97\n14.7\n17.164930\n\n\n98\n15.0\n18.555388\n\n\n99\n15.2\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df_train.temp,df_train.ice_sales,'o')\n\n\n\n\n\n상상: 온도가 -4.1인 지점에서 “썰매축제”가 열렸다고 가정하자. 그래서 사람이 갑자기 많이 왔음. 그래서 아이스크림이 많이 팔렸음.\n\n\n\n4. 분석\n- 선형회귀로 적합\n\n# step1 \nX,y = df_train[['temp']], df_train['ice_sales']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_train['ice_sales_hat'] = predictr.predict(X)\n\n\npredictr.coef_\n\narray([-0.64479089])\n\n\n\n온도가 올라가면 아이스크림 판매량이 증가하는게 상식인데 우리 상식이랑 맞지 않는데?\n\n- 시각화\n\n# plt.plot(df_train.temp,df_train.ice_sales,'o')\n# plt.plot(df_train.temp,df_train.ice_sales_hat,'--')\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\n\n\n\n\n\n이상치 하나때문에 결과가 이상해진다. –&gt; Linear model이 가진 약점.\n\n- 새로운 unseen data를 가정, 데이터는 온도가 12.5~18 에 걸쳐있다고 가정한다.\n\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\nice_sales_hat\n\n\n\n\n0\n-4.1\n200.000000\n20.989373\n\n\n1\n-3.7\n9.234175\n20.731457\n\n\n2\n-3.0\n9.642778\n20.280103\n\n\n3\n-1.3\n9.657894\n19.183959\n\n\n4\n-0.5\n9.987787\n18.668126\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n17.508688\n10.350324\n\n\n96\n13.4\n17.105376\n9.705533\n\n\n97\n14.7\n17.164930\n8.867305\n\n\n98\n15.0\n18.555388\n8.673867\n\n\n99\n15.2\n18.787014\n8.544909\n\n\n\n\n100 rows × 3 columns\n\n\n\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(12.5,18,50)})\ndf_test['ice_sales_hat'] = predictr.predict(XX)\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--')\n\n\n\n\n\n온도가 올라갈수록 아이스크림 판매량은 줄어든다는 해석 (더 온도가 올라간다면 판매량이 음수가 나올 수도 있겠음 )\n저 정도의 아웃라이어는 모형에서 제외하는게 타당하지 않나? (하지만 저러한 아웃라이어가 데이터의 가치가 있을 수도 있음. 그런데 또 데이터의 가치가 있는지 없는지는 어떻게 판단하지?)\n\n\n\n5. Discussion\n- 딱히 정답이 없음.. –&gt; 여러가지 방식으로 해결할 수 있다."
  },
  {
    "objectID": "posts/5_STBDA2023/10wk-038.html#a.-분할이-정해졌을때-haty을-결정하는-방법",
    "href": "posts/5_STBDA2023/10wk-038.html#a.-분할이-정해졌을때-haty을-결정하는-방법",
    "title": "10wk-038: 아이스크림 – 의사결정나무 원리",
    "section": "A. 분할이 정해졌을때 \\(\\hat{y}\\)을 결정하는 방법?",
    "text": "A. 분할이 정해졌을때 \\(\\hat{y}\\)을 결정하는 방법?\n- step1~4\n\n## step1\nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1) \n## step3 \npredictr.fit(X,y) \n## step4 -- pass \n# predictr.predict(X) \n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n- tree 시각화 \\(\\to\\) 분할파악\n\nsklearn.tree.plot_tree(predictr)\n\n[Text(0.5, 0.75, 'x[0] &lt;= 5.05\\nsquared_error = 111.946\\nsamples = 100\\nvalue = 33.973'),\n Text(0.25, 0.25, 'squared_error = 34.94\\nsamples = 45\\nvalue = 24.788'),\n Text(0.75, 0.25, 'squared_error = 49.428\\nsamples = 55\\nvalue = 41.489')]\n\n\n\n\n\n- 분할에 따른 \\(\\hat{y}\\) 계산\n\ndf_train[df_train.temp&lt;= 5.05].sales.mean(),df_train[df_train.temp&gt; 5.05].sales.mean()\n\n(24.787609205775055, 41.489079055828356)"
  },
  {
    "objectID": "posts/5_STBDA2023/10wk-038.html#b.-분할을-결정하는-방법",
    "href": "posts/5_STBDA2023/10wk-038.html#b.-분할을-결정하는-방법",
    "title": "10wk-038: 아이스크림 – 의사결정나무 원리",
    "section": "B. 분할을 결정하는 방법?",
    "text": "B. 분할을 결정하는 방법?\n\n\\(5.05\\) 라는 분할기준은 어떻게 결정하는건데??\n\n- 예비학습\n\npredictr.score(X,y)\n\n0.6167038863844929\n\n\n\n이 값이 내부적으로 어떻게 계산된거지?\n\n\npredictr.score??\n\n\nSignature: predictr.score(X, y, sample_weight=None)\nSource:   \n    def score(self, X, y, sample_weight=None):\n        \"\"\"Return the coefficient of determination of the prediction.\n        The coefficient of determination :math:`R^2` is defined as\n        :math:`(1 - \\\\frac{u}{v})`, where :math:`u` is the residual\n        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n        The best possible score is 1.0 and it can be negative (because the\n        model can be arbitrarily worse). A constant model that always predicts\n        the expected value of `y`, disregarding the input features, would get\n        a :math:`R^2` score of 0.0.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed\n            kernel matrix or a list of generic objects instead with shape\n            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n            is the number of samples used in the fitting for the estimator.\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            True values for `X`.\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights.\n        Returns\n        -------\n        score : float\n            :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n        Notes\n        -----\n        The :math:`R^2` score used when calling ``score`` on a regressor uses\n        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n        with default value of :func:`~sklearn.metrics.r2_score`.\n        This influences the ``score`` method of all the multioutput\n        regressors (except for\n        :class:`~sklearn.multioutput.MultiOutputRegressor`).\n        \"\"\"\n        from .metrics import r2_score\n        y_pred = self.predict(X)\n        return r2_score(y, y_pred, sample_weight=sample_weight)\nFile:      ~/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/base.py\nType:      method\n\n\n\n\ny_hat = y_pred = predictr.predict(X)\nsklearn.metrics.r2_score(y,y_pred)\n\n0.6167038863844929\n\n\n- 좋은 분할을 판단하는 기준? – 여기에서 r2_score가 이용됨\n- 우선 논의를 편하게하기 위해서 \\(({\\bf X},{\\bf y})\\)와 경계값 \\(c\\)를 줄때 \\(\\hat{\\bf y}\\)을 계산해주는 함수를 구현하자.\n\ndef fit_predict(X,y,c):\n    X = np.array(X).reshape(-1)\n    y = np.array(y) \n    yhat = y*0   # 일단 0으로 넣어놓자!\n    yhat[X&lt;=c] = y[X&lt;=c].mean()\n    yhat[X&gt;c] = y[X&gt;c].mean()\n    return yhat\n\n- 서로 다른 분할에 대하여 시각화를 진행\n\nyhat_bad = fit_predict(X,y,c=-1)\nyhat_good = fit_predict(X,y,c=5)\nfig, ax = plt.subplots(1,2) \nax[0].plot(X,y,'o',alpha=0.5)\nax[0].plot(X,yhat_bad,'--.')\nax[0].set_title('bad')\nax[1].plot(X,y,'o',alpha=0.5)\nax[1].plot(X,yhat_good,'--.')\nax[1].set_title('good')\n\nText(0.5, 1.0, 'good')\n\n\n\n\n\n\n딱봐도 오른쪽이 좋은 분할같은데, 컴퓨터한테 이걸 어떻게 설명하지?\n\n- 좋은분할을 구하는 이유는 좋은 yhat을 얻기 위함이다. 그렇다면 좋은 yhat을 얻게 해주는 분할이 좋은 분할이라 해석할 수 있다. \\(\\to\\) 아이디어: 그런데 좋은 yhat은 sklearn.metrics.r2_score(y,yhat)의 값이 높지 않을까?\n- 그렇다면 위의 그림에서 왼쪽보다 오른쪽이더 좋은 분할이라면 r2_score(y,yhat_good)의 값이 r2_score(y,yhat_bad) 값보다 높을 것!\n\nsklearn.metrics.r2_score(y,yhat_bad), sklearn.metrics.r2_score(y,yhat_good)\n\n(0.13932141536746745, 0.6167038863844928)\n\n\n- 트리의 max_depth=1 일 경우 분할을 결정하는 방법 – 노가다..\n\n적당한 \\(c\\)를 고른다.\n분할 \\((-\\infty,c), [c,\\infty)\\) 를 생성하고 yhat를 계산한다.\nr2_score(y,yhat)를 계산하고 기록한다.\n1-3의 과정을 무한반복 한다. 그리고 r2_score(y,yhat)의 값을 가장 작게 만드는 \\(c\\)가 무엇인지 찾는다.\n\n\ncuts = np.arange(-5,15)\nfig = plt.figure()\ndef func(frame):\n    ax = fig.gca()\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.5)\n    c = cuts[frame] \n    yhat = fit_predict(X,y,c)\n    ax.plot(X,yhat,'.')\n    r2 = sklearn.metrics.r2_score(y,yhat)\n    ax.set_title(f'c={c}, r2_score={r2:.2f}')\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames=20\n)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- tree가 찾은 값 5.05를 우리가 직접 찾아보자.\n\n# 좀 더 촘촘하게\ncuts = np.arange(-5,15,0.05).round(2)\nscore = np.array([sklearn.metrics.r2_score(y,fit_predict(X,y,c)) for c in cuts])\nplt.plot(cuts,score)\n\n\n\n\n- 방법1: 시각화로 찾는방법\n\npd.DataFrame({'cut':cuts,'score':score})\\\n.plot.line(x='cut',y='score',backend='plotly')\n\n                                                \n\n\n- 방법2: 정석\n\ncuts[score.argmax()]\n\n5.0\n\n\n\nscore.argmax() # 최댓값이 있는 인덱스 리턴.\n\n200\n\n\n- max_depth=2일 경우? max_depth=1의 결과로 발생한 2개의 조각을 각각 전체자료로 생각하고, max_depth=1일 때의 분할방법을 반복적용한다.\n- X=[temp,type] 와 같은 경우라면? 설명변수를 하나씩 고정하여 각각 최적분할을 생성하고 r2_score관점에서 가장 우수한 설명변수를 선택"
  },
  {
    "objectID": "posts/5_STBDA2023/05wk-020.html",
    "href": "posts/5_STBDA2023/05wk-020.html",
    "title": "05wk-020: StandardScaler를 이용한 전처리",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.preprocessing \n\n\n\n3. StandardScaler()\n- 예제자료 로드\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv').loc[:7,['toeic','gpa']]\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\n\n\n\n\n0\n135\n0.051535\n\n\n1\n935\n0.355496\n\n\n2\n485\n2.228435\n\n\n3\n65\n1.179701\n\n\n4\n445\n3.962356\n\n\n5\n65\n1.846885\n\n\n6\n290\n0.309928\n\n\n7\n730\n0.336081\n\n\n\n\n\n\n\n- 스케일러를 생성\n\nsclr = sklearn.preprocessing.StandardScaler()\nsclr.fit_transform(df)\n\narray([[-0.8680409 , -0.98104887],\n       [ 1.81575704, -0.73905505],\n       [ 0.3061207 ,  0.75205327],\n       [-1.10287322, -0.08287854],\n       [ 0.17193081,  2.13248542],\n       [-1.10287322,  0.44828929],\n       [-0.34805505, -0.77533368],\n       [ 1.12803382, -0.75451182]])\n\n\n\n토익과 gpa 스케일이 얼핏 비슷하게 맞춰졌다.\n\n- 계산식\n\n(df.toeic - df.toeic.mean())/df.toeic.std(ddof=0) # 계산식\n\n0   -0.868041\n1    1.815757\n2    0.306121\n3   -1.102873\n4    0.171931\n5   -1.102873\n6   -0.348055\n7    1.128034\nName: toeic, dtype: float64\n\n\n\nnote: Alternatively, ddof=0 can be set to normalize by N instead of N-1:\n\n\n?df.toeic.std\n\n\nSignature:\ndf.toeic.std(\n    axis=None,\n    skipna=True,\n    level=None,\n    ddof=1,\n    numeric_only=None,\n    **kwargs,\n)\nDocstring:\nReturn sample standard deviation over requested axis.\nNormalized by N-1 by default. This can be changed using the ddof argument.\nParameters\n----------\naxis : {index (0)}\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nddof : int, default 1\n    Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n    where N represents the number of elements.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\nReturns\n-------\nscalar or Series (if level specified) \nNotes\n-----\nTo have the same behaviour as `numpy.std`, use `ddof=0` (instead of the\ndefault `ddof=1`)\nExamples\n--------\n&gt;&gt;&gt; df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n...                   'age': [21, 25, 62, 43],\n...                   'height': [1.61, 1.87, 1.49, 2.01]}\n...                  ).set_index('person_id')\n&gt;&gt;&gt; df\n           age  height\nperson_id\n0           21    1.61\n1           25    1.87\n2           62    1.49\n3           43    2.01\nThe standard deviation of the columns can be found as follows:\n&gt;&gt;&gt; df.std()\nage       18.786076\nheight     0.237417\nAlternatively, `ddof=0` can be set to normalize by N instead of N-1:\n&gt;&gt;&gt; df.std(ddof=0)\nage       16.269219\nheight     0.205609\nFile:      ~/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/generic.py\nType:      method\n\n\n\n\n\n4. 비교\n- MinMaxScaler와 StandardScaler는 데이터의 스케일을 조정하는 두 가지 일반적인 방법이다.\n\nMinMaxScaler:\n\n작동 원리: 데이터를 0과 1 사이의 값으로 조정\n장점: 원하는 범위 내로 데이터를 조정할 때 유용. 특히 신경망에서는 활성화 함수의 범위와 일치하도록 입력 값을 조정하는 데 유용.1\n단점: 이상치에 매우 민감하다. 이상치 때문에 전체 데이터의 스케일이 크게 영향받을 수 있음.\n\nStandardScaler:\n\n작동 원리: 데이터의 평균을 0, 표준편차를 1로 만드는 방식으로 조정.\n장점: 이상치에 MinMaxScaler보다 덜 민감함. 많은 통계적 기법들, 특히 PCA 같은 선형 알고리즘에서 잘 작동함.2\n단점: MinMaxScaler와 달리, 표준화된 데이터의 값이 특정 범위 내에 있음을 보장하지 않음.3\n\n\n- 무식한 설명 (1)\n\nMinMaxScaler: 컴퓨터공학과, 전자공학과 느낌\nStandardScaler: 통계학과 느낌\n\n- 무식한 설명 (2)\n\nMinMaxScaler: 데이터가 기본적으로 0$\\(1 혹은 -1\\)$1 사이의 범위에 있다고 가정한다.\nStandardScaler: 데이터가 기본적으로 정규분포를 따른다고 가정하는 모형들과 잘 맞는다.\n\n- 둘 중 어느 것을 선택할지 결정하기 위한 고려사항:\n\n이상치가 많으면 StandardScaler가 더 적합할 수 있다.\n모델의 알고리즘과 특성에 따라 선택해야 한다. 예를 들어, 신경망은 일반적으로 0과 1 사이의 값이나 -1과 1 사이의 값으로 입력을 받는 활성화 함수를 사용하므로 MinMaxScaler가 적합할 수 있다.\n\n결론적으로, 두 스케일링 방법 중 어느 것이 더 좋은지는 사용 사례와 데이터의 특성에 따라 다르기 때문에, 가능한 경우 둘 다 시도해보고 모델의 성능을 비교하는 것이 좋다.\n- 둘 중 어느 것을 선택할지 결정하기 위한 고려사항 – 무식한 설명\n\n보통은 아무거나 해도 큰일 안남.\n아주 특수한 경우4를 제외하고는 어차피 이론적인 선택기준은 없음.\n\n\nPCA의 경우 이미 정규분포를 따른다고 가정을 하고 쓰는 모형이라서 이러한 특수한 경우를 제외하고는 이론적인 선택기준은 없다.\n\n\n\n\n\n\nFootnotes\n\n\nsigmoid, tanh와 같은 활성화 함수의 출력값과 맞추는 용도↩︎\n그야 PCA는 정규분포를 가정하고 만든 알고리즘이라~↩︎\nMinMaxScaler도 딱히 엄격하게 보장하는건 아니야↩︎\nClassical PCA↩︎"
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-029.html",
    "href": "posts/5_STBDA2023/07wk-029.html",
    "title": "07wk-029: 체중감량(교호작용) / 회귀분석",
    "section": "",
    "text": "1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-y6dmKt32J5hobALnT8wigT&si=ScK7ryQpemMS4cJd\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\nimport sklearn.model_selection\n\n\n\n3. Data\n\n# n = 10000\n# Supplement = np.random.choice([True, False], n)\n# Exercise = np.random.choice([False, True], n)\n# Weight_Loss = np.where(\n#     (~Supplement & (~Exercise)),\n#     np.random.normal(loc=0, scale=1, size=n),  \n#     np.where(\n#         (Supplement & (Exercise)),\n#         np.random.normal(loc=15.00, scale=1, size=n),\n#         np.where(\n#             (~Supplement & (Exercise)),\n#             np.random.normal(loc=5.00, scale=1, size=n),\n#             np.random.normal(loc=0.5, scale=1, size=n)\n#         )\n#     )\n# )\n# df = pd.DataFrame({\n#     'Supplement': Supplement,\n#     'Exercise': Exercise,\n#     'Weight_Loss': Weight_Loss\n# })\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/weightloss.csv')\n\n\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n\n\n1\nTrue\nFalse\n1.604542\n\n\n2\nTrue\nTrue\n13.824148\n\n\n3\nTrue\nTrue\n13.004505\n\n\n4\nTrue\nTrue\n13.701128\n\n\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n\n\n9996\nFalse\nFalse\n-0.217816\n\n\n9997\nFalse\nTrue\n4.072701\n\n\n9998\nTrue\nFalse\n-0.253796\n\n\n9999\nFalse\nFalse\n-1.399092\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n- 운동과 체중감량보조제를 병행하면 시너지가 나는 것 같음\n\n운동만 하면 –&gt; 5kg빠짐\n약만 먹으면 –&gt; 0.5정도 빠짐\n운동 + 약 –&gt; 15kg정도 빠짐\n\n\n\n4. 분석\n- 분석1: 모형을 아래와 같이 본다. – 언더피팅\n\n\\({\\bf X}\\): Supplement, Exercise\n\\({\\bf y}\\): Weight_Loss\n\n\n# step1\nX = df_train[['Supplement','Exercise']]\ny = df_train['Weight_Loss']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n#---#\nprint(f'train score = {predictr.score(X,y):.4f}')\n\ntrain score = 0.8208\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n-2.373106\n7.374557\n\n\nTrue\n2.845934\n12.593598\n\n\n\n\n\n\n\n\n운동을 하면 10키로 감량효과가 있다고 추정하고 있음.\n보충제를 먹으면 5키로 감량효과가 있다고 추정하고 있음.\n대충 (10,5)의 숫자를 바꿔가면서 적합해봤는데 이게 최선이라는 의미임\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\nWeight_Loss_hat\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n-2.373106\n\n\n1\nTrue\nFalse\n1.604542\n2.845934\n\n\n2\nTrue\nTrue\n13.824148\n12.593598\n\n\n3\nTrue\nTrue\n13.004505\n12.593598\n\n\n4\nTrue\nTrue\n13.701128\n12.593598\n\n\n\n\n\n\n\n- 추가효과가 있는지 조사.\n\n\n\nindex\n약의효과\n운동의효과\n추가효과\n\n\n\n\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(1\\)\n\\(1\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(2\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(3\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\n- 분석2: 모형을 아래와 같이 본다. – 딱 맞아요\n\n\\({\\bf X}\\): Supplement, Exercise, Supplement \\(\\times\\) Exercise\n\\({\\bf y}\\): Weight_Loss\n\n\nNote: 기본적인 운동의 효과 및 보조제의 효과는 각각 Supplement, Exercise 로 적합하고 운동과 보조제의 시너지는 Supplement\\(\\times\\)Exercise 로 적합한다.\n\n\n# step1 \nX = df_train.eval('Interaction = Supplement * Exercise')[['Supplement','Exercise','Interaction']]\ny = df_train['Weight_Loss']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n#---#\nprint(f'train score = {predictr.score(X,y):.4f}')\n\ntrain score = 0.9728\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\n운동의 효과는 5정도 감량효과가 있다고 추정함.\n보충제를 먹으면 0.5키로 감량효과가 있다고 추정함.\n다만 운동을 하면서 보충제를 함께 먹을 경우 발생하는 추가적인 시너지효과가 9.5정도라고 추정하는 것임."
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-024.html",
    "href": "posts/5_STBDA2023/06wk-024.html",
    "title": "06wk-024: 취업+각종영어점수, RidgeCV",
    "section": "",
    "text": "다중공선성이 있을 경우 Ridge를 쓰면된다. 단, 알파를 잘 선택해야 한다. 그런데 일일이 노가다를 하면서 찾기 보다 어떤 range를 주면 그 범위에서 찾아줬으면 좋겠다. –&gt; RidgeCV\n\n\n1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport sklearn.linear_model\n\n\n\n3. Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n\n4. RidgeCV\n- RidgeCV 클래스에서 모형을 선택해보자.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.RidgeCV()\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV()\n\n\n\npredictr.score(X,y)\n\n0.9999996840224916\n\n\n\npredictr.score(XX,yy)\n\n0.11914945950111888\n\n\n- alpha들의 후보를 우리가 직접 선정하자.\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.RidgeCV(alphas=[5e2, 5e3, 5e4, 5e5, 5e6, 5e7, 5e8]) # alpha의 후보들...\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCVRidgeCV(alphas=[500.0, 5000.0, 50000.0, 500000.0, 5000000.0, 50000000.0,\n                500000000.0])\n\n\n\npredictr.score(X,y)\n\n0.7521268560159359\n\n\n\npredictr.score(XX,yy)\n\n0.7450309251010895\n\n\n\npredictr.alpha_\n\n50000000.0\n\n\n참고로 이 적합결과는 아래의 코드를 실행한것과 같다\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.Ridge(alpha=50000000.0)\n## step3\npredictr.fit(X,y)\n## step4 -- pass \n\nRidge(alpha=50000000.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=50000000.0)\n\n\n\npredictr.score(X,y)\n\n0.7521268560159359\n\n\n\npredictr.score(XX,yy)\n\n0.7450309251010894"
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-031.html",
    "href": "posts/5_STBDA2023/07wk-031.html",
    "title": "07wk-031: 체중감량(교호작용) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-xhSSJ1GlUjFhUgzvVa3aIH&si=JA3pd69Mv9QGXx0z\n\n실제로 교호작용이 있는데, 교호작용을 고려하지 않으면 –&gt; 언더피팅\n실제로 교호작용이 있는데, 교호작용을 고려하면 –&gt; 잘했지\n실제로 교호작용이 없어요, 교호작용을 고려하지 않으면 –&gt; 잘했찌\n실제로 교호작용이 없는데, 교호작용을 고려하면 –&gt; 오버피팅\n\n내가 임의대로 교호작용이 없는데 넣었다면 오버피팅의 원인이 될 수 있다.\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport sklearn.linear_model\nimport sklearn.tree\n\n\n\n3. Data\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/weightloss.csv')\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n\n\n1\nTrue\nFalse\n1.604542\n\n\n2\nTrue\nTrue\n13.824148\n\n\n3\nTrue\nTrue\n13.004505\n\n\n4\nTrue\nTrue\n13.701128\n\n\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n\n\n9996\nFalse\nFalse\n-0.217816\n\n\n9997\nFalse\nTrue\n4.072701\n\n\n9998\nTrue\nFalse\n-0.253796\n\n\n9999\nFalse\nFalse\n-1.399092\n\n\n\n\n10000 rows × 3 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n- 운동과 체중감량보조제를 병행하면 시너지가 나는 것 같음\n\n\n4. 분석\n- 분석1: 선형회귀 (교호작용 고려 X)\n\n# step 1\nX,y = df_train[['Supplement','Exercise']], df_train['Weight_Loss']\n# step 2 \npredictr = sklearn.linear_model.LinearRegression()\n# step 3 \npredictr.fit(X,y)\n# step 4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n-2.373106\n7.374557\n\n\nTrue\n2.845934\n12.593598\n\n\n\n\n\n\n\n- 분석2: 의사결정나무\n\n# step 1\nX,y = df_train[['Supplement','Exercise']], df_train['Weight_Loss']\n# step 2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step 3 \npredictr.fit(X,y)\n# step 4 \ndf_train['Weight_Loss_hat'] = predictr.predict(X)\n\n\ndf_train\n\n\n\n\n\n\n\n\nSupplement\nExercise\nWeight_Loss\nWeight_Loss_hat\n\n\n\n\n0\nFalse\nFalse\n-0.877103\n0.021673\n\n\n1\nTrue\nFalse\n1.604542\n0.497573\n\n\n2\nTrue\nTrue\n13.824148\n14.966363\n\n\n3\nTrue\nTrue\n13.004505\n14.966363\n\n\n4\nTrue\nTrue\n13.701128\n14.966363\n\n\n...\n...\n...\n...\n...\n\n\n9995\nTrue\nFalse\n1.558841\n0.497573\n\n\n9996\nFalse\nFalse\n-0.217816\n0.021673\n\n\n9997\nFalse\nTrue\n4.072701\n4.991314\n\n\n9998\nTrue\nFalse\n-0.253796\n0.497573\n\n\n9999\nFalse\nFalse\n-1.399092\n0.021673\n\n\n\n\n10000 rows × 4 columns\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363\n\n\n\n\n\n\n\n\ndf_train.pivot_table(index='Supplement',columns='Exercise',values='Weight_Loss_hat')\n\n\n\n\n\n\n\nExercise\nFalse\nTrue\n\n\nSupplement\n\n\n\n\n\n\nFalse\n0.021673\n4.991314\n\n\nTrue\n0.497573\n14.966363"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html",
    "href": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-zw88937ohvzYyltCjK-_mg&si=gQE8ICf9c9TvFaxy"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html#a.-test-답을-모름-제출해야-알-수-있음",
    "href": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html#a.-test-답을-모름-제출해야-알-수-있음",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "A. test – 답을 모름, 제출해야 알 수 있음",
    "text": "A. test – 답을 모름, 제출해야 알 수 있음\n- 제출결과는 리더보드에서 확인할 수 있음."
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "href": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "B. train – 스스로 풀어보고 채점할 수 있음",
    "text": "B. train – 스스로 풀어보고 채점할 수 있음\n- 캐글에서 code \\(\\to\\) New Notebook 클릭\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n\nimport pandas as pd\n\n\n# tr=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n# tst=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntr = pd.read_csv('./titanic/train.csv')\ntst = pd.read_csv('./titanic/test.csv')\n\n# 예비학습 – accuracy의 계산\n\ndf = pd.DataFrame({'Surv':[1,0,1,1,0],'Sex':['f','m','f','m','m']})\n\n\ndf\n\n\n\n\n\n\n\n\nSurv\nSex\n\n\n\n\n0\n1\nf\n\n\n1\n0\nm\n\n\n2\n1\nf\n\n\n3\n1\nm\n\n\n4\n0\nm\n\n\n\n\n\n\n\n- Surv 열의 선택\n\ndf.Surv\n\n0    1\n1    0\n2    1\n3    1\n4    0\nName: Surv, dtype: int64\n\n\n- Sex 열의 선택\n\ndf.Sex\n\n0    f\n1    m\n2    f\n3    m\n4    m\nName: Sex, dtype: object\n\n\n- Sex == f이면 생존(1), 그렇지 않으면 사망(0)이라고 예측\n\n(df.Sex == 'f')*1\n\n0    1\n1    0\n2    1\n3    0\n4    0\nName: Sex, dtype: int64\n\n\n- 결과를 정리하면 아래와 같음\n\npd.DataFrame({'real': df.Surv, 'estimate': (df.Sex == 'f')*1})\n\n\n\n\n\n\n\n\nreal\nestimate\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n1\n0\n\n\n4\n0\n0\n\n\n\n\n\n\n\n- accuracy를 손으로 계산하면 \\(\\frac{4}{5}=0.8\\).\n- 컴퓨터로 accuracy를 계산한다면\n\n(df.Surv == (df.Sex == 'f')*1).sum()/5 # 방법1\n\n0.8\n\n\n\n(df.Surv == (df.Sex == 'f')*1).mean() # 방법2\n\n0.8\n\n\n#\n- 실제자료의 accuracy를 구해보자.\n\ntr = pd.read_csv('./titanic/train.csv')\n\n\n(tr.Survived == (tr.Sex == 'female')).mean()\n\n0.7867564534231201"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html",
    "href": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html",
    "title": "타이타닉 튜토리얼",
    "section": "",
    "text": "I decided to write this kernel because Titanic: Machine Learning from Disaster is one of my favorite competitions on Kaggle. This is a beginner level kernel which focuses on Exploratory Data Analysis and Feature Engineering. A lot of people start Kaggle with this competition and they get lost in extremely long tutorial kernels. This is a short kernel compared to the other ones. I hope this will be a good guide for starters and inspire them with new feature engineering ideas.\nTitanic: Machine Learning from Disaster is a great competition to apply domain knowledge for feature engineering, so I made a research and learned a lot about Titanic. There are many secrets to be revealed beneath the Titanic dataset. I tried to find out some of those secret factors that had affected the survival of passengers when the Titanic was sinking. I believe there are other features still waiting to be discovered.\nThis kernel has 3 main sections; Exploratory Data Analysis, Feature Engineering and Model, and it can achieve top 2% (0.83732) public leaderboard score with a tuned Random Forest Classifier. It takes 60 seconds to run whole notebook. If you have any idea that might improve this kernel, please be sure to comment, or fork and experiment as you like. If you didn’t understand any part, feel free to ask.\n::: {#cell-2 .cell _cell_guid=‘79c7e3d0-c299-4dcb-8224-4455121ee9b0’ _uuid=‘d629ff2d2480ee46fbb7e2d37f6b5fab8052498a’ execution_count=1}\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42\n:::\n\nTraining set has 891 rows and test set has 418 rows\nTraining set have 12 features and test set have 11 features\nOne extra feature in training set is Survived feature, which is the target variable\n\n::: {#cell-4 .cell _uuid=‘467443fda7135a8ce89c4d537da3f3a8546e2384’ execution_count=2}\ndef concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('titanic/train.csv')\ndf_test = pd.read_csv('titanic/test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)\n\nNumber of Training Examples = 891\nNumber of Test Examples = 418\n\nTraining X Shape = (891, 12)\nTraining y Shape = 891\n\nTest X Shape = (418, 11)\nTest y Shape = 418\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\nIndex(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n:::"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#introduction",
    "href": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#introduction",
    "title": "타이타닉 튜토리얼",
    "section": "",
    "text": "I decided to write this kernel because Titanic: Machine Learning from Disaster is one of my favorite competitions on Kaggle. This is a beginner level kernel which focuses on Exploratory Data Analysis and Feature Engineering. A lot of people start Kaggle with this competition and they get lost in extremely long tutorial kernels. This is a short kernel compared to the other ones. I hope this will be a good guide for starters and inspire them with new feature engineering ideas.\nTitanic: Machine Learning from Disaster is a great competition to apply domain knowledge for feature engineering, so I made a research and learned a lot about Titanic. There are many secrets to be revealed beneath the Titanic dataset. I tried to find out some of those secret factors that had affected the survival of passengers when the Titanic was sinking. I believe there are other features still waiting to be discovered.\nThis kernel has 3 main sections; Exploratory Data Analysis, Feature Engineering and Model, and it can achieve top 2% (0.83732) public leaderboard score with a tuned Random Forest Classifier. It takes 60 seconds to run whole notebook. If you have any idea that might improve this kernel, please be sure to comment, or fork and experiment as you like. If you didn’t understand any part, feel free to ask.\n::: {#cell-2 .cell _cell_guid=‘79c7e3d0-c299-4dcb-8224-4455121ee9b0’ _uuid=‘d629ff2d2480ee46fbb7e2d37f6b5fab8052498a’ execution_count=1}\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42\n:::\n\nTraining set has 891 rows and test set has 418 rows\nTraining set have 12 features and test set have 11 features\nOne extra feature in training set is Survived feature, which is the target variable\n\n::: {#cell-4 .cell _uuid=‘467443fda7135a8ce89c4d537da3f3a8546e2384’ execution_count=2}\ndef concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('titanic/train.csv')\ndf_test = pd.read_csv('titanic/test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)\n\nNumber of Training Examples = 891\nNumber of Test Examples = 418\n\nTraining X Shape = (891, 12)\nTraining y Shape = 891\n\nTest X Shape = (418, 11)\nTest y Shape = 418\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\nIndex(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n:::"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#exploratory-data-analysis",
    "href": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#exploratory-data-analysis",
    "title": "타이타닉 튜토리얼",
    "section": "1. Exploratory Data Analysis",
    "text": "1. Exploratory Data Analysis\n\n1.1 Overview\n\nPassengerId is the unique id of the row and it doesn’t have any effect on target\nSurvived is the target variable we are trying to predict (0 or 1):\n\n1 = Survived\n0 = Not Survived\n\nPclass (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has 3 unique values (1, 2 or 3):\n\n1 = Upper Class\n2 = Middle Class\n3 = Lower Class\n\nName, Sex and Age are self-explanatory\nSibSp is the total number of the passengers’ siblings and spouse\nParch is the total number of the passengers’ parents and children\nTicket is the ticket number of the passenger\nFare is the passenger fare\nCabin is the cabin number of the passenger\nEmbarked is port of embarkation and it is a categorical feature which has 3 unique values (C, Q or S):\n\nC = Cherbourg\nQ = Queenstown\nS = Southampton\n\n\n::: {#cell-7 .cell _uuid=‘f02f321f8fd8b8c7c2a4aedb36ebe868ae51004e’ execution_count=3}\nprint(df_train.info())\ndf_train.sample(3)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\nNone\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n633\n634\n0\n1\nParr, Mr. William Henry Marsh\nmale\nNaN\n0\n0\n112052\n0.00\nNaN\nS\n\n\n801\n802\n1\n2\nCollyer, Mrs. Harvey (Charlotte Annie Tate)\nfemale\n31.0\n1\n1\nC.A. 31921\n26.25\nNaN\nS\n\n\n200\n201\n0\n3\nVande Walle, Mr. Nestor Cyriel\nmale\n28.0\n0\n0\n345770\n9.50\nNaN\nS\n\n\n\n\n\n\n:::\n::: {#cell-8 .cell _uuid=‘851ccf74127831d31ea0d7273b686f9a7cf20eee’ execution_count=4}\nprint(df_test.info())\ndf_test.sample(3)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 418 entries, 0 to 417\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  418 non-null    int64  \n 1   Pclass       418 non-null    int64  \n 2   Name         418 non-null    object \n 3   Sex          418 non-null    object \n 4   Age          332 non-null    float64\n 5   SibSp        418 non-null    int64  \n 6   Parch        418 non-null    int64  \n 7   Ticket       418 non-null    object \n 8   Fare         417 non-null    float64\n 9   Cabin        91 non-null     object \n 10  Embarked     418 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 36.0+ KB\nNone\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n35\n927\n3\nKatavelas, Mr. Vassilios (Catavelas Vassilios\")\"\nmale\n18.5\n0\n0\n2682\n7.2292\nNaN\nC\n\n\n29\n921\n3\nSamaan, Mr. Elias\nmale\nNaN\n2\n0\n2662\n21.6792\nNaN\nC\n\n\n135\n1027\n3\nCarlsson, Mr. Carl Robert\nmale\n24.0\n0\n0\n350409\n7.8542\nNaN\nS\n\n\n\n\n\n\n:::\n\n\n1.2 Missing Values\nAs seen from below, some columns have missing values. display_missing function shows the count of missing values in every column in both training and test set. * Training set have missing values in Age, Cabin and Embarked columns * Test set have missing values in Age, Cabin and Fare columns\nIt is convenient to work on concatenated training and test set while dealing with missing values, otherwise filled data may overfit to training or test set samples. The count of missing values in Age, Embarked and Fare are smaller compared to total sample, but roughly 80% of the Cabin is missing. Missing values in Age, Embarked and Fare can be filled with descriptive statistical measures but that wouldn’t work for Cabin.\n::: {#cell-10 .cell _kg_hide-input=‘true’ _uuid=‘d4e8f7b72e2bd165cafa71d67c95f008e7c6101d’ execution_count=5}\ndef display_missing(df):    \n    for col in df.columns.tolist():          \n        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n    print('\\n')\n    \nfor df in dfs:\n    print('{}'.format(df.name))\n    display_missing(df)\n\nTraining Set\nPassengerId column missing values: 0\nSurvived column missing values: 0\nPclass column missing values: 0\nName column missing values: 0\nSex column missing values: 0\nAge column missing values: 177\nSibSp column missing values: 0\nParch column missing values: 0\nTicket column missing values: 0\nFare column missing values: 0\nCabin column missing values: 687\nEmbarked column missing values: 2\n\n\nTest Set\nPassengerId column missing values: 0\nPclass column missing values: 0\nName column missing values: 0\nSex column missing values: 0\nAge column missing values: 86\nSibSp column missing values: 0\nParch column missing values: 0\nTicket column missing values: 0\nFare column missing values: 1\nCabin column missing values: 327\nEmbarked column missing values: 0\n\n\n\n:::\n\n1.2.1 Age\nMissing values in Age are filled with median age, but using median age of the whole data set is not a good choice. Median age of Pclass groups is the best choice because of its high correlation with Age (0.408106) and Survived (0.338481). It is also more logical to group ages by passenger classes instead of other features.\n::: {#cell-12 .cell _kg_hide-input=‘true’ execution_count=6}\ndf_all_corr = df_all.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'Age']\n\n\n\n\n\n\n\n\nFeature 1\nFeature 2\nCorrelation Coefficient\n\n\n\n\n0\nAge\nAge\n1.000000\n\n\n9\nAge\nPclass\n0.408106\n\n\n18\nAge\nSibSp\n0.243699\n\n\n21\nAge\nFare\n0.178740\n\n\n26\nAge\nParch\n0.150917\n\n\n30\nAge\nSurvived\n0.077221\n\n\n41\nAge\nPassengerId\n0.028814\n\n\n\n\n\n\n:::\nIn order to be more accurate, Sex feature is used as the second level of groupby while filling the missing Age values. As seen from below, Pclass and Sex groups have distinct median Age values. When passenger class increases, the median age for both males and females also increases. However, females tend to have slightly lower median Age than males. The median ages below are used for filling the missing values in Age feature.\n\nage_by_pclass_sex = df_all.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('Median age of all passengers: {}'.format(df_all['Age'].median()))\n\n# Filling the missing values in Age with the medians of Sex and Pclass groups\ndf_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n\nMedian age of Pclass 1 females: 36.0\nMedian age of Pclass 1 males: 42.0\nMedian age of Pclass 2 females: 28.0\nMedian age of Pclass 2 males: 29.5\nMedian age of Pclass 3 females: 22.0\nMedian age of Pclass 3 males: 25.0\nMedian age of all passengers: 28.0\n\n\n\n\n1.2.2 Embarked\nEmbarked is a categorical feature and there are only 2 missing values in whole data set. Both of those passengers are female, upper class and they have the same ticket number. This means that they know each other and embarked from the same port together. The mode Embarked value for an upper class female passenger is C (Cherbourg), but this doesn’t necessarily mean that they embarked from that port.\n::: {#cell-16 .cell _kg_hide-input=‘true’ execution_count=8}\ndf_all[df_all['Embarked'].isnull()]\n\n\n\n\n\n\n\n\nAge\nCabin\nEmbarked\nFare\nName\nParch\nPassengerId\nPclass\nSex\nSibSp\nSurvived\nTicket\n\n\n\n\n61\n38.0\nB28\nNaN\n80.0\nIcard, Miss. Amelie\n0\n62\n1\nfemale\n0\n1.0\n113572\n\n\n829\n62.0\nB28\nNaN\n80.0\nStone, Mrs. George Nelson (Martha Evelyn)\n0\n830\n1\nfemale\n0\n1.0\n113572\n\n\n\n\n\n\n:::\nWhen I googled Stone, Mrs. George Nelson (Martha Evelyn), I found that she embarked from S (Southampton) with her maid Amelie Icard, in this page Martha Evelyn Stone: Titanic Survivor.\n\nMrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28.\n\nMissing values in Embarked are filled with S with this information.\n\n# Filling the missing values in Embarked with S\ndf_all['Embarked'] = df_all['Embarked'].fillna('S')\n\n\n\n1.2.3 Fare\nThere is only one passenger with missing Fare value. We can assume that Fare is related to family size (Parch and SibSp) and Pclass features. Median Fare value of a male with a third class ticket and no family is a logical choice to fill the missing value.\n::: {#cell-20 .cell _kg_hide-input=‘true’ execution_count=10}\ndf_all[df_all['Fare'].isnull()]\n\n\n\n\n\n\n\n\nAge\nCabin\nEmbarked\nFare\nName\nParch\nPassengerId\nPclass\nSex\nSibSp\nSurvived\nTicket\n\n\n\n\n1043\n60.5\nNaN\nS\nNaN\nStorey, Mr. Thomas\n0\n1044\n3\nmale\n0\nNaN\n3701\n\n\n\n\n\n\n:::\n\nmed_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\ndf_all['Fare'] = df_all['Fare'].fillna(med_fare)\n\n\n\n1.2.4 Cabin\nCabin feature is little bit tricky and it needs further exploration. The large portion of the Cabin feature is missing and the feature itself can’t be ignored completely because some the cabins might have higher survival rates. It turns out to be the first letter of the Cabin values are the decks in which the cabins are located. Those decks were mainly separated for one passenger class, but some of them were used by multiple passenger classes.  * On the Boat Deck there were 6 rooms labeled as T, U, W, X, Y, Z but only the T cabin is present in the dataset * A, B and C decks were only for 1st class passengers * D and E decks were for all classes * F and G decks were for both 2nd and 3rd class passengers * From going A to G, distance to the staircase increases which might be a factor of survival\n::: {#cell-23 .cell _kg_hide-input=‘true’ execution_count=12}\n# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\ndf_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ndf_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n                                                                        'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n\ndef get_pclass_dist(df):\n    \n    # Creating a dictionary for every passenger class count in every deck\n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    decks = df.columns.levels[0]    \n    \n    for deck in decks:\n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count \n            except KeyError:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)    \n    deck_percentages = {}\n\n    # Creating a dictionary for every passenger class percentage in every deck\n    for col in df_decks.columns:\n        deck_percentages[col] = [(count / df_decks[col].sum()) * 100 for count in df_decks[col]]\n        \n    return deck_counts, deck_percentages\n\ndef display_pclass_dist(percentages):\n    \n    df_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, pclass1, color='#b5ffb9', edgecolor='white', width=bar_width, label='Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#f9bc86', edgecolor='white', width=bar_width, label='Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#a3acff', edgecolor='white', width=bar_width, label='Passenger Class 3')\n\n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Passenger Class Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Passenger Class Distribution in Decks', size=18, y=1.05)   \n    \n    plt.show()    \n\nall_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\ndisplay_pclass_dist(all_deck_per)\n\n\n\n:::\n\n100% of A, B and C decks are 1st class passengers\nDeck D has 87% 1st class and 13% 2nd class passengers\nDeck E has 83% 1st class, 10% 2nd class and 7% 3rd class passengers\nDeck F has 62% 2nd class and 38% 3rd class passengers\n100% of G deck are 3rd class passengers\nThere is one person on the boat deck in T cabin and he is a 1st class passenger. T cabin passenger has the closest resemblance to A deck passengers so he is grouped with A deck\nPassengers labeled as M are the missing values in Cabin feature. I don’t think it is possible to find those passengers’ real Deck so I decided to use M like a deck\n\n\n# Passenger in the T deck is changed to A\nidx = df_all[df_all['Deck'] == 'T'].index\ndf_all.loc[idx, 'Deck'] = 'A'\n\n::: {#cell-26 .cell _kg_hide-input=‘true’ execution_count=14}\ndf_all_decks_survived = df_all.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n                                                                                   'Embarked', 'Pclass', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name':'Count'}).transpose()\n\ndef get_survived_dist(df):\n    \n    # Creating a dictionary for every survival count in every deck\n    surv_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}}\n    decks = df.columns.levels[0]    \n\n    for deck in decks:\n        for survive in range(0, 2):\n            surv_counts[deck][survive] = df[deck][survive][0]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count / df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    return surv_counts, surv_percentages\n\ndef display_surv_dist(percentages):\n    \n    df_survived_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85    \n\n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, not_survived, color='#b5ffb9', edgecolor='white', width=bar_width, label=\"Not Survived\")\n    plt.bar(bar_count, survived, bottom=not_survived, color='#f9bc86', edgecolor='white', width=bar_width, label=\"Survived\")\n \n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Survival Percentage in Decks', size=18, y=1.05)\n    \n    plt.show()\n\nall_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\ndisplay_surv_dist(all_surv_per)\n\n\n\n:::\nAs I suspected, every deck has different survival rates and that information can’t be discarded. Deck B, C, D and E have the highest survival rates. Those decks are mostly occupied by 1st class passengers. M has the lowest survival rate which is mostly occupied by 2nd and 3rd class passengers. To conclude, cabins used by 1st class passengers have higher survival rates than cabins used by 2nd and 3rd class passengers. In my opinion M (Missing Cabin values) has the lowest survival rate because they couldn’t retrieve the cabin data of the victims. That’s why I believe labeling that group as M is a reasonable way to handle the missing data. It is a unique group with shared characteristics. Deck feature has high-cardinality right now so some of the values are grouped with each other based on their similarities. * A, B and C decks are labeled as ABC because all of them have only 1st class passengers * D and E decks are labeled as DE because both of them have similar passenger class distribution and same survival rate * F and G decks are labeled as FG because of the same reason above * M deck doesn’t need to be grouped with other decks because it is very different from others and has the lowest survival rate.\n\ndf_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\ndf_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')\n\ndf_all['Deck'].value_counts()\n\nM      1014\nABC     182\nDE       87\nFG       26\nName: Deck, dtype: int64\n\n\nAfter filling the missing values in Age, Embarked, Fare and Deck features, there is no missing value left in both training and test set. Cabin is dropped because Deck feature is used instead of it.\n::: {#cell-30 .cell _kg_hide-input=‘true’ execution_count=16}\n# Dropping the Cabin feature\ndf_all.drop(['Cabin'], inplace=True, axis=1)\n\ndf_train, df_test = divide_df(df_all)\ndfs = [df_train, df_test]\n\nfor df in dfs:\n    display_missing(df)\n\nAge column missing values: 0\nEmbarked column missing values: 0\nFare column missing values: 0\nName column missing values: 0\nParch column missing values: 0\nPassengerId column missing values: 0\nPclass column missing values: 0\nSex column missing values: 0\nSibSp column missing values: 0\nSurvived column missing values: 0\nTicket column missing values: 0\nDeck column missing values: 0\n\n\nAge column missing values: 0\nEmbarked column missing values: 0\nFare column missing values: 0\nName column missing values: 0\nParch column missing values: 0\nPassengerId column missing values: 0\nPclass column missing values: 0\nSex column missing values: 0\nSibSp column missing values: 0\nTicket column missing values: 0\nDeck column missing values: 0\n\n\n\n:::\n\n\n\n1.3 Target Distribution\n\n38.38% (342/891) of training set is Class 1\n61.62% (549/891) of training set is Class 0\n\n::: {#cell-32 .cell _kg_hide-input=‘true’ _uuid=‘c70aa13b7a552beb976574d52c1cd3da1cc1ee5c’ execution_count=17}\nsurvived = df_train['Survived'].value_counts()[1]\nnot_survived = df_train['Survived'].value_counts()[0]\nsurvived_per = survived / df_train.shape[0] * 100\nnot_survived_per = not_survived / df_train.shape[0] * 100\n\nprint('{} of {} passengers survived and it is the {:.2f}% of the training set.'.format(survived, df_train.shape[0], survived_per))\nprint('{} of {} passengers didnt survive and it is the {:.2f}% of the training set.'.format(not_survived, df_train.shape[0], not_survived_per))\n\nplt.figure(figsize=(10, 8))\nsns.countplot(df_train['Survived'])\n\nplt.xlabel('Survival', size=15, labelpad=15)\nplt.ylabel('Passenger Count', size=15, labelpad=15)\nplt.xticks((0, 1), ['Not Survived ({0:.2f}%)'.format(not_survived_per), 'Survived ({0:.2f}%)'.format(survived_per)])\nplt.tick_params(axis='x', labelsize=13)\nplt.tick_params(axis='y', labelsize=13)\n\nplt.title('Training Set Survival Distribution', size=15, y=1.05)\n\nplt.show()\n\n342 of 891 passengers survived and it is the 38.38% of the training set.\n549 of 891 passengers didnt survive and it is the 61.62% of the training set.\n\n\n\n\n:::\n\n\n1.4 Correlations\nFeatures are highly correlated with each other and dependent to each other. The highest correlation between features is 0.549500 in training set and 0.577147 in test set (between Fare and Pclass). The other features are also highly correlated. There are 9 correlations in training set and 6 correlations in test set that are higher than 0.1.\n::: {#cell-34 .cell _kg_hide-input=‘true’ execution_count=18}\ndf_train_corr = df_train.drop(['PassengerId'], axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\ndf_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n\ndf_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\ndf_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)\n:::\n\n# Training set high correlations\ncorr = df_train_corr_nd['Correlation Coefficient'] &gt; 0.1\ndf_train_corr_nd[corr]\n\n\n\n\n\n\n\n\nFeature 1\nFeature 2\nCorrelation Coefficient\n\n\n\n\n6\nFare\nPclass\n0.549500\n\n\n8\nAge\nPclass\n0.417667\n\n\n10\nParch\nSibSp\n0.414838\n\n\n12\nPclass\nSurvived\n0.338481\n\n\n14\nFare\nSurvived\n0.257307\n\n\n16\nAge\nSibSp\n0.249747\n\n\n18\nParch\nFare\n0.216225\n\n\n20\nParch\nAge\n0.176733\n\n\n22\nSibSp\nFare\n0.159651\n\n\n24\nAge\nFare\n0.124061\n\n\n\n\n\n\n\n\n# Test set high correlations\ncorr = df_test_corr_nd['Correlation Coefficient'] &gt; 0.1\ndf_test_corr_nd[corr]\n\n\n\n\n\n\n\n\nFeature 1\nFeature 2\nCorrelation Coefficient\n\n\n\n\n6\nPclass\nFare\n0.577489\n\n\n8\nAge\nPclass\n0.526789\n\n\n10\nAge\nFare\n0.345347\n\n\n12\nSibSp\nParch\n0.306895\n\n\n14\nParch\nFare\n0.230410\n\n\n16\nFare\nSibSp\n0.172032\n\n\n\n\n\n\n\n::: {#cell-37 .cell _kg_hide-input=‘true’ execution_count=21}\nfig, axs = plt.subplots(nrows=2, figsize=(20, 20))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n\nfor i in range(2):    \n    axs[i].tick_params(axis='x', labelsize=14)\n    axs[i].tick_params(axis='y', labelsize=14)\n    \naxs[0].set_title('Training Set Correlations', size=15)\naxs[1].set_title('Test Set Correlations', size=15)\n\nplt.show()\n\n\n\n:::\n\n\n1.5 Target Distribution in Features\n\n1.5.1 Continuous Features\nBoth of the continuous features (Age and Fare) have good split points and spikes for a decision tree to learn. One potential problem for both features is, the distribution has more spikes and bumps in training set, but it is smoother in test set. Model may not be able to generalize to test set because of this reason.\n\nDistribution of Age feature clearly shows that children younger than 15 has a higher survival rate than any of the other age groups\nIn distribution of Fare feature, the survival rate is higher on distribution tails. The distribution also has positive skew because of the extremely large outliers\n\n::: {#cell-40 .cell _kg_hide-input=‘true’ execution_count=22}\ncont_features = ['Age', 'Fare']\nsurv = df_train['Survived'] == 1\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\nplt.subplots_adjust(right=1.5)\n\nfor i, feature in enumerate(cont_features):    \n    # Distribution of survival in feature\n    sns.distplot(df_train[~surv][feature], label='Not Survived', hist=True, color='#e74c3c', ax=axs[0][i])\n    sns.distplot(df_train[surv][feature], label='Survived', hist=True, color='#2ecc71', ax=axs[0][i])\n    \n    # Distribution of feature in dataset\n    sns.distplot(df_train[feature], label='Training Set', hist=False, color='#e74c3c', ax=axs[1][i])\n    sns.distplot(df_test[feature], label='Test Set', hist=False, color='#2ecc71', ax=axs[1][i])\n    \n    axs[0][i].set_xlabel('')\n    axs[1][i].set_xlabel('')\n    \n    for j in range(2):        \n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n    \n    axs[0][i].legend(loc='upper right', prop={'size': 20})\n    axs[1][i].legend(loc='upper right', prop={'size': 20})\n    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n\naxs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\naxs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n        \nplt.show()\n\n\n\n:::\n\n\n1.5.2 Categorical Features\nEvery categorical feature has at least one class with high mortality rate. Those classes are very helpful to predict whether the passenger is a survivor or victim. Best categorical features are Pclass and Sex because they have the most homogenous distributions.\n\nPassengers boarded from Southampton has a lower survival rate unlike other ports. More than half of the passengers boarded from Cherbourg had survived. This observation could be related to Pclass feature\nParch and SibSp features show that passengers with only one family member has a higher survival rate\n\n::: {#cell-42 .cell _kg_hide-input=‘true’ execution_count=23}\ncat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=df_train)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n\nplt.show()\n\n\n\n:::\n\n\n\n1.6 Conclusion\nMost of the features are correlated with each other. This relationship can be used to create new features with feature transformation and feature interaction. Target encoding could be very useful as well because of the high correlations with Survived feature.\nSplit points and spikes are visible in continuous features. They can be captured easily with a decision tree model, but linear models may not be able to spot them.\nCategorical features have very distinct distributions with different survival rates. Those features can be one-hot encoded. Some of those features may be combined with each other to make new features.\nCreated a new feature called Deck and dropped Cabin feature at the Exploratory Data Analysis part.\n::: {#cell-44 .cell _kg_hide-input=‘true’ execution_count=24}\ndf_all = concat_df(df_train, df_test)\ndf_all.head()\n\n\n\n\n\n\n\n\nAge\nDeck\nEmbarked\nFare\nName\nParch\nPassengerId\nPclass\nSex\nSibSp\nSurvived\nTicket\n\n\n\n\n0\n22.0\nM\nS\n7.2500\nBraund, Mr. Owen Harris\n0\n1\n3\nmale\n1\n0.0\nA/5 21171\n\n\n1\n38.0\nABC\nC\n71.2833\nCumings, Mrs. John Bradley (Florence Briggs Th...\n0\n2\n1\nfemale\n1\n1.0\nPC 17599\n\n\n2\n26.0\nM\nS\n7.9250\nHeikkinen, Miss. Laina\n0\n3\n3\nfemale\n0\n1.0\nSTON/O2. 3101282\n\n\n3\n35.0\nABC\nS\n53.1000\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n0\n4\n1\nfemale\n1\n1.0\n113803\n\n\n4\n35.0\nM\nS\n8.0500\nAllen, Mr. William Henry\n0\n5\n3\nmale\n0\n0.0\n373450\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#feature-engineering",
    "href": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#feature-engineering",
    "title": "타이타닉 튜토리얼",
    "section": "2. Feature Engineering",
    "text": "2. Feature Engineering\n\n2.1 Binning Continuous Features\n\n2.1.1 Fare\nFare feature is positively skewed and survival rate is extremely high on the right end. 13 quantile based bins are used for Fare feature. Even though the bins are too much, they provide decent amount of information gain. The groups at the left side of the graph has the lowest survival rate and the groups at the right side of the graph has the highest survival rate. This high survival rate was not visible in the distribution graph. There is also an unusual group (15.742, 23.25] in the middle with high survival rate that is captured in this process.\n\ndf_all['Fare'] = pd.qcut(df_all['Fare'], 13)\n\n::: {#cell-49 .cell _kg_hide-input=‘true’ execution_count=26}\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=df_all)\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()\n\n\n\n:::\n\n\n2.1.2 Age\nAge feature has a normal distribution with some spikes and bumps and 10 quantile based bins are used for Age. The first bin has the highest survival rate and 4th bin has the lowest survival rate. Those were the biggest spikes in the distribution. There is also an unusual group (34.0, 40.0] with high survival rate that is captured in this process.\n\ndf_all['Age'] = pd.qcut(df_all['Age'], 10)\n\n::: {#cell-52 .cell _kg_hide-input=‘true’ execution_count=28}\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=df_all)\n\nplt.xlabel('Age', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()\n\n\n\n:::\n\n\n\n2.2 Frequency Encoding\nFamily_Size is created by adding SibSp, Parch and 1. SibSp is the count of siblings and spouse, and Parch is the count of parents and children. Those columns are added in order to find the total size of families. Adding 1 at the end, is the current passenger. Graphs have clearly shown that family size is a predictor of survival because different values have different survival rates. * Family Size with 1 are labeled as Alone * Family Size with 2, 3 and 4 are labeled as Small * Family Size with 5 and 6 are labeled as Medium * Family Size with 7, 8 and 11 are labeled as Large\n::: {#cell-54 .cell _kg_hide-input=‘true’ execution_count=29}\ndf_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0][0])\nsns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()\n\n\n\n:::\nThere are too many unique Ticket values to analyze, so grouping them up by their frequencies makes things easier.\nHow is this feature different than Family_Size? Many passengers travelled along with groups. Those groups consist of friends, nannies, maids and etc. They weren’t counted as family, but they used the same ticket.\nWhy not grouping tickets by their prefixes? If prefixes in Ticket feature has any meaning, then they are already captured in Pclass or Embarked features because that could be the only logical information which can be derived from the Ticket feature.\nAccording to the graph below, groups with 2,3 and 4 members had a higher survival rate. Passengers who travel alone has the lowest survival rate. After 4 group members, survival rate decreases drastically. This pattern is very similar to Family_Size feature but there are minor differences. Ticket_Frequency values are not grouped like Family_Size because that would basically create the same feature with perfect correlation. This kind of feature wouldn’t provide any additional information gain.\n\ndf_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')\n\n::: {#cell-57 .cell _kg_hide-input=‘true’ execution_count=31}\nfig, axs = plt.subplots(figsize=(12, 9))\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=df_all)\n\nplt.xlabel('Ticket Frequency', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=15, y=1.05)\n\nplt.show()\n\n\n\n:::\n\n\n2.3 Title & Is Married\nTitle is created by extracting the prefix before Name feature. According to graph below, there are many titles that are occuring very few times. Some of those titles doesn’t seem correct and they need to be replaced. Miss, Mrs, Ms, Mlle, Lady, Mme, the Countess, Dona titles are replaced with Miss/Mrs/Ms because all of them are female. Values like Mlle, Mme and Dona are actually the name of the passengers, but they are classified as titles because Name feature is split by comma. Dr, Col, Major, Jonkheer, Capt, Sir, Don and Rev titles are replaced with Dr/Military/Noble/Clergy because those passengers have similar characteristics. Master is a unique title. It is given to male passengers below age 26. They have the highest survival rate among all males.\nIs_Married is a binary feature based on the Mrs title. Mrs title has the highest survival rate among other female titles. This title needs to be a feature because all female titles are grouped with each other.\n\ndf_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1\n\n::: {#cell-60 .cell _kg_hide-input=‘true’ execution_count=33}\nfig, axs = plt.subplots(nrows=2, figsize=(20, 20))\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[0])\n\naxs[0].tick_params(axis='x', labelsize=10)\naxs[1].tick_params(axis='x', labelsize=15)\n\nfor i in range(2):    \n    axs[i].tick_params(axis='y', labelsize=15)\n\naxs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')\n\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n\nplt.show()\n\n\n\n:::\n\n\n2.4 Target Encoding\nextract_surname function is used for extracting surnames of passengers from the Name feature. Family feature is created with the extracted surname. This is necessary for grouping passengers in the same family.\n\ndef extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families\n\ndf_all['Family'] = extract_surname(df_all['Name'])\ndf_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]\n\nFamily_Survival_Rate is calculated from families in training set since there is no Survived feature in test set. A list of family names that are occuring in both training and test set (non_unique_families), is created. The survival rate is calculated for families with more than 1 members in that list, and stored in Family_Survival_Rate feature.\nAn extra binary feature Family_Survival_Rate_NA is created for families that are unique to the test set. This feature is also necessary because there is no way to calculate those families’ survival rate. This feature implies that family survival rate is not applicable to those passengers because there is no way to retrieve their survival rate.\nTicket_Survival_Rate and Ticket_Survival_Rate_NA features are also created with the same method. Ticket_Survival_Rate and Family_Survival_Rate are averaged and become Survival_Rate, and Ticket_Survival_Rate_NA and Family_Survival_Rate_NA are also averaged and become Survival_Rate_NA.\n\n# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] &gt; 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] &gt; 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]\n\n\nmean_survival_rate = np.mean(df_train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA\n\n\nfor df in [df_train, df_test]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) / 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) / 2    \n\n\n\n2.5 Feature Transformation\n\n2.5.1 Label Encoding Non-Numerical Features\nEmbarked, Sex, Deck , Title and Family_Size_Grouped are object type, and Age and Fare features are category type. They are converted to numerical type with LabelEncoder. LabelEncoder basically labels the classes from 0 to n. This process is necessary for models to learn from those features.\n\nnon_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n\nfor df in dfs:\n    for feature in non_numeric_features:        \n        df[feature] = LabelEncoder().fit_transform(df[feature])\n\n\n\n2.5.2 One-Hot Encoding the Categorical Features\nThe categorical features (Pclass, Sex, Deck, Embarked, Title) are converted to one-hot encoded features with OneHotEncoder. Age and Fare features are not converted because they are ordinal unlike the previous ones.\n\ncat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\nencoded_features = []\n\nfor df in dfs:\n    for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n\ndf_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[6:]], axis=1)\n\n\n\n\n2.6 Conclusion\nAge and Fare features are binned. Binning helped dealing with outliers and it revealed some homogeneous groups in those features. Family_Size is created by adding Parch and SibSp features and 1. Ticket_Frequency is created by counting the occurence of Ticket values.\nName feature is very useful. First, Title and Is_Married features are created from the title prefix in the names. Second, Family_Survival_Rate and Family_Survival_Rate_NA features are created by target encoding the surname of the passengers. Ticket_Survival_Rate is created by target encoding the Ticket feature. Survival_Rate feature is created by averaging the Family_Survival_Rate and Ticket_Survival_Rate features.\nFinally, the non-numeric type features are label encoded and categorical features are one-hot encoded. Created 5 new features (Family_Size, Title, Is_Married, Survival_Rate and Survival_Rate_NA) and dropped the useless features after encoding.\n::: {#cell-73 .cell _kg_hide-input=‘true’ execution_count=40}\ndf_all = concat_df(df_train, df_test)\ndrop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n\ndf_all.drop(columns=drop_cols, inplace=True)\n\ndf_all.head()\n\n\n\n\n\n\n\n\nAge\nDeck_1\nDeck_2\nDeck_3\nDeck_4\nEmbarked_1\nEmbarked_2\nEmbarked_3\nFamily_Size_Grouped_1\nFamily_Size_Grouped_2\n...\nPclass_3\nSex_1\nSex_2\nSurvival_Rate\nSurvival_Rate_NA\nTicket_Frequency\nTitle_1\nTitle_2\nTitle_3\nTitle_4\n\n\n\n\n0\n2\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n0.0\n1.0\n0.383838\n0.0\n1\n0.0\n0.0\n0.0\n1.0\n\n\n1\n7\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n1.000000\n1.0\n2\n0.0\n0.0\n1.0\n0.0\n\n\n2\n4\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n1.0\n1.0\n0.0\n0.383838\n0.0\n1\n0.0\n0.0\n1.0\n0.0\n\n\n3\n7\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.383838\n0.0\n2\n0.0\n0.0\n1.0\n0.0\n\n\n4\n7\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n1.0\n0.0\n1.0\n0.383838\n0.0\n1\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n5 rows × 26 columns\n\n\n:::"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#model",
    "href": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#model",
    "title": "타이타닉 튜토리얼",
    "section": "3. Model",
    "text": "3. Model\n\nX_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\ny_train = df_train['Survived'].values\nX_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\n\nX_train shape: (891, 26)\ny_train shape: (891,)\nX_test shape: (418, 26)\n\n\n\n3.1 Random Forest\nCreated 2 RandomForestClassifier’s. One of them is a single model and the other is for k-fold cross validation.\nThe highest accuracy of the single_best_model is 0.82775 in public leaderboard. However, it doesn’t perform better in k-fold cross validation. It is a good model to start experimenting and hyperparameter tuning.\nThe highest accuracy of leaderboard_model is 0.83732 in public leaderboard with 5-fold cross validation. This model is created for leaderboard score and it is tuned to overfit slightly. It is designed to overfit because the estimated probabilities of X_test in every fold are going to be divided by N (fold count). If this model is used as a single model, it would struggle to predict lots of samples correctly.\nWhich model should I use? * leaderboard_model overfits to test set so it’s not suggested to use models like this in real life projects. * single_best_model is a good model to start experimenting and learning about decision trees.\n\nsingle_best_model = RandomForestClassifier(criterion='gini', \n                                           n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1)\n\nleaderboard_model = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1) \n\nStratifiedKFold is used for stratifying the target variable. The folds are made by preserving the percentage of samples for each class in target variable (Survived).\n\nN = 5\noob = 0\nprobs = pd.DataFrame(np.zeros((len(X_test), N * 2)), columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N + 1) for j in range(2)])\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)], index=df_all.columns)\nfprs, tprs, scores = [], [], []\n\nskf = StratifiedKFold(n_splits=N, random_state=N, shuffle=True)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n    print('Fold {}\\n'.format(fold))\n    \n    # Fitting the model\n    leaderboard_model.fit(X_train[trn_idx], y_train[trn_idx])\n    \n    # Computing Train AUC score\n    trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_train[trn_idx], leaderboard_model.predict_proba(X_train[trn_idx])[:, 1])\n    trn_auc_score = auc(trn_fpr, trn_tpr)\n    # Computing Validation AUC score\n    val_fpr, val_tpr, val_thresholds = roc_curve(y_train[val_idx], leaderboard_model.predict_proba(X_train[val_idx])[:, 1])\n    val_auc_score = auc(val_fpr, val_tpr)  \n      \n    scores.append((trn_auc_score, val_auc_score))\n    fprs.append(val_fpr)\n    tprs.append(val_tpr)\n    \n    # X_test probabilities\n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 1]\n    importances.iloc[:, fold - 1] = leaderboard_model.feature_importances_\n        \n    oob += leaderboard_model.oob_score_ / N\n    print('Fold {} OOB Score: {}\\n'.format(fold, leaderboard_model.oob_score_))   \n    \nprint('Average OOB Score: {}'.format(oob))\n\nFold 1\n\nFold 1 OOB Score: 0.8553370786516854\n\nFold 2\n\nFold 2 OOB Score: 0.844319775596073\n\nFold 3\n\nFold 3 OOB Score: 0.8513323983169705\n\nFold 4\n\nFold 4 OOB Score: 0.8359046283309958\n\nFold 5\n\nFold 5 OOB Score: 0.8260869565217391\n\nAverage OOB Score: 0.8425961674834928\n\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    1.3s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    1.2s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    1.2s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    1.2s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    1.2s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n\n\n\n\n3.2 Feature Importance\n::: {#cell-81 .cell _kg_hide-input=‘true’ execution_count=44}\nimportances['Mean_Importance'] = importances.mean(axis=1)\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 20))\nsns.barplot(x='Mean_Importance', y=importances.index, data=importances)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Random Forest Classifier Mean Feature Importance Between Folds', size=15)\n\nplt.show()\n\n\n\n:::\n\n\n3.3 ROC Curve\n::: {#cell-83 .cell _kg_hide-input=‘true’ execution_count=45}\ndef plot_roc_curve(fprs, tprs):\n    \n    tprs_interp = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    f, ax = plt.subplots(figsize=(15, 15))\n    \n    # Plotting ROC for each fold and computing AUC scores\n    for i, (fpr, tpr) in enumerate(zip(fprs, tprs), 1):\n        tprs_interp.append(np.interp(mean_fpr, fpr, tpr))\n        tprs_interp[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        ax.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC Fold {} (AUC = {:.3f})'.format(i, roc_auc))\n        \n    # Plotting ROC for random guessing\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8, label='Random Guessing')\n    \n    mean_tpr = np.mean(tprs_interp, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    \n    # Plotting the mean ROC\n    ax.plot(mean_fpr, mean_tpr, color='b', label='Mean ROC (AUC = {:.3f} $\\pm$ {:.3f})'.format(mean_auc, std_auc), lw=2, alpha=0.8)\n    \n    # Plotting the standard deviation around the mean ROC Curve\n    std_tpr = np.std(tprs_interp, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='$\\pm$ 1 std. dev.')\n    \n    ax.set_xlabel('False Positive Rate', size=15, labelpad=20)\n    ax.set_ylabel('True Positive Rate', size=15, labelpad=20)\n    ax.tick_params(axis='x', labelsize=15)\n    ax.tick_params(axis='y', labelsize=15)\n    ax.set_xlim([-0.05, 1.05])\n    ax.set_ylim([-0.05, 1.05])\n\n    ax.set_title('ROC Curves of Folds', size=20, y=1.02)\n    ax.legend(loc='lower right', prop={'size': 13})\n    \n    plt.show()\n\nplot_roc_curve(fprs, tprs)\n\n\n\n:::\n\n\n3.4 Submission\n\nclass_survived = [col for col in probs.columns if col.endswith('Prob_1')]\nprobs['1'] = probs[class_survived].sum(axis=1) / N\nprobs['0'] = probs.drop(columns=class_survived).sum(axis=1) / N\nprobs['pred'] = 0\npos = probs[probs['1'] &gt;= 0.5].index\nprobs.loc[pos, 'pred'] = 1\n\ny_pred = probs['pred'].astype(int)\n\nsubmission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = df_test['PassengerId']\nsubmission_df['Survived'] = y_pred.values\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head(10)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n891\n892\n0\n\n\n892\n893\n1\n\n\n893\n894\n0\n\n\n894\n895\n0\n\n\n895\n896\n1\n\n\n896\n897\n0\n\n\n897\n898\n1\n\n\n898\n899\n0\n\n\n899\n900\n1\n\n\n900\n901\n0"
  },
  {
    "objectID": "posts/5_STBDA2023/A3.html",
    "href": "posts/5_STBDA2023/A3.html",
    "title": "A3: 개발환경의 변천사",
    "section": "",
    "text": "주의사항!!\n\n\n\n본 강의노트는 작성자의 상상에 근거하여 작성되었으며, 사실이 아닌 내용이 포함되어 있을 수 있습니다. (특히 옛날사람들의 코딩습관들)"
  },
  {
    "objectID": "posts/5_STBDA2023/A3.html#a.-0세대-프로그래머-프롬프트",
    "href": "posts/5_STBDA2023/A3.html#a.-0세대-프로그래머-프롬프트",
    "title": "A3: 개발환경의 변천사",
    "section": "A. 0세대 프로그래머 (프롬프트)",
    "text": "A. 0세대 프로그래머 (프롬프트)\n- 특징: 프롬프트만 쓴다..\n# 실습1 – python 사용\n- 윈도우에서 anaconda prompt 실행 -&gt; python\n(base) C:\\Users\\python&gt;python\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; [1,2,3]+[4]\n[1, 2, 3, 4]\n&gt;&gt;&gt; a=[1,2,3]+[4]\n&gt;&gt;&gt; a\n[1, 2, 3, 4]\n- 2개를 실행할 수도 있음. (두 환경은 각각 서로 독립적인 파이썬, 변수가 공유되지 않음) \\(\\star\\)\n- 아쉬운점: `?list’와 같이 도움말 기능이 동작하지 않음\n&gt;&gt;&gt; ?list\n  File \"&lt;stdin&gt;\", line 1\n    ?list\n    ^\nSyntaxError: invalid syntax\n&gt;&gt;&gt; \n#\n# 실습2 – ipython 사용\n- 윈도우에서 anaconda prompt 실행 -&gt; ipython\n(base) C:\\Users\\python&gt;ipython\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.29.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: a=[1,2,3]\n\nIn [2]: a\nOut[2]: [1, 2, 3]\n\nIn [3]: a+[4]\nOut[3]: [1, 2, 3, 4]\n\n\n\nsyntax hilighting이 된다. + ?기능 + 자동완성도 됨.\n\n\n- ?list가 가능\nIn [4]: ?list\nInit signature: list(iterable=(), /)\nDocstring:\nBuilt-in mutable sequence.\n\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\nType:           type\nSubclasses:     _HashedSeq, StackSummary, DeferredConfigList, SList, _ImmutableLineList, FormattedText, NodeList, _ExplodedList, Stack, _Accumulator, ...\n\n- 색깔이 알록달록해서 문법을 보기 편하다. (구문강조)\n#\n# 실습3 – 0세대 프로그래머의 삶 with python\n- 1부터 10까지 합을 구하는 프로그램을 만들고 싶음\n- 시도1: python을 키고 아래와 같이 실행\n(base) C:\\Users\\python&gt;python\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; total = 0\n&gt;&gt;&gt; for i in range(10):\n...     total=total+i\n...\n&gt;&gt;&gt; total\n45\n&gt;&gt;&gt;\n- 반성: 정답은 55인데 45가 출력되었다! \\(\\to\\) range(10)을 range(1,11)으로 바꿔야겠다!\n- 시도2: range(1,11)을 바꿔야겠다고 생각하고 다시 입력하다가 오타가 발생\n&gt;&gt;&gt; total =0\n&gt;&gt;&gt; for i in range(1,11):\n...     total = totla +i\n...\n\n앗 totla이라고 잘못쳤다.\n\n- 반성: 다음에는 정신을 똑바로 차려야겠다.\n- 불편한점: … 다..\n#\n# 실습4 – 1세대 프로그래머의 삶 with ipython\n- ipython을 사용한 프로그래머는 좀더 상황이 낫다\n(base) C:\\Users\\python&gt;ipython\nPython 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.29.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: total = 0\n\nIn [2]: for i in range(1,11):\n   ...:     total = total + i\n   ...:\n\nIn [3]: total\nOut[3]: 55\n\n편한점1: 자동으로 들여쓰기가 되어서 편함\n편한점2: 화살표를 이용해서 for문을 쓰는 도중에 위아래로 이동가능\n불편한점1: 화살표로 이동할수는 있는데 마우스로는 이동할 수 없다.\n불편한점2: 내가 작성한 코드를 관리하기 어렵다.\n\n#"
  },
  {
    "objectID": "posts/5_STBDA2023/A3.html#b.-1세대-프로그래머-스크립트활용",
    "href": "posts/5_STBDA2023/A3.html#b.-1세대-프로그래머-스크립트활용",
    "title": "A3: 개발환경의 변천사",
    "section": "B. 1세대 프로그래머 (스크립트활용)",
    "text": "B. 1세대 프로그래머 (스크립트활용)\n- 특징: 스크립트 + 프롬프트를 사용\n# 실습1 – python + .py\n- 메모장을 키고 아래의 내용을 적는다.\ntotal = 0 \nfor i in range(1,11): \n    total = total + i\nprint(total)\n- 파일이름을 mysum.py로 저장한다.\n- anaconda prompt에서 mysum.py파일이 저장된 폴더로 이동 -&gt; 실행\n(base) C:\\Users\\python&gt;cd Desktop\n\n(base) C:\\Users\\python\\Desktop&gt;dir\n C 드라이브의 볼륨에는 이름이 없습니다.\n 볼륨 일련 번호: 9AFD-A05F\n\n C:\\Users\\python\\Desktop 디렉터리\n\n2022-03-27  오전 11:32    &lt;DIR&gt;          .\n2022-03-27  오전 11:32    &lt;DIR&gt;          ..\n2022-03-27  오전 12:01             2,306 Chrome.lnk\n2022-03-26  오후 08:32             2,332 Microsoft Edge.lnk\n2022-03-27  오전 11:33                71 mysum.py\n               3개 파일               4,709 바이트\n               2개 디렉터리  743,643,467,776 바이트 남음\n\n(base) C:\\Users\\python\\Desktop&gt;python mysum.py\n55\n\n(base) C:\\Users\\python\\Desktop&gt;\n- 소감\n\n편한점1: 마우스를 이용하여 이동가능\n편한점2: 내가 작업한 내용은 바탕화면의 메모장에 저장이 되어있음\n아쉬운점: ipython의 장점은 활용못함 (구문강조, 도움말기능)\n\n#\n# 실습2 – ipython + .py\n- 전체적인 개발방식\n\n메모장: 코드를 편집, 저장\nipython: anaconda prompt처럼 메모장의 코드를 실행하고 결과를 확인 + 구문강조, 도움말확인기능 등을 이용하여 짧은 코드를 빠르게 작성\n\n- 기능\n\nipython에서 !python mysum.py를 입력하면 anaconda prompt에서 python mysum.py를 입력한 것과 같은 효과\nipython에서 %run mysum을 입력하면 메모장에서 mysum.py에 입력된 내용을 복사해서 ipython에 붙여넣어 실행한것과 같은 효과\n\n#"
  },
  {
    "objectID": "posts/5_STBDA2023/A3.html#c.-2세대-프로그래머-ide-활용",
    "href": "posts/5_STBDA2023/A3.html#c.-2세대-프로그래머-ide-활용",
    "title": "A3: 개발환경의 변천사",
    "section": "C. 2세대 프로그래머 (IDE 활용)",
    "text": "C. 2세대 프로그래머 (IDE 활용)\n- 메모장과 ipython을 하나로 통합한 프로그램이 등장!\n\njupyter notebook, jupyter lab\nspyder\nidle\nVScode\n…\n\n- 주피터의 트릭 (실제로 주피터는 ipython에 기생할 뿐 아무런 역할도 안해요)\n\n주피터를 실행\n새노트북을 생성 (파이썬으로 선택)\n\n\n컴퓨터는 내부적으로 ipython을 실행하고 그 ipython이랑 여러분이 방금만든 그 노트북과 연결\n\n\n처음보이는 cell에 1+1을 입력 -&gt; 쉬프트엔터 -&gt; 결과2가 출력\n\n\n처음보이는 cell하나 = 자동으로 열린 하나의 메모장\ncell 1+1을 입력 = 메모장에 1+1을 적음\n쉬프트+엔터후 결과2를 출력 = cell의 내용을 복사 -&gt; ipython에 붙여넣음 -&gt; ipython 계산된 결과를 복사 -&gt; cell로 돌아와 붙여넣기\n\n\n새로운 cell을 추가하고 2+2을 입력 -&gt; 쉬프트엔터 -&gt; 결과4가 출력\n\n\n새로운 cell을 추가 = 새로운 메모장 추가\ncell 2+2을 입력 = 새로운 메모장에 2+2를 적음\n쉬프트+엔터후 결과4를 출력 = cell의 내용을 복사 -&gt; ipython에 붙여넣음 -&gt; ipython 계산된 결과를 복사 -&gt; cell로 돌아와 붙여넣기\n\n- 중요한 사실들\n\nIDE는 내부적으로 연산을 수행하는 능력이 없다. (생각해볼것: 왜 R을 꼭 설치하고 Rstudio를 설치해야 했을까?)\n주피터에서 커널을 재시작한다는 의미는 메모장이 열린채로 ipython을 껐다가 다시 실행한다는 의미\n주피터는 단순히 ’메모장의 내용을 복사하여 붙여넣는 기계’라고 볼 수 있다. 이렇게 생각하면 주피터를 꼭 ipython에 연결할 이유는 없다. 실제로 주피터에 R을 연결해서 쓸 수 있다. 즉 하나의 IDE가 여러개의 언어와 연결될 수 있다.\nJupyterLab이라는 프로그램은 크롬에 있는 내용과 ipython간의 통신을 제어하는 프로그램일 뿐이다."
  },
  {
    "objectID": "posts/5_STBDA2023/A3.html#d.-3세대-프로그래머-가상환경",
    "href": "posts/5_STBDA2023/A3.html#d.-3세대-프로그래머-가상환경",
    "title": "A3: 개발환경의 변천사",
    "section": "D. 3세대 프로그래머 (가상환경)",
    "text": "D. 3세대 프로그래머 (가상환경)"
  },
  {
    "objectID": "posts/5_STBDA2023/A3.html#e.-4세대-프로그래머-원격컴퓨터",
    "href": "posts/5_STBDA2023/A3.html#e.-4세대-프로그래머-원격컴퓨터",
    "title": "A3: 개발환경의 변천사",
    "section": "E. 4세대 프로그래머 (원격컴퓨터)",
    "text": "E. 4세대 프로그래머 (원격컴퓨터)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-zEjYbXMD4e-nS0_-nx9Zac&si=eA9ME6Pe4ecmtIgi"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#a.-데이터",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#a.-데이터",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"./titanic/train.csv\")\ntst = TabularDataset(\"./titanic/test.csv\")\n\n- 피처엔지니어링\n\ntr.eval('Fsize = SibSp + Parch')\ntst.eval('Fsize = SibSp + Parch')\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n0\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n1\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n0\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n0\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\nSpector, Mr. Woolf\nmale\nNaN\n0\n0\nA.5. 3236\n8.0500\nNaN\nS\n0\n\n\n414\n1306\n1\nOliva y Ocana, Dona. Fermina\nfemale\n39.0\n0\n0\nPC 17758\n108.9000\nC105\nC\n0\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\n0\n0\nSOTON/O.Q. 3101262\n7.2500\nNaN\nS\n0\n\n\n416\n1308\n3\nWare, Mr. Frederick\nmale\nNaN\n0\n0\n359309\n8.0500\nNaN\nS\n0\n\n\n417\n1309\n3\nPeter, Master. Michael J\nmale\nNaN\n1\n1\n2668\n22.3583\nNaN\nC\n2\n\n\n\n\n418 rows × 12 columns"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#b.-predictor-생성",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230913_062514/\""
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#c.-적합fit",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#c.-적합fit",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr.eval('Fsize = SibSp + Parch'))\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230913_062514/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.16\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   675.50 GB / 982.82 GB (68.7%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    37202.13 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.1s = Fit runtime\n    12 features in original data used to generate 29 features in processed data.\n    Train Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.15s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.17s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8324   = Validation score   (accuracy)\n    1.03s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.11s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8324   = Validation score   (accuracy)\n    1.26s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.32s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8547   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230913_062514/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f32015ceb20&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.854749       0.039301  2.819397                0.000427           0.273874            2       True         14\n1         LightGBMLarge   0.832402       0.002563  0.319035                0.002563           0.319035            1       True         13\n2       NeuralNetFastAI   0.832402       0.006479  1.026765                0.006479           1.026765            1       True         10\n3        NeuralNetTorch   0.832402       0.007522  1.262704                0.007522           1.262704            1       True         12\n4              CatBoost   0.826816       0.003330  0.395544                0.003330           0.395544            1       True          7\n5               XGBoost   0.826816       0.003958  0.108880                0.003958           0.108880            1       True         11\n6              LightGBM   0.821229       0.002757  0.170070                0.002757           0.170070            1       True          4\n7      RandomForestEntr   0.821229       0.021223  0.248606                0.021223           0.248606            1       True          6\n8            LightGBMXT   0.815642       0.003093  0.200616                0.003093           0.200616            1       True          3\n9      RandomForestGini   0.815642       0.021101  0.267509                0.021101           0.267509            1       True          5\n10       ExtraTreesEntr   0.810056       0.022094  0.251629                0.022094           0.251629            1       True          9\n11       ExtraTreesGini   0.804469       0.020723  0.259421                0.020723           0.259421            1       True          8\n12       KNeighborsDist   0.648045       0.003651  0.007448                0.003651           0.007448            1       True          2\n13       KNeighborsUnif   0.648045       0.029592  0.008329                0.029592           0.008329            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.854749\n0.039301\n2.819397\n0.000427\n0.273874\n2\nTrue\n14\n\n\n1\nLightGBMLarge\n0.832402\n0.002563\n0.319035\n0.002563\n0.319035\n1\nTrue\n13\n\n\n2\nNeuralNetFastAI\n0.832402\n0.006479\n1.026765\n0.006479\n1.026765\n1\nTrue\n10\n\n\n3\nNeuralNetTorch\n0.832402\n0.007522\n1.262704\n0.007522\n1.262704\n1\nTrue\n12\n\n\n4\nCatBoost\n0.826816\n0.003330\n0.395544\n0.003330\n0.395544\n1\nTrue\n7\n\n\n5\nXGBoost\n0.826816\n0.003958\n0.108880\n0.003958\n0.108880\n1\nTrue\n11\n\n\n6\nLightGBM\n0.821229\n0.002757\n0.170070\n0.002757\n0.170070\n1\nTrue\n4\n\n\n7\nRandomForestEntr\n0.821229\n0.021223\n0.248606\n0.021223\n0.248606\n1\nTrue\n6\n\n\n8\nLightGBMXT\n0.815642\n0.003093\n0.200616\n0.003093\n0.200616\n1\nTrue\n3\n\n\n9\nRandomForestGini\n0.815642\n0.021101\n0.267509\n0.021101\n0.267509\n1\nTrue\n5\n\n\n10\nExtraTreesEntr\n0.810056\n0.022094\n0.251629\n0.022094\n0.251629\n1\nTrue\n9\n\n\n11\nExtraTreesGini\n0.804469\n0.020723\n0.259421\n0.020723\n0.259421\n1\nTrue\n8\n\n\n12\nKNeighborsDist\n0.648045\n0.003651\n0.007448\n0.003651\n0.007448\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.648045\n0.029592\n0.008329\n0.029592\n0.008329\n1\nTrue\n1"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#d.-예측-predict",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr.eval('Fsize = SibSp + Parch'))).mean()\n\n0.9438832772166106\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst.eval('Fsize = SibSp + Parch'))).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"./titanic_sub/autogluon(Fsize)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#submission-result",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#submission-result",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "Submission Result",
    "text": "Submission Result\n\n\n\nSub and Description\nPublic Score\n\n\n\n\nautogulon_sub\n0.75358\n\n\nautogulon(Fsize)_sub\n0.77272"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "",
    "text": "Autogluon (Fsize, Drop SibSp/Parch)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#a.-데이터",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#a.-데이터",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"./titanic/train.csv\")\ntst = TabularDataset(\"./titanic/test.csv\")\n\n- 피처엔지니어링\n\n_tr = tr.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)\n_tst = tst.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#b.-predictor-생성",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230913_063109/\""
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#c.-적합fit",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#c.-적합fit",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(_tr) # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230913_063109/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.16\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   675.47 GB / 982.82 GB (68.7%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    36873.71 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    10 features in original data used to generate 27 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.2s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8101   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8268   = Validation score   (accuracy)\n    0.18s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.56s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.7989   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8268   = Validation score   (accuracy)\n    1.14s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8212   = Validation score   (accuracy)\n    0.11s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.838    = Validation score   (accuracy)\n    1.62s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8268   = Validation score   (accuracy)\n    0.32s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 5.93s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230913_063109/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fc481da6880&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.036541  2.535042                0.000440           0.277847            2       True         14\n1        NeuralNetTorch   0.837989       0.007182  1.621377                0.007182           1.621377            1       True         12\n2         LightGBMLarge   0.826816       0.002590  0.316955                0.002590           0.316955            1       True         13\n3              LightGBM   0.826816       0.002826  0.182086                0.002826           0.182086            1       True          4\n4              CatBoost   0.826816       0.003359  0.564035                0.003359           0.564035            1       True          7\n5       NeuralNetFastAI   0.826816       0.007859  1.137854                0.007859           1.137854            1       True         10\n6               XGBoost   0.821229       0.003699  0.113863                0.003699           0.113863            1       True         11\n7      RandomForestEntr   0.821229       0.021228  0.249363                0.021228           0.249363            1       True          6\n8      RandomForestGini   0.815642       0.021727  0.271968                0.021727           0.271968            1       True          5\n9            LightGBMXT   0.810056       0.003175  0.201514                0.003175           0.201514            1       True          3\n10       ExtraTreesGini   0.804469       0.021523  0.275308                0.021523           0.275308            1       True          8\n11       ExtraTreesEntr   0.798883       0.022917  0.252218                0.022917           0.252218            1       True          9\n12       KNeighborsDist   0.653631       0.003518  0.007811                0.003518           0.007811            1       True          2\n13       KNeighborsUnif   0.653631       0.026963  0.007720                0.026963           0.007720            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.036541\n2.535042\n0.000440\n0.277847\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.007182\n1.621377\n0.007182\n1.621377\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.826816\n0.002590\n0.316955\n0.002590\n0.316955\n1\nTrue\n13\n\n\n3\nLightGBM\n0.826816\n0.002826\n0.182086\n0.002826\n0.182086\n1\nTrue\n4\n\n\n4\nCatBoost\n0.826816\n0.003359\n0.564035\n0.003359\n0.564035\n1\nTrue\n7\n\n\n5\nNeuralNetFastAI\n0.826816\n0.007859\n1.137854\n0.007859\n1.137854\n1\nTrue\n10\n\n\n6\nXGBoost\n0.821229\n0.003699\n0.113863\n0.003699\n0.113863\n1\nTrue\n11\n\n\n7\nRandomForestEntr\n0.821229\n0.021228\n0.249363\n0.021228\n0.249363\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.815642\n0.021727\n0.271968\n0.021727\n0.271968\n1\nTrue\n5\n\n\n9\nLightGBMXT\n0.810056\n0.003175\n0.201514\n0.003175\n0.201514\n1\nTrue\n3\n\n\n10\nExtraTreesGini\n0.804469\n0.021523\n0.275308\n0.021523\n0.275308\n1\nTrue\n8\n\n\n11\nExtraTreesEntr\n0.798883\n0.022917\n0.252218\n0.022917\n0.252218\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.003518\n0.007811\n0.003518\n0.007811\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.026963\n0.007720\n0.026963\n0.007720\n1\nTrue\n1\n\n\n\n\n\n\n\n- validation set의 의미:"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#d.-예측-predict",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(_tr)).mean()\n\n0.9438832772166106\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(_tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"./titanic_sub/autogluon(Fsize,Drop)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#submission-result",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#submission-result",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "Submission Result",
    "text": "Submission Result\n\n\n\nSub and Description\nPublic Score\n\n\n\n\nautogulon_sub\n0.75358\n\n\nautogulon(Fsize)_sub\n0.77272\n\n\nautogulon(Fsize,Drop)\n0.78947\n\n\n\n- drop 했을 때 성능이 올라가는 이유?\n\nver2 : autogluon // 그냥\nver3 : autogluon // Fsize X,Y –&gt; X+Y+l = Z\nver4 : autogluon // Fsize, Drop"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-011.out.html",
    "href": "posts/5_STBDA2023/03wk-011.out.html",
    "title": "03wk-011: Medical Cost, 회귀분석",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-zwUq3ZIN2SNas0l8htamMO&si=dH2sszdMPMFTGeEV"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-011.out.html#a.-data-정리",
    "href": "posts/5_STBDA2023/03wk-011.out.html#a.-data-정리",
    "title": "03wk-011: Medical Cost, 회귀분석",
    "section": "A. Data 정리",
    "text": "A. Data 정리\n\ndf.columns\n\nIndex(['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges'], dtype='object')\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\n\nX = pd.get_dummies(df.drop(['charges'],axis=1))\ny = df[['charges']]\n\n\nX\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\nsex_female\nsex_male\nsmoker_no\nsmoker_yes\nregion_northeast\nregion_northwest\nregion_southeast\nregion_southwest\n\n\n\n\n0\n19\n27.900\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n1\n18\n33.770\n1\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\n28\n33.000\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n3\n33\n22.705\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n32\n28.880\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\n30.970\n3\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1334\n18\n31.920\n0\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1335\n18\n36.850\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1336\n21\n25.800\n0\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1337\n61\n29.070\n0\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n1338 rows × 11 columns\n\n\n\n\ny\n\n\n\n\n\n\n\n\ncharges\n\n\n\n\n0\n16884.92400\n\n\n1\n1725.55230\n\n\n2\n4449.46200\n\n\n3\n21984.47061\n\n\n4\n3866.85520\n\n\n...\n...\n\n\n1333\n10600.54830\n\n\n1334\n2205.98080\n\n\n1335\n1629.83350\n\n\n1336\n2007.94500\n\n\n1337\n29141.36030\n\n\n\n\n1338 rows × 1 columns"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-011.out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/03wk-011.out.html#b.-predictor-생성",
    "title": "03wk-011: Medical Cost, 회귀분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression()\npredictr\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-011.out.html#c.-학습",
    "href": "posts/5_STBDA2023/03wk-011.out.html#c.-학습",
    "title": "03wk-011: Medical Cost, 회귀분석",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-011.out.html#d.-예측",
    "href": "posts/5_STBDA2023/03wk-011.out.html#d.-예측",
    "title": "03wk-011: Medical Cost, 회귀분석",
    "section": "D. 예측",
    "text": "D. 예측\n\ndf.assign(yhat = predictr.predict(X))\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\nyhat\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n25293.713028\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n3448.602834\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n6706.988491\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n3754.830163\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n5592.493386\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n12351.323686\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n3511.930809\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n4149.132486\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n1246.584939\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n37085.623268\n\n\n\n\n1338 rows × 8 columns"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-011.out.html#e.-평가",
    "href": "posts/5_STBDA2023/03wk-011.out.html#e.-평가",
    "title": "03wk-011: Medical Cost, 회귀분석",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y) # R^2\n\n0.7509130345985207\n\n\n\n0.7 이상이면 망한모형까지는 아님 (대회용으로는 부적절할 수 있으나 대충 쓸 수는 있는 정도)"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-017.html",
    "href": "posts/5_STBDA2023/04wk-017.html",
    "title": "04wk-017: 취업, 로지스틱을 더 깊게",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.linear_model\n\n\n\n3. 데이터 불러오기 \\(\\to\\) 학습\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\n\n\n\n\n0\n135\n0.051535\n0\n\n\n1\n935\n0.355496\n0\n\n\n2\n485\n2.228435\n0\n\n\n3\n65\n1.179701\n0\n\n\n4\n445\n3.962356\n1\n\n\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n\n\n496\n310\n2.601212\n1\n\n\n497\n225\n0.042323\n0\n\n\n498\n320\n1.041416\n0\n\n\n499\n375\n3.626883\n1\n\n\n\n\n500 rows × 3 columns\n\n\n\n\nX = df[['toeic','gpa']]\ny = df[['employment']]\npredictr = sklearn.linear_model.LogisticRegression()\npredictr.fit(X,y)\n\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n\n4. yhat이 나오는 방식?\n- 확인: 무슨 수식에 의하여 나오긴함\n\npredictr.coef_, predictr.intercept_\n\n(array([[0.00571598, 2.46520018]]), array([-8.45433334]))\n\n\n\npd.Series(predictr.coef_[0], index=X.columns)\n\ntoeic    0.005716\ngpa      2.465200\ndtype: float64\n\n\n\nu = X.toeic*0.00571598 + X.gpa*2.46520018 -8.45433334\nv = 1/(1+np.exp(-u))\nv # 확률같은것임\n\n0      0.000523\n1      0.096780\n2      0.453003\n3      0.005627\n4      0.979312\n         ...   \n495    0.976295\n496    0.432939\n497    0.000855\n498    0.016991\n499    0.932777\nLength: 500, dtype: float64\n\n\n\n((v &gt; 0.5) == predictr.predict(X)).mean()\n\n1.0\n\n\n- 하여튼 아래와 같은 구조임\n(구조1)\n\n\n\n\nflowchart LR\n  A[\"predictr (not learned)\"]\n  B[\"predictr (learned)\"]\n  C(\"v (=prob)\")\n  D(\"yhat\")\n  A --&gt; |\"`.fit(X,y)`\"| B --&gt; |\"`????`\" | C --&gt; |\"`v&gt;0.5`\"| D\n\n\n\n\n\n(구조2) – 단순화\n\n\n\n\nflowchart LR\n  A[\"predictr (not learned)\"]\n  B[\"predictr (learned)\"]\n  C(\"yhat\")\n  A --&gt; |\"`.fit(X,y)`\"| B --&gt; |\"`.predict(X)`\" | C\n\n\n\n\n\n- v 값을 알고 싶다면 어쩌지?\n\nv[:5].round(3)\n\n0    0.001\n1    0.097\n2    0.453\n3    0.006\n4    0.979\ndtype: float64\n\n\n\npredictr.predict_proba(X)[:5].round(3) # 오른쪽 값이 v임. (취업이 될 확률 = y가 1일 확률)\n\narray([[0.999, 0.001],\n       [0.903, 0.097],\n       [0.547, 0.453],\n       [0.994, 0.006],\n       [0.021, 0.979]])\n\n\n\n좌측값은 취업이 안될 확률/ 우측값은 취업이 될 확률\n\n\n[p.sum() for p in predictr.predict_proba(X)[:5]]\n\n[1.0, 1.0, 1.0, 1.0, 1.0]\n\n\n\n당연한 말이지만 좌측의 취업이 안될 확률과 우측의 취업이 될 확률을 더하면 \\(1\\)이 된다."
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-025.html",
    "href": "posts/5_STBDA2023/06wk-025.html",
    "title": "06wk-025: 취업+각종영어점수, Lasso",
    "section": "",
    "text": "다중공선성이 있을 때 그것을 깨기위한 두번째 방법인 라쏘에 대해 소개한다."
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-025.html#a.-정확한-설명",
    "href": "posts/5_STBDA2023/06wk-025.html#a.-정확한-설명",
    "title": "06wk-025: 취업+각종영어점수, Lasso",
    "section": "A. 정확한 설명",
    "text": "A. 정확한 설명\n- 어려워요.."
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-025.html#b.-직관적-설명-엄밀하지-않은-설명",
    "href": "posts/5_STBDA2023/06wk-025.html#b.-직관적-설명-엄밀하지-않은-설명",
    "title": "06wk-025: 취업+각종영어점수, Lasso",
    "section": "B. 직관적 설명 (엄밀하지 않은 설명)",
    "text": "B. 직관적 설명 (엄밀하지 않은 설명)\n- 느낌: 몇 개의 toeic coef들로 쉽게 0.01을 만들게 해서는 안된다.\n\n아이디어1: 0.01을 동일한 값으로 균등하게 배분한다. – Ridge, L2-penalty\n아이디어2: 아주 적은숫자의 coef만을 살려두고 나머지 coef값은 0으로 강제한다. – Lasso, L1-penalty\n\n- 계수값이 0이라는 의미: 그 변수를 제거한것과 같은 효과\n- 아이디어2의 기원: y ~ toeic + gpa 가 트루이지만, y ~ toeic0 + gpa 으로 적합해도 괜찮잖아?\n- 진짜 학습된 계수값이 대부분 0인지 확인해보자.\n\nplt.plot(predictr.coef_[1:])"
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-025.html#c.-alpha-에-따른-변화-관찰",
    "href": "posts/5_STBDA2023/06wk-025.html#c.-alpha-에-따른-변화-관찰",
    "title": "06wk-025: 취업+각종영어점수, Lasso",
    "section": "C. \\(\\alpha\\) 에 따른 변화 관찰",
    "text": "C. \\(\\alpha\\) 에 따른 변화 관찰\n- 여러개의 predictor 학습\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n## step1 \ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train[['employment_score']]\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test[['employment_score']]\n## step2 \nalphas = np.linspace(0,2,100) # 0부터 2까지 100개 (등간격)\npredictrs = [sklearn.linear_model.Lasso(alpha=alpha) for alpha in alphas]\n## step3\nfor predictr in predictrs: \n    predictr.fit(X,y)\n## step4 : pass \n\n\nplt.plot(predictrs[0].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[0].alpha))\nplt.plot(predictrs[50].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[50].alpha))\nplt.plot(predictrs[-1].coef_[1:],label=r'$\\alpha={}$'.format(predictrs[-1].alpha))\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fef9c3e4730&gt;\n\n\n\n\n\n\n\\(\\alpha\\)가 \\(0\\)일때는 모든계수가 살아있음.\n\n- predictor 들의 toeic 계수합은 여전히 0.01 근처\n\nprint(f'alpha={predictrs[0].alpha:.4f}\\tsum(toeic_coef)={predictrs[0].coef_[1:].sum()}')\nprint(f'alpha={predictrs[50].alpha:.4f}\\tsum(toeic_coef)={predictrs[50].coef_[1:].sum()}')\nprint(f'alpha={predictrs[-1].alpha:.4f}\\tsum(toeic_coef)={predictrs[-1].coef_[1:].sum()}')\n\nalpha=0.0000    sum(toeic_coef)=0.010291301406468494\nalpha=1.0101    sum(toeic_coef)=0.009986115762478664\nalpha=2.0000    sum(toeic_coef)=0.009864586871194559\n\n\n- number of non-zero coefs 를 시각화\n\nnon_zero_coefs = [(abs(predictr.coef_[1:])&gt;0).sum() for predictr in predictrs]\n\n\nplt.plot(alphas, non_zero_coefs)"
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-025.html#d.-coef를-0으로-만드는-수학적-장치",
    "href": "posts/5_STBDA2023/06wk-025.html#d.-coef를-0으로-만드는-수학적-장치",
    "title": "06wk-025: 취업+각종영어점수, Lasso",
    "section": "D. coef를 0으로 만드는 수학적 장치",
    "text": "D. coef를 0으로 만드는 수학적 장치\n- Ridge(복습): coef의 값들을 엔빵하는 수학적 장치\n\n패널티: 유사토익들의 계수값을 제곱한뒤 합치고(=L2-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n- Lasso: coef의 값들을 대부분 0으로 만드는 수학적 장치\n\n패널티: 유사토익들의 계수값의 절대값을 구한뒤에 합치고(=L1-norm을 구하고), 그 값이 0에서 떨어져 있을 수록 패널티를 줄꺼야!\n\n- 사실 L1, L2 패널티에 따라서 이러한 결과가 나오는 것은 이해하기 어렵다. (그래서 취업/대학원 진학시 단골질문중 하나)\n\nLasso와 Ridge의 coefficient에 관한 질문은 많이 나온다."
  },
  {
    "objectID": "posts/5_STBDA2023/10wk-036.html",
    "href": "posts/5_STBDA2023/10wk-036.html",
    "title": "10wk-036: 애니메이션",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation\nimport IPython\n\n\n\n3. FuncAnimation\n- 모티브\n\nk = 6\nx = np.linspace(0,10,100)\ny = np.sin(0.1*k*x) \nplt.plot(x,y)\n\n\n\n\n\\(k=1,2,3,\\dots\\)로 바꾸면서 변화하는 그림을 연속으로 출력되게 하여 애니메이션으로 보고 싶다. 따라서\n\n하나의 고정된 그림을 정의하고\n그림안의 내용물을 frame에 따라 바꾸는 동작을 정의하여\n\n이들을 결합하는 전략을 생각해보자.\n- 위의코드는 아래와 같다.\n\nk = 4\nx = np.linspace(0,10,100)\ny = np.sin(0.1*k*x) \nfig = plt.figure() # 하나의 고정된 그림을 정의하는 코드 \nax = fig.gca() \nax.plot(x,y) # 고정된 그림에서 내용물을 frame에 따라서 바꾸는 동작을 정의하는 코드 \n\n\n\n\n- 애니메이션 (잘못된 예제) – ax.clear의 역할\n\nfig = plt.figure()  # 그림이 그려지는 전체 창\n\ndef func_(frame):\n    ax = fig.gca() #  \"축\"은 실제로 데이터가 플로팅되는 영역\n    x = np.linspace(0,10,100)\n    y = np.sin(0.1*frame*x)\n    ax.plot(x,y)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nfig = plt.figure()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig, # 하나의 고정된 그림\n    func_, # 고정된 그림에서 내용물을 frame에 따라서 바꾸는 동작을 함수로 정의하고, 그 함수를 넣음\n    frames=30 \n) \n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 애니메이션\n\nfig = plt.figure() # 하나의 고정된 그림을 정의하는 코드 \n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca() \n    ax.clear()\n    x = np.linspace(0,10,100)\n    y = np.sin(0.1*frame*x) \n    ax.plot(x,y) # 고정된 그림에서 내용물을 frame에 따라서 바꾸는 동작을 정의하는 코드 \n\n\nani = matplotlib.animation.FuncAnimation(\n    fig, # 하나의 고정된 그림\n    func, # 고정된 그림에서 내용물을 frame에 따라서 바꾸는 동작을 함수로 정의하고, 그 함수를 넣음\n    frames=30 \n) \n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n4. HW\n\nx = np.linspace(0,10,100)\ny = np.sin(5*x) \nplt.plot(x,y)\n\n\n\n\n위의 그림을 이용하여 애니메이션을 만들어라. 이때 frame이 짝수일경우는 color=’C0’로 frame이 홀수일 경우는 color=’C1’으로 그린 그림이 나오도록 애니메이션을 구성하라.\n\nfig = plt.figure()\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\ndef func2(frame):\n    ax = fig.gca()\n    ax.clear()\n    x = np.linspace(0,10,100)\n    y = np.sin(5*frame*x) \n    if frame % 2 == 0:\n        ax.plot(x,y, color='C0')\n    else:\n        ax.plot(x,y, color='C1')\n\n\nfig = plt.figure() # 하나의 고정된 그림을 정의하는 코드 \n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func2,\n    frames=30)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-028.html",
    "href": "posts/5_STBDA2023/07wk-028.html",
    "title": "07wk-028: 선형모형의 적",
    "section": "",
    "text": "여기서 선형모형이라는 것은 회귀나 로지스틱류…"
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-028.html#a.-결측치의-존재",
    "href": "posts/5_STBDA2023/07wk-028.html#a.-결측치의-존재",
    "title": "07wk-028: 선형모형의 적",
    "section": "A. 결측치의 존재",
    "text": "A. 결측치의 존재\n- 문제: 데이터에서 누락된 값이 있는 경우, 선형모델이 돌아가지 않음. (“NaN이 있어서 모형을 적합할 수 없습니다”라는 에러 발생)\n- 해결방법\n\n방법1: 결측치를 제거\n\n결측치가 포함된 열을 제거\n결측치가 포함된 행을 제거\n위의 두 방법을 혼합\n\n방법2: 결측치를 impute\n\ntrain 에서는 fit_transform, test 에서는 transform\ntrain, test에서 모두 fit_transform\n임의의 값 (예를들면 -999)로 일괄 impute\ninterploation (이미지나 시계열 자료)"
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-028.html#b.-다중공선성의-존재",
    "href": "posts/5_STBDA2023/07wk-028.html#b.-다중공선성의-존재",
    "title": "07wk-028: 선형모형의 적",
    "section": "B. 다중공선성의 존재",
    "text": "B. 다중공선성의 존재\n- 문제: 데이터의 설명변수가 역할이 겹칠경우 선형모형의 일반화 성능이 좋지 않음.\n- 해결방법\n\n방법1: 변수제거\n\nX의 corr을 파악하고 (혹은 히트맵을 그리고) 느낌적으로 제거\nPCA등 차원축소기법을 이용한 제거\n\n방법2: 공선성을 가지는 변수를 모아 새로운 변수로 변환\n\n느낌적으로 변환 (예시 Fsize = Sibsp + Parch + 1, 이후 Sibsp, Parch 는 drop)\nPCA를 이용한 변환\n\n방법3: Ridge, Lasso 등 패널티계열을 사용\n\nRigde\nLasso\nElastic net\n\n\n- 방법1-1 (X의 corr을 파악하고 느낌적으로 제거) 의 예시\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nX = df.loc[:,'gpa':'toeic2']\nX\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\n0\n0.051535\n135\n129.566309\n133.078481\n121.678398\n\n\n1\n0.355496\n935\n940.563187\n935.723570\n939.190519\n\n\n2\n2.228435\n485\n493.671390\n493.909118\n475.500970\n\n\n3\n1.179701\n65\n62.272565\n55.957257\n68.521468\n\n\n4\n3.962356\n445\n449.280637\n438.895582\n433.598274\n\n\n...\n...\n...\n...\n...\n...\n\n\n495\n4.288465\n280\n276.680902\n274.502675\n277.868536\n\n\n496\n2.601212\n310\n296.940263\n301.545000\n306.725610\n\n\n497\n0.042323\n225\n206.793217\n228.335345\n222.115146\n\n\n498\n1.041416\n320\n327.461442\n323.019899\n329.589337\n\n\n499\n3.626883\n375\n370.966595\n364.668477\n371.853566\n\n\n\n\n500 rows × 5 columns\n\n\n\ncorr 조사\n\nX.corr()\n\n\n\n\n\n\n\n\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\n\n\n\n\ngpa\n1.000000\n-0.033983\n-0.035722\n-0.037734\n-0.034828\n\n\ntoeic\n-0.033983\n1.000000\n0.999435\n0.999322\n0.999341\n\n\ntoeic0\n-0.035722\n0.999435\n1.000000\n0.998746\n0.998828\n\n\ntoeic1\n-0.037734\n0.999322\n0.998746\n1.000000\n0.998721\n\n\ntoeic2\n-0.034828\n0.999341\n0.998828\n0.998721\n1.000000\n\n\n\n\n\n\n\nheatmap 플랏\n\nsns.heatmap(X.corr(),annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ngpa는 남기고 toeic, toeic, toeic0, toeic1, toeic2 중에 하나만 남기면 되겠구나… // 설명변수끼리는 독립적인 정보를 갖고있는게 좋음."
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-028.html#c.-관련이-없는-변수의-존재",
    "href": "posts/5_STBDA2023/07wk-028.html#c.-관련이-없는-변수의-존재",
    "title": "07wk-028: 선형모형의 적",
    "section": "C. 관련이 없는 변수의 존재",
    "text": "C. 관련이 없는 변수의 존재\n- 문제: 데이터에서 불필요한 설명변수가 너무 많을 경우 선형모형의 일반화 성능이 좋지 않음.\n\n불필요한 설명변수임의 쉬운 예시: 고객이름, ID, Index 관련 변수\n\n- 해결방법\n\n방법1: 변수제거\n\n(y,X)의 corr을 파악하고 (혹은 히트맵을 그리고) 느낌적으로 제거\nPCA를 이용한 제거\nLasso를 이용한 제거\n\n방법2: 더 많은 데이터를 확보 (궁극기술, 그런데 차원의 저주때문에 힘듬)\n\n- 방법1-1의 예시\n\nnp.random.seed(1)\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_balance = pd.DataFrame((np.random.randn(500,3)).reshape(500,3)*1,columns = ['balance'+str(i) for i in range(3)])\ndf_train = pd.concat([df,df_balance],axis=1)\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\n0\n135\n0.051535\n0\n1.624345\n-0.611756\n-0.528172\n\n\n1\n935\n0.355496\n0\n-1.072969\n0.865408\n-2.301539\n\n\n2\n485\n2.228435\n0\n1.744812\n-0.761207\n0.319039\n\n\n3\n65\n1.179701\n0\n-0.249370\n1.462108\n-2.060141\n\n\n4\n445\n3.962356\n1\n-0.322417\n-0.384054\n1.133769\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n-1.326490\n0.308204\n1.115489\n\n\n496\n310\n2.601212\n1\n1.008196\n-3.016032\n-1.619646\n\n\n497\n225\n0.042323\n0\n2.005141\n-0.187626\n-0.148941\n\n\n498\n320\n1.041416\n0\n1.165335\n0.196645\n-0.632590\n\n\n499\n375\n3.626883\n1\n-0.209847\n1.897161\n-1.381391\n\n\n\n\n500 rows × 6 columns\n\n\n\n\ndf_train.corr()\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\ntoeic\n1.000000\n-0.033983\n0.260183\n0.002682\n0.110530\n0.024664\n\n\ngpa\n-0.033983\n1.000000\n0.711022\n-0.025197\n0.005272\n0.020794\n\n\nemployment\n0.260183\n0.711022\n1.000000\n-0.007348\n0.036706\n0.032284\n\n\nbalance0\n0.002682\n-0.025197\n-0.007348\n1.000000\n-0.059167\n0.040035\n\n\nbalance1\n0.110530\n0.005272\n0.036706\n-0.059167\n1.000000\n-0.030215\n\n\nbalance2\n0.024664\n0.020794\n0.032284\n0.040035\n-0.030215\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(df_train.corr(),annot=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n설명변수 balance0,1,2(=X3,X4,X5)는 반응변수 employment(=y)와 관련이 없어 -&gt; X3,X4,X5는 제외하자.\n반응변수 employment(=y)와 관련이 있는 설명변수인 toiec,gpa (=X1,X2)는 남기자.\n공선성체크: 설명변수 toeic, gpa (=X1,X2)의 corr은 -0.034 로 높지 않으니 다중공선성문제를 걱정할 필요가 없음.\n\n- 방법1-3의 예시\n\nnp.random.seed(1)\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\ndf_balance = pd.DataFrame((np.random.randn(500,3)).reshape(500,3)*1,columns = ['balance'+str(i) for i in range(3)])\ndf_train = pd.concat([df,df_balance],axis=1)\ndf_train\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\n\n\n\n\n0\n135\n0.051535\n0\n1.624345\n-0.611756\n-0.528172\n\n\n1\n935\n0.355496\n0\n-1.072969\n0.865408\n-2.301539\n\n\n2\n485\n2.228435\n0\n1.744812\n-0.761207\n0.319039\n\n\n3\n65\n1.179701\n0\n-0.249370\n1.462108\n-2.060141\n\n\n4\n445\n3.962356\n1\n-0.322417\n-0.384054\n1.133769\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n-1.326490\n0.308204\n1.115489\n\n\n496\n310\n2.601212\n1\n1.008196\n-3.016032\n-1.619646\n\n\n497\n225\n0.042323\n0\n2.005141\n-0.187626\n-0.148941\n\n\n498\n320\n1.041416\n0\n1.165335\n0.196645\n-0.632590\n\n\n499\n375\n3.626883\n1\n-0.209847\n1.897161\n-1.381391\n\n\n\n\n500 rows × 6 columns\n\n\n\n\n# step1\nX,y = df_train[['toeic','gpa','balance0','balance1','balance2']], df_train['employment']\n# step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(\n    Cs = [0.1, 1, 10, 100],\n    penalty='l1',\n    solver='liblinear',\n    random_state=42\n)\n# step3 \npredictr.fit(X,y)\n# step4 -- pass\n\nLogisticRegressionCV(Cs=[0.1, 1, 10, 100], penalty='l1', random_state=42,\n                     solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionCVLogisticRegressionCV(Cs=[0.1, 1, 10, 100], penalty='l1', random_state=42,\n                     solver='liblinear')\n\n\n\npredictr.coef_\n\narray([[0.00260249, 1.41401358, 0.        , 0.        , 0.        ]])\n\n\n\ns = pd.Series(predictr.coef_.reshape(-1))\ns.index= X.columns\ns\n\ntoeic       0.002602\ngpa         1.414014\nbalance0    0.000000\nbalance1    0.000000\nbalance2    0.000000\ndtype: float64\n\n\n\n라쏘를 이용해서 변수선택을 이용하는 것도 합리적이다."
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-028.html#d.-이상치의-존재",
    "href": "posts/5_STBDA2023/07wk-028.html#d.-이상치의-존재",
    "title": "07wk-028: 선형모형의 적",
    "section": "D. 이상치의 존재",
    "text": "D. 이상치의 존재\n- 문제: 이상치가 존재할 경우 전체 모형이 무너질 수 있음\n- 이상치가 있을 경우 해결할 수 있는 방법\n\n방법1: 이상치를 제거하고 분석한다.\n\n느낌적으로 제거함.\n이상치를 감지하는 지표을 사용하여 제거한 이후 분석\n이상치를 자동으로 감지하는 모형을 사용하여 이상치를 제거한 이후 분석\n\n방법2: 로버스트 선형회귀 계열을 이용\n\nsklearn.linear_model.HuberRegressor 등\n\n\n방법3: 이상치를 완화시키는 변환을 사용\n\nsklearn.preprocessing.PowerTransformer 이용\n\n\n- 방법3-1의 예시\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 50\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})[:10]\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n50.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n5\n-0.3\n10.205951\n\n\n6\n0.3\n8.486925\n\n\n7\n0.4\n8.817227\n\n\n8\n0.4\n8.273155\n\n\n9\n0.7\n8.863784\n\n\n\n\n\n\n\n\ntransformr = sklearn.preprocessing.PowerTransformer()\nx,y = transformr.fit_transform(df_train).T\n\n\nsns.scatterplot(df_train,x='temp',y='ice_sales',label='before')\nsns.scatterplot(x=x,y=y,label='after')\n\n&lt;Axes: xlabel='temp', ylabel='ice_sales'&gt;\n\n\n\n\n\n—참고—\nPowerTransformer()는 자료가 정규분포가 아닌 경우 강제로 정규화하는 변환이다.\n\nx = np.random.exponential(scale=10, size=1000)\ntransformr = sklearn.preprocessing.PowerTransformer(method='box-cox')  # 'box-cox' 또는 'yeo-johnson' 중 선택 가능, 디폴트는 이오존슨\ny = transformr.fit_transform(x.reshape(-1, 1))\n\n\nfig, ax = plt.subplots(1,2)\nax[0].hist(x,bins=25)\nax[1].hist(y,bins=25)\nfig.set_figwidth(12)\n\n\n\n\n데이터를 막 바꿔서 분석해도 되나? \\(\\to\\) 되돌릴 수 있는 변환이라면 괜찮음.\n\\(X\\to \\tilde{X}\\) // \\(\\tilde{X} \\to X\\)"
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-028.html#e.-교호작용의-존재",
    "href": "posts/5_STBDA2023/07wk-028.html#e.-교호작용의-존재",
    "title": "07wk-028: 선형모형의 적",
    "section": "E. 교호작용의 존재",
    "text": "E. 교호작용의 존재\n\ncorrelation과는 다른 개념. 변수간의 시너지 효과라고 생각하면 된다. 혹은 더 안좋아지거나.\n\n- 문제: 설명 변수 간의 상호 작용이 있는 경우 이를 고려하지 않으면 모델이 데이터를 잘 설명하지 못할 수 있음.\n- 해결: 고려하면 됩니당.."
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-033.html",
    "href": "posts/5_STBDA2023/07wk-033.html",
    "title": "07wk-033: 취업(다중공선성) / 의사결정나무",
    "section": "",
    "text": "의사결정나무는 다중공선성이 있는 문제에 대해서도 잘 동작한다.\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-y9gv3aEYpTCSbjj97u8zHM&si=YVaZvfWaat_E0tQv\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\n\n\n\n3. Data\n\nnp.random.seed(43052)\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n\nX,y = df_train.loc[:,'gpa':],df_train['employment_score']\nXX,yy = df_test.loc[:,'gpa':],df_test['employment_score']\n\n\n실제 kaggle에서는 yy를 모르는 상황임\n\n\n\n4. 분석\n- 분석0: 선형회귀\n\nlr = sklearn.linear_model.LinearRegression()\nlr.fit(X,y)\ndf_train['employment_score_hat'] = lr.predict(X)\ndf_test['employment_score_hat'] = lr.predict(XX)\n\n\nprint(f'train score: {lr.score(X,y):.4f}')\nprint(f'test score: {lr.score(XX,yy):.4f}')\n\ntrain score: 1.0000\ntest score: 0.1171\n\n\n- 분석1: 의사결정나무\n\n## step1 -- pass \n## step2 \npredictr = sklearn.tree.DecisionTreeRegressor(random_state=42)\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment_score_hat'] = predictr.predict(X)\ndf_test['employment_score_hat'] = predictr.predict(XX)\n#---#\nprint(f'train score: {predictr.score(X,y):.4f}')\nprint(f'test score: {predictr.score(XX,yy):.4f}')\n\ntrain score: 1.0000\ntest score: 0.8300\n\n\n\n오버핏이긴한데 나쁘지 않음..\n\n- 분석2: Lasso\n\n## step1 -- pass \n## step2 \npredictr = sklearn.linear_model.LassoCV(verbose=False)\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment_score_hat'] = predictr.predict(X)\ndf_test['employment_score_hat'] = predictr.predict(XX)\n#---#\nprint(f'train score: {predictr.score(X,y):.4f}')\nprint(f'test score: {predictr.score(XX,yy):.4f}')\n\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.405e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.201e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.824e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.914e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.018e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.785e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.007e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.099e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.120e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.084e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.171e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.346e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.655e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.898e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.112e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.258e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.147e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.844e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.174e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.206e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.447e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.739e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.238e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.561e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.673e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.799e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.659e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.574e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.414e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.699e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.640e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.357e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.353e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.341e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.380e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.561e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.144e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.869e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.042e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.151e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.362e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.393e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.565e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.660e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.788e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.947e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.008e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.410e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.400e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.299e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.387e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.721e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.116e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.587e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.865e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.254e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.614e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.732e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.196e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.200e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.779e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.594e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.524e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.851e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.160e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.410e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.755e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.131e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.388e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.581e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.547e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.379e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.953e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/mp/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.392e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\ntrain score: 0.8994\ntest score: 0.8587\n\n\n- 총평: Lasso가 좋긴해요. 그런데 의사결정나무도 나쁘지 않아요.\n\n참고로 Lasso는 엄청 발전된 모델\n의사결정나무는 아주 초기모델임"
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-030.html",
    "href": "posts/5_STBDA2023/07wk-030.html",
    "title": "07wk-030: 아이스크림(교호작용) / 선형회귀",
    "section": "",
    "text": "1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-zwhGQx-SVKqORNSrWm0TYi&si=igNDnZPiKmBIpvlx\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\nimport sklearn.model_selection\n\n\n\n3. 예비학습\n\ndf = pd.DataFrame({'X1':[2,3,4,1],'X2':['A','B','A','C']})\ndf \n\n\n\n\n\n\n\n\nX1\nX2\n\n\n\n\n0\n2\nA\n\n\n1\n3\nB\n\n\n2\n4\nA\n\n\n3\n1\nC\n\n\n\n\n\n\n\n\npd.get_dummies(df)\n\n\n\n\n\n\n\n\nX1\nX2_A\nX2_B\nX2_C\n\n\n\n\n0\n2\nTrue\nFalse\nFalse\n\n\n1\n3\nFalse\nTrue\nFalse\n\n\n2\n4\nTrue\nFalse\nFalse\n\n\n3\n1\nFalse\nFalse\nTrue\n\n\n\n\n\n\n\n\nX2_A, X2_B, X2_C는 셋다 있을 필요는 없지 않나? –&gt; 공선성문제가 생길수도 있음.\n\n\npd.get_dummies(df,drop_first=True)\n\n\n\n\n\n\n\n\nX1\nX2_B\nX2_C\n\n\n\n\n0\n2\nFalse\nFalse\n\n\n1\n3\nTrue\nFalse\n\n\n2\n4\nFalse\nFalse\n\n\n3\n1\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n4. Data\n- load\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n- 시각화 및 해석\n\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales,'o',label='choco')\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales,'o',label='vanilla')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fccf21e3460&gt;\n\n\n\n\n\n\n온도에 따른 아이스크림 판매량이 아이스크림의 tpye에 따라 동일하다면 기울기가 동일하고 절편이 다른 두 직선이 나올것임.\n하지만 현재는 초코보다 바닐라맛이 기온의 영향을 많이 받아보임 \\(\\to\\) (바닐라아이스크림,온도)는 (초코아이스크림,온도)보다 궁합이 좋다. \\(\\to\\) 아이스크림 type과 온도사이에는 교호작용이 존재한다.\n\n\n\n5. 분석1\n- 분석1: 모형을 아래와 같이 본다.\n\n\\({\\bf X}\\): temp, type\n\\({\\bf y}\\): sales\n\n\n# step1 \nX,y = pd.get_dummies(df_train[['temp','type']],drop_first=True), df_train['sales']\n# step2\npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n\n\npredictr.score(X,y)\n\n0.9249530603100549\n\n\n\n점수가 잘나왔다고 너무 좋아하지 마세요.\n시각화를 반드시 해보고 더 맞출수 있는 여지가 있는지 항상 확인할 것\n\n\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales,'o',label='choco',color='C0',alpha=0.5)\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales_hat,'--',color='C0')\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales,'o',label='vanilla',color='C1',alpha=0.5)\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales_hat,'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fcce8687bb0&gt;\n\n\n\n\n\n\n이 모형은 초코/바닐라에 대한 기울기차이를 “표현”할 수 없다. 이러한 상황은 “모형의 표현력이 약하다” 혹은 “언더피팅”인 상황이라고 한다.\n\n\n\n6. 분석2 (교호작용항 추가)\n- 모형을 아래와 같이 본다.\n\n\\({\\bf X}\\): temp, type, temp \\(\\times\\) type\n\\({\\bf y}\\): sales\n\n\npd.get_dummies(df_train[['temp','type']],drop_first=True).eval('interaction = temp*type_vanilla')\n\n\n\n\n\n\n\n\ntemp\ntype_vanilla\ninteraction\n\n\n\n\n0\n-4.1\nFalse\n-0.0\n\n\n1\n-3.7\nFalse\n-0.0\n\n\n2\n-3.0\nFalse\n-0.0\n\n\n3\n-1.3\nFalse\n-0.0\n\n\n4\n-0.5\nFalse\n-0.0\n\n\n...\n...\n...\n...\n\n\n95\n12.4\nTrue\n12.4\n\n\n96\n13.4\nTrue\n13.4\n\n\n97\n14.7\nTrue\n14.7\n\n\n98\n15.0\nTrue\n15.0\n\n\n99\n15.2\nTrue\n15.2\n\n\n\n\n200 rows × 3 columns\n\n\n\n\n# step1 \nX = pd.get_dummies(df_train[['temp','type']],drop_first=True).eval('interaction = temp*type_vanilla')\ny = df_train['sales']\n# step2\npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n\n\npredictr.score(X,y)\n\n0.9865793819066231\n\n\n\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales,'o',label='choco',color='C0',alpha=0.5)\nplt.plot(df_train[df_train.type=='choco'].temp,df_train[df_train.type=='choco'].sales_hat,'--',color='C0')\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales,'o',label='vanilla',color='C1',alpha=0.5)\nplt.plot(df_train[df_train.type=='vanilla'].temp,df_train[df_train.type=='vanilla'].sales_hat,'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fcce8019b40&gt;\n\n\n\n\n\n기울기는 temp\\(\\times\\)type의 인터럭션 텀으로 맞출 수 있게 되었다.\n\nNote: 초코/바닐라에 대한 절편차이는 type로, 초코/바닐라에 대한 기울기 차이는 temp\\(\\times\\)type로 표현한다."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "import numpy as np\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt\n\n\n\n(1) 아래는 \\(X_i \\overset{iid}{\\sim} N(3,2^2)\\) 를 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tnp.random.randn(10000)*2+3\nx\n\n&lt;tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])&gt;\n\n\n함수 \\(L(\\mu,\\sigma)\\)을 최대화하는 \\((\\mu,\\sigma)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\mu\\)의 초기값은 2로 \\(\\sigma\\)의 초기값은 3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2}\\]\n(2) 아래는 \\(X_i \\overset{iid}{\\sim} Ber(0.8)\\)을 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tf.constant(np.random.binomial(1,0.8,(10000,)))\nx\n\n&lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([0, 0, 1, ..., 1, 1, 1])&gt;\n\n\n함수 \\(L(p)\\)을 최대화하는 \\(p\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(p\\)의 초기값은 0.3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=p^{x_i}(1-p)^{1-x_i}\\]\n(3) 아래의 모형에 따라서 \\(\\{Y_i\\}_{i=1}^{10000}\\)를 생성하는 코드를 작성하라. - \\(Y_i \\overset{iid}{\\sim} N(\\mu_i,1)\\) - \\(\\mu_i = \\beta_0 + \\beta_1 x_i = 0.5 + 2 x_i\\) , where \\(x_i = \\frac{i}{10000}\\)\n함수 \\(L(\\beta_0,\\beta_1)\\)을 최대화하는 \\((\\beta_0,\\beta_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\beta_0,\\beta_1\\)의 초기값은 모두 1로 설정할 것)\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]\n\n\n\n아래와 같은 선형모형을 고려하자.\n\\[y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i.\\]\n이때 오차항은 정규분포로 가정한다. 즉 \\(\\epsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)\\)라고 가정한다.\n관측데이터가 아래와 같을때 아래의 물음에 답하라.\n\nx= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\ny= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 ,\n              63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286])\n# X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4],\n#               [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]])\n\n(1) MSE loss를 최소화 하는 \\(\\beta_0,\\beta_1\\)의 해석해를 구하라.\n(2) 경사하강법과 MSE loss의 도함수를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것.\n(3) tf.keras.optimizers의 apply_gradients()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n(4) tf.keras.optimizers의 minimize()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n\n\n\n(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i, \\quad i=0,1,\\dots, 999\\]\n여기에서 \\(t_i=\\frac{2\\pi i}{1000}\\) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.array(range(1000))* np.pi/1000\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.2)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n(2) 아래와 같은 모형을 고려하자.\n\\[y_i \\sim Ber(\\pi_i), ~ \\text{where} ~ \\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\]\n위의 모형에서 관측한 데이터는 아래와 같다.\n\ntf.random.set_seed(43052)\nx = tnp.linspace(-1,1,2000)\ny = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64)\nplt.plot(x,y,'.',alpha=0.05)\n\n\n\n\ntf.keras를 이용하여 \\(w_0,w_1\\)을 추정하라. (참고: \\(w_0, w_1\\)에 대한 참값은 -1과 5이다.)\n\n\n\n아래의 모형을 고려하자.\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x&gt;0 \\end{cases}\\)\n아래는 위의 모형에서 생성한 샘플이다.\n\n## data\nnp.random.seed(43052)\nN=100\nx= np.linspace(-1,1,N).reshape(N,1)\ny= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1)\n\n(1) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n$ = _0+_1x $\n\n\ntf.random.set_seed(43054)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n&lt;keras.callbacks.History at 0x7f6b142800d0&gt;\n\n\n케라스에 의해 추정된 \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\)을 구하라.\n(2) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n\\(\\boldsymbol{u}= x\\boldsymbol{W}^{(1)}+\\boldsymbol{b}^{(1)}\\)\n\\(\\boldsymbol{v}= \\text{relu}(u)\\)\n\\(y= \\boldsymbol{v}\\boldsymbol{W}^{(2)}+b^{(2)}\\)\n\n\ntf.random.set_seed(43056)\n## 1단계\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2))\nnet.add(tf.keras.layers.Activation('relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f6af6c39f30&gt;\n\n\n\\({\\boldsymbol u}\\)를 이용하여 \\({\\boldsymbol v}\\)를 만드는 코드와 \\({\\boldsymbol v}\\)를 이용하여 \\(y\\)를 만드는 코드를 작성하라.\n(3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라.\n(곤이) (2) 모형은 활성화함수로 relu를 사용하였다.\n(철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다.\n(아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다.\n(짝귀) (1) 의 모형은 오버피팅의 위험이 있다.\n\n\n\n(1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다.\n(2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다.\n(3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다.\n(4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다.\n(5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html#경사하강법과-tf.gradienttape의-사용방법-30점",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html#경사하강법과-tf.gradienttape의-사용방법-30점",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "(1) 아래는 \\(X_i \\overset{iid}{\\sim} N(3,2^2)\\) 를 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tnp.random.randn(10000)*2+3\nx\n\n&lt;tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])&gt;\n\n\n함수 \\(L(\\mu,\\sigma)\\)을 최대화하는 \\((\\mu,\\sigma)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\mu\\)의 초기값은 2로 \\(\\sigma\\)의 초기값은 3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2}\\]\n(2) 아래는 \\(X_i \\overset{iid}{\\sim} Ber(0.8)\\)을 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tf.constant(np.random.binomial(1,0.8,(10000,)))\nx\n\n&lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([0, 0, 1, ..., 1, 1, 1])&gt;\n\n\n함수 \\(L(p)\\)을 최대화하는 \\(p\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(p\\)의 초기값은 0.3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=p^{x_i}(1-p)^{1-x_i}\\]\n(3) 아래의 모형에 따라서 \\(\\{Y_i\\}_{i=1}^{10000}\\)를 생성하는 코드를 작성하라. - \\(Y_i \\overset{iid}{\\sim} N(\\mu_i,1)\\) - \\(\\mu_i = \\beta_0 + \\beta_1 x_i = 0.5 + 2 x_i\\) , where \\(x_i = \\frac{i}{10000}\\)\n함수 \\(L(\\beta_0,\\beta_1)\\)을 최대화하는 \\((\\beta_0,\\beta_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\beta_0,\\beta_1\\)의 초기값은 모두 1로 설정할 것)\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html#회귀분석의-이론적해와-tf.keras.optimizer-이용방법-20점",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html#회귀분석의-이론적해와-tf.keras.optimizer-이용방법-20점",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "아래와 같은 선형모형을 고려하자.\n\\[y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i.\\]\n이때 오차항은 정규분포로 가정한다. 즉 \\(\\epsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)\\)라고 가정한다.\n관측데이터가 아래와 같을때 아래의 물음에 답하라.\n\nx= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\ny= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 ,\n              63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286])\n# X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4],\n#               [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]])\n\n(1) MSE loss를 최소화 하는 \\(\\beta_0,\\beta_1\\)의 해석해를 구하라.\n(2) 경사하강법과 MSE loss의 도함수를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것.\n(3) tf.keras.optimizers의 apply_gradients()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n(4) tf.keras.optimizers의 minimize()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html#keras를-이용한-풀이-30점",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html#keras를-이용한-풀이-30점",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i, \\quad i=0,1,\\dots, 999\\]\n여기에서 \\(t_i=\\frac{2\\pi i}{1000}\\) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.array(range(1000))* np.pi/1000\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.2)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n(2) 아래와 같은 모형을 고려하자.\n\\[y_i \\sim Ber(\\pi_i), ~ \\text{where} ~ \\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\]\n위의 모형에서 관측한 데이터는 아래와 같다.\n\ntf.random.set_seed(43052)\nx = tnp.linspace(-1,1,2000)\ny = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64)\nplt.plot(x,y,'.',alpha=0.05)\n\n\n\n\ntf.keras를 이용하여 \\(w_0,w_1\\)을 추정하라. (참고: \\(w_0, w_1\\)에 대한 참값은 -1과 5이다.)"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html#piecewise-linear-regression-15점",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html#piecewise-linear-regression-15점",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "아래의 모형을 고려하자.\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x&gt;0 \\end{cases}\\)\n아래는 위의 모형에서 생성한 샘플이다.\n\n## data\nnp.random.seed(43052)\nN=100\nx= np.linspace(-1,1,N).reshape(N,1)\ny= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1)\n\n(1) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n$ = _0+_1x $\n\n\ntf.random.set_seed(43054)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n&lt;keras.callbacks.History at 0x7f6b142800d0&gt;\n\n\n케라스에 의해 추정된 \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\)을 구하라.\n(2) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n\\(\\boldsymbol{u}= x\\boldsymbol{W}^{(1)}+\\boldsymbol{b}^{(1)}\\)\n\\(\\boldsymbol{v}= \\text{relu}(u)\\)\n\\(y= \\boldsymbol{v}\\boldsymbol{W}^{(2)}+b^{(2)}\\)\n\n\ntf.random.set_seed(43056)\n## 1단계\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2))\nnet.add(tf.keras.layers.Activation('relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f6af6c39f30&gt;\n\n\n\\({\\boldsymbol u}\\)를 이용하여 \\({\\boldsymbol v}\\)를 만드는 코드와 \\({\\boldsymbol v}\\)를 이용하여 \\(y\\)를 만드는 코드를 작성하라.\n(3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라.\n(곤이) (2) 모형은 활성화함수로 relu를 사용하였다.\n(철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다.\n(아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다.\n(짝귀) (1) 의 모형은 오버피팅의 위험이 있다."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html#다음을-잘-읽고-참과-거짓을-판단하라.-5점",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html#다음을-잘-읽고-참과-거짓을-판단하라.-5점",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "(1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다.\n(2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다.\n(3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다.\n(4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다.\n(5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_16_(11주차)_5월16일.html",
    "href": "posts/3_STBDA2022/2022_05_16_(11주차)_5월16일.html",
    "title": "[STBDA] 11wk. MaxPool2D, Conv2D",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-wlEuWT75L4hqGNEwoPpjWw\n\n\n\nimports\n\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 첫시도\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(500,activation='relu')) # flatten(): x의 shape을 네트워크 모양에 따라 바꾸지 않아도 된다.\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\nnet1.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.1752 - accuracy: 0.7889\nEpoch 2/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4585 - accuracy: 0.8347\nEpoch 3/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4124 - accuracy: 0.8537\nEpoch 4/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.3948 - accuracy: 0.8602\nEpoch 5/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.3813 - accuracy: 0.8649\n\n\n&lt;keras.callbacks.History at 0x7f7fc8338f40&gt;\n\n\n\nnet1.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 735us/step - loss: 0.4072 - accuracy: 0.8473\n\n\n[0.40723177790641785, 0.8472999930381775]\n\n\n\nnet1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 500)               392500    \n                                                                 \n dense_1 (Dense)             (None, 500)               250500    \n                                                                 \n dense_2 (Dense)             (None, 500)               250500    \n                                                                 \n dense_3 (Dense)             (None, 500)               250500    \n                                                                 \n dense_4 (Dense)             (None, 10)                5010      \n                                                                 \n=================================================================\nTotal params: 1,149,010\nTrainable params: 1,149,010\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 두번째 시도 : 네트워크를 다르게 설계\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Conv2D(30,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.MaxPool2D())\nnet2.add(tf.keras.layers.Conv2D(30,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.MaxPool2D())\nnet2.add(tf.keras.layers.Flatten())\n#net2.add(tf.keras.layers.Dense(500,activation='relu'))\nnet2.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet2.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\nnet2.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.8491 - accuracy: 0.7960\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4071 - accuracy: 0.8539\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3550 - accuracy: 0.8719\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3221 - accuracy: 0.8831\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3096 - accuracy: 0.8877\n\n\n&lt;keras.callbacks.History at 0x7f7f847d1430&gt;\n\n\n\nnet2.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 845us/step - loss: 0.3697 - accuracy: 0.8627\n\n\n[0.36973485350608826, 0.8626999855041504]\n\n\n\nnet2.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 27, 27, 30)        150       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 30)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 12, 12, 30)        3630      \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 6, 6, 30)         0         \n 2D)                                                             \n                                                                 \n flatten_1 (Flatten)         (None, 1080)              0         \n                                                                 \n dense_5 (Dense)             (None, 10)                10810     \n                                                                 \n=================================================================\nTotal params: 14,590\nTrainable params: 14,590\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n14590/ 1149010\n\n0.012697887746842934\n\n\n파라미터를 네트워크1에 비해 1.2%정도 밖에 안쓰고 있는데 성능은 더 좋다.\n\nnet2.layers\n\n[&lt;keras.layers.convolutional.conv2d.Conv2D at 0x7f7f847bea90&gt;,\n &lt;keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f7fc00ef160&gt;,\n &lt;keras.layers.convolutional.conv2d.Conv2D at 0x7f7fc00e7fd0&gt;,\n &lt;keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f7fc007b280&gt;,\n &lt;keras.layers.reshaping.flatten.Flatten at 0x7f7fc008f2e0&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f7fc0075cd0&gt;]\n\n\n\n\\(x\\)라는 데이터가 들어가서 6장의 레이어를 순차적으로 통과하는 구조이다.\n\n\nc1, m1, c2, m2, flttn, dns = net2.layers\n\n\nprint(X.shape) # 입력이미지 = 2D\nprint(c1(X).shape) #2D\nprint(m1(c1(X)).shape)  #2D\nprint(c2(m1(c1(X))).shape) #2D\nprint(m2(c2(m1(c1(X)))).shape) #2D\nprint(flttn(m2(c2(m1(c1(X))))).shape)# 1D\nprint(dns(flttn(m2(c2(m1(c1(X)))))).shape)# 1D\n\n(60000, 28, 28, 1)\n(60000, 27, 27, 30)\n(60000, 13, 13, 30)\n(60000, 12, 12, 30)\n(60000, 6, 6, 30)\n(60000, 1080)\n(60000, 10)\n\n\n\n\nMaxPool2D\n\n테스트1\n- 레이어생성\n\nm=tf.keras.layers.MaxPool2D()\n\n- 입력데이터\n\nXXX = tnp.arange(1*4*4*1).reshape(1,4,4,1)\nXXX.reshape(1,4,4)\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]]])&gt;\n\n\n- 입력데이터가 레이어를 통과한 모습\n\nm(XXX).reshape(1,2,2)\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]]])&gt;\n\n\n- MaxPool2D layer의 역할: (2,2)윈도우를 만들고 (2,2)윈도우에서 max를 뽑아 값을 기록, 윈도우를 움직이면서 반복\n\n\n테스트2\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n&lt;tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])&gt;\n\n\n\nm(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 7,  9, 11],\n        [19, 21, 23],\n        [31, 33, 35]]])&gt;\n\n\n\n\n테스트3\n\nm=tf.keras.layers.MaxPool2D(pool_size=(3, 3))\n\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n&lt;tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])&gt;\n\n\n\nm(XXX).reshape(1,2,2)\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[14, 17],\n        [32, 35]]])&gt;\n\n\n\n\n테스트4\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n\n\nXXX = tnp.arange(1*5*5*1).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24]]])&gt;\n\n\n\nm(XXX).reshape(1,2,2)\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 6,  8],\n        [16, 18]]])&gt;\n\n\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2, 2),padding=\"same\")\n\n\nXXX = tnp.arange(1*5*5*1).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24]]])&gt;\n\n\n\n차원이 안맞는 부분은 버린다.. (이게 Default)\n\n\nm(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 6,  8,  9],\n        [16, 18, 19],\n        [21, 23, 24]]])&gt;\n\n\n\n\n테스트5\n\nXXX = tnp.arange(2*4*4*1).reshape(2,4,4,1)\nXXX.reshape(2,4,4)\n\n&lt;tf.Tensor: shape=(2, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]],\n\n       [[16, 17, 18, 19],\n        [20, 21, 22, 23],\n        [24, 25, 26, 27],\n        [28, 29, 30, 31]]])&gt;\n\n\n\nm(XXX).reshape(2,2,2)\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]],\n\n       [[21, 23],\n        [29, 31]]])&gt;\n\n\n\n\n테스트6\n\nXXX = tnp.arange(1*4*4*3).reshape(1,4,4,3)\n\n\nXXX[...,0]\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  3,  6,  9],\n        [12, 15, 18, 21],\n        [24, 27, 30, 33],\n        [36, 39, 42, 45]]])&gt;\n\n\n\nm(XXX)[...,0]\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[15, 21],\n        [39, 45]]])&gt;\n\n\n\n\n\nConv2D\n\n테스트1\n- 레이어생성\n\ncnv = tf.keras.layers.Conv2D(1,(2,2))\n\n- XXX생성\n\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1) # data type이 float형이어야 에러가 안남! 4-dim\nXXX.reshape(1,4,4)\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])&gt;\n\n\n\ncnv(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 0.9716801,  2.230203 ,  3.4887257],\n        [ 6.0057716,  7.2642946,  8.522818 ],\n        [11.039864 , 12.298387 , 13.55691  ]]], dtype=float32)&gt;\n\n\n\nXXX에서 cnv(XXX)로 가는 맵핑을 찾는건 쉽지 않아보인다.\n심지어 랜덤으로 결정되는 부분도 있어보임\n\n- 코드정리 + 시드통일\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(2,2))\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1)\n\n- conv의 입출력\n\nprint(XXX.reshape(1,4,4))\nprint(cnv(XXX).reshape(1,3,3))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.]\n  [ 4.  5.  6.  7.]\n  [ 8.  9. 10. 11.]\n  [12. 13. 14. 15.]]], shape=(1, 4, 4), dtype=float64)\ntf.Tensor(\n[[[ 0.36673212 -0.5838206  -1.5343733 ]\n  [-3.4354792  -4.3860326  -5.336584  ]\n  [-7.23769    -8.188243   -9.138796  ]]], shape=(1, 3, 3), dtype=float32)\n\n\n\ntype(cnv.weights) # 길이가 2인 list\n\nlist\n\n\n- conv연산 추론\n\ntf.reshape(cnv.weights[0],(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.7749905 , -0.50674176],\n       [ 0.7824232 , -0.45124376]], dtype=float32)&gt;\n\n\n\n## XXX\ntf.Tensor(\n[[[ 0.  1.  2.  3.]\n  [ 4.  5.  6.  7.]\n  [ 8.  9. 10. 11.]\n  [12. 13. 14. 15.]]]\n\n## cnv(XXX)\n[[[ 0.36673212 -0.5838206  -1.5343733 ]\n  [-3.4354792  -4.3860326  -5.336584  ]\n  [-7.23769    -8.188243   -9.138796  ]]]\n## cnv.weights\narray([[-0.7749905 , -0.50674176],\n       [ 0.7824232 , -0.45124376]], dtype=float32)&gt;\n\n# my version\nprint(0*-0.7749905 + 1*-0.50674176 + 4*0.7824232 + 5*-0.45124376 + 0) # bias=0\nprint(1*-0.7749905 + 2*-0.50674176 + 5*0.7824232 + 6*-0.45124376  + 0) # bias=0\nprint(2*-0.7749905 + 3*-0.50674176 + 6*0.7824232 + 7*-0.45124376 + 0) # bias=0\n\n0.3667322399999997\n-0.5838205799999998\n-1.5343734000000002\n\n\n\n# element wise하게 곱한다.\n# ver. 교수님\n0 * -0.13014299 + 1 * -0.23927206 + 4 * -0.20175874 + 5 * -0.6158894 + 0\n\n-4.1257540200000005\n\n\n\nweights를 위에 처럼 다 쓰긴 너무 귀찮아..\n\n- 내가 정의한 weights를 대입하여 conv 연산 확인\n\ncnv.get_weights()[0].shape\n\n(2, 2, 1, 1)\n\n\n\nw = np.array([1/4,1/4,1/4,1/4],dtype=np.float32).reshape(2, 2, 1, 1)\nb = np.array([3],dtype=np.float32)\n\n\ncnv.set_weights([w,b])\n\n\ncnv.weights\n\n[&lt;tf.Variable 'conv2d_9/kernel:0' shape=(2, 2, 1, 1) dtype=float32, numpy=\n array([[[[0.25]],\n \n         [[0.25]]],\n \n \n        [[[0.25]],\n \n         [[0.25]]]], dtype=float32)&gt;,\n &lt;tf.Variable 'conv2d_9/bias:0' shape=(1,) dtype=float32, numpy=array([3.], dtype=float32)&gt;]\n\n\n\nbias=3\n\n\nXXX.reshape(1,4,4)\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])&gt;\n\n\n\ncnv(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 5.5,  6.5,  7.5],\n        [ 9.5, 10.5, 11.5],\n        [13.5, 14.5, 15.5]]], dtype=float32)&gt;\n\n\n\nnp.mean([0,1,4,5])+3, np.mean([1,2,5,6])+3, np.mean([2,3,6,7])+3\n\n(5.5, 6.5, 7.5)\n\n\n\n\ntf.keras.layers.Conv2D(1,kernel_size=(2,2)) 요약\n- 요약\n\nsize=(2,2)인 윈도우를 만듬.\nXXX에 윈도우를 통과시켜서 (2,2)크기의 sub XXX 를 얻음. sub XXX의 각 원소에 conv2d.weights[0]의 각 원소를 element-wise하게 곱한다.\n(2)의 결과를 모두 더한다. 그리고 그 결과에 다시 conv2d.weights[1]을 수행\n윈도우를 이동시키면서 반복!\n\n\n\n테스트2\n- 레이어와 XXX생성\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(3,3)) # kernel size: 3x3\nXXX = tnp.arange(1*5*5*1,dtype=tf.float64).reshape(1,5,5,1)\n\n\nXXX.reshape(1,5,5) ## 입력: XXX\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.],\n        [10., 11., 12., 13., 14.],\n        [15., 16., 17., 18., 19.],\n        [20., 21., 22., 23., 24.]]])&gt;\n\n\n\ncnv(XXX)\n\n&lt;tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=\narray([[[[-2.1662652 ],\n         [-1.4241629 ],\n         [-0.68206024]],\n\n        [[ 1.5442457 ],\n         [ 2.2863474 ],\n         [ 3.0284495 ]],\n\n        [[ 5.254756  ],\n         [ 5.996857  ],\n         [ 6.73896   ]]]], dtype=float32)&gt;\n\n\n\ncnv.get_weights()\n\n[array([[[[ 0.25344223]],\n \n         [[ 0.4941373 ]],\n \n         [[ 0.21964025]]],\n \n \n        [[[ 0.5367935 ]],\n \n         [[-0.38218382]],\n \n         [[-0.1195904 ]]],\n \n \n        [[[-0.52197355]],\n \n         [[ 0.5757177 ]],\n \n         [[-0.3138811 ]]]], dtype=float32),\n array([0.], dtype=float32)]\n\n\n\n하나는 kernel에 대한 weight, 다른 하나는 bias에 대한 weight(=0)\n\n\ntf.reshape(cnv.weights[0],(3,3)) ## 커널의 가중치\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 0.25344223,  0.4941373 ,  0.21964025],\n       [ 0.5367935 , -0.38218382, -0.1195904 ],\n       [-0.52197355,  0.5757177 , -0.3138811 ]], dtype=float32)&gt;\n\n\n\ncnv(XXX).reshape(1,3,3) ## 출력: conv(XXX)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[-2.1662652 , -1.4241629 , -0.68206024],\n        [ 1.5442457 ,  2.2863474 ,  3.0284495 ],\n        [ 5.254756  ,  5.996857  ,  6.73896   ]]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(XXX.reshape(1,5,5)[0,:3,:3] * tf.reshape(cnv.weights[0],(3,3)))\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=-2.1662647128105164&gt;\n\n\n\ntf.reduce_sum(XXX.reshape(1,5,5)[0,:3,1:4]  * tf.reshape(cnv.weights[0],(3,3)))\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=-1.4241626560688019&gt;\n\n\n\ntf.reduce_sum(XXX.reshape(1,5,5)[0,:3,2:5]  * tf.reshape(cnv.weights[0],(3,3)))\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=-0.6820605993270874&gt;\n\n\n\n\n테스트3\n\n\nXXX = tf.constant([[3,3,2,1,0],[0,0,1,3,1],[3,1,2,2,3],[2,0,0,2,2],[2,0,0,0,1]],dtype=tf.float64).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[3., 3., 2., 1., 0.],\n        [0., 0., 1., 3., 1.],\n        [3., 1., 2., 2., 3.],\n        [2., 0., 0., 2., 2.],\n        [2., 0., 0., 0., 1.]]])&gt;\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\ndtype은 항상 float\nshape은 항상 4D로 나와야 한다.\n\n\n\n\ncnv = tf.keras.layers.Conv2D(1,(3,3)) # 3x3 window를 갖는 커널\n\n\ncnv.weights\n\n[]\n\n\n\n일단 cnv(XXX) 이걸 돌려줘야 weight이 생긴다.\n\n\ncnv(XXX).reshape(1,3,3) \n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 0.5037645 , -1.312168  ,  0.6543627 ],\n        [ 2.4390626 ,  1.264232  , -0.26046228],\n        [ 2.5233214 ,  0.00339425,  1.0914648 ]]], dtype=float32)&gt;\n\n\n\nlayer에 통과하는 순간 weight이 정의된다.\n\n\ncnv.weights[0]\n\n&lt;tf.Variable 'conv2d_18/kernel:0' shape=(3, 3, 1, 1) dtype=float32, numpy=\narray([[[[-0.01447701]],\n\n        [[-0.02373308]],\n\n        [[ 0.40413225]]],\n\n\n       [[[ 0.47097933]],\n\n        [[ 0.52465725]],\n\n        [[-0.37146354]]],\n\n\n       [[[ 0.42013115]],\n\n        [[-0.37700126]],\n\n        [[-0.35089916]]]], dtype=float32)&gt;\n\n\n\ncnv.weights[1] # bias\n\n&lt;tf.Variable 'conv2d_18/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;\n\n\n\nbias는 \\(0\\)이니까 무시..\n\n\n_w = tf.constant([[0,1,2],[2,2,0],[0,1,2]],dtype=tf.float64).reshape(3,3,1,1)\n_b = tf.constant([0],dtype=tf.float64)\n\n\ncnv.set_weights([_w,_b])\n\n\ncnv(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[12., 12., 17.],\n        [10., 17., 19.],\n        [ 9.,  6., 14.]]], dtype=float32)&gt;\n\n\n\n\n테스트4: X의 channel, Conv channel 늘려보기.\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(2,2))\nXXX = tnp.arange(2*5*5*1,dtype=tf.float64).reshape(2,5,5,1) # 무조건 4D, float형\n\n\nprint(XXX.reshape(2,5,5))\ncnv(XXX) # weights를 초기화 시키기 위해서 레이어를 1회 통과\ncnv.set_weights([w,b])\nprint(cnv(XXX).reshape(2,4,4))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.  4.]\n  [ 5.  6.  7.  8.  9.]\n  [10. 11. 12. 13. 14.]\n  [15. 16. 17. 18. 19.]\n  [20. 21. 22. 23. 24.]]\n\n [[25. 26. 27. 28. 29.]\n  [30. 31. 32. 33. 34.]\n  [35. 36. 37. 38. 39.]\n  [40. 41. 42. 43. 44.]\n  [45. 46. 47. 48. 49.]]], shape=(2, 5, 5), dtype=float64)\ntf.Tensor(\n[[[ 6.  7.  8.  9.]\n  [11. 12. 13. 14.]\n  [16. 17. 18. 19.]\n  [21. 22. 23. 24.]]\n\n [[31. 32. 33. 34.]\n  [36. 37. 38. 39.]\n  [41. 42. 43. 44.]\n  [46. 47. 48. 49.]]], shape=(2, 4, 4), dtype=float32)\n\n\n## 첫번째 observation\n[[[ 0.  1.  2.  3.  4.]\n  [ 5.  6.  7.  8.  9.]\n  [10. 11. 12. 13. 14.]\n  [15. 16. 17. 18. 19.]\n  [20. 21. 22. 23. 24.]]\n\n ## 두번째 observation\n [[25. 26. 27. 28. 29.]\n  [30. 31. 32. 33. 34.]\n  [35. 36. 37. 38. 39.]\n  [40. 41. 42. 43. 44.]\n  [45. 46. 47. 48. 49.]]]\n\nprint(cnv.weights[0]) # weights\nprint(cnv.weights[1]) # bias\n\n&lt;tf.Variable 'conv2d_24/kernel:0' shape=(2, 2, 1, 1) dtype=float32, numpy=\narray([[[[0.25]],\n\n        [[0.25]]],\n\n\n       [[[0.25]],\n\n        [[0.25]]]], dtype=float32)&gt;\n&lt;tf.Variable 'conv2d_24/bias:0' shape=(1,) dtype=float32, numpy=array([3.], dtype=float32)&gt;\n\n\n\nnp.mean([0,1,5,6])+3,np.mean([25,26,30,31])+3,\n\n(6.0, 31.0)\n\n\n\n첫번째 obs에 컨볼루션 연산, 두번째 obs도 반복..\n\n\n\n테스트5 : obs 그대로(고정), 출력 채널이 바뀔 때\n- 여기부터 어려워짐..\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2),activation='relu')\nXXX = tnp.arange(1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[0. 1.]\n  [2. 3.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\ncnv(XXX)\n\n&lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=array([[[[0.       , 1.4783237, 0.       , 1.3432117]]]], dtype=float32)&gt;\n\n\n\ncnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수\n\n&lt;tf.Variable 'conv2d_27/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.14884505,  0.1156525 , -0.4904389 , -0.0581924 ]],\n\n        [[-0.05579314,  0.28279382, -0.27906388, -0.15229541]]],\n\n\n       [[[-0.02228564,  0.08160239,  0.44105333,  0.49807286]],\n\n        [[-0.51174355,  0.34410834, -0.38598925,  0.16645378]]]],\n      dtype=float32)&gt;\n\n\n\n1개의 채널에서 4개의 채널로 뻥튀기 됨.\n\n\ncnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.14884505, -0.05579314],\n       [-0.02228564, -0.51174355]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=-1.635595053434372&gt;\n\n\n- 계산결과를 확인하기 쉽게 하기 위한 약간의 트릭\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2))\nXXX = tnp.array([1]*1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[1. 1.]\n  [1. 1.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\n이렇게 XXX를 설정하면 cnv(XXX)의 결과는 단지 cnv의 weight들의 sum이 된다.\n\n\ncnv(XXX)\n\n&lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=array([[[[1.1908543, 0.       , 0.       , 0.       ]]]], dtype=float32)&gt;\n\n\n\ncnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수\n\n&lt;tf.Variable 'conv2d_24/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]],\n\n        [[-0.36398047,  0.07347518, -0.08780673,  0.46633136]]],\n\n\n       [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]],\n\n        [[ 0.33916563, -0.08248386,  0.11705655, -0.49948823]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.08230966, -0.36398047],\n       [ 0.19759327,  0.33916563]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0])\n#tf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.090468764&gt;\n\n\n\n\n테스트6\n- 결과확인을 쉽게하기 위해서 XXX를 1로 통일\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2))\nXXX = tnp.array([1]*1*2*2*3,dtype=tf.float64).reshape(1,2,2,3)\n\n\ncnv(XXX)\n\n&lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 0.3297621, -0.4498347, -1.0487393, -1.580095 ]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0] ## (2,2)는 커널의 사이즈 // 3은 XXX의채널 // 4는 cnv(XXX)의 채널\n\n&lt;tf.Variable 'conv2d_33/kernel:0' shape=(2, 2, 3, 4) dtype=float32, numpy=\narray([[[[-0.06956434, -0.12789628, -0.10784459, -0.32920673],\n         [-0.30761963,  0.06209785, -0.07421023,  0.3941219 ],\n         [ 0.16699678, -0.38913035, -0.13020593, -0.29443866]],\n\n        [[ 0.28664726, -0.0697116 ,  0.09893084, -0.4221446 ],\n         [-0.23161241, -0.16410837, -0.36420006,  0.12424195],\n         [-0.14245945,  0.36286396, -0.10751781,  0.1733647 ]]],\n\n\n       [[[ 0.02764335,  0.15547717, -0.42024496, -0.31893867],\n         [ 0.22414821,  0.3619454 , -0.00282967, -0.3503708 ],\n         [ 0.4610079 , -0.17417148,  0.00401336, -0.29777044]],\n\n        [[-0.1620284 , -0.42066965, -0.01578814, -0.4240524 ],\n         [ 0.37925082,  0.24236053,  0.3949356 , -0.20996472],\n         [-0.30264795, -0.28889188, -0.3237777 ,  0.37506342]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0][...,0] ## cnv(XXX)의 첫번째 채널결과를 얻기 위해서 사용하는 w\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\narray([[[-0.06956434, -0.30761963,  0.16699678],\n        [ 0.28664726, -0.23161241, -0.14245945]],\n\n       [[ 0.02764335,  0.22414821,  0.4610079 ],\n        [-0.1620284 ,  0.37925082, -0.30264795]]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0]) ### cnv(XXX)의 첫번째 채널의 결과\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.32976213&gt;\n\n\n\nprint(tf.reduce_sum(cnv.weights[0][...,0]))\nprint(tf.reduce_sum(cnv.weights[0][...,1]))\nprint(tf.reduce_sum(cnv.weights[0][...,2]))\nprint(tf.reduce_sum(cnv.weights[0][...,3])) ### cnv(XXX)의 결과\n\ntf.Tensor(0.32976213, shape=(), dtype=float32)\ntf.Tensor(-0.44983464, shape=(), dtype=float32)\ntf.Tensor(-1.0487392, shape=(), dtype=float32)\ntf.Tensor(-1.5800952, shape=(), dtype=float32)\n\n\n\nw_red = cnv.weights[0][...,0][...,0]\nw_green = cnv.weights[0][...,0][...,1]\nw_blue = cnv.weights[0][...,0][...,2]\n\n\ntf.reduce_sum(XXX[...,0] * w_red + XXX[...,1] * w_green + XXX[...,2] * w_blue) ## cnv(XXX)의 첫채널 출력결과\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.32976213097572327&gt;\n\n\n\n\n\nhw\n아래와 같은 흑백이미지가 있다고 하자.\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n위의 이미지에 아래와 같은 weight를 가진 필터를 적용하여 convolution한 결과를 계산하라. (bias는 0으로 가정한다)\n-1 1\n-1 1"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_18_(7주차)_4월18일.html",
    "href": "posts/3_STBDA2022/2022_04_18_(7주차)_4월18일.html",
    "title": "[STBDA] 7wk. Piece-wise LR / Logistic Regression",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-ws3T1xD-bBU46dtduUlwmP\n\n\n\nimports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\n\npiece-wise linear regression\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x&gt;0 \\end{cases}\\)\n\nnp.random.seed(43052)\nN=100\nx = np.linspace(-1,1,N)\nlamb = lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3 \ny= np.array(list(map(lamb,x)))\ny\n\narray([-0.88497385, -0.65454563, -0.61676249, -0.84702584, -0.84785569,\n       -0.79220455, -1.3777105 , -1.27341781, -1.41643729, -1.26404671,\n       -0.79590224, -0.78824395, -0.86064773, -0.52468679, -1.18247354,\n       -0.29327295, -0.69373049, -0.90561768, -1.07554911, -0.7225404 ,\n       -0.69867774, -0.34811037,  0.11188474, -1.05046296, -0.03840085,\n       -0.38356861, -0.24299798, -0.58403161, -0.20344022, -0.13872303,\n       -0.529586  , -0.27814478, -0.10852781, -0.38294596,  0.02669763,\n       -0.23042603, -0.77720364, -0.34287396, -0.04512022, -0.30180793,\n       -0.26711438, -0.51880349, -0.53939672, -0.32052379, -0.32080763,\n        0.28917092,  0.18175206, -0.48988124, -0.08084459,  0.37706178,\n        0.14478908,  0.07621827, -0.071864  ,  0.05143365,  0.33932009,\n       -0.35071776,  0.87742867,  0.51370399,  0.34863976,  0.55855514,\n        1.14196717,  0.86421076,  0.72957843,  0.57342304,  1.54803332,\n        0.98840018,  1.11129366,  1.42410801,  1.44322465,  1.25926455,\n        1.12940772,  1.46516829,  1.16365096,  1.45560853,  1.9530553 ,\n        2.45940445,  1.52921129,  1.8606463 ,  1.86406718,  1.5866523 ,\n        1.49033473,  2.35242686,  2.12246412,  2.41951931,  2.43615052,\n        1.96024441,  2.65843789,  2.46854394,  2.76381882,  2.78547462,\n        2.56568465,  3.15212157,  3.11482949,  3.17901774,  3.31268904,\n        3.60977818,  3.40949166,  3.30306495,  3.74590922,  3.85610433])\n\n\n\nplt.plot(x,y,'.')\n\n\n\n\n\n풀이1: 단순회귀모형\n\nx= x.reshape(N,1)\ny= y.reshape(N,1) \n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n&lt;keras.callbacks.History at 0x7f88c01c7820&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)&gt;]\n\n\n\nyhat = x * 2.2616348 + 0.6069048\nyhat = net.predict(x)\n\n4/4 [==============================] - 0s 470us/step\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,yhat,'--')\n\n\n\n\n- 실패: 이 모형은 epoch을 10억번 돌려도 실패할 모형임 - 왜? 아키텍처 설계자체가 틀렸음 - 꺽인부분을 표현하기에는 아키텍처의 표현력이 너무 부족하다 -&gt; under fit의 문제\n\n\n풀이2: 비선형 활성화 함수의 도입\n- 여기에서 비선형 활성화 함수는 relu\n- 네트워크를 아래와 같이 수정하자.\n(수정전) hat은 생략\n\n#collapse\ngv('''\n\"x\" -&gt; \"x*w,    bias=True\"[label=\"*w\"] ;\n\"x*w,    bias=True\" -&gt; \"y\"[label=\"indentity\"] ''')\n\n\n\n\n(수정후) hat은 생략\n\n#collapse\ngv('''\n\"x\" -&gt; \"x*w,    bias=True\"[label=\"*w\"] ;\n\"x*w,    bias=True\" -&gt; \"y\"[label=\"relu\"] ''')\n\n\n\n\n\n마지막에 \\(f(x)=x\\) 라는 함수대신에 relu 를 취하는 것으로 구조를 약간 변경\n활성화함수(acitivation function)를 indentity에서 relu로 변경\n\n- relu함수란?\n\n_x = np.linspace(-1,1,100)\ntf.nn.relu(_x)\n\n&lt;tf.Tensor: shape=(100,), dtype=float64, numpy=\narray([0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.01010101, 0.03030303, 0.05050505, 0.07070707, 0.09090909,\n       0.11111111, 0.13131313, 0.15151515, 0.17171717, 0.19191919,\n       0.21212121, 0.23232323, 0.25252525, 0.27272727, 0.29292929,\n       0.31313131, 0.33333333, 0.35353535, 0.37373737, 0.39393939,\n       0.41414141, 0.43434343, 0.45454545, 0.47474747, 0.49494949,\n       0.51515152, 0.53535354, 0.55555556, 0.57575758, 0.5959596 ,\n       0.61616162, 0.63636364, 0.65656566, 0.67676768, 0.6969697 ,\n       0.71717172, 0.73737374, 0.75757576, 0.77777778, 0.7979798 ,\n       0.81818182, 0.83838384, 0.85858586, 0.87878788, 0.8989899 ,\n       0.91919192, 0.93939394, 0.95959596, 0.97979798, 1.        ])&gt;\n\n\n\nplt.plot(_x,_x)\nplt.plot(_x,tf.nn.relu(_x))\n\n\n\n\n\n파란색을 주황색으로 바꿔주는 것이 렐루함수임\n\\(f(x)=\\max(0,x)=\\begin{cases} 0 & x\\leq 0 \\\\ x & x&gt;0 \\end{cases}\\)\n\n- 아키텍처: \\(\\hat{y}_i=relu(\\hat{w}_0+\\hat{w}_1x_i)\\), \\(relu(x)=\\max(0,x)\\)\n- 풀이시작\n1단계\n\nnet2 = tf.keras.Sequential() \n\n2단계\n\ntf.random.set_seed(43053)\nl1 = tf.keras.layers.Dense(1, input_shape=(1,)) \na1 = tf.keras.layers.Activation(tf.nn.relu) \n\n\nnet2.add(l1)\n\n\nnet2.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7f888c109820&gt;]\n\n\n\nnet2.add(a1)\n\n\nnet2.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7f888c109820&gt;,\n &lt;keras.layers.core.activation.Activation at 0x7f888c109250&gt;]\n\n\n\nl1.get_weights()\n\n[array([[0.3830086]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nnet2.get_weights()\n\n[array([[0.3830086]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nl1에 있는 weight가 net2에 그대로 들어가게 된다.\n왜 시드고정이 안되는거지??\n\n(네트워크 상황 확인)\n\nu1= l1(x)\n#u1= x@l1.weights[0] + l1.weights[1]\n\n\nv1= a1(u1)\n#v1= tf.nn.relu(u1) \n\n\nplt.plot(x,x)\nplt.plot(x,u1,'--r')\nplt.plot(x,v1,'--b')\n\n\n\n\n3단계\n\nnet2.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\n\n4단계\n\nnet2.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f888c0b6d90&gt;\n\n\n- result\n\n# 다 똑같은 코드.\nyhat = tf.nn.relu(x@l1.weights[0] + l1.weights[1]) \nyhat = net2.predict(x)\nyhat = net2(x)\nyhat = a1(l1(x))\nyhat = net2.layers[1](net2.layers[0](x))\n\n4/4 [==============================] - 0s 438us/step\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,yhat,'--')\n\n\n\n\n- discussion - 이것 역시 수백억번 에폭을 반복해도 이 이상 적합이 힘들다 \\(\\to\\) 모형의 표현력이 떨어진다. - 해결책: 주황색점선이 2개 있다면 어떨까?\n\n\n풀이3: 노드수추가 + 레이어추가\n목표: 2개의 주황색 점선을 만들자.\n1단계\n\nnet3 = tf.keras.Sequential()\n\n2단계\n\n# tf.random.set_seed(43053)\ntf.keras.utils.set_random_seed(43053)\nl1 = tf.keras.layers.Dense(2,input_shape=(1,)) # 출력을 2로 하면 직선이 2개 만들어짐.\na1 = tf.keras.layers.Activation(tf.nn.relu)\n\n\nnet3.add(l1)\nnet3.add(a1) \n\n\nnet3.layers # 2개가 들어가있음!\n\n[&lt;keras.layers.core.dense.Dense at 0x7f8840271d60&gt;,\n &lt;keras.layers.core.activation.Activation at 0x7f888c268c40&gt;]\n\n\n(네트워크 상황 확인)\n\nl1(x).shape\n# l1(x) : (100,1) -&gt; (100,2) \n\nTensorShape([100, 2])\n\n\n\n출력차원이 2라고 했으니까!\n\n\nplt.plot(x,x) # 입력 (파란)\nplt.plot(x,l1(x),'--') # 출력 (주황, 초록)\n\n\n\n\n\nplt.plot(x,x)\nplt.plot(x,a1(l1(x)),'--')\n\n\n\n\n- 이 상태에서는 yhat이 안나온다. 왜? - 차원이 안맞음. a1(l1(x))의 차원은 (N,2)인데 최종적인 yhat의 차원은 (N,1)이어야 함. (선이 하나여야 하잖아..) - 차원이 어찌저찌 맞다고 쳐도 relu를 통과하면 항상 yhat&gt;0 임. 따라서 음수값을 가지는 y는 0으로 밖에 맞출 수 없음.\n- 해결책: a1(l1(x))에 연속으로(Sequential하게!) 또 다른 레이어를 설계! (N,2) -&gt; (N,1) 이 되도록! - yhat= bias + weight1 * a1(l1(x))[0] + weight2 * a1(l1(x))[1]\n- 즉 a1(l1(x)) 를 새로운 입력으로 해석하고 출력을 만들어주는 선형모형을 다시태우면 된다. - 입력차원: 2 - 출력차원: 1\n\nnet3.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7f8840271d60&gt;,\n &lt;keras.layers.core.activation.Activation at 0x7f888c268c40&gt;]\n\n\n\n# tf.random.set_seed(43053) \ntf.keras.utils.set_random_seed(43053)\nl2 = tf.keras.layers.Dense(1, input_shape=(2,)) # 출력차원은 1, 입력차원은 2\n\n\nnet3.add(l2) \n\n\nnet3.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7f8840271d60&gt;,\n &lt;keras.layers.core.activation.Activation at 0x7f888c268c40&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f88401fdaf0&gt;]\n\n\n\nnet3.summary()\n\nModel: \"sequential_32\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_39 (Dense)            (None, 2)                 4         \n                                                                 \n activation_37 (Activation)  (None, 2)                 0         \n                                                                 \n dense_40 (Dense)            (None, 1)                 3         \n                                                                 \n=================================================================\nTotal params: 7\nTrainable params: 7\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nDense layer \\(\\to\\) activation \\(\\to\\) Dense layer\n\n- 추정해야할 파라메터수가 4,0,3으로 나온다.\n- 수식표현: \\(X \\to X@W^{(1)}+b^{(1)} \\to relu(X@W^{(1)}+b^{(1)}) \\to relu(X@W^{(1)}+b^{(1)})@W^{(2)}+b^{(2)}=yhat\\)\n\n\\(X\\): (N,1)\n\\(W^{(1)}\\): (1,2) ==&gt; 파라메터 2개 추정\n\\(b^{(1)}\\): (2,) ==&gt; 파라메터 2개가 추가 // 여기까지 추정할 파라메터는 4개\n\\(W^{(2)}\\): (2,1) ==&gt; 파라메터 2개 추정\n\\(b^{(2)}\\): (1,) ==&gt; 파라메터 1개가 추가 // 따라서 3개\n\n- 참고: 추정할 파라메터수가 많다 = 복잡한 모형이다. - 초거대AI: 추정할 파라메터수가 엄청 많은..\n\nnet3.weights\n\n[&lt;tf.Variable 'dense_39/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[ 0.8359591, -0.8971499]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_39/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 0.8359591],\n        [-0.8971499]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]\n\n\n\nl1.weights\n\n[&lt;tf.Variable 'dense_39/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[ 0.8359591, -0.8971499]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_39/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;]\n\n\n\nl2.weights\n\n[&lt;tf.Variable 'dense_40/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 0.8359591],\n        [-0.8971499]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]\n\n\n- 좀 더 간단한 수식표현: \\(X \\to (u_1 \\to v_1) \\to (u_2 \\to v_2) = yhat\\) - \\(u_1= X@W^{(1)}+b^{(1)}\\) - \\(v_1= relu(u_1)\\) - \\(u_2= v_1@W^{(2)}+b^{(2)}\\) - \\(v_2= indentity(u_2):=yhat\\)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -&gt; \"u1[:,0]\"[label=\"*W1[0,0]\"]\n    \"X\" -&gt; \"u1[:,1]\"[label=\"*W1[0,1]\"]\n    \"u1[:,0]\" -&gt; \"v1[:,0]\"[label=\"relu\"]\n    \"u1[:,1]\" -&gt; \"v1[:,1]\"[label=\"relu\"]\n    label = \"Layer 1\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"v1[:,0]\" -&gt; \"yhat\"[label=\"*W2[0,0]\"]\n    \"v1[:,1]\" -&gt; \"yhat\"[label=\"*W2[1,0]\"]\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\n\n하나는 주황색선, 하나는 초록색선이 만들어진다.\n너무 복잡한데..?\n\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -&gt; \"node1\"\n    \"X\" -&gt; \"node2\"\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -&gt; \"yhat\"\n    \"node2\" -&gt; \"yhat\"\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\n\n좀 더 간단한 형태의 아키텍쳐\n\n3단계\n\nnet3.compile(loss='mse',optimizer=tf.optimizers.SGD(0.1))\n\n4단계\n\nnet3.fit(x,y,epochs=1000,verbose=0, batch_size=N) \n\n&lt;keras.callbacks.History at 0x7f88400fba30&gt;\n\n\n- 결과확인\n\nnet3.weights\n\n[&lt;tf.Variable 'dense_39/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[ 1.6574922, -0.8332077]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_39/bias:0' shape=(2,) dtype=float32, numpy=array([-0.0729818,  0.8324026], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 1.6503255],\n        [-1.1728343]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/bias:0' shape=(1,) dtype=float32, numpy=array([0.95030355], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'.') \nplt.plot(x,net3(x),'--')\n\n\n\n\n\n잘 맞춤!\n\n- 분석\n\nplt.plot(x,y,'.') \nplt.plot(x,l1(x),'--')\n\n\n\n\n\nplt.plot(x,y,'.') \nplt.plot(x,a1(l1(x)),'--')\n\n\n\n\n\nplt.plot(x,y,'.') \nplt.plot(x,l2(a1(l1(x))),'--')\n\n\n\n\n- 마지막 2개의 그림을 분석\n\nl2.weights\n\n[&lt;tf.Variable 'dense_40/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 1.6503255],\n        [-1.1728343]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/bias:0' shape=(1,) dtype=float32, numpy=array([0.95030355], dtype=float32)&gt;]\n\n\n\nfig, (ax1,ax2,ax3) = plt.subplots(1,3) \nfig.set_figwidth(12) \nax1.plot(x,y,'.')\nax1.plot(x,a1(l1(x))[:,0],'--r')\nax1.plot(x,a1(l1(x))[:,1],'--b')\nax2.plot(x,y,'.')\nax2.plot(x,a1(l1(x))[:,0]*1.6328746,'--r')\nax2.plot(x,a1(l1(x))[:,1]*(-1.2001747)+1.0253307,'--b')\nax3.plot(x,y,'.')\nax3.plot(x,a1(l1(x))[:,0]*1.6328746+a1(l1(x))[:,1]*(-1.2001747)+1.0253307,'--')\n\n\n\n\n\n\n풀이3의 실패\n\n# tf.random.set_seed(43054) \ntf.keras.utils.set_random_seed(43052)\n## 1단계\nnet3 = tf.keras.Sequential() \n## 2단계\nnet3.add(tf.keras.layers.Dense(2))\nnet3.add(tf.keras.layers.Activation('relu')) \nnet3.add(tf.keras.layers.Dense(1)) # 출력 1\n## 3단계 \nnet3.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\n## 4단계 \nnet3.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f8821a15c70&gt;\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net3(x),'--')\n\n\n\n\n- 엥? 에폭이 부족한가?\n\nnet3.fit(x,y,epochs=10000,verbose=0,batch_size=N)\nplt.plot(x,y,'.')\nplt.plot(x,net3(x),'--')\n\n\n\n\n\n똑같은데…? 결국 에폭문제가 아니였음.\n\n- 실패분석\n\nl1,a1,l2 = net3.layers\n\n\nl2.weights\n\n[&lt;tf.Variable 'dense_46/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[0.5306579],\n        [1.7407396]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_46/bias:0' shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;]\n\n\n\nfig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) \nfig.set_figwidth(16) \nax1.plot(x,y,'.')\nax1.plot(x,l1(x)[:,0],'--r')\nax1.plot(x,l1(x)[:,1],'--b')\nax2.plot(x,y,'.')\nax2.plot(x,a1(l1(x))[:,0],'--r')\nax2.plot(x,a1(l1(x))[:,1],'--b')\nax3.plot(x,y,'.')\nax3.plot(x,a1(l1(x))[:,0]*0.5306579,'--r')\nax3.plot(x,a1(l1(x))[:,1]*(1.7407396)+(-0.60076195),'--b')\nax4.plot(x,y,'.')\nax4.plot(x,a1(l1(x))[:,0]*0.5306579+a1(l1(x))[:,1]*(1.7407396)+(-0.60076195),'--')\n\n\n\n\n\n보니까 빨간색선이 하는 역할을 없음\n그런데 생각해보니까 이 상황에서는 빨간색선이 할수 있는 일이 별로 없음\n왜? 지금은 나름 파란색선에 의해서 최적화가 된 상태임 \\(\\to\\) 빨간선이 뭔가 하려고하면 최적화된 상태가 깨질 수 있음 (loss 증가)\n즉 이 상황 자체가 나름 최적화된 상태이다. 이러한 현상을 “global minimum을 찾지 못하고 local minimum에 빠졌다” 라고 표현한다.\n\n확인:\n\nnet3.weights\n\n[&lt;tf.Variable 'dense_45/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[-0.3914748,  1.9987504]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_45/bias:0' shape=(2,) dtype=float32, numpy=array([-0.392621,  0.34811 ], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_46/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[0.5306579],\n        [1.7407396]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_46/bias:0' shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;]\n\n\n\nW1= tf.Variable(tnp.array([[-0.03077251,  1.8713338 ]]))\nb1= tf.Variable(tnp.array([-0.04834982,  0.3259186 ]))\nW2= tf.Variable(tnp.array([[0.65121335],[1.8592643 ]]))\nb2= tf.Variable(tnp.array([-0.60076195])) \n\n\nwith tf.GradientTape() as tape: \n    u = tf.constant(x) @ W1 + b1 \n    v = tf.nn.relu(u) \n    yhat = v@W2 + b2 \n    loss = tf.losses.mse(y,yhat) \n\n\ntape.gradient(loss,[W1,b1,W2,b2])\n\n[&lt;tf.Tensor: shape=(1, 2), dtype=float64, numpy=array([[ 0.00000000e+00, -4.77330119e-05]])&gt;,\n &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([0.0000000e+00, 3.1478608e-06])&gt;,\n &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n array([[ 0.00000000e+00],\n        [-4.74910706e-05]])&gt;,\n &lt;tf.Tensor: shape=(1,), dtype=float64, numpy=array([-2.43031263e-05])&gt;]\n\n\n예상대로 계수값이 거의 다 0이다.\n\n\n풀이4: 노드수를 더 추가한다면?\n- 노드수를 더 추가해보면 어떻게 될까? (주황색 점선이 더 여러개 있다면?)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -&gt; \"node1\"\n    \"X\" -&gt; \"node2\"\n    \"X\" -&gt; \"...\"\n    \"X\" -&gt; \"node512\"\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -&gt; \"yhat\"\n    \"node2\" -&gt; \"yhat\"\n    \"...\" -&gt; \"yhat\"\n    \"node512\" -&gt; \"yhat\"\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\n\ntf.random.set_seed(43056)\nnet4= tf.keras.Sequential()\nnet4.add(tf.keras.layers.Dense(512,activation='relu')) # 이렇게 해도됩니다. \nnet4.add(tf.keras.layers.Dense(1))         \nnet4.compile(loss='mse',optimizer=tf.optimizers.SGD(0.1)) \nnet4.fit(x,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7f8821327160&gt;\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net4(x),'--')\n\n\n\n\n\n잘된다..\n한두개의 노드가 역할을 못해도 다른노드들이 잘 보완해주는듯!\n\n- 노드수가 많으면 무조건 좋다? -&gt; 대부분 나쁘지 않음. 그런데 종종 맞추지 말아야할것도 맞춤.. (overfit)\n\nnp.random.seed(43052)\nN=100 \n_x = np.linspace(0,1,N).reshape(N,1) \n_y = np.random.normal(loc=0,scale=0.001,size=(N,1))\nplt.plot(_x,_y)\n\n\n\n\n\ntf.random.set_seed(43052) \nnet4 = tf.keras.Sequential()\nnet4.add(tf.keras.layers.Dense(512,activation='relu'))\nnet4.add(tf.keras.layers.Dense(1))\nnet4.compile(loss='mse',optimizer=tf.optimizers.SGD(0.5))\nnet4.fit(_x,_y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f88210403a0&gt;\n\n\n\nplt.plot(_x,_y)\nplt.plot(_x,net4(_x),'--')\n\n\n\n\n\n맞추지 말아야 할 것까지 맞춘다..\n가장 좋은 fit은 직선으로 맞추는것.\n이 예제는 추후 다시 공부할 예정\n\n\n\n\nLogistic regression\n\nmotive\n- 현실에서 이런 경우가 많음 - \\(x\\)가 커질수록 (혹은 작아질수록) 성공확률이 올라간다.\nex) 전자제품에 열을 많이 가할수록 불량률일 증가한다.\nex) 성적이 좋을수록 합격률이 증가한다.\n- 이러한 모형은 아래와 같이 설계할 수 있음 &lt;– 외우세요!! - \\(y_i \\sim Ber(\\pi_i)\\), where \\(\\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\n\\(\\hat{y}_i =\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\\(loss=-\\frac{1}{n}\\sum_{i=1}^{n}\\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\n\n- 위와 같은 손실함수를 BCEloss라고 부른다. (BCE는 Binary Cross Entropy의 약자)\n\n\n예제\n\nN = 2000\n\n\nx = tnp.linspace(-1,1,N).reshape(N,1)\nw0 = -1 \nw1 = 5 \nu = w0 + x*w1 \n#v = tf.constant(np.exp(u)/(1+np.exp(u))) # v=πi \nv = tf.nn.sigmoid(u) \ny = tf.constant(np.random.binomial(1,v),dtype=tf.float64) \n\n\nplt.plot(x,y,'.',alpha=0.02)\nplt.plot(x,v,'--r')\n\n\n\n\n- 이 아키텍처(yhat을 얻어내는 과정)를 다어어그램으로 나타내면 아래와 같다.\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x\" -&gt; \"x*w, bias=True\"[label=\"*w\"]\n    \"x*w, bias=True\" -&gt; \"yhat\"[label=\"sigmoid\"]\n    label = \"Layer 1\"\n}\n''')\n\n\n\n\n- 또는 간단하게 아래와 같이 쓸 수 있다.\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    x\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    x -&gt; \"node1=yhat\"\n    label = \"Layer 1: sigmoid\"\n}\n''')\n\n\n\n\n- 케라스를 이용하여 적합을 해보면\n\n\\(loss=-\\frac{1}{n}\\sum_{i=1}^{n}\\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nbceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat))\nnet.compile(loss=bceloss_fn, optimizer=tf.optimizers.SGD(0.1))\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7f8820868070&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_55/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[4.4220047]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_55/bias:0' shape=(1,) dtype=float32, numpy=array([-0.84613276], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'.',alpha=0.1) # 관측데이터\nplt.plot(x,v,'--r') # 추정하고 싶은 확률\nplt.plot(x,net(x),'--b')  # 네트워크에 의한 추정 결과.\n\n\n\n\n\n거의 비슷하게 잘 추정되었다.\n\n\n\nMSE loss?\n- mse loss를 쓰면 왜 안되는지?\nMSE loss를 써서 다시 돌려보자.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nmseloss_fn = lambda y,yhat: tf.reduce_mean((y-yhat)**2)\nnet.compile(loss=mseloss_fn, optimizer=tf.optimizers.SGD(0.1))\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7f88201c2580&gt;\n\n\n\nplt.plot(x,y,'.',alpha=0.1)\nplt.plot(x,v,'--r')\nplt.plot(x,net(x),'--b')\n\n\n\n\n\n일단 BCE loss와 비교해보니까 동일 초기값, 동일 epochs에서 적합이 별로임\n\n\n\nMSE loss vs BCE loss\n- MSEloss, BCEloss의 시각화\n\nw0, w1 = np.meshgrid(np.arange(-10,3,0.2), np.arange(-1,10,0.2), indexing='ij')\nw0, w1 = w0.reshape(-1), w1.reshape(-1)\n\ndef loss_fn1(w0,w1):\n    u = w0+w1*x \n    yhat = np.exp(u)/(np.exp(u)+1)\n    return mseloss_fn(y,yhat) \n\ndef loss_fn2(w0,w1):\n    u = w0+w1*x \n    yhat = np.exp(u)/(np.exp(u)+1)\n    return bceloss_fn(y,yhat) \n\nloss1 = list(map(loss_fn1,w0,w1))\nloss2 = list(map(loss_fn2,w0,w1))\n\n\nfig = plt.figure()\nfig.set_figwidth(9)\nfig.set_figheight(9)\nax1=fig.add_subplot(1,2,1,projection='3d')\nax2=fig.add_subplot(1,2,2,projection='3d')\nax1.elev=15\nax2.elev=15\nax1.azim=75\nax2.azim=75\nax1.scatter(w0,w1,loss1,s=0.1)\nax2.scatter(w0,w1,loss2,s=0.1) \n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f8803ecb910&gt;\n\n\n\n\n\n\n왼쪽곡면(MSEloss)보다 오른쪽곡면(BCEloss)이 좀더 예쁘게 생김 -&gt; 오른쪽 곡면에서 더 학습이 잘될것 같음\n\n\n\n학습과정 시각화예시1\n- 파라메터학습과정 시각화 // 옵티마이저: SGD, 초기값: (w0,w1) = (-3.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n&lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])&gt;\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_mse.compile(optimizer=tf.optimizers.SGD(0.1),loss=mseloss_fn) \nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 75ms/step - loss: 0.2461\n\n\n&lt;keras.callbacks.History at 0x7f8803dd3940&gt;\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_bce.compile(optimizer=tf.optimizers.SGD(0.1),loss=bceloss_fn) \nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 85ms/step - loss: 1.1333\n\n\n&lt;keras.callbacks.History at 0x7f882080f0d0&gt;\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[0.40601483],\n         [0.3624403 ]], dtype=float32)],\n [array([[ 1.363407  ],\n         [-0.33031225]], dtype=float32)])\n\n\n\n둘이 weight이 다르니까 강제로 맞춰놓자.\n\n\nnet_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-3.],\n         [-1.]], dtype=float32)],\n [array([[-3.],\n         [-1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29): \n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) \n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) \n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"SGD, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n왼쪽 MSE, 오른쪽 BCE\nMSE의 경우 너무 천천히 수렴을 한다.\n동일 학습조건에서 오른쪽이 학습이 더 잘된다.\n\n\n\n학습과정 시각화예시2\n이번에는 똑같은 초깃값에 다른 옵티마이저(Adam)를 써보자.\n- 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-3.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n&lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])&gt;\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) \nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 92ms/step - loss: 0.2902\n\n\n&lt;keras.callbacks.History at 0x7f88211b1430&gt;\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) \nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 98ms/step - loss: 0.8690\n\n\n&lt;keras.callbacks.History at 0x7f88402718b0&gt;\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[1.1395919],\n         [1.3250527]], dtype=float32)],\n [array([[-0.77553874],\n         [-0.59953135]], dtype=float32)])\n\n\n\nnet_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-3.],\n         [-1.]], dtype=float32)],\n [array([[-3.],\n         [-1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29): \n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) \n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) \n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"Adam, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nAdam을 쓴게 확실히 학습을 효율적으로 빨리함!\n\n동일한 loss function에 동일한 초깃값이라 해도 Adam을 쓰면 학습이 더 빠르다.\n그럼 MSE+Adam 조합으로 하면 적합이 잘 되겠구나? 라고 생각할 수 있는데 그건 아님.\n\n\n\n학습과정 시각화예시3\n이번에는 Adam을 쓸건데 다른 초깃값에서 시작해보자.\n- 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-10.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n&lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])&gt;\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) \nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 93ms/step - loss: 0.1645\n\n\n&lt;keras.callbacks.History at 0x7f87f3edb880&gt;\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) \nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 101ms/step - loss: 0.5978\n\n\n&lt;keras.callbacks.History at 0x7f87f1ce8400&gt;\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-0.9448344],\n         [ 1.4777311]], dtype=float32)],\n [array([[0.47022673],\n         [1.2034082 ]], dtype=float32)])\n\n\n\nnet_mse.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-10.],\n         [ -1.]], dtype=float32)],\n [array([[-10.],\n         [ -1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29): \n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) \n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) \n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"Adam, Winit=(-10,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n아무리 아담이라고 해도 이건 힘듬\n\n학습을 잘 되게 하기 위해서 옵티마이저를 개선하는 것은 되게 좋은 방법이지만 그것보다 근본적으로 loss function을 좀 더 예쁘게 만드려는 노력이 훨씬 더 좋은 생각..\n\n\ndiscussion / Summary\n- discussion - mse_loss는 경우에 따라서 엄청 수렴속도가 느릴수도 있음. - 근본적인 문제점: mse_loss일 경우 loss function의 곡면이 예쁘지 않음. (전문용어로 convex가 아니라고 말함) - 좋은 옵티마지어를 이용하면 mse_loss일 경우에도 수렴속도를 올릴 수 있음 (학습과정 시각화예시2). 그렇지만 이는 근본적인 해결책은 아님. (학습과정 시각화예시3)\n- 요약: 왜 logistic regression에서 mse loss를 쓰면 안되는가? - mse loss를 사용하면 손실함수가 convex하지 않으니까! - 그리고 bce loss를 사용하면 손실함수가 convex하니까!"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_01_(9주차)_5월2일(1).html",
    "href": "posts/3_STBDA2022/2022_05_01_(9주차)_5월2일(1).html",
    "title": "[STBDA] 9wk. Likelihood function",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-wrJ6CivKKBuJUN7ukm5OHr\n\n\n\nimports\n\nimport numpy as np\nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt \n\n\n\n우도함수와 최대우도추정량\n(예제)\n\\(X_i \\overset{iid}{\\sim} Ber(p)\\)에서 얻은 샘플이 아래와 같다고 하자.\n\nx=[0,1,0,1] \nx\n\n[0, 1, 0, 1]\n\n\n\\(p\\)는 얼마라고 볼 수 있는가? –&gt; 0.5\n왜?? \\(p\\)가 0.5라고 주장할 수 있는 이론적 근거, 혹은 논리체계가 무엇인가?\n- suppose: \\(p=0.1\\) 이라고 하자.\n그렇다면 \\((x_1,x_2,x_3,x_4)=(0,1,0,1)\\)와 같은 샘플이 얻어질 확률이 아래와 같다.\n\n0.9 * 0.1 * 0.9 * 0.1\n\n0.008100000000000001\n\n\n- suppose: \\(p=0.2\\) 이라고 하자.\n그렇다면 \\((x_1,x_2,x_3,x_4)=(0,1,0,1)\\)와 같은 샘플이 얻어질 확률이 아래와 같다.\n\n0.8 * 0.2 * 0.8 * 0.2\n\n0.025600000000000008\n\n\n- 질문1: \\(p=0.1\\)인것 같냐? 아니면 \\(p=0.2\\)인것 같냐? -&gt; \\(p=0.2\\)\n\n왜?? \\(p=0.2\\)일 확률이 더 크다! \\(\\to\\) \\(p=0.2\\)일 가능도가 더 크다!\n\n\n(여기서 잠깐 중요한것) 확률이라는 말을 함부로 쓸 수 없다.\n- 0.0256은 “\\(p=0.2\\)일 경우 샘플 (0,1,0,1)이 얻어질 확률”이지 “\\(p=0.2\\)일 확률”은 아니다.\n“\\(p=0.2\\)인 확률” 이라는 개념이 성립하려면 아래코드에서 sum([(1-p)*p*(1-p)*p for p in _plist])이 1보다는 작아야 한다. (그런데 1보다 크다)\n\n_plist = np.linspace(0.499,0.501,1000) \nsum([(1-p)*p*(1-p)*p for p in _plist])\n\n62.49983299986714\n\n\n\n확률은 다 더하면 \\(1\\)이어야 하는데 가뿐히 넘어버림..\n\n- 확률이라는 말을 쓸 수 없지만 확률의 느낌은 있음 -&gt; 가능도라는 말을 쓰자.\n\n0.0256 \\(=\\) \\(p\\)가 0.2일 경우 샘플 (0,1,0,1)이 얻어질 확률 \\(=\\) \\(p\\)가 0.2일 가능도\n\n\n- 다시 질문1로 돌아가자!\n\n질문1: \\(p=0.1\\)인 것 같냐? 아니면 \\(p=0.2\\)인 것 같냐? -&gt; 답 \\(p=0.2\\) -&gt; 왜? \\(p=0.2\\)인 가능도가 더 크니까!\n질문2: \\(p=0.2\\)인 것 같냐? 아니면 \\(p=0.3\\)인 것 같냐? -&gt; 답 \\(p=0.3\\) -&gt; 왜? \\(p=0.3\\)인 가능도가 더 크니까!\n질문3: …\n\n- 궁극의 질문: \\(p\\)가 뭐일 것 같아?\n\n\\(p\\)가 입력으로 들어가면 가능도가 계산되는 함수를 만들자.\n그 함수를 최대화하는 \\(p\\)를 찾자.\n그 \\(p\\)가 궁극의 질문에 대한 대답이 된다.\n\n- 잠깐 용어정리\n\n가능도함수 \\(=\\) 우도함수 \\(=\\) likelihood function \\(:=\\) \\(L(p)\\)\n\\(p\\)의 maximum likelihood estimator \\(=\\) p의 MLE \\(:=\\) \\(\\hat{p}^{mle}\\) \\(=\\) \\(\\text{argmax}_p L(p)\\) \\(=\\) \\(\\hat{p}\\)\n\n(예제의 풀이)\n- 이 예제의 경우 가능도함수를 정의하자.\n\n\\(L(p)\\): \\(p\\)의 가능도함수 = \\(p\\)가 모수일때 샘플 (0,1,0,1)이 얻어질 확률 = \\(p\\)가 모수일때 \\(x_1\\)이 0일 확률 \\(\\times \\dots \\times\\) \\(p\\)가 모수일때 \\(x_4\\)가 1일 확률\n\\(L(p)=\\prod_{i=1}^{4} f(x_i;p)= \\prod_{i=1}^{4}p^{x_i}(1-p)^{1-x_i}\\)\n\n\nnote: 참고로 이 과정을 일반화 하면 \\(X_1,\\dots,X_n \\overset{iid}{\\sim} Ber(p)\\) 일때 \\(p\\)의 likelihood function은 \\(\\prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}\\) 라고 볼 수 있다.\n\n\nnote: 더 일반화: \\(x_1,\\dots,x_n\\)이 pdf가 \\(f(x)\\)인 분포에서 뽑힌 서로 독립인 샘플일때 likelihood function은 \\(\\prod_{i=1}^{n}f(x_i)\\)라고 볼 수 있다.\n\n- 이 예제의 경우 \\(p\\)의 최대우도추정량을 구하면\n\\[\\hat{p}^{mle} = \\text{argmax}_p L(p) = \\text{argmax}_p  \\big\\{ p^2(1-p)^2 \\big\\}= \\frac{1}{2}\\]\n\n\n중간고사 1번\n(1) \\(N(\\mu,\\sigma)\\)에서 얻은 샘플이 아래와 같다고 할때 \\(\\mu,\\sigma\\)의 MLE를 구하여라.\n&lt;tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])&gt;\n(2) \\(Ber(p)\\)에서 얻은 샘플이 아래와 같다고 할 때 \\(p\\)의 MLE를 구하여라.\n&lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])&gt;\n(3) \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\), \\(\\epsilon_i \\overset{iid}{\\sim} N(0,1)\\) 일때 \\((\\beta_0,\\beta_1)\\)의 MLE를 구하여라. (회귀모형)\n(풀이) 가능도함수\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]\n를 최대화하는 \\(\\beta_0,\\beta_1\\)을 구하면된다. 그런데 이것은 아래를 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하는 것과 같다.\n\\[-\\log L(\\beta_0,\\beta_1) = \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\]\n위의 식은 SSE와 같다. 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하면 된다.\n중간고사 1-(3)의 다른 풀이\nstep1: 생성\n\nx= tf.constant(np.arange(1,10001)/10000)\ny= tnp.random.randn(10000) + (0.5 + 2*x) \n\nstep2: minimize MSEloss (원래는 maximize log-likelihood)\n\nmaximize likelihood였던 문제를 minimize MSEloss로 바꾸어도 되는근거? 주어진 함수(=가능도함수)를 최대화하는 \\(\\beta_0,\\beta_1\\)은 MSE를 최소화하는 \\(\\beta_0,\\beta_1\\)과 동일하므로\n\n\nbeta0= tf.Variable(1.0)\nbeta1= tf.Variable(1.0) \nfor i in range(2000):\n    with tf.GradientTape() as tape: \n        #minus_log_likelihood = tf.reduce_sum((y-beta0-beta1*x)**2)\n        loss =  tf.reduce_sum((y-beta0-beta1*x)**2)\n    slope1, slope2 = tape.gradient(loss,[beta0,beta1]) \n    beta0.assign_sub(slope1* 0.1/10000) # N=10000 \n    beta1.assign_sub(slope2* 0.1/10000) \n\n\nbeta0,beta1\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.52694947&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.956429&gt;)\n\n\n\n\\(\\beta_0 = 0.5, \\beta_1 = 2\\) 가 true값임.\n\n- 문제를 풀면서 생각해보니 손실함수는 -로그가능도함수로 선택하면 될 것 같다?\n\n손실함수를 선택하는 기준이 -로그가능도함수만 존재하는 것은 아니나 대부분 그러하긴함\n\n(4) 출제하지 못한 중간고사 문제\n아래의 모형을 생각하자. - \\(Y_i \\overset{iid}{\\sim} Ber(\\pi_i)\\) - \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}=\\frac{\\exp(-1+5x_i)}{1+\\exp(-1+5x_i)}\\)\n아래는 위의 모형에서 얻은 샘플이다.\n\nx = tnp.linspace(-1,1,2000) # sample\npi = tnp.exp(-1+5*x) / (1+tnp.exp(-1+5*x)) # sample이 나올 확률\ny = np.random.binomial(1,pi)\ny = tf.constant(y)\n\n함수 \\(L(w_0,w_1)\\)을 최대화하는 \\((w_0,w_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\((w_0,w_1)\\)의 초기값은 모두 0.1로 설정할 것)\n\\[L(w_0,w_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(x_i)={\\pi_i}^{y_i}(1-\\pi_i)^{1-y_i},\\quad \\pi_i=\\text{sigmoid}(w_0+w_1x_i)\\]\n(풀이1)\n\nw0hat = tf.Variable(1.0) \nw1hat = tf.Variable(1.0) \n\n\nfor i in range(1000): \n    with tf.GradientTape() as tape: \n        pihat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n        pdf = pihat**y * (1-pihat)**(1-y) \n        logL = tf.reduce_mean(tnp.log(pdf)) \n    slope1,slope2 = tape.gradient(logL,[w0hat,w1hat])\n    w0hat.assign_add(slope1*0.1) \n    w1hat.assign_add(slope2*0.1) \n\n\nw0hat,w1hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.88308984&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.144893&gt;)\n\n\n\ntrue값인 \\(-1\\)과 \\(5\\)에 근접한 값이 나와 적합이 잘 되었다.\n\n(해석) - 로지스틱에서 가능도함수와 BCEloss의 관계\n\\(L(w_0,w_1)\\)를 최대화하는 \\(w_0,w_1\\)은 아래를 최소화하는 \\(w_0,w_1\\)와 같다.\n\\[-\\log L(w_0,w_1) = - \\sum_{i=1}^{n}\\big(y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i)\\big)\\]\n이것은 최적의 \\(w_0,w_1\\)을 \\(\\hat{w}_0,\\hat{w}_1\\)이라고 하면 \\(\\hat{\\pi}_i=\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\hat{y}_i\\)이 되고 따라서 위의 식은 \\(n\\times\\)BCEloss의 형태임을 쉽게 알 수 있다.\n결국 로지스틱 모형에서 \\((w_0,w_1)\\)의 MLE를 구하기 위해서는 BCEloss를 최소화하는 \\((w_0,w_1)\\)을 구하면 된다!\n결국 BCE Loss는 -로그가능도함수를 \\(n\\)으로 나눈 것과 같다.\nlikelihood라는 개념이없다면 회귀모형일 경우에는 MSELoss를 쓰고, 로지스틱일 경우에는 BCELoss를 쓴다고 외워야하는 문제였지만, Likelihood라는 개념이 있다면 로지스틱이든 회귀모형이든 손실함수는 -loglikelihood로 잡는군! 이렇게 넘어갈 수 있다.\n(풀이2)\n\nw0hat = tf.Variable(1.0) \nw1hat = tf.Variable(1.0) \n\n\nfor i in range(1000): \n    with tf.GradientTape() as tape: \n        yhat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n        loss = tf.losses.binary_crossentropy(y,yhat)\n    slope1,slope2 = tape.gradient(loss,[w0hat,w1hat])\n    w0hat.assign_sub(slope1*0.1) \n    w1hat.assign_sub(slope2*0.1) \n\n\nw0hat,w1hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.8830899&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.144893&gt;)\n\n\n\n\n손실함수의 설계 (선택)\n- 회귀분석이든 로지스틱이든 손실함수는 minus_log_likelihood 로 선택한다. - 그런데 (오차항이 정규분포인) 회귀분석 일때는 minus_log_likelihood 가 MSEloss가 되고 - 로지스틱일때는 minus_log_likelihood 가 BCEloss가 된다\n- minus_log_likelihood가 손실함수를 선택하는 유일한 기준은 아니다. &lt;— 참고만하세요, 이 수업에서는 안중요합니다. - 오차항이 대칭이고 서로독립이며 등분산 가정을 만족하는 어떠한 분포에서의 회귀모형이 있다고 하자. 이 회귀모형에서 \\(\\hat{\\beta}\\)은 여전히 MSEloss를 최소화하는 \\(\\beta\\)를 구함으로써 얻을 수 있다. - 이 경우 MSEloss를 쓰는 이론적근거? \\(\\hat{\\beta}\\)이 BLUE가 되기 때문임 (가우스-마코프정리)"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-wvV9xuYHvx0Gn7KDGNJbwj"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#강의영상",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#강의영상",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-wvV9xuYHvx0Gn7KDGNJbwj"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#import",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#import",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "import",
    "text": "import\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntf.config.experimental.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')\n\n\nbasis가 orthogonal하냐?\noverfitting 이슈는 변수가 많다고해서 무조건 발생하는 것은 아님! 변수가 많이 있어도 orthogonal하게 잘 넣으면 심지어 무한대의 basis를 갖고있어도 overfitting이슈가 발생하지 않는다. 이렇게 맞추는 것을 semi-parametric modeling 이라고 한다!\n\nex. 직선의 basis: 절편과 기울기"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#중간고사-관련-잡담",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#중간고사-관련-잡담",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "중간고사 관련 잡담",
    "text": "중간고사 관련 잡담\n\n중간고사 3번문제\n- 특이한모형: 오버핏이 일어날 수 없는 모형이다. - 유의미한 coef: 상수항(bias), \\(\\cos(t)\\)의 계수, \\(\\cos(2t)\\)의 계수, \\(\\cos(5t)\\)의 계수. - 유의미하지 않은 coef: \\(\\cos(3t)\\)의 계수, \\(\\cos(4t)\\)의 계수 - 유의미하지 않은 계수는 \\(n%\\)이 커질수록 0으로 추정된다 = \\(\\cos(3t)\\)와 \\(\\cos(5t)\\)는 사용자가 임의로 제외하지 않아도 결국 모형에서 알아서 제거된다 = overfit이 일어나지 않는다. 모형이 알아서 유의미한 변수만 뽑아서 fit하는 느낌\n- 3번문제는 overfit이 일어나지 않는다. 이러한 신기한 일이 일어나는 이유는 모든 설명변수가 직교하기 때문임. - 이런 모형의 장점: overfit이 일어날 위험이 없으므로 train/test로 나누어 학습할 이유가 없다. (샘플만 버리는 꼴, test에 빼둔 observation까지 모아서 학습해 \\(\\beta\\)를 좀 더 정확히 추론하는게 차라리 더 이득) - 이러한 모형에서 할일: 추정된 계수들이 0인지 아닌지만 test하면 된다. (이것을 유의성검정이라고 한다)\n- 직교기저의 예시 - 빨강과 파랑을 255,255만큼 섞으면 보라색이 된다. - 빨강과 파랑과 노랑을 각각 255,255,255만큼 섞으면 검은색이 된다. - 임의의 어떠한 색도 빨강,파랑,노랑의 조합으로 표현가능하다. 즉 \\(\\text{color}= \\text{red}*\\beta_1 + \\text{blue}*\\beta_2 + \\text{yellow}*\\beta_3\\) 이다. - (빨,파,노)는 색을 표현하는 basis이다. (적절한 \\(\\beta_1,\\beta_2,\\beta_3\\)을 구하기만 하면 임의의 색도 표현가능) - (빨,보,노)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요할때 보라색-빨간색을 하면되니까) - (빨,보,검)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요하면 보라색-빨간색을 하면되고, 노란색이 필요하면 검정색-보라색을 하면 되니까) - (빨,파,노)는 직교기저이다.\n- 3번에서 알아둘 것: (1) 직교기저의 개념 (추후 재설명) (2) 임의의 색을 표현하려면 3개의 basis가 필요함\n\n\n중간고사 1-(3)번 문제\n- 그림을 그려보자.\n\n_x= tf.constant(np.arange(1,10001)/10000)\n_y= tnp.random.randn(10000) + (0.5 + 2*_x)\nplt.plot(_x,_y,'.',alpha=0.1)\n\n\n\n\n- 저것 꼭 10000개 다 모아서 loss계산해야할까?\n\nplt.plot(_x,_y,'.',alpha=0.1)\nplt.plot(_x[::10],_y[::10],'.') # 10씩 jump해서 점이 찍힘.\n\n\n\n\n- 대충 이정도만 모아서 해도 비슷하지 않을까? \\(\\to\\) 해보자!\n\n주황색만가지고 기울기, 절편 추론을 해보자."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#경사하강법과-확률적경사하강법",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#경사하강법과-확률적경사하강법",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "경사하강법과 확률적경사하강법",
    "text": "경사하강법과 확률적경사하강법\n원래 확률적경사하강법이 딥러닝을 하려고 만든것이 아니다.(만들어진 의도와 사용이 다름) 그런데 거기에 맞게 진화를 한 것. 그래서 되게 헷갈린다…\n\nver1: 모든 샘플을 사용하여 slope계산 (gradient descent)\n- 단순회귀분석에서 샘플 10개 관측: \\((x_1,y_1),\\dots,(x_{10},y_{10})\\).\n(epoch1) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\nfor문 이 3번 돌아감.\n\n\nver2: 하나의 샘플만 사용하여 slope계산\n(epoch1) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\nfor문이 30번 돌아감.\n\n\nver3: \\(m(\\leq n)\\)개의 샘플만 사용하여 slope계산 (mini-batch)\n\\(m=3\\)이라고 하자.\n(epoch1) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n1,2,3번 observation들만 뽑아서 loss를 계산해서 그것만가지고 update \\(\\to\\) 4,5,6먼만 가지고 loss계산해서 update \\(\\to\\) 7,8,9를가지고 loss구하고 업데이트 \\(\\to\\) 남은 하나가지고 loss구하고 업데이트… // 한 에폭 끝!\n(epoch2) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\n\n용어의 정리\n\n옛날 (좀 더 엄밀)\n- ver1: gradient descent, batch gradient descent\n- ver2: stochastic gradient descent\n- ver3: mini-batch gradient descent, mini-batch stochastic gradient descent\n\n\n요즘\n- ver1: gradient descent\n- ver2: stochastic gradient descent with batch size = 1\n- ver3: stochastic gradient descent - https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고.\n\n지금은 mini-batch가 포함된 방법을 stochastic gradient descent라고 부른다. 왜냐하면 1,2는 사장된 방법. 버전 3만 쓴다.(유명한 사람들이 학회에서 그렇게 부르기 시작했음.)\nnote: 이렇게 많이 쓰는 이유? ver1,2는 사실상 없는 방법이므로\n\n\n\nver1,2,3 이외에 좀 더 지저분한 것들이 있다.\n- ver2,3에서 샘플을 셔플할 수도 있다.\n- ver3에서 일부 샘플이 학습에 참여 안하는 버전도 있다.\n- 개인적 생각: 크게3개정도만 알면 괜찮고 나머지는 그렇게 유의미하지 않아보인다.\n\n\nDiscussion\n- 핵심개념\n\n메모리사용량1: ver1 &gt; ver3 &gt; ver2\n계산속도: ver1 &gt; ver3 &gt; ver2\nlocal-min에 갇힘: ver1 &gt; ver3 &gt; ver2\n\n로컬미니멈에 갇힌가는 건 로컬미니멈을 잘 찾는다라고 생각 (정신이 제대로 박힌애)\n대충대충 학습하면 로컬미니멈에서 딱 멈춰야하는데 대충대충 계산해서 기울기가 딱 \\(0\\)이 안나오는 것. (운 좋게 탈출하는 경우도 있음.)\n- 본질: GPU 메모리가 한정되어 있어서 ver1을 쓰지는 못한다. GPU 메모리를 가장 적게쓰는것은 ver2인데 이것은 너무 불안정하다.\n- 틀리진 않지만 어색한 블로그 정리 내용들\n\n경사하강법은 종종 국소최소점에 갇히는 문제가 있다. 이를 해결하기 위해서 등장한 방법이 확률적 경사하강법이다. –&gt; 영 틀린말은 아니지만 그걸 의도하고 만든건 아님 (이건 side effect)\n경사하강법은 계산시간이 오래걸린다. 계산을 빠르게 하기 위해서 등장한 방법이 확률적 경사하강법이다. –&gt; 1회 업데이트는 빠르게 계산함. 하지만 그것이 최적의 \\(\\beta\\)를 빠르게 얻을 수 있다는 의미는 아님\n\n원래 경사하강법은 local minimum에 빠지기 쉬운 알고리즘! 그런데 그나마 둘 중에 비교를 하자면 확률적으로 하면 로컬 미니멈에 빠졌다가 어쩌다 운좋아서 튀어 나가는 경우가 있다.\n동일한 컴퓨터 자원으로 수렴을 더 빨리 시킬 수 있냐? 그건 아님 (그건 모름).\n그럼 왜 쓰냐???\n메모리 사용량만 보면 됩니다!"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#fashion_mnist-모듈",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#fashion_mnist-모듈",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "fashion_mnist 모듈",
    "text": "fashion_mnist 모듈\n\ntf.keras.datasets.fashion_mnist.load_data()\n- tf.keras.datasets.fashion_mnist.load_data 의 리턴값 조사\n\ntf.keras.datasets.fashion_mnist.load_data??\n\n\nSignature: tf.keras.datasets.fashion_mnist.load_data()\nSource:   \n@keras_export(\"keras.datasets.fashion_mnist.load_data\")\ndef load_data():\n    \"\"\"Loads the Fashion-MNIST dataset.\n    This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories,\n    along with a test set of 10,000 images. This dataset can be used as\n    a drop-in replacement for MNIST.\n    The classes are:\n    | Label | Description |\n    |:-----:|-------------|\n    |   0   | T-shirt/top |\n    |   1   | Trouser     |\n    |   2   | Pullover    |\n    |   3   | Dress       |\n    |   4   | Coat        |\n    |   5   | Sandal      |\n    |   6   | Shirt       |\n    |   7   | Sneaker     |\n    |   8   | Bag         |\n    |   9   | Ankle boot  |\n    Returns:\n      Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n    **x_train**: uint8 NumPy array of grayscale image data with shapes\n      `(60000, 28, 28)`, containing the training data.\n    **y_train**: uint8 NumPy array of labels (integers in range 0-9)\n      with shape `(60000,)` for the training data.\n    **x_test**: uint8 NumPy array of grayscale image data with shapes\n      (10000, 28, 28), containing the test data.\n    **y_test**: uint8 NumPy array of labels (integers in range 0-9)\n      with shape `(10000,)` for the test data.\n    Example:\n    ```python\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n    assert x_train.shape == (60000, 28, 28)\n    assert x_test.shape == (10000, 28, 28)\n    assert y_train.shape == (60000,)\n    assert y_test.shape == (10000,)\n    ```\n    License:\n      The copyright for Fashion-MNIST is held by Zalando SE.\n      Fashion-MNIST is licensed under the [MIT license](\n      https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE).\n    \"\"\"\n    dirname = os.path.join(\"datasets\", \"fashion-mnist\")\n    base = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n    files = [\n        \"train-labels-idx1-ubyte.gz\",\n        \"train-images-idx3-ubyte.gz\",\n        \"t10k-labels-idx1-ubyte.gz\",\n        \"t10k-images-idx3-ubyte.gz\",\n    ]\n    paths = []\n    for fname in files:\n        paths.append(get_file(fname, origin=base + fname, cache_subdir=dirname))\n    with gzip.open(paths[0], \"rb\") as lbpath:\n        y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n    with gzip.open(paths[1], \"rb\") as imgpath:\n        x_train = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(\n            len(y_train), 28, 28\n        )\n    with gzip.open(paths[2], \"rb\") as lbpath:\n        y_test = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n    with gzip.open(paths[3], \"rb\") as imgpath:\n        x_test = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(\n            len(y_test), 28, 28\n        )\n    return (x_train, y_train), (x_test, y_test)\nFile:      ~/anaconda3/envs/torch/lib/python3.8/site-packages/keras/datasets/fashion_mnist.py\nType:      function\n\n\n\n\n\n데이터생성 및 탐색\n- tf.keras.datasets.fashion_mnist.load_data()를 이용한 데이터 생성\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 2s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\n\n\n- 차원확인\n\nx_train.shape, y_train.shape, x_test.shape,y_test.shape\n\n((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))\n\n\n\n60000은 obs숫자인듯\n(28,28)은 28픽셀,28픽셀을 의미하는듯\ntrain/test는 6:1로 나눈것 같음\n\n- 첫번째 obs\n\nplt.imshow(x_train[0])\n\n&lt;matplotlib.image.AxesImage at 0x7febf458eeb0&gt;\n\n\n\n\n\n\ny_train[0]\n\n9\n\n\n\n첫번쨰 obs에 대응하는 라벨\n\n- 첫번째 obs와 동일한 라벨을 가지는 그림을 찾아보자.\n\nnp.where(y_train==9)\n\n(array([    0,    11,    15, ..., 59932, 59970, 59978]),)\n\n\n\ny_train[11]\n\n9\n\n\n\nplt.imshow(x_train[11])\n\n&lt;matplotlib.image.AxesImage at 0x7f65219f9e80&gt;\n\n\n\n\n\n\n\n데이터구조\n- \\({\\bf X}\\): (n,28,28)\n- \\({\\bf y}\\): (n,) , \\(y=0,1,2,3,\\dots,9\\)"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#예제1",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#예제1",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "예제1",
    "text": "예제1\n\n데이터 정리\n- y=0,1에 대응하는 이미지만 정리하자. (우리가 배운건 로지스틱이니까)\n\ny= y_train[(y_train==0) | (y_train==1)].reshape(-1,1)\nX= x_train[(y_train==0) | (y_train==1)].reshape(-1,784)\nyy= y_test[(y_test==0) | (y_test==1)].reshape(-1,1)\nXX= x_test[(y_test==0) | (y_test==1)].reshape(-1,784)\n\n\nX.shape, y.shape, XX.shape, yy.shape\n\n((12000, 784), (12000, 1), (2000, 784), (2000, 1))\n\n\n\n\n풀이1: 은닉층을 포함한 신경망 // epochs=100\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -&gt; \"node1\"\n    \"x2\" -&gt; \"node1\"\n    \"..\" -&gt; \"node1\"\n\n    \"x784\" -&gt; \"node1\"\n    \"x1\" -&gt; \"node2\"\n    \"x2\" -&gt; \"node2\"\n    \"..\" -&gt; \"node2\"\n    \"x784\" -&gt; \"node2\"\n\n    \"x1\" -&gt; \"...\"\n    \"x2\" -&gt; \"...\"\n    \"..\" -&gt; \"...\"\n    \"x784\" -&gt; \"...\"\n\n    \"x1\" -&gt; \"node30\"\n    \"x2\" -&gt; \"node30\"\n    \"..\" -&gt; \"node30\"\n    \"x784\" -&gt; \"node30\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -&gt; \"y\"\n    \"node2\" -&gt; \"y\"\n    \"...\" -&gt; \"y\"\n    \"node30\" -&gt; \"y\"\n    label = \"Layer 2: sigmoid\"\n}\n''')\n\n\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='sgd',loss=tf.losses.binary_crossentropy)\nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 122ms/step - loss: 220.9145\nEpoch 2/100\n1/1 [==============================] - 0s 9ms/step - loss: 6800.3174\nEpoch 3/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7045\nEpoch 4/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7012\nEpoch 5/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7004\nEpoch 6/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6997\nEpoch 7/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6991\nEpoch 8/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6985\nEpoch 9/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6979\nEpoch 10/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6976\nEpoch 11/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6973\nEpoch 12/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6970\nEpoch 13/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6968\nEpoch 14/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6966\nEpoch 15/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6964\nEpoch 16/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6963\nEpoch 17/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6961\nEpoch 18/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6959\nEpoch 19/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6958\nEpoch 20/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6956\nEpoch 21/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6955\nEpoch 22/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6953\nEpoch 23/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6952\nEpoch 24/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6951\nEpoch 25/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6949\nEpoch 26/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6948\nEpoch 27/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6947\nEpoch 28/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6946\nEpoch 29/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6945\nEpoch 30/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6944\nEpoch 31/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6943\nEpoch 32/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6942\nEpoch 33/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6942\nEpoch 34/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6941\nEpoch 35/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6940\nEpoch 36/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6940\nEpoch 37/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6939\nEpoch 38/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6939\nEpoch 39/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6938\nEpoch 40/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6937\nEpoch 41/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6937\nEpoch 42/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6936\nEpoch 43/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6936\nEpoch 44/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6935\nEpoch 45/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6935\nEpoch 46/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6934\nEpoch 47/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6934\nEpoch 48/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6934\nEpoch 49/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 50/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 51/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 52/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 53/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 54/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 55/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 56/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 57/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 58/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 59/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 60/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 61/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 62/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6933\nEpoch 63/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 64/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 65/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 66/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 67/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 68/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 69/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 70/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 71/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 72/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 73/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 74/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 75/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 76/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 77/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 78/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 79/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 80/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 81/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 82/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 83/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 84/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 85/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 86/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 87/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 88/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 89/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 90/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 91/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 92/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 93/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 94/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 95/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 96/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 97/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 98/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 99/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 100/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\n\n\n&lt;keras.callbacks.History at 0x7f640c5e9c40&gt;\n\n\n\nnp.mean((net(X)&gt;0.5) == y)\n\n0.5000833333333333\n\n\n\nnp.mean((net(XX)&gt;0.5) == yy)\n\n0.5\n\n\n\n\n풀이2: 옵티마이저 개선\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy)\nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 138ms/step - loss: 220.9145\nEpoch 2/100\n1/1 [==============================] - 0s 10ms/step - loss: 88.9490\nEpoch 3/100\n1/1 [==============================] - 0s 10ms/step - loss: 7.5895\nEpoch 4/100\n1/1 [==============================] - 0s 9ms/step - loss: 33.7521\nEpoch 5/100\n1/1 [==============================] - 0s 9ms/step - loss: 40.2290\nEpoch 6/100\n1/1 [==============================] - 0s 9ms/step - loss: 28.9675\nEpoch 7/100\n1/1 [==============================] - 0s 9ms/step - loss: 16.5128\nEpoch 8/100\n1/1 [==============================] - 0s 10ms/step - loss: 9.4911\nEpoch 9/100\n1/1 [==============================] - 0s 10ms/step - loss: 6.2027\nEpoch 10/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.2417\nEpoch 11/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.5172\nEpoch 12/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.5900\nEpoch 13/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.8605\nEpoch 14/100\n1/1 [==============================] - 0s 9ms/step - loss: 8.5884\nEpoch 15/100\n1/1 [==============================] - 0s 9ms/step - loss: 8.3991\nEpoch 16/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.4675\nEpoch 17/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.2581\nEpoch 18/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.1274\nEpoch 19/100\n1/1 [==============================] - 0s 9ms/step - loss: 4.2382\nEpoch 20/100\n1/1 [==============================] - 0s 9ms/step - loss: 3.6033\nEpoch 21/100\n1/1 [==============================] - 0s 9ms/step - loss: 3.1860\nEpoch 22/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.9233\nEpoch 23/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.7560\nEpoch 24/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.6421\nEpoch 25/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.5490\nEpoch 26/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.4612\nEpoch 27/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.3617\nEpoch 28/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.2378\nEpoch 29/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.0874\nEpoch 30/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.9117\nEpoch 31/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.7239\nEpoch 32/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.5409\nEpoch 33/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.3663\nEpoch 34/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.2210\nEpoch 35/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.1035\nEpoch 36/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0208\nEpoch 37/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9766\nEpoch 38/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9628\nEpoch 39/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.9717\nEpoch 40/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9883\nEpoch 41/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0039\nEpoch 42/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0156\nEpoch 43/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0181\nEpoch 44/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0067\nEpoch 45/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9809\nEpoch 46/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9443\nEpoch 47/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9019\nEpoch 48/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.8571\nEpoch 49/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.8146\nEpoch 50/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7768\nEpoch 51/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7489\nEpoch 52/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7294\nEpoch 53/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7186\nEpoch 54/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7125\nEpoch 55/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7080\nEpoch 56/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7044\nEpoch 57/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7002\nEpoch 58/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6949\nEpoch 59/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6884\nEpoch 60/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6806\nEpoch 61/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6715\nEpoch 62/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6615\nEpoch 63/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6510\nEpoch 64/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6404\nEpoch 65/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6302\nEpoch 66/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6209\nEpoch 67/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6127\nEpoch 68/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6060\nEpoch 69/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6007\nEpoch 70/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5963\nEpoch 71/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5924\nEpoch 72/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5888\nEpoch 73/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5853\nEpoch 74/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5816\nEpoch 75/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5778\nEpoch 76/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.5736\nEpoch 77/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5691\nEpoch 78/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5644\nEpoch 79/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5595\nEpoch 80/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5547\nEpoch 81/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5501\nEpoch 82/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5457\nEpoch 83/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5417\nEpoch 84/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5379\nEpoch 85/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5344\nEpoch 86/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5310\nEpoch 87/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5276\nEpoch 88/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5242\nEpoch 89/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5208\nEpoch 90/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5174\nEpoch 91/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5140\nEpoch 92/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5108\nEpoch 93/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5075\nEpoch 94/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5044\nEpoch 95/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5014\nEpoch 96/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4986\nEpoch 97/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4960\nEpoch 98/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.4935\nEpoch 99/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4909\nEpoch 100/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4885\n\n\n&lt;keras.callbacks.History at 0x7f640c513e50&gt;\n\n\n\nnp.mean((net(X)&gt;0.5) == y)\n\n0.98125\n\n\n\nnp.mean((net(XX)&gt;0.5) == yy)\n\n0.977\n\n\n\n\n풀이3: 컴파일시 metrics=[‘accuracy’] 추가\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 150ms/step - loss: 220.9145 - accuracy: 0.5000\nEpoch 2/100\n1/1 [==============================] - 0s 9ms/step - loss: 88.9490 - accuracy: 0.5073\nEpoch 3/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.5895 - accuracy: 0.8208\nEpoch 4/100\n1/1 [==============================] - 0s 10ms/step - loss: 33.7521 - accuracy: 0.5972\nEpoch 5/100\n1/1 [==============================] - 0s 9ms/step - loss: 40.2290 - accuracy: 0.5723\nEpoch 6/100\n1/1 [==============================] - 0s 9ms/step - loss: 28.9675 - accuracy: 0.6442\nEpoch 7/100\n1/1 [==============================] - 0s 11ms/step - loss: 16.5128 - accuracy: 0.8061\nEpoch 8/100\n1/1 [==============================] - 0s 9ms/step - loss: 9.4911 - accuracy: 0.8947\nEpoch 9/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.2027 - accuracy: 0.9355\nEpoch 10/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.2417 - accuracy: 0.9404\nEpoch 11/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.5172 - accuracy: 0.9270\nEpoch 12/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.5900 - accuracy: 0.9021\nEpoch 13/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.8605 - accuracy: 0.8788\nEpoch 14/100\n1/1 [==============================] - 0s 9ms/step - loss: 8.5884 - accuracy: 0.8647\nEpoch 15/100\n1/1 [==============================] - 0s 9ms/step - loss: 8.3991 - accuracy: 0.8664\nEpoch 16/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.4675 - accuracy: 0.8793\nEpoch 17/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.2581 - accuracy: 0.8982\nEpoch 18/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.1274 - accuracy: 0.9156\nEpoch 19/100\n1/1 [==============================] - 0s 9ms/step - loss: 4.2382 - accuracy: 0.9302\nEpoch 20/100\n1/1 [==============================] - 0s 9ms/step - loss: 3.6033 - accuracy: 0.9426\nEpoch 21/100\n1/1 [==============================] - 0s 9ms/step - loss: 3.1860 - accuracy: 0.9509\nEpoch 22/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.9233 - accuracy: 0.9551\nEpoch 23/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.7560 - accuracy: 0.9574\nEpoch 24/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.6421 - accuracy: 0.9594\nEpoch 25/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.5490 - accuracy: 0.9599\nEpoch 26/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.4612 - accuracy: 0.9603\nEpoch 27/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.3617 - accuracy: 0.9608\nEpoch 28/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.2378 - accuracy: 0.9612\nEpoch 29/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.0874 - accuracy: 0.9619\nEpoch 30/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.9117 - accuracy: 0.9630\nEpoch 31/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.7239 - accuracy: 0.9641\nEpoch 32/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.5409 - accuracy: 0.9657\nEpoch 33/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.3663 - accuracy: 0.9670\nEpoch 34/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.2210 - accuracy: 0.9685\nEpoch 35/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.1035 - accuracy: 0.9688\nEpoch 36/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0208 - accuracy: 0.9696\nEpoch 37/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9766 - accuracy: 0.9705\nEpoch 38/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9628 - accuracy: 0.9708\nEpoch 39/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9717 - accuracy: 0.9715\nEpoch 40/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9883 - accuracy: 0.9706\nEpoch 41/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0039 - accuracy: 0.9699\nEpoch 42/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0156 - accuracy: 0.9685\nEpoch 43/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0181 - accuracy: 0.9681\nEpoch 44/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0067 - accuracy: 0.9686\nEpoch 45/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9809 - accuracy: 0.9693\nEpoch 46/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9443 - accuracy: 0.9703\nEpoch 47/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9019 - accuracy: 0.9711\nEpoch 48/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.8571 - accuracy: 0.9722\nEpoch 49/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.8146 - accuracy: 0.9737\nEpoch 50/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7768 - accuracy: 0.9743\nEpoch 51/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7489 - accuracy: 0.9753\nEpoch 52/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7294 - accuracy: 0.9759\nEpoch 53/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7186 - accuracy: 0.9767\nEpoch 54/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7125 - accuracy: 0.9774\nEpoch 55/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7080 - accuracy: 0.9776\nEpoch 56/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7044 - accuracy: 0.9777\nEpoch 57/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7002 - accuracy: 0.9776\nEpoch 58/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6949 - accuracy: 0.9778\nEpoch 59/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6884 - accuracy: 0.9779\nEpoch 60/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6806 - accuracy: 0.9784\nEpoch 61/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6715 - accuracy: 0.9786\nEpoch 62/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6615 - accuracy: 0.9786\nEpoch 63/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6510 - accuracy: 0.9784\nEpoch 64/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6404 - accuracy: 0.9786\nEpoch 65/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6302 - accuracy: 0.9787\nEpoch 66/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6209 - accuracy: 0.9791\nEpoch 67/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6127 - accuracy: 0.9787\nEpoch 68/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6060 - accuracy: 0.9791\nEpoch 69/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6007 - accuracy: 0.9792\nEpoch 70/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5963 - accuracy: 0.9795\nEpoch 71/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5924 - accuracy: 0.9793\nEpoch 72/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5888 - accuracy: 0.9791\nEpoch 73/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5853 - accuracy: 0.9790\nEpoch 74/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5816 - accuracy: 0.9793\nEpoch 75/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5778 - accuracy: 0.9794\nEpoch 76/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5736 - accuracy: 0.9795\nEpoch 77/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5691 - accuracy: 0.9794\nEpoch 78/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5644 - accuracy: 0.9794\nEpoch 79/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5595 - accuracy: 0.9796\nEpoch 80/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5547 - accuracy: 0.9796\nEpoch 81/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5501 - accuracy: 0.9798\nEpoch 82/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5457 - accuracy: 0.9800\nEpoch 83/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5417 - accuracy: 0.9800\nEpoch 84/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5379 - accuracy: 0.9804\nEpoch 85/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5344 - accuracy: 0.9807\nEpoch 86/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5310 - accuracy: 0.9807\nEpoch 87/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5276 - accuracy: 0.9807\nEpoch 88/100\n1/1 [==============================] - 0s 7ms/step - loss: 0.5242 - accuracy: 0.9808\nEpoch 89/100\n1/1 [==============================] - 0s 7ms/step - loss: 0.5208 - accuracy: 0.9808\nEpoch 90/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5174 - accuracy: 0.9811\nEpoch 91/100\n1/1 [==============================] - 0s 7ms/step - loss: 0.5140 - accuracy: 0.9812\nEpoch 92/100\n1/1 [==============================] - 0s 7ms/step - loss: 0.5108 - accuracy: 0.9812\nEpoch 93/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5075 - accuracy: 0.9813\nEpoch 94/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5044 - accuracy: 0.9814\nEpoch 95/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5014 - accuracy: 0.9816\nEpoch 96/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4986 - accuracy: 0.9815\nEpoch 97/100\n1/1 [==============================] - 0s 7ms/step - loss: 0.4960 - accuracy: 0.9815\nEpoch 98/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4935 - accuracy: 0.9812\nEpoch 99/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4909 - accuracy: 0.9812\nEpoch 100/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4885 - accuracy: 0.9812\n\n\n&lt;keras.callbacks.History at 0x7f640c46bc70&gt;\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 0s 349us/step - loss: 0.4860 - accuracy: 0.9812\n\n\n[0.48598653078079224, 0.981249988079071]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 617us/step - loss: 0.4294 - accuracy: 0.9770\n\n\n[0.42936256527900696, 0.9769999980926514]\n\n\n\n\n풀이4: 확률적경사하강법 이용 // epochs=10\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=10,batch_size=120)\n\nEpoch 1/10\n100/100 [==============================] - 0s 827us/step - loss: 5.6484 - accuracy: 0.9418\nEpoch 2/10\n100/100 [==============================] - 0s 747us/step - loss: 0.5078 - accuracy: 0.9793\nEpoch 3/10\n100/100 [==============================] - 0s 734us/step - loss: 0.3784 - accuracy: 0.9818\nEpoch 4/10\n100/100 [==============================] - 0s 765us/step - loss: 0.3390 - accuracy: 0.9828\nEpoch 5/10\n100/100 [==============================] - 0s 735us/step - loss: 0.2474 - accuracy: 0.9857\nEpoch 6/10\n100/100 [==============================] - 0s 717us/step - loss: 0.2116 - accuracy: 0.9870\nEpoch 7/10\n100/100 [==============================] - 0s 734us/step - loss: 0.1724 - accuracy: 0.9889\nEpoch 8/10\n100/100 [==============================] - 0s 784us/step - loss: 0.1711 - accuracy: 0.9880\nEpoch 9/10\n100/100 [==============================] - 0s 795us/step - loss: 0.1491 - accuracy: 0.9894\nEpoch 10/10\n100/100 [==============================] - 0s 723us/step - loss: 0.1550 - accuracy: 0.9896\n\n\n&lt;keras.callbacks.History at 0x7f640c2d9fa0&gt;\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 0s 339us/step - loss: 0.1124 - accuracy: 0.9923\n\n\n[0.11242959648370743, 0.9922500252723694]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 566us/step - loss: 0.2988 - accuracy: 0.9845\n\n\n[0.29883989691734314, 0.984499990940094]"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-xOfpHJG0LrtYt4TUVgqUNy"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#강의영상",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#강의영상",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-xOfpHJG0LrtYt4TUVgqUNy"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#imports",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#imports",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "imports",
    "text": "imports\n\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#cnn",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#cnn",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "CNN",
    "text": "CNN\n\nCONV의 역할\n- 데이터생성 (그냥 흑백대비 데이터)\n\n_X1 = tnp.ones([50,25])*10\n_X1\n\n&lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       ...,\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.]])&gt;\n\n\n\n_X2 = tnp.zeros([50,25])*10\n_X2\n\n&lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])&gt;\n\n\n\ntf.concat([_X1,_X2],axis=1)\n\n&lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       ...,\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.]])&gt;\n\n\n\nplt.imshow(tf.concat([_X1,_X2], axis=1),cmap='gray')\n\n&lt;matplotlib.image.AxesImage at 0x7f37fdcce3d0&gt;\n\n\n\n\n\n\n값이 크면 흰색, 값이 작으면 검정색으로 되어있음.\n\n여기다 적당한 noise를 섞어보자.\n\n_noise_vec = tnp.random.randn(50*50) # 2500개의 vector\n_noise_vec\n\n&lt;tf.Tensor: shape=(2500,), dtype=float64, numpy=\narray([-0.06902639,  0.76575606,  0.62313143, ..., -0.15415241,\n       -0.52788634,  0.77923277])&gt;\n\n\n\n_noise = tnp.random.randn(50*50).reshape(50,50) # matrix 형태로\n_noise\n\n&lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[ 0.64448515,  0.48672712, -0.21792212, ..., -0.4029161 ,\n        -0.76942793,  0.42752944],\n       [-0.85736695,  1.27257844,  0.86595728, ...,  0.17527877,\n         1.74959789, -0.8465042 ],\n       [-2.37767743,  1.12817978,  0.80667681, ..., -1.69588932,\n         0.66389614,  0.04199325],\n       ...,\n       [ 0.35348866,  0.8854033 ,  0.57155344, ...,  1.47500872,\n        -0.56131948,  0.44347445],\n       [ 1.58838754, -1.37524759,  1.12635227, ..., -0.6870017 ,\n         0.63987008,  0.55168672],\n       [ 0.27925012,  0.04426039,  0.19833725, ...,  0.00770918,\n        -2.02424407, -0.04405339]])&gt;\n\n\n\n# image data 생성\nXXX = tf.concat([_X1,_X2],axis=1) + _noise\n\n\n# shape을 맞춰줘야함. \nXXX=XXX.reshape(1,50,50,1)\n\n\nplt.imshow(XXX.reshape(50,50),cmap='gray')\n\n&lt;matplotlib.image.AxesImage at 0x7f37fdc40490&gt;\n\n\n\n\n\n- conv layer 생성\n\nconv = tf.keras.layers.Conv2D(2,(2,2)) # (2,2) kernel\n\n\nconv.weights # 처음에는 가중치가 없음\n\n[]\n\n\n\nconv(XXX).shape # 2x2 kernel을 만들었으니까 이미지가 하나씩 날라가서 49x49의 이미지 생성\n\nTensorShape([1, 49, 49, 2])\n\n\n\nconv(XXX) # 가중치를 만들기 위해서 XXX를 conv에 한번 통과시킴\nconv.weights # 이제 가중치가 생김\n\n[&lt;tf.Variable 'conv2d/kernel:0' shape=(2, 2, 1, 2) dtype=float32, numpy=\n array([[[[-0.10361075, -0.03655446]],\n \n         [[-0.25615066, -0.6408293 ]]],\n \n \n        [[[-0.19069207, -0.6668661 ]],\n \n         [[-0.18648207, -0.534903  ]]]], dtype=float32)&gt;,\n &lt;tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;]\n\n\n\n이제 가중치가 생김.\n\n- 가중치의 값을 확인해보자.\n\nconv.weights[0] # kernel에 해당하는것\n\n&lt;tf.Variable 'conv2d/kernel:0' shape=(2, 2, 1, 2) dtype=float32, numpy=\narray([[[[-0.10361075, -0.03655446]],\n\n        [[-0.25615066, -0.6408293 ]]],\n\n\n       [[[-0.19069207, -0.6668661 ]],\n\n        [[-0.18648207, -0.534903  ]]]], dtype=float32)&gt;\n\n\n\nshape=(2, 2, 1, 2) : 커널사이즈 2x2 // XXX의 채널 1 // Conv(XXX)의 출력채널 2\n\n## 참고\nconv = tf.keras.layers.Conv2D(2,(2,2))\n\n# 여기서 unit을 2개로 받았으니까 2개의 출력 채널이 만들어진다.\n\nconv.weights[1] # bias에 해당하는것\n\n&lt;tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;\n\n\n- 필터값을 원하는 것으로 변경해보자.\n\nw0 = [[0.25,0.25],[0.25,0.25]] # 잡티를 제거하는 효과를 준다.\nw1 = [[-1.0,1.0],[-1.0,1.0]] # 경계를 찾기 좋아보이는 필터이다. (엣지검출)\n\n\nnp.array(w0), np.array(w1) # (2,2,1,2)\n\n(array([[0.25, 0.25],\n        [0.25, 0.25]]),\n array([[-1.,  1.],\n        [-1.,  1.]]))\n\n\n\nw=np.concatenate([np.array(w0).reshape(2,2,1,1),np.array(w1).reshape(2,2,1,1)],axis=-1)\nw\n\narray([[[[ 0.25, -1.  ]],\n\n        [[ 0.25,  1.  ]]],\n\n\n       [[[ 0.25, -1.  ]],\n\n        [[ 0.25,  1.  ]]]])\n\n\n\nb= np.array([0.0,0.0])\nb\n\narray([0., 0.])\n\n\n\nconv.set_weights([w,b])\nconv.get_weights()\n\n[array([[[[ 0.25, -1.  ]],\n \n         [[ 0.25,  1.  ]]],\n \n \n        [[[ 0.25, -1.  ]],\n \n         [[ 0.25,  1.  ]]]], dtype=float32),\n array([0., 0.], dtype=float32)]\n\n\n\n첫번째는 평균을 구하는 필터,\n두번째는 엣지를 검출하는 필터\n\n- 필터를 넣은 결과를 확인\n\nXXX0=conv(XXX)[...,0] # 채널0\nXXX0\n\n&lt;tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy=\narray([[[10.386606  , 10.601835  , 10.11681   , ..., -0.26418784,\n          0.18813315,  0.1402988 ],\n        [ 9.791428  , 11.018348  , 10.635733  , ..., -0.15474442,\n          0.22322088,  0.4022458 ],\n        [ 9.269302  , 10.1986265 , 10.747881  , ..., -0.42707908,\n         -0.01979728,  0.31640166],\n        ...,\n        [10.478983  , 10.708517  ,  9.69488   , ...,  0.23036546,\n         -0.513018  , -0.63264334],\n        [10.363008  , 10.302015  , 10.524318  , ...,  0.52903736,\n          0.21663941,  0.26842797],\n        [10.134163  ,  9.9984255 , 10.623085  , ...,  0.01135473,\n         -0.5159166 , -0.21918514]]], dtype=float32)&gt;\n\n\n\nXXX1=conv(XXX)[...,1] # 채널1\nXXX1\n\n&lt;tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy=\narray([[[ 1.9721851 , -1.111269  , -0.8288326 , ...,  0.60147667,\n          1.2078073 , -1.3991446 ],\n        [ 5.6358013 , -0.7281227 , -0.80233765, ..., -2.4222436 ,\n          3.9341047 , -3.2180052 ],\n        [ 2.7431278 ,  0.9741707 ,  1.222847  , ..., -0.033876  ,\n          1.6630032 , -0.31820738],\n        ...,\n        [ 3.2966785 , -2.378542  , -1.6760101 , ..., -0.11025763,\n         -2.863276  ,  2.3847747 ],\n        [-2.4317207 ,  2.1877518 , -1.298542  , ..., -0.5401355 ,\n         -0.7094564 ,  0.91661054],\n        [-3.1986256 ,  2.6556778 , -0.15703964, ..., -1.4040039 ,\n         -0.7050814 ,  1.8920072 ]]], dtype=float32)&gt;\n\n\n- 각 채널을 시각화\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)\n\n\n\n\n\nax1.imshow(XXX.reshape(50,50),cmap='gray') # original image\n\n&lt;matplotlib.image.AxesImage at 0x7f37fd4f2070&gt;\n\n\n\nax3.imshow(XXX0.reshape(49,49),cmap='gray') # 채널0을 통과시킨 이미지\n\n&lt;matplotlib.image.AxesImage at 0x7f37fd4c2700&gt;\n\n\n\nax4.imshow(XXX1.reshape(49,49),cmap='gray') # 채널1을 통과시킨 이미지\n\n&lt;matplotlib.image.AxesImage at 0x7f37fd3511c0&gt;\n\n\n\nfig\n\n\n\n\n\n2사분면: 원래이미지\n3사분면: 원래이미지 -&gt; 평균을 의미하는 conv적용\n4사분면: 원래이미지 -&gt; 엣지를 검출하는 conv적용\n\n- conv(XXX)의 각 채널에 한번더 conv를 통과시켜보자\n\nchannel0 : 평균필터\nchannel1 : 엣지필터\n\n\nconv(XXX0.reshape(1,49,49,1))[...,0] ### XXX0 -&gt; 평균필터 &lt;=&gt; XXX -&gt; 평균필터 -&gt; 평균필터\nconv(XXX0.reshape(1,49,49,1))[...,1] ### XXX0 -&gt; 엣지필터 &lt;=&gt; XXX -&gt; 평균필터 -&gt; 엣지필터\nconv(XXX1.reshape(1,49,49,1))[...,0] ### XXX1 -&gt; 평균필터 &lt;=&gt; XXX -&gt; 엣지필터 -&gt; 평균필터\nconv(XXX1.reshape(1,49,49,1))[...,1] ### XXX1 -&gt; 엣지필터 &lt;=&gt; XXX -&gt; 엣지필터 -&gt; 엣지필터\n\n&lt;tf.Tensor: shape=(1, 48, 48), dtype=float32, numpy=\narray([[[-9.447378  ,  0.20822144,  0.19534683, ..., -0.7362902 ,\n          6.962679  , -9.759062  ],\n        [-8.132881  ,  0.17446136, -3.0925026 , ..., -2.7580416 ,\n          8.053227  , -9.13332   ],\n        [-0.93447495, -2.5899773 , -1.0724258 , ...,  5.2640886 ,\n          0.94379497, -3.4876597 ],\n        ...,\n        [-9.087433  ,  1.0329819 ,  6.3635483 , ...,  0.6270219 ,\n         -6.605945  ,  9.485929  ],\n        [-1.055748  , -2.783762  ,  2.398777  , ...,  2.1154294 ,\n         -2.9223394 ,  6.874118  ],\n        [10.473776  , -6.299011  , -0.73449326, ...,  1.5050641 ,\n          0.5296016 ,  4.2231555 ]]], dtype=float32)&gt;\n\n\n\nfig,ax =plt.subplots(3,4)\n\n\n\n\n\nax[0][0].imshow(XXX.reshape(50,50),cmap='gray') # 원래이미지\nax[0][0].set_title('original image')\n\nText(0.5, 1.0, 'original image')\n\n\n\nfig\n\n\n\n\n\nax[1][0].imshow(XXX0.reshape(49,49),cmap='gray') # 원래이미지 -&gt; 평균필터\nax[1][2].imshow(XXX1.reshape(49,49),cmap='gray') # 원래이미지 -&gt; 엣지필터\n\n&lt;matplotlib.image.AxesImage at 0x7f37fd58ebe0&gt;\n\n\n\nax[1][0].set_title('Average filter')\nax[1][2].set_title('Edge filter')\n\nText(0.5, 1.0, 'Edge filter')\n\n\n\nax[2][0].imshow(conv(XXX0.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지(평균필터를 1번 통과한 이미지) -&gt; 평균필터\nax[2][1].imshow(conv(XXX0.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지(평균필터를 1번 통과한 이미지) -&gt; 엣지필터\nax[2][2].imshow(conv(XXX1.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지(엣지필터를 1번 통과한 이미지) -&gt; 평균필터\nax[2][3].imshow(conv(XXX1.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지(엣지필터를 1번 통과한 이미지) -&gt; 엣지필터\n\n&lt;matplotlib.image.AxesImage at 0x7f37fced5eb0&gt;\n\n\n\nax[2][0].set_title('Average + Average')\nax[2][1].set_title('Average + Edge')\nax[2][2].set_title('Edge + Average')\nax[2][3].set_title('Edge + Edge')\n\nText(0.5, 1.0, 'Edge + Edge')\n\n\n\nfig.set_figheight(8)\nfig.set_figwidth(16)\nfig.tight_layout()\nfig\n\n\n\n\n- 요약 - conv의 weight에 따라서 엣지를 검출하는 필터가 만들어지기도 하고 스무딩의 역할을 하는 필터가 만들어지기도 한다. 그리고 우리는 의미를 알 수 없지만 어떠한 역할을 하는 필터가 만들어질 것이다. - 이것들을 조합하다보면 우연히 이미지를 분류하기에 유리한 특징을 뽑아내는 weight가 맞춰질 수도 있겠다. - 채널수를 많이 만들고 다양한 웨이트조합을 실험하다보면 보다 복잡한 이미지의 특징을 추출할 수도 있을 것이다? - 컨볼루션 레이어의 역할 = 이미지의 특징을 추출하는 역할\n\n\n(참고) 스트라이드, 패딩\n- 참고: 스트라이드, 패딩 - 스트라이드: 윈도우가 1칸씩 이동하는 것이 아니라 2~3칸씩 이동함 - 패딩: 이미지의 가장자리에 정당한 값을 넣어서 (예를들어 0) 컨볼루션을 수행. 따라서 컨볼루션 연산 이후에도 이미지의 크기가 줄어들지 않도록 방지한다.\n\n\nMAXPOOL\n- 기본적역할: 이미지의 크기를 줄이는 것 - 이미지의의 크기를 줄여야하는 이유? 어차피 최종적으로 10차원으로 줄어야하므로 - 이미지의 크기를 줄이면서도 동시에 아주 크리티컬한 특징은 손실없이 유지하고 싶다~\n- 점점 작은 이미지가 되면서 중요한 특징들은 살아남지만 그렇지 않으면 죽는다. (캐리커쳐 느낌)\n- 평균이 아니라 max를 쓴 이유는? 그냥 평균보다 나을것이라고 생각했음.. - 그런데 사실은 꼭 그렇지만은 않아서 최근에는 꼭 맥스풀링을 고집하진 않는 추세 (평균풀링도 많이씀)\n\n\nCNN 아키텍처의 표현방법\n- 아래와 같이 아키텍처의 다이어그램형태로 표현하고 굳이 노드별로 이미지를 그리진 않음\n\n\n\n위키에서 긁어온 이미지\n\n\n- 물론 아래와 같이 그리는 경우도 있음\n\n\n\nDiscusstion about CNN\n- 격자형태로 배열된 자료를 처리하는데 특화된 신경망이다. - 시계열 (1차원격자), 이미지 (2차원격자)\n- 실제응용에서 엄청난 성공을 거두었다.\n- 이름의 유래는 컨볼루션이라는 수학적 연산을 사용했기 때문 - 컨볼루션은 조금 특별한 선형변환이다.\n- 신경과학의 원리가 심층학습에 영향을 미친 사례이다.\n\n\nCNN의 모티브\n- 희소성 + 매개변수의 공유 - 다소 철학적인 모티브임 - 희소성: 이미지를 분석하여 특징을 뽑아낼때 부분부분의 특징만 뽑으면 된다는 의미1 - 매개변수의 공유: 한 채널에는 하나의 역할을 하는 커널을 설계하면 된다는 의미 (스무딩이든 엣징이든). 즉 어떤지역은 스무딩, 어떤지역은 엣징을 할 필요가 없이 한채널에서는 엣징만, 다른채널에서는 스무딩만 수행한뒤 여러채널을 조합해서 이해하면 된다.\n- 매개변수 공유효과로 인해서 파라메터가 확 줄어든다.\n(예시) (1,6,6,1) -&gt; (1,5,5,2) - MLP방식이면 (36,50) 의 차원을 가진 매트릭스가 필요함 =&gt; 1800개의 매개변수 필요 - CNN은 8개의 매개변수 필요\n\n## MLP\n36*50\n\n1800\n\n\n\n## CNN\n(2*2) * 2 # (2x2) kernel이 2개\n\n8\n\n\n\n매개변수가 적으면 GPU에 올릴때 좋음.\n\n\n\nCNN 신경망의 기본구조\n- 기본유닛 - conv2 - activation3 - pooling4 - conv5 - conv6 - activation7 - pooling8"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#모형의-성능을-올리기-위한-노력들",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#모형의-성능을-올리기-위한-노력들",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "모형의 성능을 올리기 위한 노력들",
    "text": "모형의 성능을 올리기 위한 노력들\n\ndropout\n- 아래의 예제를 복습하자.\n\nnp.random.seed(43052)\nx = np.linspace(0,1,100).reshape(100,1)\ny = np.random.normal(loc=0,scale=0.01,size=(100,1))\nplt.plot(x,y)\n\n\n\n\n\n이 예제는 랜덤으로 만든 데이터이기 때문에 fitting을 하면 직선이 나와야 한다.\n그게 아니라면 오퍼피팅.\n\nDense layer를 \\(2048\\)로 받아서 오버피팅이 일어나기 좋은 환경을 일부러 만들고 있다.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() # 네트워크 생성\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1)) # activation이 identity로 받아지는 것.\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x,y,epochs=5000,verbose=0,batch_size=100) # 적합.\n\n&lt;keras.callbacks.History at 0x7f37e6134820&gt;\n\n\n\nplt.plot(x,y)\nplt.plot(x,net(x),'--')\n\n\n\n\n\n얘는 랜덤인데9 위와 같이 데이터를 따라가는 피팅 결과가 나오면 오버피팅이라고 할 수 있다.\n이러한 추세가 있는게 맞을수도 있지 않느냐? 라고 생각할 수 있는데 그것을 아님을 보이기 위해 train/test로 나누어서 생각해보자.\n\n- train/test로 나누어서 생각해보자.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n&lt;keras.callbacks.History at 0x7f37e5f1eee0&gt;\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\n\n\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--', label='train')\nplt.plot(x[80:],net(x[80:]),'--', label='test')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f37fd08b640&gt;\n\n\n\n\n\n\ntrain에서 추세를 따라가는게 좋은게 아니다 \\(\\to\\) 그냥 직선으로 핏하는거 이외에는 다 오버핏이다.\n\n(생각) 우리가 노드를 \\(2048\\)개를 만들었었는데 학습을 해보니 과적합이 일어났다. 즉, 노드들이 학습을 너무 열심히 했다. 학습을 좀 더 대충했으면 이렇게 세밀하게는 안따라갔을 텐데..\n- 매 에폭마다 적당히 80%의 노드들을 빼고 학습하자 \\(\\to\\) 너무 잘 학습되는 문제는 생기지 않을 것이다 (과적합이 방지될것이다?)\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dropout(0.8)) ## Dropout층 추가\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n&lt;keras.callbacks.History at 0x7f37fd5a9520&gt;\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\nplt.plot(x[80:],net(x[80:]),'--')\n\n\n\n\n\n오버핏이 확실히 줄어들었다. (완전히 없어진 것은 아니지만)\n\n\n\n\n\n\n\nNote\n\n\n\n\\(80\\%\\) 노드를 빼고 학습하면 특징을 잘 학습하지 못하는거 아니냐? 라고 생각할 수 있지만 그렇게 중요한 특징이면 노드들을 랜덤으로 빼도 결국 학습을 해낼 것이라는 믿음이 있는 것이다. 증명이 있는건 아니지만 그렇게 믿음. 그러한 직관이 있다.\n\n\n- 드랍아웃에 대한 summary - 직관: 특정노드를 랜덤으로 off시키면 학습이 방해되어 오히려 과적합이 방지되는 효과가 있다 (그렇지만 진짜 중요한 특징이라면 랜덤으로 off 되더라도 어느정도는 학습될 듯) - note: 드랍아웃을 쓰면 오버핏이 줄어드는건 맞지만 완전히 없어지는건 아니다. - note: 오버핏을 줄이는 유일한 방법이 드랍아웃만 있는것도 아니며, 드랍아웃이 오버핏을 줄이는 가장 효과적인 방법도 아니다 (최근에는 dropout보다 batch nomalization을 사용하는 추세임)\n\n\ntrain / val / test\n만약 train으로 에폭 5000정도로 열심히 학습했다고 가정해보자. 그런데 테스트를 해봤더니 오버피팅이 심한 엉뚱한 모형이 나왔다면 너무 아깝다. (비효율적)\ntrain, validation을 비교해보고, validation loss가 줄어들지 않고 오히려 overfitting이 되면서 커진다면 학습의 에폭을 줄여봐야겠다 내지는 노드를 줄여봐야 겠다 이런식으로 조정을 할 수 있다.\n- data\nFashion MNIST 데이터셋은 위 그림과 같이 운동화, 셔츠, 샌들과 같은 작은 이미지들의 모음이며, 기본 MNIST 데이터셋과 같이 열 가지로 분류될 수 있는 28×28 픽셀의 이미지 70,000개로 이루어져 있습니다.\n\n\n\nFashion MNIST 이미지 데이터셋.\n\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= x_train.reshape(-1,28,28,1)/255 ## 입력이 0~255 -&gt; 0~1로 표준화 시키는 효과 + float으로 자료형이 바뀜\ny = tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,28,28,1)/255\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nX.shape, y.shape, XX.shape, yy.shape\n\n((60000, 28, 28, 1), (60000, 10), (10000, 28, 28, 1), (10000, 10))\n\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten()) # DNN 쓸거니까 flatten()!\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#collapse_output\ncb1 = tf.keras.callbacks.TensorBoard()\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb1,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 0s 1ms/step - loss: 0.7011 - accuracy: 0.7695 - val_loss: 0.4931 - val_accuracy: 0.8338\nEpoch 2/200\n240/240 [==============================] - 0s 818us/step - loss: 0.4582 - accuracy: 0.8434 - val_loss: 0.4590 - val_accuracy: 0.8414\nEpoch 3/200\n240/240 [==============================] - 0s 829us/step - loss: 0.4142 - accuracy: 0.8560 - val_loss: 0.4216 - val_accuracy: 0.8543\nEpoch 4/200\n240/240 [==============================] - 0s 824us/step - loss: 0.3920 - accuracy: 0.8625 - val_loss: 0.3953 - val_accuracy: 0.8624\nEpoch 5/200\n240/240 [==============================] - 0s 822us/step - loss: 0.3718 - accuracy: 0.8690 - val_loss: 0.3842 - val_accuracy: 0.8654\nEpoch 6/200\n240/240 [==============================] - 0s 827us/step - loss: 0.3567 - accuracy: 0.8750 - val_loss: 0.3825 - val_accuracy: 0.8662\nEpoch 7/200\n240/240 [==============================] - 0s 823us/step - loss: 0.3477 - accuracy: 0.8770 - val_loss: 0.3805 - val_accuracy: 0.8684\nEpoch 8/200\n240/240 [==============================] - 0s 837us/step - loss: 0.3377 - accuracy: 0.8789 - val_loss: 0.3564 - val_accuracy: 0.8741\nEpoch 9/200\n240/240 [==============================] - 0s 828us/step - loss: 0.3288 - accuracy: 0.8830 - val_loss: 0.3481 - val_accuracy: 0.8758\nEpoch 10/200\n240/240 [==============================] - 0s 826us/step - loss: 0.3204 - accuracy: 0.8852 - val_loss: 0.3541 - val_accuracy: 0.8771\nEpoch 11/200\n240/240 [==============================] - 0s 832us/step - loss: 0.3136 - accuracy: 0.8880 - val_loss: 0.3541 - val_accuracy: 0.8749\nEpoch 12/200\n240/240 [==============================] - 0s 827us/step - loss: 0.3071 - accuracy: 0.8887 - val_loss: 0.3508 - val_accuracy: 0.8748\nEpoch 13/200\n240/240 [==============================] - 0s 827us/step - loss: 0.3020 - accuracy: 0.8914 - val_loss: 0.3450 - val_accuracy: 0.8796\nEpoch 14/200\n240/240 [==============================] - 0s 826us/step - loss: 0.2968 - accuracy: 0.8934 - val_loss: 0.3507 - val_accuracy: 0.8752\nEpoch 15/200\n240/240 [==============================] - 0s 824us/step - loss: 0.2929 - accuracy: 0.8946 - val_loss: 0.3494 - val_accuracy: 0.8772\nEpoch 16/200\n240/240 [==============================] - 0s 823us/step - loss: 0.2909 - accuracy: 0.8948 - val_loss: 0.3875 - val_accuracy: 0.8594\nEpoch 17/200\n240/240 [==============================] - 0s 819us/step - loss: 0.2853 - accuracy: 0.8969 - val_loss: 0.3411 - val_accuracy: 0.8792\nEpoch 18/200\n240/240 [==============================] - 0s 822us/step - loss: 0.2796 - accuracy: 0.8979 - val_loss: 0.3503 - val_accuracy: 0.8773\nEpoch 19/200\n240/240 [==============================] - 0s 818us/step - loss: 0.2745 - accuracy: 0.9007 - val_loss: 0.3381 - val_accuracy: 0.8800\nEpoch 20/200\n240/240 [==============================] - 0s 820us/step - loss: 0.2709 - accuracy: 0.9023 - val_loss: 0.3402 - val_accuracy: 0.8807\nEpoch 21/200\n240/240 [==============================] - 0s 821us/step - loss: 0.2690 - accuracy: 0.9026 - val_loss: 0.3368 - val_accuracy: 0.8805\nEpoch 22/200\n240/240 [==============================] - 0s 835us/step - loss: 0.2635 - accuracy: 0.9047 - val_loss: 0.3346 - val_accuracy: 0.8816\nEpoch 23/200\n240/240 [==============================] - 0s 827us/step - loss: 0.2608 - accuracy: 0.9053 - val_loss: 0.3386 - val_accuracy: 0.8812\nEpoch 24/200\n240/240 [==============================] - 0s 815us/step - loss: 0.2585 - accuracy: 0.9072 - val_loss: 0.3383 - val_accuracy: 0.8808\nEpoch 25/200\n240/240 [==============================] - 0s 816us/step - loss: 0.2525 - accuracy: 0.9075 - val_loss: 0.3341 - val_accuracy: 0.8809\nEpoch 26/200\n240/240 [==============================] - 0s 822us/step - loss: 0.2519 - accuracy: 0.9095 - val_loss: 0.3393 - val_accuracy: 0.8810\nEpoch 27/200\n240/240 [==============================] - 0s 815us/step - loss: 0.2499 - accuracy: 0.9101 - val_loss: 0.3312 - val_accuracy: 0.8859\nEpoch 28/200\n240/240 [==============================] - 0s 832us/step - loss: 0.2437 - accuracy: 0.9118 - val_loss: 0.3343 - val_accuracy: 0.8844\nEpoch 29/200\n240/240 [==============================] - 0s 842us/step - loss: 0.2446 - accuracy: 0.9108 - val_loss: 0.3575 - val_accuracy: 0.8759\nEpoch 30/200\n240/240 [==============================] - 0s 821us/step - loss: 0.2402 - accuracy: 0.9124 - val_loss: 0.3381 - val_accuracy: 0.8802\nEpoch 31/200\n240/240 [==============================] - 0s 824us/step - loss: 0.2363 - accuracy: 0.9141 - val_loss: 0.3459 - val_accuracy: 0.8787\nEpoch 32/200\n240/240 [==============================] - 0s 820us/step - loss: 0.2365 - accuracy: 0.9144 - val_loss: 0.3396 - val_accuracy: 0.8804\nEpoch 33/200\n240/240 [==============================] - 0s 827us/step - loss: 0.2328 - accuracy: 0.9159 - val_loss: 0.3367 - val_accuracy: 0.8837\nEpoch 34/200\n240/240 [==============================] - 0s 830us/step - loss: 0.2310 - accuracy: 0.9161 - val_loss: 0.3516 - val_accuracy: 0.8808\nEpoch 35/200\n240/240 [==============================] - 0s 820us/step - loss: 0.2293 - accuracy: 0.9173 - val_loss: 0.3541 - val_accuracy: 0.8766\nEpoch 36/200\n240/240 [==============================] - 0s 826us/step - loss: 0.2268 - accuracy: 0.9183 - val_loss: 0.3503 - val_accuracy: 0.8802\nEpoch 37/200\n240/240 [==============================] - 0s 820us/step - loss: 0.2262 - accuracy: 0.9179 - val_loss: 0.3543 - val_accuracy: 0.8797\nEpoch 38/200\n240/240 [==============================] - 0s 827us/step - loss: 0.2266 - accuracy: 0.9175 - val_loss: 0.3432 - val_accuracy: 0.8825\nEpoch 39/200\n240/240 [==============================] - 0s 817us/step - loss: 0.2190 - accuracy: 0.9208 - val_loss: 0.3364 - val_accuracy: 0.8847\nEpoch 40/200\n240/240 [==============================] - 0s 809us/step - loss: 0.2156 - accuracy: 0.9232 - val_loss: 0.3400 - val_accuracy: 0.8838\nEpoch 41/200\n240/240 [==============================] - 0s 805us/step - loss: 0.2138 - accuracy: 0.9231 - val_loss: 0.3530 - val_accuracy: 0.8776\nEpoch 42/200\n240/240 [==============================] - 0s 816us/step - loss: 0.2154 - accuracy: 0.9224 - val_loss: 0.3461 - val_accuracy: 0.8833\nEpoch 43/200\n240/240 [==============================] - 0s 836us/step - loss: 0.2105 - accuracy: 0.9242 - val_loss: 0.3517 - val_accuracy: 0.8806\nEpoch 44/200\n240/240 [==============================] - 0s 822us/step - loss: 0.2104 - accuracy: 0.9237 - val_loss: 0.3439 - val_accuracy: 0.8852\nEpoch 45/200\n240/240 [==============================] - 0s 832us/step - loss: 0.2064 - accuracy: 0.9250 - val_loss: 0.3457 - val_accuracy: 0.8840\nEpoch 46/200\n240/240 [==============================] - 0s 817us/step - loss: 0.2053 - accuracy: 0.9265 - val_loss: 0.3698 - val_accuracy: 0.8756\nEpoch 47/200\n240/240 [==============================] - 0s 824us/step - loss: 0.2085 - accuracy: 0.9249 - val_loss: 0.3476 - val_accuracy: 0.8824\nEpoch 48/200\n240/240 [==============================] - 0s 837us/step - loss: 0.1996 - accuracy: 0.9290 - val_loss: 0.3581 - val_accuracy: 0.8812\nEpoch 49/200\n240/240 [==============================] - 0s 823us/step - loss: 0.2002 - accuracy: 0.9274 - val_loss: 0.3568 - val_accuracy: 0.8802\nEpoch 50/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1991 - accuracy: 0.9289 - val_loss: 0.3607 - val_accuracy: 0.8798\nEpoch 51/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1987 - accuracy: 0.9278 - val_loss: 0.3527 - val_accuracy: 0.8832\nEpoch 52/200\n240/240 [==============================] - 0s 828us/step - loss: 0.1962 - accuracy: 0.9295 - val_loss: 0.3781 - val_accuracy: 0.8738\nEpoch 53/200\n240/240 [==============================] - 0s 832us/step - loss: 0.1952 - accuracy: 0.9300 - val_loss: 0.3603 - val_accuracy: 0.8807\nEpoch 54/200\n240/240 [==============================] - 0s 824us/step - loss: 0.1956 - accuracy: 0.9295 - val_loss: 0.3941 - val_accuracy: 0.8712\nEpoch 55/200\n240/240 [==============================] - 0s 837us/step - loss: 0.1929 - accuracy: 0.9301 - val_loss: 0.3656 - val_accuracy: 0.8806\nEpoch 56/200\n240/240 [==============================] - 0s 842us/step - loss: 0.1887 - accuracy: 0.9321 - val_loss: 0.3676 - val_accuracy: 0.8788\nEpoch 57/200\n240/240 [==============================] - 0s 827us/step - loss: 0.1892 - accuracy: 0.9321 - val_loss: 0.3638 - val_accuracy: 0.8806\nEpoch 58/200\n240/240 [==============================] - 0s 839us/step - loss: 0.1866 - accuracy: 0.9329 - val_loss: 0.3626 - val_accuracy: 0.8808\nEpoch 59/200\n240/240 [==============================] - 0s 829us/step - loss: 0.1839 - accuracy: 0.9336 - val_loss: 0.3672 - val_accuracy: 0.8797\nEpoch 60/200\n240/240 [==============================] - 0s 842us/step - loss: 0.1823 - accuracy: 0.9347 - val_loss: 0.3803 - val_accuracy: 0.8773\nEpoch 61/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1822 - accuracy: 0.9350 - val_loss: 0.3680 - val_accuracy: 0.8801\nEpoch 62/200\n240/240 [==============================] - 0s 822us/step - loss: 0.1808 - accuracy: 0.9351 - val_loss: 0.3691 - val_accuracy: 0.8815\nEpoch 63/200\n240/240 [==============================] - 0s 835us/step - loss: 0.1815 - accuracy: 0.9343 - val_loss: 0.3730 - val_accuracy: 0.8792\nEpoch 64/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1802 - accuracy: 0.9352 - val_loss: 0.3845 - val_accuracy: 0.8775\nEpoch 65/200\n240/240 [==============================] - 0s 819us/step - loss: 0.1752 - accuracy: 0.9372 - val_loss: 0.3918 - val_accuracy: 0.8770\nEpoch 66/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1748 - accuracy: 0.9375 - val_loss: 0.3770 - val_accuracy: 0.8818\nEpoch 67/200\n240/240 [==============================] - 0s 835us/step - loss: 0.1742 - accuracy: 0.9369 - val_loss: 0.3845 - val_accuracy: 0.8783\nEpoch 68/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1732 - accuracy: 0.9386 - val_loss: 0.3857 - val_accuracy: 0.8798\nEpoch 69/200\n240/240 [==============================] - 0s 832us/step - loss: 0.1738 - accuracy: 0.9373 - val_loss: 0.3766 - val_accuracy: 0.8813\nEpoch 70/200\n240/240 [==============================] - 0s 825us/step - loss: 0.1710 - accuracy: 0.9390 - val_loss: 0.3822 - val_accuracy: 0.8797\nEpoch 71/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1690 - accuracy: 0.9393 - val_loss: 0.3843 - val_accuracy: 0.8784\nEpoch 72/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1683 - accuracy: 0.9383 - val_loss: 0.3878 - val_accuracy: 0.8803\nEpoch 73/200\n240/240 [==============================] - 0s 835us/step - loss: 0.1644 - accuracy: 0.9414 - val_loss: 0.3926 - val_accuracy: 0.8792\nEpoch 74/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1649 - accuracy: 0.9420 - val_loss: 0.3928 - val_accuracy: 0.8773\nEpoch 75/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1657 - accuracy: 0.9408 - val_loss: 0.3850 - val_accuracy: 0.8816\nEpoch 76/200\n240/240 [==============================] - 0s 825us/step - loss: 0.1631 - accuracy: 0.9411 - val_loss: 0.3984 - val_accuracy: 0.8806\nEpoch 77/200\n240/240 [==============================] - 0s 829us/step - loss: 0.1638 - accuracy: 0.9415 - val_loss: 0.3905 - val_accuracy: 0.8804\nEpoch 78/200\n240/240 [==============================] - 0s 833us/step - loss: 0.1644 - accuracy: 0.9413 - val_loss: 0.4109 - val_accuracy: 0.8789\nEpoch 79/200\n240/240 [==============================] - 0s 836us/step - loss: 0.1616 - accuracy: 0.9420 - val_loss: 0.3997 - val_accuracy: 0.8764\nEpoch 80/200\n240/240 [==============================] - 0s 821us/step - loss: 0.1601 - accuracy: 0.9434 - val_loss: 0.4165 - val_accuracy: 0.8758\nEpoch 81/200\n240/240 [==============================] - 0s 817us/step - loss: 0.1573 - accuracy: 0.9438 - val_loss: 0.4006 - val_accuracy: 0.8807\nEpoch 82/200\n240/240 [==============================] - 0s 815us/step - loss: 0.1552 - accuracy: 0.9444 - val_loss: 0.4066 - val_accuracy: 0.8792\nEpoch 83/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1545 - accuracy: 0.9437 - val_loss: 0.4042 - val_accuracy: 0.8792\nEpoch 84/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1538 - accuracy: 0.9444 - val_loss: 0.4126 - val_accuracy: 0.8753\nEpoch 85/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1563 - accuracy: 0.9445 - val_loss: 0.4049 - val_accuracy: 0.8799\nEpoch 86/200\n240/240 [==============================] - 0s 811us/step - loss: 0.1579 - accuracy: 0.9440 - val_loss: 0.4054 - val_accuracy: 0.8770\nEpoch 87/200\n240/240 [==============================] - 0s 810us/step - loss: 0.1511 - accuracy: 0.9464 - val_loss: 0.4145 - val_accuracy: 0.8789\nEpoch 88/200\n240/240 [==============================] - 0s 839us/step - loss: 0.1512 - accuracy: 0.9465 - val_loss: 0.4105 - val_accuracy: 0.8780\nEpoch 89/200\n240/240 [==============================] - 0s 834us/step - loss: 0.1509 - accuracy: 0.9458 - val_loss: 0.4125 - val_accuracy: 0.8784\nEpoch 90/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1515 - accuracy: 0.9461 - val_loss: 0.4185 - val_accuracy: 0.8784\nEpoch 91/200\n240/240 [==============================] - 0s 813us/step - loss: 0.1464 - accuracy: 0.9485 - val_loss: 0.4116 - val_accuracy: 0.8797\nEpoch 92/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1492 - accuracy: 0.9474 - val_loss: 0.4180 - val_accuracy: 0.8784\nEpoch 93/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1501 - accuracy: 0.9460 - val_loss: 0.4196 - val_accuracy: 0.8797\nEpoch 94/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1460 - accuracy: 0.9485 - val_loss: 0.4242 - val_accuracy: 0.8789\nEpoch 95/200\n240/240 [==============================] - 0s 825us/step - loss: 0.1465 - accuracy: 0.9473 - val_loss: 0.4143 - val_accuracy: 0.8794\nEpoch 96/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1436 - accuracy: 0.9490 - val_loss: 0.4276 - val_accuracy: 0.8789\nEpoch 97/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1434 - accuracy: 0.9500 - val_loss: 0.4361 - val_accuracy: 0.8788\nEpoch 98/200\n240/240 [==============================] - 0s 843us/step - loss: 0.1404 - accuracy: 0.9507 - val_loss: 0.4261 - val_accuracy: 0.8783\nEpoch 99/200\n240/240 [==============================] - 0s 816us/step - loss: 0.1391 - accuracy: 0.9507 - val_loss: 0.4392 - val_accuracy: 0.8788\nEpoch 100/200\n240/240 [==============================] - 0s 822us/step - loss: 0.1437 - accuracy: 0.9487 - val_loss: 0.4266 - val_accuracy: 0.8776\nEpoch 101/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1378 - accuracy: 0.9506 - val_loss: 0.4445 - val_accuracy: 0.8767\nEpoch 102/200\n240/240 [==============================] - 0s 813us/step - loss: 0.1385 - accuracy: 0.9506 - val_loss: 0.4478 - val_accuracy: 0.8733\nEpoch 103/200\n240/240 [==============================] - 0s 824us/step - loss: 0.1372 - accuracy: 0.9520 - val_loss: 0.4424 - val_accuracy: 0.8767\nEpoch 104/200\n240/240 [==============================] - 0s 816us/step - loss: 0.1404 - accuracy: 0.9495 - val_loss: 0.4454 - val_accuracy: 0.8783\nEpoch 105/200\n240/240 [==============================] - 0s 827us/step - loss: 0.1396 - accuracy: 0.9499 - val_loss: 0.4475 - val_accuracy: 0.8739\nEpoch 106/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1340 - accuracy: 0.9526 - val_loss: 0.4597 - val_accuracy: 0.8762\nEpoch 107/200\n240/240 [==============================] - 0s 813us/step - loss: 0.1363 - accuracy: 0.9512 - val_loss: 0.4534 - val_accuracy: 0.8743\nEpoch 108/200\n240/240 [==============================] - 0s 844us/step - loss: 0.1341 - accuracy: 0.9525 - val_loss: 0.4524 - val_accuracy: 0.8772\nEpoch 109/200\n240/240 [==============================] - 0s 819us/step - loss: 0.1314 - accuracy: 0.9535 - val_loss: 0.4499 - val_accuracy: 0.8781\nEpoch 110/200\n240/240 [==============================] - 0s 821us/step - loss: 0.1329 - accuracy: 0.9530 - val_loss: 0.4555 - val_accuracy: 0.8772\nEpoch 111/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1320 - accuracy: 0.9532 - val_loss: 0.4566 - val_accuracy: 0.8757\nEpoch 112/200\n240/240 [==============================] - 0s 831us/step - loss: 0.1324 - accuracy: 0.9532 - val_loss: 0.4464 - val_accuracy: 0.8786\nEpoch 113/200\n240/240 [==============================] - 0s 831us/step - loss: 0.1300 - accuracy: 0.9534 - val_loss: 0.4571 - val_accuracy: 0.8765\nEpoch 114/200\n240/240 [==============================] - 0s 843us/step - loss: 0.1294 - accuracy: 0.9543 - val_loss: 0.4609 - val_accuracy: 0.8759\nEpoch 115/200\n240/240 [==============================] - 0s 829us/step - loss: 0.1294 - accuracy: 0.9546 - val_loss: 0.4627 - val_accuracy: 0.8769\nEpoch 116/200\n240/240 [==============================] - 0s 825us/step - loss: 0.1268 - accuracy: 0.9548 - val_loss: 0.4595 - val_accuracy: 0.8785\nEpoch 117/200\n240/240 [==============================] - 0s 813us/step - loss: 0.1279 - accuracy: 0.9546 - val_loss: 0.4736 - val_accuracy: 0.8745\nEpoch 118/200\n240/240 [==============================] - 0s 831us/step - loss: 0.1303 - accuracy: 0.9532 - val_loss: 0.4683 - val_accuracy: 0.8770\nEpoch 119/200\n240/240 [==============================] - 0s 831us/step - loss: 0.1257 - accuracy: 0.9550 - val_loss: 0.4896 - val_accuracy: 0.8696\nEpoch 120/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1275 - accuracy: 0.9539 - val_loss: 0.4760 - val_accuracy: 0.8758\nEpoch 121/200\n240/240 [==============================] - 0s 831us/step - loss: 0.1242 - accuracy: 0.9558 - val_loss: 0.4716 - val_accuracy: 0.8758\nEpoch 122/200\n240/240 [==============================] - 0s 818us/step - loss: 0.1227 - accuracy: 0.9570 - val_loss: 0.4944 - val_accuracy: 0.8706\nEpoch 123/200\n240/240 [==============================] - 0s 816us/step - loss: 0.1225 - accuracy: 0.9565 - val_loss: 0.4770 - val_accuracy: 0.8755\nEpoch 124/200\n240/240 [==============================] - 0s 809us/step - loss: 0.1215 - accuracy: 0.9564 - val_loss: 0.4969 - val_accuracy: 0.8732\nEpoch 125/200\n240/240 [==============================] - 0s 815us/step - loss: 0.1246 - accuracy: 0.9554 - val_loss: 0.4846 - val_accuracy: 0.8766\nEpoch 126/200\n240/240 [==============================] - 0s 821us/step - loss: 0.1210 - accuracy: 0.9578 - val_loss: 0.5119 - val_accuracy: 0.8697\nEpoch 127/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1213 - accuracy: 0.9571 - val_loss: 0.4964 - val_accuracy: 0.8737\nEpoch 128/200\n240/240 [==============================] - 0s 838us/step - loss: 0.1176 - accuracy: 0.9586 - val_loss: 0.4787 - val_accuracy: 0.8767\nEpoch 129/200\n240/240 [==============================] - 0s 824us/step - loss: 0.1200 - accuracy: 0.9581 - val_loss: 0.4976 - val_accuracy: 0.8733\nEpoch 130/200\n240/240 [==============================] - 0s 817us/step - loss: 0.1177 - accuracy: 0.9587 - val_loss: 0.4856 - val_accuracy: 0.8759\nEpoch 131/200\n240/240 [==============================] - 0s 837us/step - loss: 0.1165 - accuracy: 0.9590 - val_loss: 0.4881 - val_accuracy: 0.8742\nEpoch 132/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1198 - accuracy: 0.9581 - val_loss: 0.4968 - val_accuracy: 0.8737\nEpoch 133/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1182 - accuracy: 0.9581 - val_loss: 0.5100 - val_accuracy: 0.8739\nEpoch 134/200\n240/240 [==============================] - 0s 822us/step - loss: 0.1161 - accuracy: 0.9595 - val_loss: 0.5058 - val_accuracy: 0.8727\nEpoch 135/200\n240/240 [==============================] - 0s 828us/step - loss: 0.1198 - accuracy: 0.9578 - val_loss: 0.5181 - val_accuracy: 0.8730\nEpoch 136/200\n240/240 [==============================] - 0s 818us/step - loss: 0.1157 - accuracy: 0.9587 - val_loss: 0.5176 - val_accuracy: 0.8744\nEpoch 137/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1145 - accuracy: 0.9592 - val_loss: 0.5112 - val_accuracy: 0.8773\nEpoch 138/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1121 - accuracy: 0.9614 - val_loss: 0.5236 - val_accuracy: 0.8728\nEpoch 139/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1120 - accuracy: 0.9601 - val_loss: 0.5358 - val_accuracy: 0.8698\nEpoch 140/200\n240/240 [==============================] - 0s 829us/step - loss: 0.1116 - accuracy: 0.9603 - val_loss: 0.5276 - val_accuracy: 0.8717\nEpoch 141/200\n240/240 [==============================] - 0s 827us/step - loss: 0.1111 - accuracy: 0.9612 - val_loss: 0.5178 - val_accuracy: 0.8721\nEpoch 142/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1124 - accuracy: 0.9604 - val_loss: 0.5285 - val_accuracy: 0.8738\nEpoch 143/200\n240/240 [==============================] - 0s 825us/step - loss: 0.1102 - accuracy: 0.9607 - val_loss: 0.5256 - val_accuracy: 0.8734\nEpoch 144/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1095 - accuracy: 0.9608 - val_loss: 0.5328 - val_accuracy: 0.8727\nEpoch 145/200\n240/240 [==============================] - 0s 828us/step - loss: 0.1115 - accuracy: 0.9615 - val_loss: 0.5214 - val_accuracy: 0.8739\nEpoch 146/200\n240/240 [==============================] - 0s 835us/step - loss: 0.1134 - accuracy: 0.9591 - val_loss: 0.5244 - val_accuracy: 0.8730\nEpoch 147/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1082 - accuracy: 0.9619 - val_loss: 0.5270 - val_accuracy: 0.8724\nEpoch 148/200\n240/240 [==============================] - 0s 839us/step - loss: 0.1061 - accuracy: 0.9629 - val_loss: 0.5361 - val_accuracy: 0.8723\nEpoch 149/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1076 - accuracy: 0.9621 - val_loss: 0.5285 - val_accuracy: 0.8749\nEpoch 150/200\n240/240 [==============================] - 0s 813us/step - loss: 0.1068 - accuracy: 0.9626 - val_loss: 0.5590 - val_accuracy: 0.8675\nEpoch 151/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1065 - accuracy: 0.9623 - val_loss: 0.5348 - val_accuracy: 0.8748\nEpoch 152/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1056 - accuracy: 0.9636 - val_loss: 0.5363 - val_accuracy: 0.8725\nEpoch 153/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1077 - accuracy: 0.9621 - val_loss: 0.5535 - val_accuracy: 0.8683\nEpoch 154/200\n240/240 [==============================] - 0s 814us/step - loss: 0.1030 - accuracy: 0.9642 - val_loss: 0.5398 - val_accuracy: 0.8726\nEpoch 155/200\n240/240 [==============================] - 0s 836us/step - loss: 0.1076 - accuracy: 0.9618 - val_loss: 0.5322 - val_accuracy: 0.8757\nEpoch 156/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1065 - accuracy: 0.9630 - val_loss: 0.5485 - val_accuracy: 0.8730\nEpoch 157/200\n240/240 [==============================] - 0s 814us/step - loss: 0.1054 - accuracy: 0.9634 - val_loss: 0.5554 - val_accuracy: 0.8749\nEpoch 158/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1015 - accuracy: 0.9645 - val_loss: 0.5482 - val_accuracy: 0.8736\nEpoch 159/200\n240/240 [==============================] - 0s 814us/step - loss: 0.1021 - accuracy: 0.9641 - val_loss: 0.5769 - val_accuracy: 0.8652\nEpoch 160/200\n240/240 [==============================] - 0s 824us/step - loss: 0.1002 - accuracy: 0.9651 - val_loss: 0.5563 - val_accuracy: 0.8738\nEpoch 161/200\n240/240 [==============================] - 0s 827us/step - loss: 0.1059 - accuracy: 0.9627 - val_loss: 0.5700 - val_accuracy: 0.8712\nEpoch 162/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1009 - accuracy: 0.9647 - val_loss: 0.5543 - val_accuracy: 0.8725\nEpoch 163/200\n240/240 [==============================] - 0s 835us/step - loss: 0.0978 - accuracy: 0.9666 - val_loss: 0.5470 - val_accuracy: 0.8733\nEpoch 164/200\n240/240 [==============================] - 0s 818us/step - loss: 0.1007 - accuracy: 0.9648 - val_loss: 0.5643 - val_accuracy: 0.8731\nEpoch 165/200\n240/240 [==============================] - 0s 832us/step - loss: 0.1005 - accuracy: 0.9653 - val_loss: 0.5695 - val_accuracy: 0.8735\nEpoch 166/200\n240/240 [==============================] - 0s 826us/step - loss: 0.0998 - accuracy: 0.9650 - val_loss: 0.5811 - val_accuracy: 0.8703\nEpoch 167/200\n240/240 [==============================] - 0s 818us/step - loss: 0.0980 - accuracy: 0.9659 - val_loss: 0.5800 - val_accuracy: 0.8723\nEpoch 168/200\n240/240 [==============================] - 0s 851us/step - loss: 0.0967 - accuracy: 0.9665 - val_loss: 0.5753 - val_accuracy: 0.8741\nEpoch 169/200\n240/240 [==============================] - 0s 824us/step - loss: 0.0948 - accuracy: 0.9667 - val_loss: 0.5742 - val_accuracy: 0.8732\nEpoch 170/200\n240/240 [==============================] - 0s 815us/step - loss: 0.0957 - accuracy: 0.9674 - val_loss: 0.5791 - val_accuracy: 0.8729\nEpoch 171/200\n240/240 [==============================] - 0s 828us/step - loss: 0.0965 - accuracy: 0.9661 - val_loss: 0.5887 - val_accuracy: 0.8707\nEpoch 172/200\n240/240 [==============================] - 0s 831us/step - loss: 0.0956 - accuracy: 0.9674 - val_loss: 0.5799 - val_accuracy: 0.8736\nEpoch 173/200\n240/240 [==============================] - 0s 840us/step - loss: 0.0958 - accuracy: 0.9669 - val_loss: 0.5815 - val_accuracy: 0.8742\nEpoch 174/200\n240/240 [==============================] - 0s 825us/step - loss: 0.0953 - accuracy: 0.9668 - val_loss: 0.5971 - val_accuracy: 0.8684\nEpoch 175/200\n240/240 [==============================] - 0s 829us/step - loss: 0.0952 - accuracy: 0.9662 - val_loss: 0.6041 - val_accuracy: 0.8700\nEpoch 176/200\n240/240 [==============================] - 0s 827us/step - loss: 0.0900 - accuracy: 0.9691 - val_loss: 0.5917 - val_accuracy: 0.8719\nEpoch 177/200\n240/240 [==============================] - 0s 815us/step - loss: 0.0961 - accuracy: 0.9661 - val_loss: 0.5820 - val_accuracy: 0.8724\nEpoch 178/200\n240/240 [==============================] - 0s 823us/step - loss: 0.0928 - accuracy: 0.9685 - val_loss: 0.6020 - val_accuracy: 0.8708\nEpoch 179/200\n240/240 [==============================] - 0s 819us/step - loss: 0.0913 - accuracy: 0.9683 - val_loss: 0.6174 - val_accuracy: 0.8692\nEpoch 180/200\n240/240 [==============================] - 0s 824us/step - loss: 0.0953 - accuracy: 0.9662 - val_loss: 0.5866 - val_accuracy: 0.8705\nEpoch 181/200\n240/240 [==============================] - 0s 816us/step - loss: 0.0928 - accuracy: 0.9679 - val_loss: 0.6090 - val_accuracy: 0.8708\nEpoch 182/200\n240/240 [==============================] - 0s 825us/step - loss: 0.0935 - accuracy: 0.9673 - val_loss: 0.5961 - val_accuracy: 0.8719\nEpoch 183/200\n240/240 [==============================] - 0s 859us/step - loss: 0.0901 - accuracy: 0.9692 - val_loss: 0.6071 - val_accuracy: 0.8687\nEpoch 184/200\n240/240 [==============================] - 0s 818us/step - loss: 0.0890 - accuracy: 0.9694 - val_loss: 0.6067 - val_accuracy: 0.8707\nEpoch 185/200\n240/240 [==============================] - 0s 817us/step - loss: 0.0896 - accuracy: 0.9700 - val_loss: 0.6109 - val_accuracy: 0.8716\nEpoch 186/200\n240/240 [==============================] - 0s 815us/step - loss: 0.0913 - accuracy: 0.9677 - val_loss: 0.6126 - val_accuracy: 0.8727\nEpoch 187/200\n240/240 [==============================] - 0s 824us/step - loss: 0.0943 - accuracy: 0.9665 - val_loss: 0.6081 - val_accuracy: 0.8708\nEpoch 188/200\n240/240 [==============================] - 0s 843us/step - loss: 0.0858 - accuracy: 0.9699 - val_loss: 0.6124 - val_accuracy: 0.8736\nEpoch 189/200\n240/240 [==============================] - 0s 825us/step - loss: 0.0900 - accuracy: 0.9693 - val_loss: 0.6128 - val_accuracy: 0.8703\nEpoch 190/200\n240/240 [==============================] - 0s 833us/step - loss: 0.0860 - accuracy: 0.9699 - val_loss: 0.6175 - val_accuracy: 0.8726\nEpoch 191/200\n240/240 [==============================] - 0s 823us/step - loss: 0.0866 - accuracy: 0.9704 - val_loss: 0.6292 - val_accuracy: 0.8693\nEpoch 192/200\n240/240 [==============================] - 0s 831us/step - loss: 0.0866 - accuracy: 0.9709 - val_loss: 0.6300 - val_accuracy: 0.8700\nEpoch 193/200\n240/240 [==============================] - 0s 832us/step - loss: 0.0849 - accuracy: 0.9705 - val_loss: 0.6230 - val_accuracy: 0.8700\nEpoch 194/200\n240/240 [==============================] - 0s 819us/step - loss: 0.0894 - accuracy: 0.9686 - val_loss: 0.6218 - val_accuracy: 0.8737\nEpoch 195/200\n240/240 [==============================] - 0s 828us/step - loss: 0.0864 - accuracy: 0.9707 - val_loss: 0.6399 - val_accuracy: 0.8713\nEpoch 196/200\n240/240 [==============================] - 0s 815us/step - loss: 0.0855 - accuracy: 0.9704 - val_loss: 0.6343 - val_accuracy: 0.8691\nEpoch 197/200\n240/240 [==============================] - 0s 825us/step - loss: 0.0852 - accuracy: 0.9702 - val_loss: 0.6686 - val_accuracy: 0.8656\nEpoch 198/200\n240/240 [==============================] - 0s 824us/step - loss: 0.0852 - accuracy: 0.9712 - val_loss: 0.6403 - val_accuracy: 0.8706\nEpoch 199/200\n240/240 [==============================] - 0s 826us/step - loss: 0.0833 - accuracy: 0.9719 - val_loss: 0.6442 - val_accuracy: 0.8698\nEpoch 200/200\n240/240 [==============================] - 0s 814us/step - loss: 0.0849 - accuracy: 0.9702 - val_loss: 0.6350 - val_accuracy: 0.8728\n\n\n&lt;keras.callbacks.History at 0x7f37e5e348b0&gt;\n\n\n\n40에폭쯤에서 멈췄었어야 할 것 같은데?\n그래서 나온 개념이 early stopping!\n\n- 텐서보드 여는 방법1\n\n%load_ext tensorboard\n# 주피터노트북 (혹은 주피터랩)에서 텐서보드를 임베딩하여 넣을 수 있도록 도와주는 매직펑션\n\n\n#\n!rm -rf logs # lags 파일 삭제\n# !kill 313799\n!kill 507234 \n\n\n#\n# %tensorboard --logdir logs --host 0.0.0.0\n# %tensorboard --logdir logs # &lt;-- 실습에서는 이렇게 하면됩니다.\n\n\n\ntrain accuray는 점점 증가하는데 validation accuracy는 오히려 지지부진해 지다 점점 떨어진다. \\(\\to\\) 오버피팅의 징조\ntraining loss는 계속 떨어지고 있지만 validation loss는 감소하다 어느 시점이 지나면 오히려 점점 증가한다. \\(\\to\\) 오버피팅의 징조\n\n\n\n\n\n\n\n(참고사항) 파이썬 3.10의 경우 아래의 수정이 필요\n\n\n\n?/python3.10/site-packages/tensorboard/_vendor/html5lib/_trie/_base.py 을 열고\nfrom collections import Mapping ### 수정전\nfrom collections.abc import Mapping ### 수정후\n와 같이 수정한다.\n\n왜냐하면 파이썬 3.10부터 from collections import Mapping 가 동작하지 않고 from collections.abc import Mapping 가 동작하도록 문법이 바뀜\n\n\n\n- 텐서보드를 실행하는 방법2\n\n#\n# !tensorboard --logdir logs --host 0.0.0.0\n# !tensorboard --logdir logs # &lt;-- 실습에서는 이렇게 하면됩니다.\n\n\n\n조기종료\n- 텐서보드를 살펴보니 특정에폭 이후에는 오히려 과적합이 진행되는 듯 하다 (학습할수록 손해인듯 하다) \\(\\to\\) 그 특정에폭까지만 학습해보자\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 40s 164ms/step - loss: 0.5284 - accuracy: 0.8175 - val_loss: 0.4271 - val_accuracy: 0.8418\nEpoch 2/200\n240/240 [==============================] - 40s 165ms/step - loss: 0.3579 - accuracy: 0.8672 - val_loss: 0.3539 - val_accuracy: 0.8719\nEpoch 3/200\n240/240 [==============================] - 40s 165ms/step - loss: 0.3207 - accuracy: 0.8802 - val_loss: 0.3578 - val_accuracy: 0.8716\n\n\n&lt;keras.callbacks.History at 0x7f37e5586550&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 40s 165ms/step - loss: 0.2977 - accuracy: 0.8891 - val_loss: 0.3353 - val_accuracy: 0.8752\nEpoch 2/200\n240/240 [==============================] - 40s 165ms/step - loss: 0.2783 - accuracy: 0.8941 - val_loss: 0.3460 - val_accuracy: 0.8802\n\n\n&lt;keras.callbacks.History at 0x7f37e4c0ca90&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2627 - accuracy: 0.9010 - val_loss: 0.3251 - val_accuracy: 0.8799\nEpoch 2/200\n240/240 [==============================] - 39s 165ms/step - loss: 0.2463 - accuracy: 0.9068 - val_loss: 0.3460 - val_accuracy: 0.8823\n\n\n&lt;keras.callbacks.History at 0x7f37e44f8790&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2339 - accuracy: 0.9104 - val_loss: 0.3190 - val_accuracy: 0.8873\nEpoch 2/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2207 - accuracy: 0.9157 - val_loss: 0.3449 - val_accuracy: 0.8837\n\n\n&lt;keras.callbacks.History at 0x7f37e55660a0&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2111 - accuracy: 0.9188 - val_loss: 0.3703 - val_accuracy: 0.8785\nEpoch 2/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2030 - accuracy: 0.9225 - val_loss: 0.3199 - val_accuracy: 0.8922\nEpoch 3/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2002 - accuracy: 0.9236 - val_loss: 0.3231 - val_accuracy: 0.8881\n\n\n&lt;keras.callbacks.History at 0x7f37e5521a30&gt;\n\n\n- 몇 번 좀 참았다가 멈추면 좋겠다.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=5) # 좀더 참다가 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 40s 167ms/step - loss: 0.5611 - accuracy: 0.8114 - val_loss: 0.4034 - val_accuracy: 0.8518\nEpoch 2/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.3582 - accuracy: 0.8669 - val_loss: 0.3627 - val_accuracy: 0.8688\nEpoch 3/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.3252 - accuracy: 0.8784 - val_loss: 0.3576 - val_accuracy: 0.8723\nEpoch 4/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2966 - accuracy: 0.8887 - val_loss: 0.3566 - val_accuracy: 0.8767\nEpoch 5/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2789 - accuracy: 0.8959 - val_loss: 0.3215 - val_accuracy: 0.8852\nEpoch 6/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2621 - accuracy: 0.9010 - val_loss: 0.3186 - val_accuracy: 0.8844\nEpoch 7/200\n240/240 [==============================] - 40s 167ms/step - loss: 0.2466 - accuracy: 0.9085 - val_loss: 0.3297 - val_accuracy: 0.8852\nEpoch 8/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2350 - accuracy: 0.9107 - val_loss: 0.3187 - val_accuracy: 0.8905\nEpoch 9/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2266 - accuracy: 0.9143 - val_loss: 0.3132 - val_accuracy: 0.8921\nEpoch 10/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2114 - accuracy: 0.9196 - val_loss: 0.3212 - val_accuracy: 0.8938\nEpoch 11/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2059 - accuracy: 0.9206 - val_loss: 0.3292 - val_accuracy: 0.8896\nEpoch 12/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.1969 - accuracy: 0.9247 - val_loss: 0.3230 - val_accuracy: 0.8959\nEpoch 13/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.1868 - accuracy: 0.9274 - val_loss: 0.3289 - val_accuracy: 0.8929\nEpoch 14/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.1807 - accuracy: 0.9306 - val_loss: 0.3586 - val_accuracy: 0.8892\n\n\n&lt;keras.callbacks.History at 0x7f37e44f8d60&gt;\n\n\n- 텐서보드로 그려보자?\n\n#\n# %tensorboard --logdir logs --host 0.0.0.0\n# 아무것도 안나온다 -&gt; 왜? cb1을 써야 텐서보드가 나옴\n\n- 조기종료와 텐서보드를 같이 쓰려면?\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\ncb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=7) # 좀더 참다가 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2]) # callbacks에 cb1,cb2 리스트로 같이 전달.\n\nEpoch 1/200\n240/240 [==============================] - 1s 1ms/step - loss: 0.7273 - accuracy: 0.7593 - val_loss: 0.5007 - val_accuracy: 0.8295\nEpoch 2/200\n240/240 [==============================] - 0s 825us/step - loss: 0.4678 - accuracy: 0.8409 - val_loss: 0.4602 - val_accuracy: 0.8397\nEpoch 3/200\n240/240 [==============================] - 0s 823us/step - loss: 0.4223 - accuracy: 0.8521 - val_loss: 0.4262 - val_accuracy: 0.8525\nEpoch 4/200\n240/240 [==============================] - 0s 832us/step - loss: 0.3984 - accuracy: 0.8594 - val_loss: 0.4001 - val_accuracy: 0.8612\nEpoch 5/200\n240/240 [==============================] - 0s 825us/step - loss: 0.3773 - accuracy: 0.8675 - val_loss: 0.3906 - val_accuracy: 0.8614\nEpoch 6/200\n240/240 [==============================] - 0s 838us/step - loss: 0.3638 - accuracy: 0.8713 - val_loss: 0.3897 - val_accuracy: 0.8604\nEpoch 7/200\n240/240 [==============================] - 0s 833us/step - loss: 0.3516 - accuracy: 0.8749 - val_loss: 0.3808 - val_accuracy: 0.8672\nEpoch 8/200\n240/240 [==============================] - 0s 822us/step - loss: 0.3425 - accuracy: 0.8773 - val_loss: 0.3640 - val_accuracy: 0.8721\nEpoch 9/200\n240/240 [==============================] - 0s 836us/step - loss: 0.3347 - accuracy: 0.8811 - val_loss: 0.3567 - val_accuracy: 0.8739\nEpoch 10/200\n240/240 [==============================] - 0s 836us/step - loss: 0.3249 - accuracy: 0.8834 - val_loss: 0.3585 - val_accuracy: 0.8737\nEpoch 11/200\n240/240 [==============================] - 0s 825us/step - loss: 0.3175 - accuracy: 0.8868 - val_loss: 0.3582 - val_accuracy: 0.8728\nEpoch 12/200\n240/240 [==============================] - 0s 827us/step - loss: 0.3104 - accuracy: 0.8884 - val_loss: 0.3531 - val_accuracy: 0.8734\nEpoch 13/200\n240/240 [==============================] - 0s 819us/step - loss: 0.3047 - accuracy: 0.8909 - val_loss: 0.3513 - val_accuracy: 0.8758\nEpoch 14/200\n240/240 [==============================] - 0s 838us/step - loss: 0.3000 - accuracy: 0.8915 - val_loss: 0.3624 - val_accuracy: 0.8714\nEpoch 15/200\n240/240 [==============================] - 0s 828us/step - loss: 0.2962 - accuracy: 0.8937 - val_loss: 0.3492 - val_accuracy: 0.8781\nEpoch 16/200\n240/240 [==============================] - 0s 828us/step - loss: 0.2948 - accuracy: 0.8934 - val_loss: 0.3793 - val_accuracy: 0.8650\nEpoch 17/200\n240/240 [==============================] - 0s 837us/step - loss: 0.2875 - accuracy: 0.8951 - val_loss: 0.3459 - val_accuracy: 0.8783\nEpoch 18/200\n240/240 [==============================] - 0s 829us/step - loss: 0.2834 - accuracy: 0.8963 - val_loss: 0.3452 - val_accuracy: 0.8775\nEpoch 19/200\n240/240 [==============================] - 0s 843us/step - loss: 0.2788 - accuracy: 0.8977 - val_loss: 0.3482 - val_accuracy: 0.8769\nEpoch 20/200\n240/240 [==============================] - 0s 837us/step - loss: 0.2737 - accuracy: 0.8997 - val_loss: 0.3446 - val_accuracy: 0.8759\nEpoch 21/200\n240/240 [==============================] - 0s 814us/step - loss: 0.2724 - accuracy: 0.9018 - val_loss: 0.3377 - val_accuracy: 0.8797\nEpoch 22/200\n240/240 [==============================] - 0s 839us/step - loss: 0.2666 - accuracy: 0.9033 - val_loss: 0.3370 - val_accuracy: 0.8832\nEpoch 23/200\n240/240 [==============================] - 0s 827us/step - loss: 0.2638 - accuracy: 0.9039 - val_loss: 0.3398 - val_accuracy: 0.8789\nEpoch 24/200\n240/240 [==============================] - 0s 833us/step - loss: 0.2603 - accuracy: 0.9067 - val_loss: 0.3458 - val_accuracy: 0.8812\nEpoch 25/200\n240/240 [==============================] - 0s 831us/step - loss: 0.2549 - accuracy: 0.9072 - val_loss: 0.3407 - val_accuracy: 0.8779\nEpoch 26/200\n240/240 [==============================] - 0s 821us/step - loss: 0.2546 - accuracy: 0.9067 - val_loss: 0.3381 - val_accuracy: 0.8848\nEpoch 27/200\n240/240 [==============================] - 0s 823us/step - loss: 0.2519 - accuracy: 0.9093 - val_loss: 0.3401 - val_accuracy: 0.8806\nEpoch 28/200\n240/240 [==============================] - 0s 818us/step - loss: 0.2461 - accuracy: 0.9112 - val_loss: 0.3469 - val_accuracy: 0.8812\nEpoch 29/200\n240/240 [==============================] - 0s 840us/step - loss: 0.2468 - accuracy: 0.9096 - val_loss: 0.3478 - val_accuracy: 0.8807\n\n\n&lt;keras.callbacks.History at 0x7f37e5426c40&gt;\n\n\n\n#\n# 조기종료가 구현된 그림이 출력\n# %tensorboard --logdir logs --host 0.0.0.0\n\n\n\n\nEarlyStopping을 적용한 결과 텐서보드로 실행한 결과\n\n\n\n# !kill 508556\n# !rm -rf logs\n\n\n\n하이퍼파라메터 선택\n- 하이퍼파라메터 설정\n\nfrom tensorboard.plugins.hparams import api as hp\n\n\na=net.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 368us/step - loss: 0.3726 - accuracy: 0.8690\n\n\n\n!rm -rf logs\nfor u in [50,5000]:\n    for d in [0.0,0.5]:\n        for o in ['adam','sgd']:\n            logdir = 'logs/hpguebin_{}_{}_{}'.format(u,d,o)\n            with tf.summary.create_file_writer(logdir).as_default():\n                net = tf.keras.Sequential()\n                net.add(tf.keras.layers.Flatten())\n                net.add(tf.keras.layers.Dense(u,activation='relu'))\n                net.add(tf.keras.layers.Dropout(d))\n                net.add(tf.keras.layers.Dense(10,activation='softmax'))\n                net.compile(optimizer=o,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])\n                cb3 = hp.KerasCallback(logdir, {'유닛수':u, '드랍아웃비율':d, '옵티마이저':o})\n                net.fit(X,y,epochs=3,callbacks=cb3)\n                _rslt=net.evaluate(XX,yy)  # test accuracy, test recall\n                _mymetric=_rslt[1]*0.8 + _rslt[2]*0.2\n                tf.summary.scalar('애큐러시와리컬의가중평균(테스트셋)', _mymetric, step=1)\n\nEpoch 1/3\n1875/1875 [==============================] - 1s 523us/step - loss: 0.5223 - accuracy: 0.8170 - recall: 0.7559\nEpoch 2/3\n1875/1875 [==============================] - 1s 513us/step - loss: 0.3940 - accuracy: 0.8602 - recall: 0.8296\nEpoch 3/3\n1875/1875 [==============================] - 1s 517us/step - loss: 0.3578 - accuracy: 0.8711 - recall: 0.8455\n313/313 [==============================] - 0s 390us/step - loss: 0.3902 - accuracy: 0.8600 - recall: 0.8347\nEpoch 1/3\n1875/1875 [==============================] - 1s 483us/step - loss: 0.7881 - accuracy: 0.7447 - recall: 0.5665\nEpoch 2/3\n1875/1875 [==============================] - 1s 478us/step - loss: 0.5283 - accuracy: 0.8214 - recall: 0.7521\nEpoch 3/3\n1875/1875 [==============================] - 1s 465us/step - loss: 0.4769 - accuracy: 0.8361 - recall: 0.7843\n313/313 [==============================] - 0s 401us/step - loss: 0.4903 - accuracy: 0.8302 - recall: 0.7817\nEpoch 1/3\n1875/1875 [==============================] - 1s 527us/step - loss: 0.7370 - accuracy: 0.7401 - recall: 0.6217\nEpoch 2/3\n1875/1875 [==============================] - 1s 522us/step - loss: 0.5597 - accuracy: 0.7993 - recall: 0.7249\nEpoch 3/3\n1875/1875 [==============================] - 1s 520us/step - loss: 0.5226 - accuracy: 0.8110 - recall: 0.7480\n313/313 [==============================] - 0s 393us/step - loss: 0.4303 - accuracy: 0.8447 - recall: 0.7962\nEpoch 1/3\n1875/1875 [==============================] - 1s 488us/step - loss: 1.0124 - accuracy: 0.6446 - recall: 0.4268\nEpoch 2/3\n1875/1875 [==============================] - 1s 488us/step - loss: 0.7222 - accuracy: 0.7500 - recall: 0.6124\nEpoch 3/3\n1875/1875 [==============================] - 1s 488us/step - loss: 0.6513 - accuracy: 0.7767 - recall: 0.6656\n313/313 [==============================] - 0s 382us/step - loss: 0.5226 - accuracy: 0.8165 - recall: 0.7442\nEpoch 1/3\n1875/1875 [==============================] - 28s 15ms/step - loss: 0.4779 - accuracy: 0.8285 - recall: 0.7886\nEpoch 2/3\n1875/1875 [==============================] - 28s 15ms/step - loss: 0.3614 - accuracy: 0.8677 - recall: 0.8425\nEpoch 3/3\n1875/1875 [==============================] - 28s 15ms/step - loss: 0.3205 - accuracy: 0.8819 - recall: 0.8609\n313/313 [==============================] - 0s 994us/step - loss: 0.3640 - accuracy: 0.8684 - recall: 0.8475\nEpoch 1/3\n1875/1875 [==============================] - 7s 3ms/step - loss: 0.6689 - accuracy: 0.7886 - recall: 0.6448\nEpoch 2/3\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.4828 - accuracy: 0.8371 - recall: 0.7770\nEpoch 3/3\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.4422 - accuracy: 0.8490 - recall: 0.8011\n313/313 [==============================] - 0s 967us/step - loss: 0.4644 - accuracy: 0.8375 - recall: 0.7946\nEpoch 1/3\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.5713 - accuracy: 0.7975 - recall: 0.7544\nEpoch 2/3\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.4424 - accuracy: 0.8395 - recall: 0.8069\nEpoch 3/3\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.4066 - accuracy: 0.8521 - recall: 0.8217\n313/313 [==============================] - 0s 1ms/step - loss: 0.3829 - accuracy: 0.8602 - recall: 0.8310\nEpoch 1/3\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.6914 - accuracy: 0.7763 - recall: 0.6367\nEpoch 2/3\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.5047 - accuracy: 0.8304 - recall: 0.7671\nEpoch 3/3\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.4622 - accuracy: 0.8413 - recall: 0.7901\n313/313 [==============================] - 0s 985us/step - loss: 0.4639 - accuracy: 0.8398 - recall: 0.7945\n\n\n\n#\n%tensorboard --logdir logs --host 0.0.0.0"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#숙제",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#숙제",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "숙제",
    "text": "숙제\n- 아래의 네트워크에서 옵티마이저를 adam, sgd를 선택하여 각각 적합시켜보고 testset의 loss를 성능비교를 하라. epoch은 5정도로 설정하라.\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer=???,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_03_07_(1주차)_3월7일.html",
    "href": "posts/3_STBDA2022/2022_03_07_(1주차)_3월7일.html",
    "title": "[STBDA] 1wk. 강의소개 및 단순선형회귀",
    "section": "",
    "text": "(1주차) 3월7일\n\n강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-yKGpQh49tnRrA-o2Odea8r\n\n\n\n강의보충자료\n- https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp1.pdf\n- https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp2.pdf\n\n\n로드맵\n- 오늘수업할내용: 단순선형회귀\n- 단순선형회귀를 배우는 이유?\n\n우리가 배우고싶은것: 심층신경망(DNN) \\(\\to\\) 합성곱신경망(CNN) \\(\\to\\) 적대적생성신경망(GAN)\n심층신경망을 바로 이해하기 어려움\n다음의 과정으로 이해해야함: (선형대수학 \\(\\to\\)) 회귀분석 \\(\\to\\) 로지스틱회귀분석 \\(\\to\\) 심층신경망\n\n\n\n선형회귀\n- 상황극 - 나는 동네에 커피점을 하나 차렸음. - 장사를 하다보니까 날이 더울수록 아이스아메리카노의 판매량이 증가한다는 사실을 깨달았다. - 일기예보는 미리 나와있으니까 그 정보를 잘 이용하면 ‘온도 -&gt; 아이스아메리카노 판매량 예측’ 이 가능할것 같다. (내가 앞으로 얼마나 벌지 예측가능)\n- 가짜자료 생성\n\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \n\n온도 \\({\\bf x}\\)가 아래와 같다고 하자.\n\nx=tf.constant([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) # 기온 \nx\n\n&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4],\n      dtype=float32)&gt;\n\n\n아이스아메리카노의 판매량 \\({\\bf y}\\)이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자)\n\\[{\\bf y} \\approx 10.2 +2.2 {\\bf x}\\]\n\n여기에서 10.2, 2.2 의 숫자는 제가 임의로 정한것임\n식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림\n물결의의미: 현실반영. 세상은 꼭 수식대로 정확하게 이루어지지 않음.\n\n\ntf.random.set_seed(43052)\nepsilon=tf.random.normal([10])\ny=10.2 + 2.2*x + epsilon\ny\n\n&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([55.418365, 58.194283, 61.230827, 62.312557, 63.107002, 63.69569 ,\n       67.247055, 71.4365  , 73.1013  , 77.84988 ], dtype=float32)&gt;\n\n\n- 우리는 아래와 같은 자료를 모았다고 생각하자.\n\ntf.transpose(tf.concat([[x],[y]],0))\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[20.1     , 55.418365],\n       [22.2     , 58.194283],\n       [22.7     , 61.230827],\n       [23.3     , 62.312557],\n       [24.4     , 63.107002],\n       [25.1     , 63.69569 ],\n       [26.2     , 67.247055],\n       [27.3     , 71.4365  ],\n       [28.4     , 73.1013  ],\n       [30.4     , 77.84988 ]], dtype=float32)&gt;\n\n\n- 그려보자.\n\nplt.plot(x,y,'.') # 파란점, 관측한 데이터 \nplt.plot(x,10.2 + 2.2*x, '--')  # 주황색점선, 세상의 법칙 \n\n\n\n\n- 우리의 목표: 파란색점 \\(\\to\\) 주황색점선을 추론 // 데이터를 바탕으로 세상의 법칙을 추론\n- 아이디어: 데이터를 보니까 \\(x\\)와 \\(y\\)가 선형의 관계에 있는듯 보인다. 즉 모든 \\(i=1,2,\\dots, 10\\)에 대하여 아래를 만족하는 적당한 a,b (혹은 \\(\\beta_0,\\beta_1\\)) 가 존재할것 같다. - \\(y_{i} \\approx ax_{i}+b\\) - \\(y_{i} \\approx \\beta_1 x_{i}+\\beta_0\\)\n- 어림짐작으로 \\(a,b\\)를 알아내보자.\n데이터를 살펴보자.\n\ntf.transpose(tf.concat([[x],[y]],0))\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[20.1     , 55.418365],\n       [22.2     , 58.194283],\n       [22.7     , 61.230827],\n       [23.3     , 62.312557],\n       [24.4     , 63.107002],\n       [25.1     , 63.69569 ],\n       [26.2     , 67.247055],\n       [27.3     , 71.4365  ],\n       [28.4     , 73.1013  ],\n       [30.4     , 77.84988 ]], dtype=float32)&gt;\n\n\n적당히 왼쪽*2+15 = 오른쪽의 관계가 성립하는것 같다.\n따라서 \\(a=2, b=15\\) 혹은 \\(\\beta_0=15, \\beta_1=2\\) 로 추론할 수 있겠다.\n- 누군가가 \\((\\beta_0,\\beta_1)=(14,2)\\) 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까)\n- 새로운 주장으로 인해서 \\((\\beta_0,\\beta_1)=(15,2)\\) 로 볼 수도 있고 \\((\\beta_0,\\beta_1)=(14,2)\\) 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? - 후보1: \\((\\beta_0,\\beta_1)=(15,2)\\) - 후보2: \\((\\beta_0,\\beta_1)=(14,2)\\)\n- 가능한 \\(y_i \\approx \\beta_0 + \\beta_1 x_i\\) 이 되도록 만드는 \\((\\beta_0,\\beta_1)\\) 이 좋을 것이다. \\(\\to\\) 후보 1,2를 비교해보자.\n(관찰에 의한 비교)\n후보1에 대해서 \\(i=1,2\\)를 넣고 관찰하여 보자.\n\n20.1 * 2 + 15 , 55.418365 # i=1 \n\n(55.2, 55.418365)\n\n\n\n22.2 * 2 + 15 , 58.194283 # i=2\n\n(59.4, 58.194283)\n\n\n후보2에 대하여 \\(i=1,2\\)를 넣고 관찰하여 보자.\n\n20.1 * 2 + 14 , 55.418365 # i=1 \n\n(54.2, 55.418365)\n\n\n\n22.2 * 2 + 14 , 58.194283 # i=2\n\n(58.4, 58.194283)\n\n\n\\(i=1\\)인 경우에는 후보1이 더 잘맞는것 같은데 \\(i=2\\)인 경우는 후보2가 더 잘맞는것 같다.\n(좀 더 체계적인 비교)\n\\(i=1,2,3, \\dots, 10\\) 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자.\n후보 1,2에 대하여 \\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)를 계산하여 비교해보자.\n\nsum1=0 \nfor i in range(10):\n    sum1=sum1+(y[i]-15-2*x[i])**2 \n\n\nsum2=0 \nfor i in range(10):\n    sum2=sum2+(y[i]-14-2*x[i])**2 \n\n\nsum1,sum2\n\n(&lt;tf.Tensor: shape=(), dtype=float32, numpy=14.734169&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=31.521088&gt;)\n\n\n후보1이 더 \\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)의 값이 작다.\n후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다.\n- 그런데 이 알고리즘은 현실적으로 구현이 불가능하다. (무한번 계산하기도 힘들고, 언제 멈출지도 애매함)\n- 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 \\(\\beta_0,\\beta_1\\)을 찾으면 된다.\n\\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)\n그런데 결국 \\(\\beta_0, \\beta_1\\)에 대한 이차식인데 이 식을 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하기 위해서는 아래를 연립하여 풀면된다.\n\\(\\begin{cases} \\frac{\\partial}{\\partial \\beta_0}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0 \\\\ \\frac{\\partial}{\\partial \\beta_1}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0 \\end{cases}\\)\n- 풀어보자.\n\\(\\begin{cases} \\sum_{i=1}^{10} -2(y_i -\\beta_0 -\\beta_1 x_i)=0 \\\\ \\sum_{i=1}^{10} -2x_i(y_i -\\beta_0 -\\beta_1 x_i)=0 \\end{cases}\\)\n정리하면\n\\[\\hat{\\beta}_0= \\bar{y}-\\hat{\\beta}_1 \\bar{x}\\]\n\\[\\hat{\\beta}_1= \\frac{S_{xy}}{S_{xx}}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\]\n- 따라서 최적의 추정치 \\((\\hat{\\beta}_0,\\hat{\\beta}_1)\\)를 이용한 추세선을 아래와 같이 계산할 수 있음.\n\nSxx= sum((x-sum(x)/10)**2)\nSxx\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=87.848976&gt;\n\n\n\nSxy=  sum((x-sum(x)/10)*(y-sum(y)/10))\nSxy\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=194.64737&gt;\n\n\n\nbeta1_estimated = Sxy/Sxx \nbeta1_estimated\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157044&gt;\n\n\n\nbeta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 \nbeta0_estimated\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.944572&gt;\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,beta0_estimated + beta1_estimated * x, '--') # 주황색선: 세상의 법칙을 추정한선 \nplt.plot(x,10.2 + 2.2* x, '--') # 초록색선: ture, 세상의법칙 \n\n\n\n\n\nNote: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다.\n\n- 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다.\n\n공식이 좀 복잡함..\n\\(x\\)가 여러개일 경우 확장이 어려움\n\n- 단점을 극복하기 위해서 우리가 지금까지 했던논의를 매트릭스로 바꾸어서 다시 써보자.\n- 모형의 매트릭스화\n우리의 모형은 아래와 같다.\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10\\)\n풀어서 쓰면\n\\(\\begin{cases} y_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ y_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \\dots \\\\ y_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \\end{cases}\\)\n아래와 같이 쓸 수 있다.\n$\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\dots \\\\\ny_{10}\n\\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\dots & \\dots \\\\\n1 & x_{10}\n\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\]\n\n\\[\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\dots \\\\\n\\epsilon_{10}\n\\end{bmatrix}\\]\n$\n\n벡터와 매트릭스 형태로 정리하면\n\\({\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}\\)\n- 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\)\n이것을 벡터표현으로 하면 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\)\n풀어보면\n\\(loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n- 미분하는 과정의 매트릭스화\nloss를 최소화하는 \\({\\boldsymbol \\beta}\\)를 구해야하므로 loss를 \\({\\boldsymbol \\beta}\\)로 미분한식을 0이라고 놓고 풀면 된다.\n\\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n$= 0 - {}^- {}^ + 2{}^ $\n따라서 \\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0\\)을 풀면 아래와 같다.\n$= ({}){-1}{}^ $\n- 공식도 매트릭스로 표현하면: $= ({}){-1}{}^ $ &lt;– 외우세요\n- 적용을 해보자.\n(X를 만드는 방법1)\n\nX=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # \nX\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)&gt;\n\n\n(X를 만드는 방법2)\n\nfrom tensorflow.python.ops.numpy_ops import np_config\nnp_config.enable_numpy_behavior()\n\n\nX=tf.concat([[[1.0]*10],[x]],0).T\nX\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)&gt;\n\n\n\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.944702, 2.215706], dtype=float32)&gt;\n\n\n- 잘 구해진다.\n- 그런데..\n\nbeta0_estimated,beta1_estimated\n\n(&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.944572&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157044&gt;)\n\n\n값이 좀 다르다..?\n- 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다.\n\nimport tensorflow.experimental.numpy as tnp \n\n\nx=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) \ny=10.2 + 2.2*x + epsilon \n\n\nbeta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2)\nbeta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 \n\n\nbeta0_estimated, beta1_estimated\n\n(&lt;tf.Tensor: shape=(), dtype=float64, numpy=9.944573294798559&gt;,\n &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.2157046054834106&gt;)\n\n\n\nX=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457329, 2.21570461])&gt;\n\n\n\n\n앞으로 할것\n- 선형대수학의 미분이론..\n- 실습 (tensorflow에서 매트릭스를 자유롭게 다루기!)"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-25-DL-4wk.html",
    "href": "posts/4_DL2023/2023-09-25-DL-4wk.html",
    "title": "[DL] 4wk. Numerical Computation and Machine Learning Basics",
    "section": "",
    "text": "1. Overflow and Underflow\n\nBasic limit: Computers use only finite bytes in calculating numerical values in various operations (rounding error/quantization).\nUnderflow: Nearly 0 becomes 0 (log operation can fail).\nOverflow: Going to infinite."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-i",
    "href": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-i",
    "title": "[DL] 4wk. Numerical Computation and Machine Learning Basics",
    "section": "",
    "text": "1. Overflow and Underflow\n\nBasic limit: Computers use only finite bytes in calculating numerical values in various operations (rounding error/quantization).\nUnderflow: Nearly 0 becomes 0 (log operation can fail).\nOverflow: Going to infinite."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xii",
    "href": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xii",
    "title": "[DL] 4wk. Numerical Computation and Machine Learning Basics",
    "section": "Numerical Computation XII",
    "text": "Numerical Computation XII\n\nLocal minimum\n\n\\(u^\\top H u\\) is positive for all \\(u\\) (positive definite \\(H\\)).\n\nLocal maximum\n\n\\(u^\\top H u\\) is negative for all \\(u\\) (negative definite \\(H\\)).\n\nSaddle point\n\n\\(u^\\top H u\\) is negative/positive for some \\(u\\).\n\n\n\n\n\nFigure4: Example of a saddle point."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xiii",
    "href": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xiii",
    "title": "[DL] 4wk. Numerical Computation and Machine Learning Basics",
    "section": "Numerical Computation XIII",
    "text": "Numerical Computation XIII\n\nNote that the change of gradient w.r.t. each coordinate can be different.\nNewton’s method (second order approximation).\n\n\\[f(x) \\approx f(x_0) + (x-x_0)^\\top \\nabla_x f(x_0) + (x-x_0)^\\top H(f)(x_0)(x-x_0)\\]\n\\[x^* = x_0 - H(f)(x_0)^{-1}\\nabla_x f(x_0)\\]\nHessian이 크다? 곡률이크다\n곡률이 크면 천천히 움직이고, 곡률이 작으면 빠르게 움직인다.\nOptimal하게 움직일 수 있게 해준다."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xiv",
    "href": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xiv",
    "title": "[DL] 4wk. Numerical Computation and Machine Learning Basics",
    "section": "Numerical Computation XIV",
    "text": "Numerical Computation XIV\n\n\n\nFigure 5\n\n\n\nHessian을 쓰면 지그재그로 안가고 직선으로 갈 수 있다.\n\n- Restriction on a function \\(f\\), Lipschitz continuous.\n\\[\\forall x,y |f(x)-f(y)| \\leq \\cal{L}||x-y||.\\]"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xv",
    "href": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xv",
    "title": "[DL] 4wk. Numerical Computation and Machine Learning Basics",
    "section": "Numerical Computation XV",
    "text": "Numerical Computation XV\nIdeas from the analysis of convex optimization algorithms can be helpful in proving the convergence of deep learning algorithms, but in general, the importance of convex optimization is greatly diminished in the context of deep learning."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xvi",
    "href": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xvi",
    "title": "[DL] 4wk. Numerical Computation and Machine Learning Basics",
    "section": "Numerical Computation XVI",
    "text": "Numerical Computation XVI\n4. Constrained Optimization\n\\[\\arg \\min_x f(x) \\text{ subject to } x \\in \\cal{S}.\\]\n\nHere, points that lie within the set \\(\\cal{S}\\) are called feasible points.\nSimple approach: projection into \\(\\cal{S}\\) when \\(x = x − \\epsilon ∇x f(x).\\)\nStructured approach: \\(f(x, y) = f (\\theta)\\) where \\(x = \\sin(\\theta)\\), \\(y = \\cos(\\theta)\\). Searching minimizing with a constraint \\(\\sqrt{x^2 + y^2}=1\\)"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xvii",
    "href": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xvii",
    "title": "[DL] 4wk. Numerical Computation and Machine Learning Basics",
    "section": "Numerical Computation XVII",
    "text": "Numerical Computation XVII\n\nKarush–Kuhn–Tucker(KKT) approach provides a very general solution to constrained optimization\nsetup\n\n\n\\(\\cal S = \\{\\forall_i, g(x)=0, \\forall j, h(x) \\leq 0 \\}\\)\nObject: \\(\\min_{x\\in {\\cal S}} f(x).\\)\nGeneralized Lagrangian.\n\n\\[L(x, \\lambda, \\alpha) = f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^n \\alpha_j h_j(x), \\quad \\text{  where  } \\lambda_i &gt; 0 \\text{  and  } \\alpha_j \\geq 0 \\]"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xviii",
    "href": "posts/4_DL2023/2023-09-25-DL-4wk.html#numerical-computation-xviii",
    "title": "[DL] 4wk. Numerical Computation and Machine Learning Basics",
    "section": "Numerical Computation XVIII",
    "text": "Numerical Computation XVIII\n\nGoal: \\(\\min_{x\\in \\cal{S}} f(x) = \\min_x \\max_{\\lambda} \\max_{\\alpha\\geq 0}L(x,\\lambda, \\alpha).\\)\n\n\n강의록 읽어보고 생각정리 및 질문 생각해오기. (토론)\n\n\n슬라이드에 있는 머신러닝 Basic과 연관이 되어있으면 좋음…"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-18-DL-3wk.html",
    "href": "posts/4_DL2023/2023-09-18-DL-3wk.html",
    "title": "[DL] 3wk. Applied Math and Machine Learning Basics (2)",
    "section": "",
    "text": "- 독립 : 공간을 줄여도 차지하는 비중은 달라지지 않는다.\n\n\n\n예를들어 A공간은 전체공간의 반을 차지하고 있는데, 이를 B공간으로 옮기다고 해도 A가 차지하는 비중은 전체B공간의 반이다.\n\n\n- Covariance: 선형관계가 얼마나 강한지?"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-18-DL-3wk.html#probability",
    "href": "posts/4_DL2023/2023-09-18-DL-3wk.html#probability",
    "title": "[DL] 3wk. Applied Math and Machine Learning Basics (2)",
    "section": "",
    "text": "- 독립 : 공간을 줄여도 차지하는 비중은 달라지지 않는다.\n\n\n\n예를들어 A공간은 전체공간의 반을 차지하고 있는데, 이를 B공간으로 옮기다고 해도 A가 차지하는 비중은 전체B공간의 반이다.\n\n\n- Covariance: 선형관계가 얼마나 강한지?"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "",
    "text": "판다스– 인덱싱(2), 판다스–새로운열의할당(1), 아이스크림을 많이 먹으면 걸리는 병(1)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#데이터",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#데이터",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "데이터",
    "text": "데이터\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/movie.csv')\ndf\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nnum_critic_for_reviews\nduration\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\ngross\ngenres\n...\nnum_user_for_reviews\nlanguage\ncountry\ncontent_rating\nbudget\ntitle_year\nactor_2_facebook_likes\nimdb_score\naspect_ratio\nmovie_facebook_likes\n\n\n\n\n0\nColor\nJames Cameron\n723.0\n178.0\n0.0\n855.0\nJoel David Moore\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\n...\n3054.0\nEnglish\nUSA\nPG-13\n237000000.0\n2009.0\n936.0\n7.9\n1.78\n33000\n\n\n1\nColor\nGore Verbinski\n302.0\n169.0\n563.0\n1000.0\nOrlando Bloom\n40000.0\n309404152.0\nAction|Adventure|Fantasy\n...\n1238.0\nEnglish\nUSA\nPG-13\n300000000.0\n2007.0\n5000.0\n7.1\n2.35\n0\n\n\n2\nColor\nSam Mendes\n602.0\n148.0\n0.0\n161.0\nRory Kinnear\n11000.0\n200074175.0\nAction|Adventure|Thriller\n...\n994.0\nEnglish\nUK\nPG-13\n245000000.0\n2015.0\n393.0\n6.8\n2.35\n85000\n\n\n3\nColor\nChristopher Nolan\n813.0\n164.0\n22000.0\n23000.0\nChristian Bale\n27000.0\n448130642.0\nAction|Thriller\n...\n2701.0\nEnglish\nUSA\nPG-13\n250000000.0\n2012.0\n23000.0\n8.5\n2.35\n164000\n\n\n4\nNaN\nDoug Walker\nNaN\nNaN\n131.0\nNaN\nRob Walker\n131.0\nNaN\nDocumentary\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n7.1\nNaN\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\nColor\nScott Smith\n1.0\n87.0\n2.0\n318.0\nDaphne Zuniga\n637.0\nNaN\nComedy|Drama\n...\n6.0\nEnglish\nCanada\nNaN\nNaN\n2013.0\n470.0\n7.7\nNaN\n84\n\n\n4912\nColor\nNaN\n43.0\n43.0\nNaN\n319.0\nValorie Curry\n841.0\nNaN\nCrime|Drama|Mystery|Thriller\n...\n359.0\nEnglish\nUSA\nTV-14\nNaN\nNaN\n593.0\n7.5\n16.00\n32000\n\n\n4913\nColor\nBenjamin Roberds\n13.0\n76.0\n0.0\n0.0\nMaxwell Moody\n0.0\nNaN\nDrama|Horror|Thriller\n...\n3.0\nEnglish\nUSA\nNaN\n1400.0\n2013.0\n0.0\n6.3\nNaN\n16\n\n\n4914\nColor\nDaniel Hsia\n14.0\n100.0\n0.0\n489.0\nDaniel Henney\n946.0\n10443.0\nComedy|Drama|Romance\n...\n9.0\nEnglish\nUSA\nPG-13\nNaN\n2012.0\n719.0\n6.3\n2.35\n660\n\n\n4915\nColor\nJon Gunn\n43.0\n90.0\n16.0\n16.0\nBrian Herzlinger\n86.0\n85222.0\nDocumentary\n...\n84.0\nEnglish\nUSA\nPG\n1100.0\n2004.0\n23.0\n6.6\n1.85\n456\n\n\n\n\n4916 rows × 28 columns\n\n\n\n- 열의 이름을 출력하여 보자.\n\ndf.columns\n\nIndex(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n      dtype='object')\n\n\n\ndf.keys()\n\nIndex(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n      dtype='object')"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#기본인덱싱-df-인덱싱공부-1단계-내용",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#기본인덱싱-df-인덱싱공부-1단계-내용",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "기본인덱싱 (df 인덱싱공부 1단계 내용)",
    "text": "기본인덱싱 (df 인덱싱공부 1단계 내용)\n- color ~ num_voted_user 를 뽑고 + aspect_ratio 도 추가적으로 뽑고싶다. -&gt; loc으로는 못하겠어요..\n\ndf.loc[:,['color':'num_voted_users','aspect_ratio']] # 이건 안됨.\n\nSyntaxError: invalid syntax (&lt;ipython-input-7-0b4cb2e2977c&gt;, line 1)\n\n\n\ndf.loc[:,'color':'num_voted_users'].head(2) # 이건 잘 됨.\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nnum_critic_for_reviews\nduration\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\ngross\ngenres\nactor_1_name\nmovie_title\nnum_voted_users\n\n\n\n\n0\nColor\nJames Cameron\n723.0\n178.0\n0.0\n855.0\nJoel David Moore\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\nCCH Pounder\nAvatar\n886204\n\n\n1\nColor\nGore Verbinski\n302.0\n169.0\n563.0\n1000.0\nOrlando Bloom\n40000.0\n309404152.0\nAction|Adventure|Fantasy\nJohnny Depp\nPirates of the Caribbean: At World's End\n471220\n\n\n\n\n\n\n\n- (팁) 복잡한 조건은 iloc으로 쓰는게 편할때가 있다. \\(\\to\\) 그런데 df.columns 변수들이 몇번인지 알아보기 힘듬 \\(\\to\\) 아래와 같이 하면 열의 이름을 인덱스와 함께 출력할 수 있음\n\npd.Series(df.columns) ## 제일 편함.\n\n0                         color\n1                 director_name\n2        num_critic_for_reviews\n3                      duration\n4       director_facebook_likes\n5        actor_3_facebook_likes\n6                  actor_2_name\n7        actor_1_facebook_likes\n8                         gross\n9                        genres\n10                 actor_1_name\n11                  movie_title\n12              num_voted_users\n13    cast_total_facebook_likes\n14                 actor_3_name\n15         facenumber_in_poster\n16                plot_keywords\n17              movie_imdb_link\n18         num_user_for_reviews\n19                     language\n20                      country\n21               content_rating\n22                       budget\n23                   title_year\n24       actor_2_facebook_likes\n25                   imdb_score\n26                 aspect_ratio\n27         movie_facebook_likes\ndtype: object\n\n\n\nlist(range(13))+[26]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 26]\n\n\n\ndf.iloc[:,list(range(13))+[26]].head(2)\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nnum_critic_for_reviews\nduration\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\ngross\ngenres\nactor_1_name\nmovie_title\nnum_voted_users\naspect_ratio\n\n\n\n\n0\nColor\nJames Cameron\n723.0\n178.0\n0.0\n855.0\nJoel David Moore\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\nCCH Pounder\nAvatar\n886204\n1.78\n\n\n1\nColor\nGore Verbinski\n302.0\n169.0\n563.0\n1000.0\nOrlando Bloom\n40000.0\n309404152.0\nAction|Adventure|Fantasy\nJohnny Depp\nPirates of the Caribbean: At World's End\n471220\n2.35"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#actor라는-단어가-포함된-column-선택",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#actor라는-단어가-포함된-column-선택",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "actor라는 단어가 포함된 column 선택",
    "text": "actor라는 단어가 포함된 column 선택\n- 다시 열의 이름들을 확인\n\ndf.columns\n\nIndex(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n      dtype='object')\n\n\n\n- 방법1\n\n'actor' in 'actor_1_facebook_likes'\n\nTrue\n\n\n\n# list(map(lambda x: 'actor' in x, df.columns))\n\n\ndf.columns[list(map(lambda x: 'actor' in x, df.columns))]\n\nIndex(['actor_3_facebook_likes', 'actor_2_name', 'actor_1_facebook_likes',\n       'actor_1_name', 'actor_3_name', 'actor_2_facebook_likes'],\n      dtype='object')\n\n\n\ndf.iloc[:,list(map(lambda x : 'actor' in x, df.columns) )]\n\n\n\n\n\n\n\n\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\nactor_1_name\nactor_3_name\nactor_2_facebook_likes\n\n\n\n\n0\n855.0\nJoel David Moore\n1000.0\nCCH Pounder\nWes Studi\n936.0\n\n\n1\n1000.0\nOrlando Bloom\n40000.0\nJohnny Depp\nJack Davenport\n5000.0\n\n\n2\n161.0\nRory Kinnear\n11000.0\nChristoph Waltz\nStephanie Sigman\n393.0\n\n\n3\n23000.0\nChristian Bale\n27000.0\nTom Hardy\nJoseph Gordon-Levitt\n23000.0\n\n\n4\nNaN\nRob Walker\n131.0\nDoug Walker\nNaN\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n318.0\nDaphne Zuniga\n637.0\nEric Mabius\nCrystal Lowe\n470.0\n\n\n4912\n319.0\nValorie Curry\n841.0\nNatalie Zea\nSam Underwood\n593.0\n\n\n4913\n0.0\nMaxwell Moody\n0.0\nEva Boehnke\nDavid Chandler\n0.0\n\n\n4914\n489.0\nDaniel Henney\n946.0\nAlan Ruck\nEliza Coupe\n719.0\n\n\n4915\n16.0\nBrian Herzlinger\n86.0\nJohn August\nJon Gunn\n23.0\n\n\n\n\n4916 rows × 6 columns\n\n\n\n\n\n- 방법2\n\ndf.loc[:,list(map(lambda x : 'actor' in x, df.columns) )]\n\n\n\n\n\n\n\n\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\nactor_1_name\nactor_3_name\nactor_2_facebook_likes\n\n\n\n\n0\n855.0\nJoel David Moore\n1000.0\nCCH Pounder\nWes Studi\n936.0\n\n\n1\n1000.0\nOrlando Bloom\n40000.0\nJohnny Depp\nJack Davenport\n5000.0\n\n\n2\n161.0\nRory Kinnear\n11000.0\nChristoph Waltz\nStephanie Sigman\n393.0\n\n\n3\n23000.0\nChristian Bale\n27000.0\nTom Hardy\nJoseph Gordon-Levitt\n23000.0\n\n\n4\nNaN\nRob Walker\n131.0\nDoug Walker\nNaN\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n318.0\nDaphne Zuniga\n637.0\nEric Mabius\nCrystal Lowe\n470.0\n\n\n4912\n319.0\nValorie Curry\n841.0\nNatalie Zea\nSam Underwood\n593.0\n\n\n4913\n0.0\nMaxwell Moody\n0.0\nEva Boehnke\nDavid Chandler\n0.0\n\n\n4914\n489.0\nDaniel Henney\n946.0\nAlan Ruck\nEliza Coupe\n719.0\n\n\n4915\n16.0\nBrian Herzlinger\n86.0\nJohn August\nJon Gunn\n23.0\n\n\n\n\n4916 rows × 6 columns\n\n\n\n\n\n- 방법3\n\ndf.iloc[:,map(lambda x : 'actor' in x, df.columns)]\n\n\n\n\n\n\n\n\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\nactor_1_name\nactor_3_name\nactor_2_facebook_likes\n\n\n\n\n0\n855.0\nJoel David Moore\n1000.0\nCCH Pounder\nWes Studi\n936.0\n\n\n1\n1000.0\nOrlando Bloom\n40000.0\nJohnny Depp\nJack Davenport\n5000.0\n\n\n2\n161.0\nRory Kinnear\n11000.0\nChristoph Waltz\nStephanie Sigman\n393.0\n\n\n3\n23000.0\nChristian Bale\n27000.0\nTom Hardy\nJoseph Gordon-Levitt\n23000.0\n\n\n4\nNaN\nRob Walker\n131.0\nDoug Walker\nNaN\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n318.0\nDaphne Zuniga\n637.0\nEric Mabius\nCrystal Lowe\n470.0\n\n\n4912\n319.0\nValorie Curry\n841.0\nNatalie Zea\nSam Underwood\n593.0\n\n\n4913\n0.0\nMaxwell Moody\n0.0\nEva Boehnke\nDavid Chandler\n0.0\n\n\n4914\n489.0\nDaniel Henney\n946.0\nAlan Ruck\nEliza Coupe\n719.0\n\n\n4915\n16.0\nBrian Herzlinger\n86.0\nJohn August\nJon Gunn\n23.0\n\n\n\n\n4916 rows × 6 columns\n\n\n\n\n\n- 방법4\n\ndf.loc[:,map(lambda x : 'actor' in x, df.columns)]\n\n\n\n\n\n\n\n\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\nactor_1_name\nactor_3_name\nactor_2_facebook_likes\n\n\n\n\n0\n855.0\nJoel David Moore\n1000.0\nCCH Pounder\nWes Studi\n936.0\n\n\n1\n1000.0\nOrlando Bloom\n40000.0\nJohnny Depp\nJack Davenport\n5000.0\n\n\n2\n161.0\nRory Kinnear\n11000.0\nChristoph Waltz\nStephanie Sigman\n393.0\n\n\n3\n23000.0\nChristian Bale\n27000.0\nTom Hardy\nJoseph Gordon-Levitt\n23000.0\n\n\n4\nNaN\nRob Walker\n131.0\nDoug Walker\nNaN\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n318.0\nDaphne Zuniga\n637.0\nEric Mabius\nCrystal Lowe\n470.0\n\n\n4912\n319.0\nValorie Curry\n841.0\nNatalie Zea\nSam Underwood\n593.0\n\n\n4913\n0.0\nMaxwell Moody\n0.0\nEva Boehnke\nDavid Chandler\n0.0\n\n\n4914\n489.0\nDaniel Henney\n946.0\nAlan Ruck\nEliza Coupe\n719.0\n\n\n4915\n16.0\nBrian Herzlinger\n86.0\nJohn August\nJon Gunn\n23.0\n\n\n\n\n4916 rows × 6 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#s로-끝나는-column-선택",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#s로-끝나는-column-선택",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "s로 끝나는 column 선택",
    "text": "s로 끝나는 column 선택\n\n## 참고 (iterable object -&gt; for문 안에 넣어서 돌릴 수 있다.)\n_map = df.loc[:,map(lambda x: x[-1] == 's', df.columns)]\nset(dir(_map)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\n_gen = iter(_map)\n\n\n_gen.__next__()\n\n'num_critic_for_reviews'\n\n\n\n_gen.__next__()\n\n'director_facebook_likes'\n\n\n- 방법1\n\ndf.iloc[:,map(lambda x: 's' == x[-1],df.columns )]\n\n\n\n\n\n\n\n\nnum_critic_for_reviews\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_1_facebook_likes\ngross\ngenres\nnum_voted_users\ncast_total_facebook_likes\nplot_keywords\nnum_user_for_reviews\nactor_2_facebook_likes\nmovie_facebook_likes\n\n\n\n\n0\n723.0\n0.0\n855.0\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\n886204\n4834\navatar|future|marine|native|paraplegic\n3054.0\n936.0\n33000\n\n\n1\n302.0\n563.0\n1000.0\n40000.0\n309404152.0\nAction|Adventure|Fantasy\n471220\n48350\ngoddess|marriage ceremony|marriage proposal|pi...\n1238.0\n5000.0\n0\n\n\n2\n602.0\n0.0\n161.0\n11000.0\n200074175.0\nAction|Adventure|Thriller\n275868\n11700\nbomb|espionage|sequel|spy|terrorist\n994.0\n393.0\n85000\n\n\n3\n813.0\n22000.0\n23000.0\n27000.0\n448130642.0\nAction|Thriller\n1144337\n106759\ndeception|imprisonment|lawlessness|police offi...\n2701.0\n23000.0\n164000\n\n\n4\nNaN\n131.0\nNaN\n131.0\nNaN\nDocumentary\n8\n143\nNaN\nNaN\n12.0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n1.0\n2.0\n318.0\n637.0\nNaN\nComedy|Drama\n629\n2283\nfraud|postal worker|prison|theft|trial\n6.0\n470.0\n84\n\n\n4912\n43.0\nNaN\n319.0\n841.0\nNaN\nCrime|Drama|Mystery|Thriller\n73839\n1753\ncult|fbi|hideout|prison escape|serial killer\n359.0\n593.0\n32000\n\n\n4913\n13.0\n0.0\n0.0\n0.0\nNaN\nDrama|Horror|Thriller\n38\n0\nNaN\n3.0\n0.0\n16\n\n\n4914\n14.0\n0.0\n489.0\n946.0\n10443.0\nComedy|Drama|Romance\n1255\n2386\nNaN\n9.0\n719.0\n660\n\n\n4915\n43.0\n16.0\n16.0\n86.0\n85222.0\nDocumentary\n4285\n163\nactress name in title|crush|date|four word tit...\n84.0\n23.0\n456\n\n\n\n\n4916 rows × 12 columns\n\n\n\n- 방법2\n\ndf.loc[:,map(lambda x: 's' == x[-1],df.columns )]\n\n\n\n\n\n\n\n\nnum_critic_for_reviews\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_1_facebook_likes\ngross\ngenres\nnum_voted_users\ncast_total_facebook_likes\nplot_keywords\nnum_user_for_reviews\nactor_2_facebook_likes\nmovie_facebook_likes\n\n\n\n\n0\n723.0\n0.0\n855.0\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\n886204\n4834\navatar|future|marine|native|paraplegic\n3054.0\n936.0\n33000\n\n\n1\n302.0\n563.0\n1000.0\n40000.0\n309404152.0\nAction|Adventure|Fantasy\n471220\n48350\ngoddess|marriage ceremony|marriage proposal|pi...\n1238.0\n5000.0\n0\n\n\n2\n602.0\n0.0\n161.0\n11000.0\n200074175.0\nAction|Adventure|Thriller\n275868\n11700\nbomb|espionage|sequel|spy|terrorist\n994.0\n393.0\n85000\n\n\n3\n813.0\n22000.0\n23000.0\n27000.0\n448130642.0\nAction|Thriller\n1144337\n106759\ndeception|imprisonment|lawlessness|police offi...\n2701.0\n23000.0\n164000\n\n\n4\nNaN\n131.0\nNaN\n131.0\nNaN\nDocumentary\n8\n143\nNaN\nNaN\n12.0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n1.0\n2.0\n318.0\n637.0\nNaN\nComedy|Drama\n629\n2283\nfraud|postal worker|prison|theft|trial\n6.0\n470.0\n84\n\n\n4912\n43.0\nNaN\n319.0\n841.0\nNaN\nCrime|Drama|Mystery|Thriller\n73839\n1753\ncult|fbi|hideout|prison escape|serial killer\n359.0\n593.0\n32000\n\n\n4913\n13.0\n0.0\n0.0\n0.0\nNaN\nDrama|Horror|Thriller\n38\n0\nNaN\n3.0\n0.0\n16\n\n\n4914\n14.0\n0.0\n489.0\n946.0\n10443.0\nComedy|Drama|Romance\n1255\n2386\nNaN\n9.0\n719.0\n660\n\n\n4915\n43.0\n16.0\n16.0\n86.0\n85222.0\nDocumentary\n4285\n163\nactress name in title|crush|date|four word tit...\n84.0\n23.0\n456\n\n\n\n\n4916 rows × 12 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#c-혹은-d로-시작하는-column-선택",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#c-혹은-d로-시작하는-column-선택",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "c 혹은 d로 시작하는 column 선택",
    "text": "c 혹은 d로 시작하는 column 선택\n\ndf.columns\n\nIndex(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n      dtype='object')\n\n\n\n_str = 'color'\n(_str[0] == 'c') or (_str[0] =='d')\n\nTrue\n\n\n\nlist(map(lambda x: (x[0] == 'c') or (x[0] =='d'), df.columns)) # list안해도 이미 iterable object.\n\n[True,\n True,\n False,\n True,\n True,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n True,\n False,\n False,\n False,\n False,\n False,\n False,\n True,\n True,\n False,\n False,\n False,\n False,\n False,\n False]\n\n\n- 방법1\n\ndf.iloc[:,map(lambda x: 'c' == x[0] or 'd' == x[0] ,df.columns )]\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nduration\ndirector_facebook_likes\ncast_total_facebook_likes\ncountry\ncontent_rating\n\n\n\n\n0\nColor\nJames Cameron\n178.0\n0.0\n4834\nUSA\nPG-13\n\n\n1\nColor\nGore Verbinski\n169.0\n563.0\n48350\nUSA\nPG-13\n\n\n2\nColor\nSam Mendes\n148.0\n0.0\n11700\nUK\nPG-13\n\n\n3\nColor\nChristopher Nolan\n164.0\n22000.0\n106759\nUSA\nPG-13\n\n\n4\nNaN\nDoug Walker\nNaN\n131.0\n143\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\nColor\nScott Smith\n87.0\n2.0\n2283\nCanada\nNaN\n\n\n4912\nColor\nNaN\n43.0\nNaN\n1753\nUSA\nTV-14\n\n\n4913\nColor\nBenjamin Roberds\n76.0\n0.0\n0\nUSA\nNaN\n\n\n4914\nColor\nDaniel Hsia\n100.0\n0.0\n2386\nUSA\nPG-13\n\n\n4915\nColor\nJon Gunn\n90.0\n16.0\n163\nUSA\nPG\n\n\n\n\n4916 rows × 7 columns\n\n\n\n- 방법2\n\ndf.loc[:,map(lambda x: 'c' == x[0] or 'd' == x[0] ,df.columns )]\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nduration\ndirector_facebook_likes\ncast_total_facebook_likes\ncountry\ncontent_rating\n\n\n\n\n0\nColor\nJames Cameron\n178.0\n0.0\n4834\nUSA\nPG-13\n\n\n1\nColor\nGore Verbinski\n169.0\n563.0\n48350\nUSA\nPG-13\n\n\n2\nColor\nSam Mendes\n148.0\n0.0\n11700\nUK\nPG-13\n\n\n3\nColor\nChristopher Nolan\n164.0\n22000.0\n106759\nUSA\nPG-13\n\n\n4\nNaN\nDoug Walker\nNaN\n131.0\n143\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\nColor\nScott Smith\n87.0\n2.0\n2283\nCanada\nNaN\n\n\n4912\nColor\nNaN\n43.0\nNaN\n1753\nUSA\nTV-14\n\n\n4913\nColor\nBenjamin Roberds\n76.0\n0.0\n0\nUSA\nNaN\n\n\n4914\nColor\nDaniel Hsia\n100.0\n0.0\n2386\nUSA\nPG-13\n\n\n4915\nColor\nJon Gunn\n90.0\n16.0\n163\nUSA\nPG\n\n\n\n\n4916 rows × 7 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#방법1-concat",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#방법1-concat",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "방법1: concat",
    "text": "방법1: concat\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\n_df = pd.DataFrame({'c':[3,4,5]}) \n_df\n\n\n\n\n\n\n\n\nc\n\n\n\n\n0\n3\n\n\n1\n4\n\n\n2\n5\n\n\n\n\n\n\n\n\npd.concat([df,_df],axis=1)\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n3\n4\n5"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#방법2-4가지-컨셉에-따른-할당",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#방법2-4가지-컨셉에-따른-할당",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "방법2: 4가지 컨셉에 따른 할당",
    "text": "방법2: 4가지 컨셉에 따른 할당\n\n# 컨셉1: 불가능\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.c = pd.Series([1,2,3]) \ndf\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\n\n# 컨셉2: 가능\n(예시1) – 사실상 이렇게 해야함.\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf['c']=[3,4,5]\ndf\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n3\n4\n5\n\n\n\n\n\n\n\n(예시2) - 굳이 이렇게까지 할필요는 없음.\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf[['c','d']]=np.array([[3,4,5],[4,5,6]]).T # 굳이.. \ndf\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n(예시3)\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf['c'],df['d']=[3,4,5],[4,5,6]\ndf\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n\n\n# 컨셉3: 불가능\n(예시1)\n\name({'a':[1,2,3],'b':[2,3,4]})\ndfdf = pd.DataFr\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.iloc[:,2] = [3,4,5] \ndf\n\nIndexError: iloc cannot enlarge its target object\n\n\n\n\n# 컨셉4: 가능\n(예시1)\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.loc[:,'c'] = [3,4,5] \ndf\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n3\n4\n5\n\n\n\n\n\n\n\n(예시2) – 굳이 쓸 필요가 없다.\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.loc[:,['c','d']] = np.array([[3,4,5],[4,5,6]]).T # 이거 솔직히 되는지 몰랐어요.. \ndf\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n(예시3)\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.loc[:,'c'],df.loc[:,'d'] = [3,4,5],[4,5,6] \ndf\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#방법3-.assign으로-할당-star-제-최애",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#방법3-.assign으로-할당-star-제-최애",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "방법3: .assign으로 할당 (\\(\\star\\)) – 제 최애",
    "text": "방법3: .assign으로 할당 (\\(\\star\\)) – 제 최애\n\n확장성이 있고 다양한 상황에 사용하기 좋음.\n\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.assign(c=[3,4,5]) \n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n3\n4\n5\n\n\n\n\n\n\n\n\ndf.assign(c=[3,4,5],d=[4,5,6]) \n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n\ndf.assign(c=[3,4,5]).assign(d=[4,5,6]) # 1-&gt;2, 2-&gt;3 으로 가는 과정이 메모리 공간안에 모두 저장되어 있다.\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\n오오오오 원래 df가 살아있음."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#방법4-.eval을-이용한-할당",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#방법4-.eval을-이용한-할당",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "방법4: .eval을 이용한 할당",
    "text": "방법4: .eval을 이용한 할당\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.eval('c=[3,4,5]')\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n3\n4\n5\n\n\n\n\n\n\n\n\ndf.eval('c=[3,4,5]').eval('d=[4,5,6]')\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n\n이 방법은 좀 꺼려하는데 아래의 예제를 통해 이유를 알아보자."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#연습해보기",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#연습해보기",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "연습해보기",
    "text": "연습해보기\n\n# 데이터프레임 생성\n\ndf=pd.DataFrame({'x':np.random.randn(1000),'y':np.random.randn(1000)})\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n-0.528686\n-0.822504\n\n\n1\n-0.570925\n0.177597\n\n\n2\n-2.095003\n0.334422\n\n\n3\n-0.382900\n0.573522\n\n\n4\n-0.971033\n-1.840163\n\n\n...\n...\n...\n\n\n995\n0.172025\n-0.770867\n\n\n996\n-0.086068\n-0.087574\n\n\n997\n0.691668\n0.850134\n\n\n998\n-0.359600\n0.913740\n\n\n999\n0.568702\n-0.808420\n\n\n\n\n1000 rows × 2 columns\n\n\n\n\n\n# 새로운열 r을 생성하고 \\(r=\\sqrt{x^2 + y^2}\\)를 계산\n- 방법1: 브로드캐스팅\n\ndf.assign(r=np.sqrt(df.x**2 + df.y**2))\n\n\n\n\n\n\n\n\nx\ny\nr\n\n\n\n\n0\n1.085469\n-1.427839\n1.793590\n\n\n1\n-1.473272\n-1.527442\n2.122171\n\n\n2\n-1.007274\n-1.312202\n1.654229\n\n\n3\n1.220634\n-0.474995\n1.309796\n\n\n4\n-0.101496\n1.636326\n1.639470\n\n\n...\n...\n...\n...\n\n\n995\n-0.668557\n-0.435391\n0.797831\n\n\n996\n0.455894\n0.796826\n0.918026\n\n\n997\n-1.004412\n1.843344\n2.099229\n\n\n998\n-2.115145\n-1.971965\n2.891796\n\n\n999\n0.861141\n-0.193742\n0.882667\n\n\n\n\n1000 rows × 3 columns\n\n\n\n- 방법2: lambda + map을 이용한 개별원소 계산\n\ndf.assign(r=list(map(lambda x,y: np.sqrt(x**2+y**2), df.x,df.y)))\n\n\n\n\n\n\n\n\nx\ny\nr\n\n\n\n\n0\n1.085469\n-1.427839\n1.793590\n\n\n1\n-1.473272\n-1.527442\n2.122171\n\n\n2\n-1.007274\n-1.312202\n1.654229\n\n\n3\n1.220634\n-0.474995\n1.309796\n\n\n4\n-0.101496\n1.636326\n1.639470\n\n\n...\n...\n...\n...\n\n\n995\n-0.668557\n-0.435391\n0.797831\n\n\n996\n0.455894\n0.796826\n0.918026\n\n\n997\n-1.004412\n1.843344\n2.099229\n\n\n998\n-2.115145\n-1.971965\n2.891796\n\n\n999\n0.861141\n-0.193742\n0.882667\n\n\n\n\n1000 rows × 3 columns\n\n\n\n- 방법3: eval\n\ndf.eval('r=sqrt(x**2+y**2)')\n\n\n\n\n\n\n\n\nx\ny\nr\n\n\n\n\n0\n-0.528686\n-0.822504\n0.977764\n\n\n1\n-0.570925\n0.177597\n0.597910\n\n\n2\n-2.095003\n0.334422\n2.121527\n\n\n3\n-0.382900\n0.573522\n0.689594\n\n\n4\n-0.971033\n-1.840163\n2.080650\n\n\n...\n...\n...\n...\n\n\n995\n0.172025\n-0.770867\n0.789828\n\n\n996\n-0.086068\n-0.087574\n0.122788\n\n\n997\n0.691668\n0.850134\n1.095962\n\n\n998\n-0.359600\n0.913740\n0.981954\n\n\n999\n0.568702\n-0.808420\n0.988415\n\n\n\n\n1000 rows × 3 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#toy-exam",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#toy-exam",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "Toy exam",
    "text": "Toy exam\n- 교재의 예제상황은 예를들면 아래와 같다.\n(숨은진짜상황1)\n\\[\\text{아이스크림 판매량} = 20 + 2 \\times \\text{온도} + \\epsilon\\]\n\nnp.random.seed(1) \ntemp= np.array([-10.2, -5.2, 0.1, 10.1, 12.2, 14.7, \n                25.4, 26.8, 28.9, 35.1, 32.2, 34.6])\neps= np.random.normal(size=12,scale=5)\nicecream= 20 + temp * 2 + eps\n\n\nplt.plot(temp,icecream,'.')\n\n\n\n\n\n온도와 아이스크림 판매량의 산점도\n\n(숨은진짜상황2)\n\\[\\text{소아마비 반응수치} = 30 + 0.5 \\times \\text{온도} + \\epsilon\\] - 좌변은 소아마비임을 나타내는 어떠한 반응수치라고 생각하자.\n\nnp.random.seed(2) \neps = np.random.normal(size=12,scale=5) \ndisease = 30+ temp* 0.5 + eps\n\n\nplt.plot(temp,disease,'.')\n\n\n\n\n\n온도와 소아마비의 산점도\n\n(우리가 데이터로부터 관측한 상황)\n- 아이스크림과 질병의 산점도를 그려보자.\n\nplt.plot(icecream,disease,'.')\n\n\n\n\n\n양의 상관관계에 있다.\n\n- 아이스크림 중 어떠한 물질이 소아마비를 일으키는것이 분명하므로 (인과성이 분명해보이니까) 아래와 같은 모형을 세우자. &lt;– 여기서부터 틀렸음\n\\[{\\tt disease}_i =\\beta_0 +\\beta_1 {\\tt icecream}_i +\\epsilon_i,\\quad \\textbf{for} ~~ i=1,2,\\dots, 12\\]\n- 적절한 \\(\\beta_0\\)와 \\(\\beta_1\\)을 추정하면 우리는 아이스크림과 소아마비의 관계를 알 수 있다. &lt;– 틀린주장\n\n틀린 모형\n도데체 우리가 뭘 잘못했는가?\n\n- 두 변수 사이에 상관관계가 있어도 실제 원인은 다른 변수에 숨겨져 있는 경우가 많다.\n(ex1)\n\n온도 \\(\\to\\) 익사\n온도 \\(\\to\\) 아이스크림\n아이스크림과 익사자도 양의 상관관계에 있을것이다.\n아이스크림을 먹이면 물에 빠져 죽는다 \\(\\to\\) 틀린주장\n사실 기온이 숨겨진 원인이다. 기온이 증가하면 아이스크림 판매량도 증가하고 폭염때문에 익사사고율도 높아지는 구조이다.\n\n(ex2)\n\n인구수 \\(\\to\\) 교회\n인구수 \\(\\to\\) 범죄건수\n지역별 교회와 범죄건수를 살펴보면 상관관계가 높게 나올것임\n교회를 지으면 범죄건수도 증가한다? \\(\\to\\) 틀린주장\n사실 인구가 숨겨진 요인임\n\n- ex2, ex1에 대하여 바른 분석을 하려면?\n\nex2: 인구가 비슷한 도시끼리 묶어서 비교해보면 교회와 범죄의 건수는 양의 상관관계에 있지 않을것임\nex1: 온도가 비슷한 그룹끼리 묶어보자.\n\n- 올바른 분석: 온도가 비슷한 그룹끼리 묶어서 그려보자. \\(\\to\\) 상관계수가 줄어들 것이다.\n\nplt.plot(icecream[:6],disease[:6],'.')\n\n\n\n\n\nplt.plot(icecream[6:],disease[6:],'.')\n\n\n\n\n\n진짜로 선형관계가 약해졌다.."
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#a.-dropna",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#a.-dropna",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "A. dropna()",
    "text": "A. dropna()\n- 결측치가 하나라도 포함된 모든 행을 제거하는 방법\n\ndf = pd.DataFrame({\n    'A': [1,2,3,np.nan,5,6,7],\n    'B': [11,np.nan,33,np.nan,55,66,77], \n    'C': [111,222,333,np.nan,555,666,np.nan]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n11.0\n111.0\n\n\n1\n2.0\nNaN\n222.0\n\n\n2\n3.0\n33.0\n333.0\n\n\n3\nNaN\nNaN\nNaN\n\n\n4\n5.0\n55.0\n555.0\n\n\n5\n6.0\n66.0\n666.0\n\n\n6\n7.0\n77.0\nNaN\n\n\n\n\n\n\n\n\ndf.dropna() # 결측치가 하나라도 포함되어 있으면 제거.\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n11.0\n111.0\n\n\n2\n3.0\n33.0\n333.0\n\n\n4\n5.0\n55.0\n555.0\n\n\n5\n6.0\n66.0\n666.0"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#b.-_",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#b.-_",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "B. _",
    "text": "B. _\n- 파이썬이 계산한 최근 결과는 _에 저장된다.\n\na = [1,2,3]\na + [4] \n\n[1, 2, 3, 4]\n\n\n\n_\n\n[1, 2, 3, 4]\n\n\n\n_ + [5] \n\n[1, 2, 3, 4, 5]\n\n\n\n_.pop()\n\n5\n\n\n\n_ + 1\n\n6"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#a.-열의-이름에서-공백제거",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#a.-열의-이름에서-공백제거",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "A. 열의 이름에서 공백제거",
    "text": "A. 열의 이름에서 공백제거\n\ndf.columns = df.columns.str.replace(' ','')"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#b.-결측치제거",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#b.-결측치제거",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "B. 결측치제거",
    "text": "B. 결측치제거\n\ndf = df.loc[:,df.isna().mean()&lt;0.5] # 결측치가 50퍼 이상인 컬럼을 제거\n\n\ndf = df.dropna()\n\n\ndf\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#c.-position-칼럼의-변환",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#c.-position-칼럼의-변환",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "C. Position 칼럼의 변환",
    "text": "C. Position 칼럼의 변환\n\nx = 'CAM'\n[k for k,v in position_dict.items() if x in v][0] # 리스트로 결과가 출력되니까 [0]으로 원소를 뽑아줌.\n\n'MIDFIELDER'\n\n\n\n# x를 넣었을 때 'MIDFIELDER'를 출력하는 함수를 만들고 싶음.\ndf.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v][0])\n\n0        MIDFIELDER\n1        MIDFIELDER\n2           FORWARD\n3        MIDFIELDER\n4          DEFENDER\n            ...    \n16703           RES\n16704           RES\n16706           RES\n16707           RES\n16708           SUB\nName: Position, Length: 14398, dtype: object\n\n\n\ndf['Position'] = df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v][0])\n\n\ndf.Position.value_counts()\n\nSUB           6009\nRES           2396\nMIDFIELDER    2188\nDEFENDER      2150\nFORWARD       1148\nGOALKEEPER     507\nName: Position, dtype: int64"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#d.-wage-칼럼의-변환",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#d.-wage-칼럼의-변환",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "D. Wage 칼럼의 변환",
    "text": "D. Wage 칼럼의 변환\n\ndf.Wage\n\n0        €250K\n1        €140K\n2        €135K\n3        €350K\n4         €45K\n         ...  \n16703     €650\n16704     €950\n16706     €550\n16707     €700\n16708     €500\nName: Wage, Length: 14398, dtype: object\n\n\n\ndf.Wage.str[1:].str.replace('K', '000') # object형이 들어감.\n\n0        250000\n1        140000\n2        135000\n3        350000\n4         45000\n          ...  \n16703       650\n16704       950\n16706       550\n16707       700\n16708       500\nName: Wage, Length: 14398, dtype: object\n\n\n\ndf.Wage.str[1:].str.replace('K', '000').astype(int) # int형태로 바꿔준다.\n\n0        250000\n1        140000\n2        135000\n3        350000\n4         45000\n          ...  \n16703       650\n16704       950\n16706       550\n16707       700\n16708       500\nName: Wage, Length: 14398, dtype: int64\n\n\n\ndf['Wage'] = df.Wage.str[1:].str.replace('K','000').astype(int)"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#e.-시각화",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#e.-시각화",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "E. 시각화",
    "text": "E. 시각화\n\nfig = ggplot(df.query('Position==\"DEFENDER\" or Position==\"FORWARD\"'))\nfig\n\n\n\n\n\npoint = geom_point(aes(x='ShotPower', y='SlidingTackle', color='Position', size='Wage', alpha='Wage'), position='jitter')\nfig + point\n\n\n\n\n\nfig = ggplot(df.query('Position==\"DEFENDER\" or Position==\"FORWARD\"'))\npoint = geom_point(aes(x='ShotPower',y='SlidingTackle',color='Position',size='Wage',alpha='Wage'),position='jitter')\nfig + point"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#a.-열의-이름에서-공백제거-1",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#a.-열의-이름에서-공백제거-1",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "A. 열의 이름에서 공백제거",
    "text": "A. 열의 이름에서 공백제거\n\ndf.columns = df.columns.str.replace(' ','')"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#b.-결측치제거-1",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#b.-결측치제거-1",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "B. 결측치제거",
    "text": "B. 결측치제거\n- 실수로 df.dropna()를 먼저 사용.\n\ndf = df.dropna() # 2개의 열이 거의 missing인 상태라 모든 관측치가 날라감.\n\n\ndf\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n\n\n0 rows × 65 columns\n\n\n\n- 이럴경우는 다시 처음부터 실행해야함.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2021/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\n\n\ndf.columns = df.columns.str.replace(' ','')\n\n\ndf = df.loc[:,df.isna().mean()&lt;0.5]\n\n\ndf = df.dropna()"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#c.-position-칼럼의-변환-1",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#c.-position-칼럼의-변환-1",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "C. Position 칼럼의 변환",
    "text": "C. Position 칼럼의 변환\n- 실수로 아래와 같이 코드를 입력했다고 치자.\n\ndf['Position'] = df.Position.str.split('&gt;').str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v])\ndf.Position\n\n0        [MIDFIELDER]\n1        [MIDFIELDER]\n2           [FORWARD]\n3        [MIDFIELDER]\n4          [DEFENDER]\n             ...     \n16703           [RES]\n16704           [RES]\n16706           [RES]\n16707           [RES]\n16708           [SUB]\nName: Position, Length: 14398, dtype: object\n\n\n- 하지만 지금와서 고쳐보려고 해봤자 늦음..\n\ndf['Position'] = df.Position.str.split('&gt;').str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop())\n\nAttributeError: Can only use .str accessor with string values!\n\n\n- 다시 처음부터..\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2021/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\ndf.columns = df.columns.str.replace(' ','')\ndf = df.loc[:,df.isna().mean()&lt;0.5]\ndf = df.dropna()\n\n\n# step1\n[k for k,v in position_dict.items() if 'CAM' in v][0]\n\n'MIDFIELDER'\n\n\n\n# step2\n# 'CAM' --&gt; x로 바꾼다. // x가 들어갔을 때 결과를 뱉어주는 함수를 만든다. (lambda)\nlambda x: [k for k,v in position_dict.items() if x in v][0]\n\n&lt;function __main__.&lt;lambda&gt;(x)&gt;\n\n\n\n# step3\n# 이러한 함수를 적용해 준다.\nlist(map(lambda x: [k for k,v in position_dict.items() if x in v][0], [s.split(\"&gt;\")[-1] for s in df.Position]))[:10]\n\n['MIDFIELDER',\n 'MIDFIELDER',\n 'FORWARD',\n 'MIDFIELDER',\n 'DEFENDER',\n 'MIDFIELDER',\n 'MIDFIELDER',\n 'SUB',\n 'SUB',\n 'MIDFIELDER']\n\n\n\ndf['Position'] = df.Position.str.split('&gt;').str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop())\ndf.head()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n\n\n5 rows × 63 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#a.-열의-이름에서-공백제거-2",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#a.-열의-이름에서-공백제거-2",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "A. 열의 이름에서 공백제거",
    "text": "A. 열의 이름에서 공백제거\n\ndf.set_axis(df.columns.str.replace(' ',''),axis=1) # 원본 데이터를 손상시키지 X\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16705\n240558\n18 L. Clayton\n17\nhttps://cdn.sofifa.com/players/240/558/18_60.png\nEngland\nhttps://cdn.sofifa.com/flags/gb-eng.png\n53\n70\nCheltenham Town\nhttps://cdn.sofifa.com/teams/1936/30.png\n...\n12.0\n55.0\n54.0\n52.0\n50.0\n59.0\nGK\n52.0\n€238K\nNaN\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n16709\n178453\n07 A. Censori\n17\nhttps://cdn.sofifa.com/players/178/453/07_60.png\nItaly\nhttps://cdn.sofifa.com/flags/it.png\n28\n38\nArezzo\nhttps://cdn.sofifa.com/teams/110907/30.png\n...\nNaN\n7.0\n1.0\n36.0\n6.0\n9.0\nST\n36.0\nNaN\nNaN\n\n\n\n\n16710 rows × 65 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#b.-결측치제거-2",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#b.-결측치제거-2",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "B. 결측치제거",
    "text": "B. 결측치제거\n\n_df = df.set_axis(df.columns.str.replace(' ',''),axis=1)\n_df\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16705\n240558\n18 L. Clayton\n17\nhttps://cdn.sofifa.com/players/240/558/18_60.png\nEngland\nhttps://cdn.sofifa.com/flags/gb-eng.png\n53\n70\nCheltenham Town\nhttps://cdn.sofifa.com/teams/1936/30.png\n...\n12.0\n55.0\n54.0\n52.0\n50.0\n59.0\nGK\n52.0\n€238K\nNaN\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n16709\n178453\n07 A. Censori\n17\nhttps://cdn.sofifa.com/players/178/453/07_60.png\nItaly\nhttps://cdn.sofifa.com/flags/it.png\n28\n38\nArezzo\nhttps://cdn.sofifa.com/teams/110907/30.png\n...\nNaN\n7.0\n1.0\n36.0\n6.0\n9.0\nST\n36.0\nNaN\nNaN\n\n\n\n\n16710 rows × 65 columns\n\n\n\n\ndf.set_axis(df.columns.str.replace(' ',''),axis=1)\\\n.loc[:,lambda _df: _df.isna().mean()&lt;0.5].dropna()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#c.-position-칼럼의-변환-2",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#c.-position-칼럼의-변환-2",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "C. Position 칼럼의 변환",
    "text": "C. Position 칼럼의 변환\n_df는 이전까지 변환시킨 결과값들을 가지고 있음.\n\ndf.set_axis(df.columns.str.replace(' ',''),axis=1)\\\n.loc[:,lambda _df: _df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda _df: _df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#d.-wage-칼럼의-변환-1",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#d.-wage-칼럼의-변환-1",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "D. Wage 칼럼의 변환",
    "text": "D. Wage 칼럼의 변환\n\n_df = _\n\n\n# 리스트 컴프리헨션을 써서 해도됨.\nlist(map(lambda x: int(x[:-1])*1000 if x[-1] == 'K' else int(x), [s[1:] for s in _df.Wage]))[:10]\n\n[250000, 140000, 135000, 350000, 45000, 160000, 61000, 115000, 72000, 190000]\n\n\n\ndf.set_axis(df.columns.str.replace(' ',''),axis=1)\\\n.loc[:,lambda _df: _df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda _df: _df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\\\n.assign(Wage = lambda _df: _df.Wage.str[1:].str.replace('K','000').astype(int))\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClubLogo\n...\nSlidingTackle\nGKDiving\nGKHandling\nGKKicking\nGKPositioning\nGKReflexes\nBestPosition\nBestOverallRating\nReleaseClause\nDefensiveAwareness\n\n\n\n\n0\n212198\nBruno Fernandes\n26\nhttps://cdn.sofifa.com/players/212/198/22_60.png\nPortugal\nhttps://cdn.sofifa.com/flags/pt.png\n88\n89\nManchester United\nhttps://cdn.sofifa.com/teams/11/30.png\n...\n65.0\n12.0\n14.0\n15.0\n8.0\n14.0\nCAM\n88.0\n€206.9M\n72.0\n\n\n1\n209658\nL. Goretzka\n26\nhttps://cdn.sofifa.com/players/209/658/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.com/teams/21/30.png\n...\n77.0\n13.0\n8.0\n15.0\n11.0\n9.0\nCM\n87.0\n€160.4M\n74.0\n\n\n2\n176580\nL. Suárez\n34\nhttps://cdn.sofifa.com/players/176/580/22_60.png\nUruguay\nhttps://cdn.sofifa.com/flags/uy.png\n88\n88\nAtlético de Madrid\nhttps://cdn.sofifa.com/teams/240/30.png\n...\n38.0\n27.0\n25.0\n31.0\n33.0\n37.0\nST\n88.0\n€91.2M\n42.0\n\n\n3\n192985\nK. De Bruyne\n30\nhttps://cdn.sofifa.com/players/192/985/22_60.png\nBelgium\nhttps://cdn.sofifa.com/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.com/teams/10/30.png\n...\n53.0\n15.0\n13.0\n5.0\n10.0\n13.0\nCM\n91.0\n€232.2M\n68.0\n\n\n4\n224334\nM. Acuña\n29\nhttps://cdn.sofifa.com/players/224/334/22_60.png\nArgentina\nhttps://cdn.sofifa.com/flags/ar.png\n84\n84\nSevilla FC\nhttps://cdn.sofifa.com/teams/481/30.png\n...\n82.0\n8.0\n14.0\n13.0\n13.0\n14.0\nLB\n84.0\n€77.7M\n80.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16703\n259718\nF. Gebhardt\n19\nhttps://cdn.sofifa.com/players/259/718/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n52\n66\nFC Basel 1893\nhttps://cdn.sofifa.com/teams/896/30.png\n...\n10.0\n53.0\n45.0\n47.0\n52.0\n57.0\nGK\n52.0\n€361K\n6.0\n\n\n16704\n251433\nB. Voll\n20\nhttps://cdn.sofifa.com/players/251/433/22_60.png\nGermany\nhttps://cdn.sofifa.com/flags/de.png\n58\n69\nF.C. Hansa Rostock\nhttps://cdn.sofifa.com/teams/27/30.png\n...\n10.0\n59.0\n60.0\n56.0\n55.0\n61.0\nGK\n58.0\n€656K\n5.0\n\n\n16706\n262846\n�. Dobre\n20\nhttps://cdn.sofifa.com/players/262/846/22_60.png\nRomania\nhttps://cdn.sofifa.com/flags/ro.png\n53\n63\nFC Academica Clinceni\nhttps://cdn.sofifa.com/teams/113391/30.png\n...\n12.0\n57.0\n52.0\n53.0\n48.0\n58.0\nGK\n53.0\n€279K\n5.0\n\n\n16707\n241317\n21 Xue Qinghao\n19\nhttps://cdn.sofifa.com/players/241/317/21_60.png\nChina PR\nhttps://cdn.sofifa.com/flags/cn.png\n47\n60\nShanghai Shenhua FC\nhttps://cdn.sofifa.com/teams/110955/30.png\n...\n9.0\n49.0\n48.0\n45.0\n38.0\n52.0\nGK\n47.0\n€223K\n21.0\n\n\n16708\n259646\nA. Shaikh\n18\nhttps://cdn.sofifa.com/players/259/646/22_60.png\nIndia\nhttps://cdn.sofifa.com/flags/in.png\n47\n67\nATK Mohun Bagan FC\nhttps://cdn.sofifa.com/teams/113146/30.png\n...\n13.0\n49.0\n41.0\n39.0\n45.0\n49.0\nGK\n47.0\n€259K\n7.0\n\n\n\n\n14398 rows × 63 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#e.-시각화-1",
    "href": "posts/2_DV2022/2023-10-06-05wk-supp-t.html#e.-시각화-1",
    "title": "[DV2023] supp-1: FIFA23 자료의 시각화",
    "section": "E. 시각화",
    "text": "E. 시각화\n\ntidydata = df.set_axis(df.columns.str.replace(' ',''),axis=1)\\\n.loc[:,lambda _df: _df.isna().mean()&lt;0.5].dropna()\\\n.assign(Position = lambda _df: _df.Position.str.split(\"&gt;\").str[-1].apply(lambda x: [k for k,v in position_dict.items() if x in v].pop()))\\\n.assign(Wage = lambda _df: _df.Wage.str[1:].str.replace('K','000').astype(int))\n\n\nfig = ggplot(tidydata.query('Position==\"DEFENDER\" or Position==\"FORWARD\"'))\npoint = geom_point(aes(x='ShotPower',y='SlidingTackle',color='Position',size='Wage',alpha='Wage'),position='jitter')\nfig + point"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html",
    "title": "05wk-1",
    "section": "",
    "text": "seaborn(2)–scatterplot, mpl미세먼지팁(2)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#plt-복습",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#plt-복습",
    "title": "05wk-1",
    "section": "plt 복습",
    "text": "plt 복습\n\nplt.plot(x1,y1,'o')\nplt.plot(x2,y2,'o')"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-array",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-array",
    "title": "05wk-1",
    "section": "sns: array",
    "text": "sns: array\n\nsns.scatterplot(data=None,x=x1,y=y1)\nsns.scatterplot(data=None,x=x2,y=y2)\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-wide-df",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-wide-df",
    "title": "05wk-1",
    "section": "sns: wide df",
    "text": "sns: wide df\n\nsns.scatterplot(data=pd.DataFrame({'x':x1,'y':y1}),x='x',y='y')\nsns.scatterplot(data=pd.DataFrame({'x':x2,'y':y2}),x='x',y='y')\n#sns.scatterplot(data=None,x=x2,y=y2)\n\n&lt;AxesSubplot:xlabel='x', ylabel='y'&gt;\n\n\n\n\n\n\n억지로 그리긴 했는데 이 경우는 wide하게 만든 df는 별로 경쟁력이 없음"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-long-df",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-long-df",
    "title": "05wk-1",
    "section": "sns: long df",
    "text": "sns: long df\n\nx= np.concatenate([x1,x2])\ny= np.concatenate([y1,y2])\ncat = ['x1']*len(x1) + ['x2']*len(x2)\ndf2 = pd.DataFrame({'x':x,'y':y,'cat':cat})\ndf2\n\n\n\n\n\n\n\n\nx\ny\ncat\n\n\n\n\n0\n2.023919\n-0.400176\nx1\n\n\n1\n1.229622\n-1.763752\nx1\n\n\n2\n-0.413211\n2.293004\nx1\n\n\n3\n-1.343073\n0.404232\nx1\n\n\n4\n1.062845\n0.030775\nx1\n\n\n...\n...\n...\n...\n\n\n1995\n2.226805\n3.683857\nx2\n\n\n1996\n2.768263\n2.678292\nx2\n\n\n1997\n2.525295\n2.815478\nx2\n\n\n1998\n1.750193\n2.289812\nx2\n\n\n1999\n1.153290\n2.095922\nx2\n\n\n\n\n2000 rows × 3 columns\n\n\n\n\nsns.scatterplot(data=df2,x='x',y='y',hue='cat') \n\n&lt;AxesSubplot:xlabel='x', ylabel='y'&gt;"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#예제1",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#예제1",
    "title": "05wk-1",
    "section": "예제1",
    "text": "예제1\n\nfig,ax = plt.subplots(1,3,figsize=(12,4))\nax[0].plot([1,2,4,3],'--o')\nsns.scatterplot(x=x1,y=y1,ax=ax[1])\nsns.scatterplot(x=x1,y=y1,ax=ax[2])\nsns.scatterplot(x=x2,y=y2,ax=ax[2])\nax[2].plot([1,2,4,3],'-r',lw=5)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#예제2",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#예제2",
    "title": "05wk-1",
    "section": "예제2",
    "text": "예제2\n\nimport cv2\n\n\n!wget https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg \nimg = cv2.imread('Unequalized_Hawkes_Bay_NZ.jpg',0)\n!rm Unequalized_Hawkes_Bay_NZ.jpg \n\n--2022-10-05 16:33:56--  https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\nResolving upload.wikimedia.org (upload.wikimedia.org)... 103.102.166.240, 2001:df2:e500:ed1a::2:b\nConnecting to upload.wikimedia.org (upload.wikimedia.org)|103.102.166.240|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 110895 (108K) [image/jpeg]\nSaving to: ‘Unequalized_Hawkes_Bay_NZ.jpg’\n\nUnequalized_Hawkes_ 100%[===================&gt;] 108.30K   548KB/s    in 0.2s    \n\n2022-10-05 16:33:57 (548 KB/s) - ‘Unequalized_Hawkes_Bay_NZ.jpg’ saved [110895/110895]\n\n\n\n\nimg2 = cv2.equalizeHist(img)\n\n\nimg.reshape(-1)\n\narray([127, 145, 149, ..., 146, 145, 144], dtype=uint8)\n\n\n\nfig,ax = plt.subplots(2,2,figsize=(10,5))\nax[0,0].imshow(img,vmin=0,vmax=255,cmap='gray')\nsns.histplot(img.reshape(-1),ax=ax[0,1],bins=15,lw=0,kde=True,color='C1')\nax[0,1].set_xlim(0,255)\nax[1,0].imshow(img2,vmin=0,vmax=255,cmap='gray')\nsns.histplot(img2.reshape(-1),ax=ax[1,1],bins=15,lw=0,kde=True,color='C1')\n\n&lt;AxesSubplot:ylabel='Count'&gt;\n\n\n\n\n\n- seaborn: figure-level vs axes-level 의 개념\nref: https://seaborn.pydata.org/tutorial/function_overview.html#figure-level-vs-axes-level-functions"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#축-간격조정",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#축-간격조정",
    "title": "05wk-1",
    "section": "축 간격조정",
    "text": "축 간격조정\n\nimport matplotlib as mpl\n\n\nfig, ax = plt.subplots()\nax.plot([(xi/30)**2 for xi in range(30)],'--o')\nax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(3)) # 큰 눈금간격을 3으로\nax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(1)) # 작은 눈금간격을 1로"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#축-삭제",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#축-삭제",
    "title": "05wk-1",
    "section": "축 삭제",
    "text": "축 삭제\n\nfig, ax = plt.subplots()\nax.plot([(xi/30)**2 for xi in range(30)],'--o')\nax.xaxis.set_major_locator(mpl.ticker.NullLocator()) # x축 눈금삭제\nax.yaxis.set_major_locator(mpl.ticker.NullLocator()) # y축 눈금삭제"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#축-범위조정",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#축-범위조정",
    "title": "05wk-1",
    "section": "축 범위조정",
    "text": "축 범위조정\n\nfig, ax = plt.subplots()\nax.plot([(xi/30)**2 for xi in range(30)],'--o')\nax.set_ylim(-1,2) \nax.set_xlim(-5,35)\n#plt.ylim(-1,2)\n#plt.xlim(-5,35)\n\n(-5.0, 35.0)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#gcf-gca",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#gcf-gca",
    "title": "05wk-1",
    "section": "gcf, gca",
    "text": "gcf, gca\n- gcf\n\nplt.plot([1,2,3,2])\nfig = plt.gcf()\n\n\n\n\n\nfig.suptitle('suptitle')\n\nText(0.5, 0.98, 'suptitle')\n\n\n\nfig\n\n\n\n\n- gca\n\nfig\n\n\n\n\n\nax = fig.gca()\n\n\nax.set_title('title') \nfig"
  },
  {
    "objectID": "posts/2_DV2022/2022-11-09-10wk-2.html",
    "href": "posts/2_DV2022/2022-11-09-10wk-2.html",
    "title": "10wk-2 심슨의 역설",
    "section": "",
    "text": "심슨의 역설을 bar plot으로 시각화하는 방법과 왜 생기게 되는지에 대해 알아보자."
  },
  {
    "objectID": "posts/2_DV2022/2022-11-09-10wk-2.html#버클리대학교의-입학데이터",
    "href": "posts/2_DV2022/2022-11-09-10wk-2.html#버클리대학교의-입학데이터",
    "title": "10wk-2 심슨의 역설",
    "section": "버클리대학교의 입학데이터",
    "text": "버클리대학교의 입학데이터\n\nhttps://github.com/pinkocto/Quarto-Blog/blob/main/posts/DV/ds.pdf\n\n- 주장: 버클리대학에 gender bias가 존재한다.\n\n1973년 가을학기의 입학통계에 따르면 지원하는 남성이 여성보다 훨씬 많이 합격했고, 그 차이가 너무 커서 우연의 일치라 보기 어렵다.\n\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\nresult\ngender\ncount\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224"
  },
  {
    "objectID": "posts/2_DV2022/2022-11-09-10wk-2.html#시각화1-전체합격률",
    "href": "posts/2_DV2022/2022-11-09-10wk-2.html#시각화1-전체합격률",
    "title": "10wk-2 심슨의 역설",
    "section": "시각화1: 전체합격률",
    "text": "시각화1: 전체합격률\n- df1\n\ndf.groupby(['gender','result']).agg({'count':np.sum}).reset_index()\n\n\n\n\n\n\n\n\ngender\nresult\ncount\n\n\n\n\n0\nfemale\nfail\n1063\n\n\n1\nfemale\npass\n772\n\n\n2\nmale\nfail\n1291\n\n\n3\nmale\npass\n1400\n\n\n\n\n\n\n\n- df2\n\n# df.query('gender ==\"female\"')\n\n\ndf.groupby('gender').agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1)\n\n\n\n\n\n\n\n\ngender\ncount2\n\n\n\n\n0\nfemale\n1835\n\n\n1\nmale\n2691\n\n\n\n\n\n\n\n- merge: 두개의 데이터프레임을 합친다\n\ndf.groupby(['gender','result']).agg({'count':np.sum}).reset_index()\\\n.merge(df.groupby('gender').agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1))\n\n\n\n\n\n\n\n\ngender\nresult\ncount\ncount2\n\n\n\n\n0\nfemale\nfail\n1063\n1835\n\n\n1\nfemale\npass\n772\n1835\n\n\n2\nmale\nfail\n1291\n2691\n\n\n3\nmale\npass\n1400\n2691\n\n\n\n\n\n\n\n- 비율계산\n\ndf.groupby(['gender','result']).agg({'count':np.sum}).reset_index()\\\n.merge(df.groupby('gender').agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1))\\\n.eval('rate = count/count2')\n\n\n\n\n\n\n\n\ngender\nresult\ncount\ncount2\nrate\n\n\n\n\n0\nfemale\nfail\n1063\n1835\n0.579292\n\n\n1\nfemale\npass\n772\n1835\n0.420708\n\n\n2\nmale\nfail\n1291\n2691\n0.479747\n\n\n3\nmale\npass\n1400\n2691\n0.520253\n\n\n\n\n\n\n\n- 시각화\n\ndata1= df.groupby(['gender','result']).agg({'count':np.sum}).reset_index()\\\n.merge(df.groupby('gender').agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1))\\\n.eval('rate = count/count2')\nggplot(data1.query('result==\"pass\"'))+geom_col(aes(x='gender',fill='gender',y='rate')) # 합격률만 시각화.\n\n\n\n\n- 결론: 남자의 합격률이 더 높다. \\(\\to\\) 성차별이 있어보인다(?)"
  },
  {
    "objectID": "posts/2_DV2022/2022-11-09-10wk-2.html#시각화2-학과별-합격률",
    "href": "posts/2_DV2022/2022-11-09-10wk-2.html#시각화2-학과별-합격률",
    "title": "10wk-2 심슨의 역설",
    "section": "시각화2: 학과별 합격률",
    "text": "시각화2: 학과별 합격률\n- df2\n\ndf.groupby(['department','gender']).agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1)\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount2\n\n\n\n\n0\nA\nfemale\n108\n\n\n1\nA\nmale\n825\n\n\n2\nB\nfemale\n25\n\n\n3\nB\nmale\n560\n\n\n4\nC\nfemale\n593\n\n\n5\nC\nmale\n325\n\n\n6\nD\nfemale\n375\n\n\n7\nD\nmale\n417\n\n\n8\nE\nfemale\n393\n\n\n9\nE\nmale\n191\n\n\n10\nF\nfemale\n341\n\n\n11\nF\nmale\n373\n\n\n\n\n\n\n\n- merge\n\ndf.merge(df.groupby(['department','gender']).agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1))\\\n.eval('rate = count/count2')\n\n\n\n\n\n\n\n\ndepartment\nresult\ngender\ncount\ncount2\nrate\n\n\n\n\n0\nA\nfail\nfemale\n19\n108\n0.175926\n\n\n1\nA\npass\nfemale\n89\n108\n0.824074\n\n\n2\nA\nfail\nmale\n314\n825\n0.380606\n\n\n3\nA\npass\nmale\n511\n825\n0.619394\n\n\n4\nB\nfail\nfemale\n7\n25\n0.280000\n\n\n5\nB\npass\nfemale\n18\n25\n0.720000\n\n\n6\nB\nfail\nmale\n208\n560\n0.371429\n\n\n7\nB\npass\nmale\n352\n560\n0.628571\n\n\n8\nC\nfail\nfemale\n391\n593\n0.659359\n\n\n9\nC\npass\nfemale\n202\n593\n0.340641\n\n\n10\nC\nfail\nmale\n204\n325\n0.627692\n\n\n11\nC\npass\nmale\n121\n325\n0.372308\n\n\n12\nD\nfail\nfemale\n244\n375\n0.650667\n\n\n13\nD\npass\nfemale\n131\n375\n0.349333\n\n\n14\nD\nfail\nmale\n279\n417\n0.669065\n\n\n15\nD\npass\nmale\n138\n417\n0.330935\n\n\n16\nE\nfail\nfemale\n299\n393\n0.760814\n\n\n17\nE\npass\nfemale\n94\n393\n0.239186\n\n\n18\nE\nfail\nmale\n137\n191\n0.717277\n\n\n19\nE\npass\nmale\n54\n191\n0.282723\n\n\n20\nF\nfail\nfemale\n103\n341\n0.302053\n\n\n21\nF\npass\nfemale\n238\n341\n0.697947\n\n\n22\nF\nfail\nmale\n149\n373\n0.399464\n\n\n23\nF\npass\nmale\n224\n373\n0.600536\n\n\n\n\n\n\n\n- 시각화\n\ndata2=df.merge(df.groupby(['department','gender']).agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1))\\\n.eval('rate = count/count2')\nggplot(data2.query('result==\"pass\"'))+geom_col(aes(x='gender',fill='gender',y='rate'))\\\n+facet_wrap('department')\n\n\n\n\n\n학과별로 살펴보니 오히려 A,B,F,D의 경우 여성의 합격률이 높다.\n\n- 교재에서 설명한 이유: 여성이 합격률이 낮은 학과에만 많이 지원하였기 때문\n\nggplot(data2.query('result==\"pass\"'))+geom_col(aes(x='department',y='count2',fill='gender'),position='dodge')\n\n\n\n\n\n살펴보니 합격률이 높은 A,B학과의 경우 상대적으로 남성이 많이 지원하였음. 합격률이 낮은 C,D학과는 상대적으로 여성이 많이 지원함. D,F의 지원수는 비슷"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-19-7wk-2.html",
    "href": "posts/2_DV2022/2022-10-19-7wk-2.html",
    "title": "07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)",
    "section": "",
    "text": "아이스크림을 많이 먹으면 걸리는 병(2)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-19-7wk-2.html#자료생성-좀-더-그럴듯한-자료-만들기",
    "href": "posts/2_DV2022/2022-10-19-7wk-2.html#자료생성-좀-더-그럴듯한-자료-만들기",
    "title": "07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)",
    "section": "자료생성: 좀 더 그럴듯한 자료 (만들기)",
    "text": "자료생성: 좀 더 그럴듯한 자료 (만들기)\n- 지난 시간의 toy example은 데이터가 너무 작아서 억지스러움 \\(\\to\\) 기상자료개방포털, 회원가입해야 자료받을 수 있음.\n\n_df=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv')\n_df\n\n\n\n\n\n\n\n\n지점번호\n지점명\n일시\n평균기온(℃)\n최고기온(℃)\n최고기온시각\n최저기온(℃)\n\n\n\n\n0\n146\n전주\n2020-01-01\n-0.5\n4.3\n15:09\n-6.4\n\n\n1\n146\n전주\n2020-01-02\n1.4\n6.5\n14:12\n-3.0\n\n\n2\n146\n전주\n2020-01-03\n2.6\n7.6\n13:32\n-0.5\n\n\n3\n146\n전주\n2020-01-04\n2.0\n7.7\n13:51\n-2.6\n\n\n4\n146\n전주\n2020-01-05\n2.5\n8.6\n14:05\n-3.2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n651\n146\n전주\n2021-10-13\n19.9\n25.5\n14:29\n15.6\n\n\n652\n146\n전주\n2021-10-14\n20.4\n25.5\n13:36\n17.0\n\n\n653\n146\n전주\n2021-10-15\n18.3\n22.0\n13:47\n15.7\n\n\n654\n146\n전주\n2021-10-16\n12.8\n17.4\n0:01\n6.5\n\n\n655\n146\n전주\n2021-10-17\n6.7\n12.4\n15:18\n2.2\n\n\n\n\n656 rows × 7 columns\n\n\n\n- 평균기온만 선택\n\npd.Series(_df.columns)\n\n0       지점번호\n1        지점명\n2         일시\n3    평균기온(℃)\n4    최고기온(℃)\n5     최고기온시각\n6    최저기온(℃)\ndtype: object\n\n\n\ntemp = np.array(_df.iloc[:,3])\ntemp[:5]\n\narray([-0.5,  1.4,  2.6,  2. ,  2.5])\n\n\n\n# 숨은진짜상황1: 온도 \\(\\to\\) 아이스크림 판매량\n- 아래와 같은 관계가 있다고 하자.\n\\[\\text{아이스크림 판매량} = 20 + 2 \\times \\text{온도} + \\epsilon\\]\n\nnp.random.seed(1)\neps = np.random.normal(size=len(temp), scale=10) \nicecream = 20 + 2*temp + eps\n\n\nplt.plot(temp,icecream,'o',alpha=0.3)\nplt.xlabel(\"temp\",size=15)\nplt.ylabel(\"icecream\",size=15)\n\nText(0, 0.5, 'icecream')\n\n\n\n\n\n\n\n# 숨은진짜상황1: 온도 \\(\\to\\) 아이스크림 판매량\n- 아래와 같은 관계가 있다고 하자.\n\\[\\text{소아마비 반응수치} = 30 + 0.5 \\times \\text{온도} + \\epsilon\\]\n\nnp.random.seed(2) \neps=np.random.normal(size=len(temp),scale=1)\ndisease= 30 + 0.5 * temp + eps\n\n\nplt.plot(temp,disease,'o',alpha=0.3)\nplt.xlabel(\"temp\",size=15)\nplt.ylabel(\"disease\",size=15)\n\nText(0, 0.5, 'disease')\n\n\n\n\n\n\n\n# 우리가 관측한 상황 (온도는 은닉되어있음)\n\nplt.plot(icecream,disease,'o',alpha=0.3)\nplt.xlabel(\"icecream\",size=15)\nplt.ylabel(\"disease\",size=15)\n\nText(0, 0.5, 'disease')\n\n\n\n\n\n\nnp.corrcoef(icecream,disease)\n\narray([[1.        , 0.86298975],\n       [0.86298975, 1.        ]])\n\n\n\n0.86정도.."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-19-7wk-2.html#직관-여름만-뽑아서-plot-해보자.",
    "href": "posts/2_DV2022/2022-10-19-7wk-2.html#직관-여름만-뽑아서-plot-해보자.",
    "title": "07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)",
    "section": "직관: 여름만 뽑아서 plot 해보자.",
    "text": "직관: 여름만 뽑아서 plot 해보자.\n- temp&gt;25 (여름으로 간주) 인 관측치만 플랏\n\nplt.plot(icecream[temp&gt;25],disease[temp&gt;25], 'o', color='C1') ## 평균기온이 25도가 넘어가면 여름이라 생각 \n\n\n\n\n- 전체적인 산점도\n\nfig , ((ax1,ax2), (ax3,ax4)) = plt.subplots(2,2,figsize=(8,6)) \nax1.plot(temp,icecream,'o',alpha=0.2); ax1.set_xlabel('temp'); ax1.set_ylabel('icecream'); ax1.set_title(\"hidden1\")\nax2.plot(temp,disease,'o',alpha=0.2); ax2.set_xlabel('temp'); ax2.set_ylabel('disease'); ax2.set_title(\"hidden2\")\nax3.plot(icecream,disease,'o',alpha=0.2); ax3.set_xlabel('icecream'); ax3.set_ylabel('disease'); ax3.set_title(\"observed\")\nax4.plot(icecream,disease,'o',alpha=0.2); ax4.set_xlabel('icecream'); ax4.set_ylabel('disease'); ax4.set_title(\"observed\")\nax4.plot(icecream[temp&gt;25],disease[temp&gt;25],'o',label='temp&gt;25')\nax4.legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-19-7wk-2.html#ggplot-온도구간을-세분화-하여-시각화",
    "href": "posts/2_DV2022/2022-10-19-7wk-2.html#ggplot-온도구간을-세분화-하여-시각화",
    "title": "07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)",
    "section": "ggplot: 온도구간을 세분화 하여 시각화",
    "text": "ggplot: 온도구간을 세분화 하여 시각화\n- 목표: 모든 온도구간에 대하여 각각 색을 다르게 하여 그려보자.\n\n사실 지금 변수는 온도, 아이스크림판매량, 소아마비\n온도가 유사한 지역을 색으로 묶으면 3차원 플랏이 가능함\n\n\n# df로 자료정리\n- 일단 데이터 프레임을 정리하자.\n\ndf = pd.DataFrame({'temp':temp,'icecream':icecream,'disease':disease})\ndf\n\n\n\n\n\n\n\n\ntemp\nicecream\ndisease\n\n\n\n\n0\n-0.5\n35.243454\n29.333242\n\n\n1\n1.4\n16.682436\n30.643733\n\n\n2\n2.6\n19.918282\n29.163804\n\n\n3\n2.0\n13.270314\n32.640271\n\n\n4\n2.5\n33.654076\n29.456564\n\n\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.633906\n\n\n652\n20.4\n76.554679\n38.920443\n\n\n653\n18.3\n68.666079\n39.882650\n\n\n654\n12.8\n42.771364\n36.613159\n\n\n655\n6.7\n30.736731\n34.902513\n\n\n\n\n656 rows × 3 columns\n\n\n\n\n\n# 구간세분화\n- 온도를 카테고리화 하자 \\(\\to\\) 적당한 구긴을 설정하기 위해서 히스토그램을 그려보자.\n\ndf.temp.hist() # ? 이거 14주차쯤 배우는데 미리 스포합니다.. 엄청 편해요 \n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nplt.hist(df.temp) # 원래는 이걸 배웠죠\n\n(array([  3.,   9.,  29.,  60.,  92.,  86.,  65.,  93., 139.,  80.]),\n array([-12.4 ,  -8.16,  -3.92,   0.32,   4.56,   8.8 ,  13.04,  17.28,\n         21.52,  25.76,  30.  ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n- 구간은 5정도로 하면 적당할 것 같다.\n\ndef cut(x): # 이거보다 더 좋은 방법이 있을 것 같긴 한데요..\n    if x&lt;0: \n        y='Temp: &lt;0'\n    elif x&lt;5: \n        y='Temp: 0~5'\n    elif x&lt;10: \n        y='Temp: 5~10'\n    elif x&lt;15: \n        y='Temp: 10~15'\n    elif x&lt;20:\n        y='Temp: 15~20'\n    elif x&lt;25: \n        y='Temp: 20~25'\n    else: \n        y='Temp: &gt;30'\n    return y \n\n\ndf.assign(temp2 = list(map(cut,df.temp)))\n\n\n\n\n\n\n\n\ntemp\nicecream\ndisease\ntemp2\n\n\n\n\n0\n-0.5\n35.243454\n29.333242\nTemp: &lt;0\n\n\n1\n1.4\n16.682436\n30.643733\nTemp: 0~5\n\n\n2\n2.6\n19.918282\n29.163804\nTemp: 0~5\n\n\n3\n2.0\n13.270314\n32.640271\nTemp: 0~5\n\n\n4\n2.5\n33.654076\n29.456564\nTemp: 0~5\n\n\n...\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.633906\nTemp: 15~20\n\n\n652\n20.4\n76.554679\n38.920443\nTemp: 20~25\n\n\n653\n18.3\n68.666079\n39.882650\nTemp: 15~20\n\n\n654\n12.8\n42.771364\n36.613159\nTemp: 10~15\n\n\n655\n6.7\n30.736731\n34.902513\nTemp: 5~10\n\n\n\n\n656 rows × 4 columns\n\n\n\n\n\n# ggplot\n- 온도를 색으로 구분하면\n\nfig = ggplot(data=df.assign(temp2 = list(map(cut,df.temp))))\np1 = geom_point(aes(x='icecream',y='disease',colour='temp2'),alpha=0.5)\nfig + p1\n\n\n\n\n- 추세선을 추가하면\n\nl1 = geom_smooth(aes(x='icecream',y='disease',colour='temp2'))\n\n\nfig+p1+l1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n\n각 온도별로 추세선은 거의 기울기가 0이다. \\(\\to\\) 온도가 비슷한 구간별로 묶어서 보니까 상관관계가 없다는 거!\n아이스크림 판매량과 소아마비의 corr은 유의미해보이지만, 온도를 통제하였을 경우 아이스크림 판매량과 소아마비의 partial corr은 유의미해보이지 않음.\n\n\n\n# 해석\n- 해피앤딩: 온도를 통제하니까 아이스크림과 질병은 관련이 없어보인다. \\(\\to\\) 아이스크림을 먹으면 소아마비를 유발한다는 이상한 결론이 나올뻔 했지만 우리는 온도라는 흑막을 잘 찾았고 결과적으로 “온도-&gt;아이스크림판매량,소아마비” 이라는 합리적인 진리를 얻을 수 있었다.\n\n온도와 같은 변수를 은닉변수라고 한다.\n\n- 또 다른 흑막? 고려할 흑막이 온도뿐이라는 보장이 어디있지? 사실 흑막2, 흑막3이 있어서 그런 흑막들을 고려하다보니까 아이스크림과 소아마비사이의 상관관계가 다시 보이면 어떡하지?\n\n이러한 이유 때문에 상관계수로 인과성을 유추하는건 사실상 불가능.\n그런데 이론적으로는 “세상의 모든 은닉변수를 통제하였을 경우에도 corr(X,Y)의 값이 1에 가깝다면 그때는 인과성이 있다고 봐도 무방함, (물론 이 경우에도 무엇이 원인인지는 통계적으로 따지는것이 불가)” 이라고 주장할 수 있다. 즉 모든 흑막을 제거한다면 “상관성=인과성”이다.\n\n- 실험계획법, 인과추론: 세상의 모든 흑막을 제거하는건 상식적으로 불가능\n\n피셔의주장(실험계획법): 그런데 실험계획을 잘하면 흑막을 제거한 효과가 있음 (무작위로 사람뽑아서 담배를 피우게 한다든가)\n인과추론: 실험계획이 사실상 불가능한 경우가 있음 \\(\\to\\) 모인 데이터에서 최대한 흑막2,3,4,.. 등이 비슷한 그룹끼리 “매칭”을 시킨다!"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-19-7wk-2.html#그냥-궁금해서-진짜-만약에-아이스크림과-소아마비가-관련있는-경우라면",
    "href": "posts/2_DV2022/2022-10-19-7wk-2.html#그냥-궁금해서-진짜-만약에-아이스크림과-소아마비가-관련있는-경우라면",
    "title": "07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)",
    "section": "그냥 궁금해서: 진짜 만약에 아이스크림과 소아마비가 관련있는 경우라면?",
    "text": "그냥 궁금해서: 진짜 만약에 아이스크림과 소아마비가 관련있는 경우라면?\n- 온도는 아이스크림 판매에 여전히 영향을 주지만\n\\[\\text{아이스크림 판매량} = 20 + 2 \\times \\text{온도} + \\epsilon\\]\n\nnp.random.seed(1)\neps=np.random.normal(size=len(temp), scale=10) \nicecream = 20 + 2 * temp + eps \n\n- 수영장이 원인이 아니라 진짜 아이스크림을 먹고 소아마비에 걸린상황이라면?\n\\[\\text{소아마비 반응수치} = 30 + 0 \\times \\text{온도} + 0.15 \\times \\text{아이스크림 판매량} + \\epsilon\\]\n\nnp.random.seed(2) \neps = np.random.normal(size=len(temp),scale=2)\ndisease= 30+ 0*temp + 0.15*icecream + eps\n\n\ndf2=pd.DataFrame({'temp':temp,'icecream':icecream,'disease':disease})\ndf2.assign(temp2=list(map(cut,df2.temp)))\n\n\n\n\n\n\n\n\ntemp\nicecream\ndisease\ntemp2\n\n\n\n\n0\n-0.5\n35.243454\n34.453002\nTemp: &lt;0\n\n\n1\n1.4\n16.682436\n32.389832\nTemp: 0~5\n\n\n2\n2.6\n19.918282\n28.715350\nTemp: 0~5\n\n\n3\n2.0\n13.270314\n35.271089\nTemp: 0~5\n\n\n4\n2.5\n33.654076\n31.461240\nTemp: 0~5\n\n\n...\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.693811\nTemp: 15~20\n\n\n652\n20.4\n76.554679\n38.924088\nTemp: 20~25\n\n\n653\n18.3\n68.666079\n41.765212\nTemp: 15~20\n\n\n654\n12.8\n42.771364\n36.842022\nTemp: 10~15\n\n\n655\n6.7\n30.736731\n37.715537\nTemp: 5~10\n\n\n\n\n656 rows × 4 columns\n\n\n\n\nggplot(data=df2.assign(temp2=list(map(cut,df2.temp))))+\\\ngeom_point(aes(x='icecream',y='disease',colour='temp2'),alpha=0.2)+\\\ngeom_smooth(aes(x='icecream',y='disease',colour='temp2'))\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n\n이번엔 partial corr도 유의미하게 나옴\n\n- 단순 corr을 봐서는 “온도-&gt;아이스크림,소아마비” 인지, “온도-&gt;아이스크림-&gt;소아마비” 인지 알기 어렵다.\n\ndf.corr()\n\n\n\n\n\n\n\n\ntemp\nicecream\ndisease\n\n\n\n\ntemp\n1.000000\n0.884366\n0.975609\n\n\nicecream\n0.884366\n1.000000\n0.862990\n\n\ndisease\n0.975609\n0.862990\n1.000000\n\n\n\n\n\n\n\n\ndf2.corr()\n\n\n\n\n\n\n\n\ntemp\nicecream\ndisease\n\n\n\n\ntemp\n1.000000\n0.884366\n0.725505\n\n\nicecream\n0.884366\n1.000000\n0.830539\n\n\ndisease\n0.725505\n0.830539\n1.000000"
  },
  {
    "objectID": "2_dv2022.html",
    "href": "2_dv2022.html",
    "title": "DV2022",
    "section": "",
    "text": "This page is organized based on the contents of the Data Visualization (2022-2) and lecture notes of Professor Guebin Choi of Jeonbuk National University."
  },
  {
    "objectID": "2_dv2022.html#contents",
    "href": "2_dv2022.html#contents",
    "title": "DV2022",
    "section": "Contents",
    "text": "Contents\n1. 시각화 차트 소개\n\nboxplot, histogram, lineplot, scatterplot\n\n2. 파이썬 데이터 시각화 패키지 사용법\n\nmatplotlib, seaborn, plotnine/ggplot2\n\n3. 데이터 시각화와 통계적 해석\n\n히스토그램 이퀄라이제이션\n표본상관계수, 앤스콤의 플랏, 무상관, 무상관과 독립\n\n4. Data Wrangling\n\nlambda, map\npandas: indexing\n\n5. 인포그래픽과 데이터시각화\n\n나이젤홈즈와 애드워드터프티, 찰스미나드의 도표"
  },
  {
    "objectID": "4_dl2023.html",
    "href": "4_dl2023.html",
    "title": "DL2023",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 25, 2023\n\n\n[DL] 4wk. Numerical Computation and Machine Learning Basics\n\n\nJiyunLim \n\n\n\n\nSep 18, 2023\n\n\n[DL] 3wk. Applied Math and Machine Learning Basics (2)\n\n\nJiyunLim \n\n\n\n\nSep 11, 2023\n\n\n[DL] 2wk. Applied Math and Machine Learning Basics\n\n\nJiyunLim \n\n\n\n\nSep 4, 2023\n\n\n딥러닝\n\n\njiyunLim \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "3_stbda2022.html",
    "href": "3_stbda2022.html",
    "title": "STBDA2022",
    "section": "",
    "text": "This page is organized based on the contents of the Big Data Analysis Special Lecture (2022-1) and lecture notes of Professor Guebin Choi of Jeonbuk National University.\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 11, 2023\n\n\n[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들\n\n\nJiyunLim \n\n\n\n\nMay 30, 2023\n\n\n[STBDA] 11wk. MaxPool2D, Conv2D\n\n\nJiyunLim \n\n\n\n\nMay 28, 2023\n\n\n[STBDA] 10wk. Softmax / 다양한 평가지표\n\n\nJiyunLim \n\n\n\n\nMay 26, 2023\n\n\n[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법\n\n\nJiyunLim \n\n\n\n\nMay 24, 2023\n\n\n[STBDA] 9wk. Likelihood function\n\n\nJiyunLim \n\n\n\n\nMay 22, 2023\n\n\n[STBDA] 중간고사\n\n\nJiyunLim \n\n\n\n\nMay 20, 2023\n\n\n[STBDA] 7wk. Piece-wise LR / Logistic Regression\n\n\nJiyunLim \n\n\n\n\nMay 18, 2023\n\n\n[STBDA] 6wk. 회귀모형 적합 with keras\n\n\nJiyunLim \n\n\n\n\nMay 16, 2023\n\n\n[STBDA] 5wk. optimizer를 이용한 최적화\n\n\nJiyunLim \n\n\n\n\nMay 14, 2023\n\n\n[STBDA] 4wk. 미분 / 경사하강법\n\n\nJiyunLim \n\n\n\n\nMay 12, 2023\n\n\n[STBDA] 3wk. 텐서플로우 intro2 (tf.GradientTape())\n\n\nJiyunLim \n\n\n\n\nMay 10, 2023\n\n\n[STBDA] 2wk. 텐서플로우 intro1 (tf.constant선언, tnp사용법)\n\n\nJiyunLim \n\n\n\n\nMay 8, 2023\n\n\n[STBDA] 1wk. 강의소개 및 단순선형회귀\n\n\nJiyunLim \n\n\n\n\nJan 1, 2023\n\n\nJupyter\n\n\nJiyunLim \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "가상환경\n\n가상환경 생성 맟 활성화\n\nconda create -n py310 python=3.10\nconda activate py310\n\n가상환경 삭제 conda remove –name [py310] –all\npytorch 설치 conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "",
    "text": "데이터가 깔끔하지 않을 때를 위한 자잘한 팁 (missing, filtering, assign, 행변환)"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#a.-열의-이름-변경",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#a.-열의-이름-변경",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "A. 열의 이름 변경",
    "text": "A. 열의 이름 변경\n- 방법1: df.columns에 대입\n\ndf = pd.DataFrame(np.random.randn(3,2))\ndf.columns \n\nRangeIndex(start=0, stop=2, step=1)\n\n\n\nlist(df.columns) # 컬럼명이 0,1로 저장되어 있음.\n\n[0, 1]\n\n\n\ndf.columns = ['A','B'] # 컬럼이름 바꾸기.\n\n\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n0.148797\n0.508274\n\n\n1\n2.450131\n0.077788\n\n\n2\n-0.089712\n0.720153\n\n\n\n\n\n\n\n- 방법2: df.set_axis()\n\ndf = pd.DataFrame(np.random.randn(5,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-2.295180\n0.196262\n0.260348\n\n\n1\n0.933123\n-1.304135\n1.446940\n\n\n2\n0.642722\n-1.138573\n0.873878\n\n\n3\n-0.658560\n0.369092\n0.063098\n\n\n4\n0.017730\n-1.110979\n0.917571\n\n\n\n\n\n\n\n\ndf.set_axis(['A','B','C'],axis=1)\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-2.295180\n0.196262\n0.260348\n\n\n1\n0.933123\n-1.304135\n1.446940\n\n\n2\n0.642722\n-1.138573\n0.873878\n\n\n3\n-0.658560\n0.369092\n0.063098\n\n\n4\n0.017730\n-1.110979\n0.917571\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(np.random.randn(5,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-1.104416\n-0.128439\n0.269068\n\n\n1\n-1.663507\n-0.083289\n-0.057443\n\n\n2\n-0.921260\n-0.534263\n-0.685721\n\n\n3\n-1.867316\n-0.438948\n-0.104675\n\n\n4\n-1.350895\n-1.563338\n-0.680217\n\n\n\n\n\n\n\n\ndf.set_axis(['A','B','C','D','E'],axis=0)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nA\n-1.104416\n-0.128439\n0.269068\n\n\nB\n-1.663507\n-0.083289\n-0.057443\n\n\nC\n-0.921260\n-0.534263\n-0.685721\n\n\nD\n-1.867316\n-0.438948\n-0.104675\n\n\nE\n-1.350895\n-1.563338\n-0.680217\n\n\n\n\n\n\n\n- 방법3: df.rename()\n\ndf = pd.DataFrame(np.random.randn(5,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.217076\n1.089656\n-0.384690\n\n\n1\n1.677108\n-1.263818\n2.244100\n\n\n2\n-1.841508\n-1.156893\n-0.738780\n\n\n3\n0.426959\n-1.423985\n-1.193719\n\n\n4\n1.125859\n-0.799723\n0.454321\n\n\n\n\n\n\n\n\ndf.rename({0:'AA', 1:'BB'},axis=1)\n# df.rename(columns={0:'AA', 1:'BB'}) ## 위와 동일한 코드\n\n\n\n\n\n\n\n\nAA\nBB\n2\n\n\n\n\n0\n0.217076\n1.089656\n-0.384690\n\n\n1\n1.677108\n-1.263818\n2.244100\n\n\n2\n-1.841508\n-1.156893\n-0.738780\n\n\n3\n0.426959\n-1.423985\n-1.193719\n\n\n4\n1.125859\n-0.799723\n0.454321"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#b.-행의-이름-변경",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#b.-행의-이름-변경",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "B. 행의 이름 변경",
    "text": "B. 행의 이름 변경\n- 방법1: df.index에 대입\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n1.033253\n1.768416\n1.413502\n\n\n1\n0.615272\n1.021584\n0.069085\n\n\n\n\n\n\n\n\nrow이름을 index\n\n\ndf.index = ['hynn','iu']\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nhynn\n1.033253\n1.768416\n1.413502\n\n\niu\n0.615272\n1.021584\n0.069085\n\n\n\n\n\n\n\n- 방법2: df.set_axis()\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n-2.130565\n0.227919\n1.023103\n\n\n1\n-0.405379\n-0.368737\n-1.174423\n\n\n\n\n\n\n\n\n# df.set_axis(list('AB'))\ndf.set_axis(['A','B'], axis=0)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nA\n-2.130565\n0.227919\n1.023103\n\n\nB\n-0.405379\n-0.368737\n-1.174423\n\n\n\n\n\n\n\n- 방법3: df.rename()\n\ndf = pd.DataFrame(np.random.randn(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.683712\n0.259077\n-1.600270\n\n\n1\n-0.283836\n-0.057625\n-0.708813\n\n\n\n\n\n\n\n\ndf.rename({1:'guebin'})\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.683712\n0.259077\n-1.600270\n\n\nguebin\n-0.283836\n-0.057625\n-0.708813\n\n\n\n\n\n\n\n\n데이터 프레임의 row name을 바꾸는 경우는 거의 없다… (Timeseries일 경우를 제외하고는?)\n\n- 방법4: 임의의 열을 행이름으로 지정!\n\ndf = pd.DataFrame({'id':[43052,43053], 'X1':[1,2],'X2':[2,3]})\ndf\n\n\n\n\n\n\n\n\nid\nX1\nX2\n\n\n\n\n0\n43052\n1\n2\n\n\n1\n43053\n2\n3\n\n\n\n\n\n\n\n\ndf = pd.DataFrame({'id':['2023-43052', '2023-43053'], 'hour':[3,2], 'height':[176,172]})\ndf\n\n\n\n\n\n\n\n\nid\nhour\nheight\n\n\n\n\n0\n2023-43052\n3\n176\n\n\n1\n2023-43053\n2\n172\n\n\n\n\n\n\n\n\ndf.set_index('id') # column에 있던 id가 row index로..\n\n\n\n\n\n\n\n\nhour\nheight\n\n\nid\n\n\n\n\n\n\n2023-43052\n3\n176\n\n\n2023-43053\n2\n172"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#ab에-대한-연습문제",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#ab에-대한-연습문제",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "# A~B에 대한 연습문제",
    "text": "# A~B에 대한 연습문제\n- 데이터 load\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.head(2)\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n\n\n2 rows × 29 columns\n\n\n\n# 예제1: 열의 이름 출력하고, 열의 이름중 공백(`)이 있을 경우 언더바(_`) 로 바꾸자.\n컬럼출력\n\ndf.columns\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Joined', 'Loaned From', 'Contract Valid Until', 'Height', 'Weight',\n       'Release Clause', 'Kit Number', 'Best Overall Rating'],\n      dtype='object')\n\n\n\ndf.columns에 직접대입\n\n\nnew_columns = [s.replace(' ','_') for s in df.columns]  \n# df.columns = new_columns\n\n\nnew_columns\n\n['ID',\n 'Name',\n 'Age',\n 'Photo',\n 'Nationality',\n 'Flag',\n 'Overall',\n 'Potential',\n 'Club',\n 'Club_Logo',\n 'Value',\n 'Wage',\n 'Special',\n 'Preferred_Foot',\n 'International_Reputation',\n 'Weak_Foot',\n 'Skill_Moves',\n 'Work_Rate',\n 'Body_Type',\n 'Real_Face',\n 'Position',\n 'Joined',\n 'Loaned_From',\n 'Contract_Valid_Until',\n 'Height',\n 'Weight',\n 'Release_Clause',\n 'Kit_Number',\n 'Best_Overall_Rating']\n\n\n\nset_axis() 이용\n\n\nnew_columns = [s.replace(' ','_') for s in df.columns]  \ndf.set_axis(new_columns,axis=1)\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub_Logo\n...\nReal_Face\nPosition\nJoined\nLoaned_From\nContract_Valid_Until\nHeight\nWeight\nRelease_Clause\nKit_Number\nBest_Overall_Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n\nrename() 이용 – 안중요함\n\n\ndct = {k:k.replace(' ', '_') for k in df.columns} # 딕셔너리 컴프리헨션\ndct\n\n{'ID': 'ID',\n 'Name': 'Name',\n 'Age': 'Age',\n 'Photo': 'Photo',\n 'Nationality': 'Nationality',\n 'Flag': 'Flag',\n 'Overall': 'Overall',\n 'Potential': 'Potential',\n 'Club': 'Club',\n 'Club Logo': 'Club_Logo',\n 'Value': 'Value',\n 'Wage': 'Wage',\n 'Special': 'Special',\n 'Preferred Foot': 'Preferred_Foot',\n 'International Reputation': 'International_Reputation',\n 'Weak Foot': 'Weak_Foot',\n 'Skill Moves': 'Skill_Moves',\n 'Work Rate': 'Work_Rate',\n 'Body Type': 'Body_Type',\n 'Real Face': 'Real_Face',\n 'Position': 'Position',\n 'Joined': 'Joined',\n 'Loaned From': 'Loaned_From',\n 'Contract Valid Until': 'Contract_Valid_Until',\n 'Height': 'Height',\n 'Weight': 'Weight',\n 'Release Clause': 'Release_Clause',\n 'Kit Number': 'Kit_Number',\n 'Best Overall Rating': 'Best_Overall_Rating'}\n\n\n\ndct = {k:k.replace(' ','_') for k in df.columns}\ndf.rename(dct, axis=1)\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub_Logo\n...\nReal_Face\nPosition\nJoined\nLoaned_From\nContract_Valid_Until\nHeight\nWeight\nRelease_Clause\nKit_Number\nBest_Overall_Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n#\n예제2: ID를 row-index로 지정하라.\n\n직접지정\n\n\n# df.index = df.ID\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\nID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n209658\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n212198\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n224334\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n192985\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n224232\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n# df.index = df.ID\n\n\nset_axis()\n\n\ndf.set_axis(df.ID, axis=0)[:3]\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\nID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n209658\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n212198\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n224334\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n\n\n3 rows × 29 columns\n\n\n\n\nrename() – 안중요함\n\n\n{idx:ID for idx, ID in zip(df[:10].index, df[:10].ID)}\n\n{0: 209658,\n 1: 212198,\n 2: 224334,\n 3: 192985,\n 4: 224232,\n 5: 212622,\n 6: 197445,\n 7: 187961,\n 8: 208333,\n 9: 210514}\n\n\n\ndct = {k:v for k,v in zip(df.index,df.ID)}\ndf.rename(dct)\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n209658\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n212198\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n224334\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n192985\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n224232\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n269526\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n267946\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n270567\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n256624\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n256376\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n\nset_index()\n\n\ndf.set_index('ID')\n\n\n\n\n\n\n\n\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\nValue\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\nID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n€91M\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n€78.5M\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n€46.5M\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n€107.5M\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n€89.5M\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n€100K\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n€100K\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n€70K\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n€90K\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n€90K\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17660 rows × 28 columns\n\n\n\n#"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#c.-pd.t",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#c.-pd.t",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "C. pd.T",
    "text": "C. pd.T\n- df.T를 이용하여 데이터를 살피면 편리함\n\ndf.T.iloc[:,:3]\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nID\n209658\n212198\n224334\n\n\nName\nL. Goretzka\nBruno Fernandes\nM. Acuña\n\n\nAge\n27\n27\n30\n\n\nPhoto\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nhttps://cdn.sofifa.net/players/224/334/23_60.png\n\n\nNationality\nGermany\nPortugal\nArgentina\n\n\nFlag\nhttps://cdn.sofifa.net/flags/de.png\nhttps://cdn.sofifa.net/flags/pt.png\nhttps://cdn.sofifa.net/flags/ar.png\n\n\nOverall\n87\n86\n85\n\n\nPotential\n88\n87\n85\n\n\nClub\nFC Bayern München\nManchester United\nSevilla FC\n\n\nClub Logo\nhttps://cdn.sofifa.net/teams/21/30.png\nhttps://cdn.sofifa.net/teams/11/30.png\nhttps://cdn.sofifa.net/teams/481/30.png\n\n\nValue\n€91M\n€78.5M\n€46.5M\n\n\nWage\n€115K\n€190K\n€46K\n\n\nSpecial\n2312\n2305\n2303\n\n\nPreferred Foot\nRight\nRight\nLeft\n\n\nInternational Reputation\n4.0\n3.0\n2.0\n\n\nWeak Foot\n4.0\n3.0\n3.0\n\n\nSkill Moves\n3.0\n4.0\n3.0\n\n\nWork Rate\nHigh/ Medium\nHigh/ High\nHigh/ High\n\n\nBody Type\nUnique\nUnique\nStocky (170-185)\n\n\nReal Face\nYes\nYes\nNo\n\n\nPosition\n&lt;span class=\"pos pos28\"&gt;SUB\n&lt;span class=\"pos pos15\"&gt;LCM\n&lt;span class=\"pos pos7\"&gt;LB\n\n\nJoined\nJul 1, 2018\nJan 30, 2020\nSep 14, 2020\n\n\nLoaned From\nNaN\nNaN\nNaN\n\n\nContract Valid Until\n2026\n2026\n2024\n\n\nHeight\n189cm\n179cm\n172cm\n\n\nWeight\n82kg\n69kg\n69kg\n\n\nRelease Clause\n€157M\n€155M\n€97.7M\n\n\nKit Number\n8.0\n8.0\n19.0\n\n\nBest Overall Rating\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n- 출력옵션 조정\n\npd.options.display.max_rows = 10\ndisplay(df.T.iloc[:,:3])\npd.reset_option(\"display.max_rows\")\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\nID\n209658\n212198\n224334\n\n\nName\nL. Goretzka\nBruno Fernandes\nM. Acuña\n\n\nAge\n27\n27\n30\n\n\nPhoto\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nhttps://cdn.sofifa.net/players/224/334/23_60.png\n\n\nNationality\nGermany\nPortugal\nArgentina\n\n\n...\n...\n...\n...\n\n\nHeight\n189cm\n179cm\n172cm\n\n\nWeight\n82kg\n69kg\n69kg\n\n\nRelease Clause\n€157M\n€155M\n€97.7M\n\n\nKit Number\n8.0\n8.0\n19.0\n\n\nBest Overall Rating\nNaN\nNaN\nNaN\n\n\n\n\n29 rows × 3 columns\n\n\n\n\n이 예제에서는 줄이는 옵션을 사용했지만 보통은 늘려서 사용함"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#d.-df.dtypes-s.dtype",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#d.-df.dtypes-s.dtype",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "D. df.dtypes, s.dtype",
    "text": "D. df.dtypes, s.dtype\n- df.dtypes\n\ndf.dtypes\n\nID                            int64\nName                         object\nAge                           int64\nPhoto                        object\nNationality                  object\nFlag                         object\nOverall                       int64\nPotential                     int64\nClub                         object\nClub Logo                    object\nValue                        object\nWage                         object\nSpecial                       int64\nPreferred Foot               object\nInternational Reputation    float64\nWeak Foot                   float64\nSkill Moves                 float64\nWork Rate                    object\nBody Type                    object\nReal Face                    object\nPosition                     object\nJoined                       object\nLoaned From                  object\nContract Valid Until         object\nHeight                       object\nWeight                       object\nRelease Clause               object\nKit Number                  float64\nBest Overall Rating          object\ndtype: object\n\n\n\n“object는 거의 string이구나” 라고 생각해도 무방.\n\n- s.dtype\n\ndf.Name.dtype # 'O': object를 의미.\n\ndtype('O')\n\n\n\ndf.Age.dtype\n\ndtype('int64')\n\n\n\ndf['International Reputation'].dtype\n\ndtype('float64')\n\n\n- ==를 이용한 자료형 체크\n\ndf.Name.dtype == np.object_\n\nTrue\n\n\n\ndf.Age.dtype == np.int64\n\nTrue\n\n\n\ndf['International Reputation'].dtype == np.float64\n\nTrue\n\n\n# 예제: df에서 int64 자료형만 출력\n- (풀이1)\n\npd.Series(list(df.dtypes))\n\n0       int64\n1      object\n2       int64\n3      object\n4      object\n5      object\n6       int64\n7       int64\n8      object\n9      object\n10     object\n11     object\n12      int64\n13     object\n14    float64\n15    float64\n16    float64\n17     object\n18     object\n19     object\n20     object\n21     object\n22     object\n23     object\n24     object\n25     object\n26     object\n27    float64\n28     object\ndtype: object\n\n\n\ndf.iloc[:,[0,2,6,7,12]]\n\n\n\n\n\n\n\n\nID\nAge\nOverall\nPotential\nSpecial\n\n\n\n\n0\n209658\n27\n87\n88\n2312\n\n\n1\n212198\n27\n86\n87\n2305\n\n\n2\n224334\n30\n85\n85\n2303\n\n\n3\n192985\n31\n91\n91\n2303\n\n\n4\n224232\n25\n86\n89\n2296\n\n\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\n19\n48\n61\n762\n\n\n17656\n267946\n17\n48\n64\n761\n\n\n17657\n270567\n25\n51\n56\n759\n\n\n17658\n256624\n18\n50\n65\n758\n\n\n17659\n256376\n20\n50\n61\n749\n\n\n\n\n17660 rows × 5 columns\n\n\n\n- (풀이2)\n\n[o == np.int64 for o in df.dtypes]\n\n[True,\n False,\n True,\n False,\n False,\n False,\n True,\n True,\n False,\n False,\n False,\n False,\n True,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False]\n\n\n\ndf.loc[:,[o == np.int64 for o in df.dtypes]]\n\n\n\n\n\n\n\n\nID\nAge\nOverall\nPotential\nSpecial\n\n\n\n\n0\n209658\n27\n87\n88\n2312\n\n\n1\n212198\n27\n86\n87\n2305\n\n\n2\n224334\n30\n85\n85\n2303\n\n\n3\n192985\n31\n91\n91\n2303\n\n\n4\n224232\n25\n86\n89\n2296\n\n\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\n19\n48\n61\n762\n\n\n17656\n267946\n17\n48\n64\n761\n\n\n17657\n270567\n25\n51\n56\n759\n\n\n17658\n256624\n18\n50\n65\n758\n\n\n17659\n256376\n20\n50\n61\n749\n\n\n\n\n17660 rows × 5 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#e.-df.sort_values",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#e.-df.sort_values",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "E. df.sort_values()",
    "text": "E. df.sort_values()\n- 예시1: 나이가 어린 순서대로 정렬\n\ndf.sort_values(by='Age')\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n17636\n263636\n22 D. Oncescu\n15\nhttps://cdn.sofifa.net/players/263/636/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n50\n72\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 1, 2021\nNaN\n2025\n190cm\n77kg\n€306K\n34.0\nNaN\n\n\n13712\n271072\nE. Topcu\n16\nhttps://cdn.sofifa.net/players/271/072/23_60.png\nRepublic of Ireland\nhttps://cdn.sofifa.net/flags/ie.png\n48\n58\nDrogheda United\nhttps://cdn.sofifa.net/teams/1572/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 8, 2022\nNaN\n2022\n183cm\n65kg\n€175K\n20.0\nNaN\n\n\n13078\n259442\n22 R. van den Berg\n16\nhttps://cdn.sofifa.net/players/259/442/22_60.png\nNetherlands\nhttps://cdn.sofifa.net/flags/nl.png\n60\n81\nPEC Zwolle\nhttps://cdn.sofifa.net/teams/1914/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMay 24, 2020\nNaN\n2024\n190cm\n73kg\n€1.8M\n33.0\nNaN\n\n\n11257\n266205\n22 Y. Koré\n16\nhttps://cdn.sofifa.net/players/266/205/22_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n59\n74\nParis FC\nhttps://cdn.sofifa.net/teams/111817/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nAug 11, 2022\nNaN\n2025\n187cm\n75kg\n€1.1M\n34.0\nNaN\n\n\n11278\n261873\n21 H. Kumagai\n16\nhttps://cdn.sofifa.net/players/261/873/21_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n52\n70\nVegalta Sendai\nhttps://cdn.sofifa.net/teams/112836/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 16, 2021\nNaN\n2023\n174cm\n64kg\n€375K\n48.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16311\n254196\n21 L. Fernández\n42\nhttps://cdn.sofifa.net/players/254/196/21_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n61\n61\nSociedad Deportiva Aucas\nhttps://cdn.sofifa.net/teams/110987/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJan 29, 2018\nNaN\n2024\n187cm\n82kg\n€75K\n1.0\nNaN\n\n\n16036\n216692\nS. Torrico\n42\nhttps://cdn.sofifa.net/players/216/692/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n72\n72\nSan Lorenzo de Almagro\nhttps://cdn.sofifa.net/teams/1013/30.png\n...\nNo\n&lt;span class=\"pos pos0\"&gt;GK\nApr 25, 2013\nNaN\n2022\n183cm\n84kg\n€375K\n12.0\nNaN\n\n\n17257\n645\n17 D. Andersson\n43\nhttps://cdn.sofifa.net/players/000/645/17_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n57\n57\nHelsingborgs IF\nhttps://cdn.sofifa.net/teams/432/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nApr 21, 2016\nNaN\n2022\n187cm\n85kg\nNaN\n39.0\nNaN\n\n\n15375\n1179\nG. Buffon\n44\nhttps://cdn.sofifa.net/players/001/179/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n79\n79\nParma\nhttps://cdn.sofifa.net/teams/50/30.png\n...\nYes\n&lt;span class=\"pos pos0\"&gt;GK\nJul 1, 2021\nNaN\n2024\n192cm\n92kg\n€3M\n1.0\nNaN\n\n\n15272\n254704\n22 K. Miura\n54\nhttps://cdn.sofifa.net/players/254/704/22_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n56\n56\nYokohama FC\nhttps://cdn.sofifa.net/teams/113197/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2005\nNaN\n2022\n177cm\n72kg\nNaN\n11.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- 예시2: 나이가 많은 순서대로 정렬\n\ndf.sort_values('Age',ascending=False)\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n15272\n254704\n22 K. Miura\n54\nhttps://cdn.sofifa.net/players/254/704/22_60.png\nJapan\nhttps://cdn.sofifa.net/flags/jp.png\n56\n56\nYokohama FC\nhttps://cdn.sofifa.net/teams/113197/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2005\nNaN\n2022\n177cm\n72kg\nNaN\n11.0\nNaN\n\n\n15375\n1179\nG. Buffon\n44\nhttps://cdn.sofifa.net/players/001/179/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n79\n79\nParma\nhttps://cdn.sofifa.net/teams/50/30.png\n...\nYes\n&lt;span class=\"pos pos0\"&gt;GK\nJul 1, 2021\nNaN\n2024\n192cm\n92kg\n€3M\n1.0\nNaN\n\n\n17257\n645\n17 D. Andersson\n43\nhttps://cdn.sofifa.net/players/000/645/17_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n57\n57\nHelsingborgs IF\nhttps://cdn.sofifa.net/teams/432/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nApr 21, 2016\nNaN\n2022\n187cm\n85kg\nNaN\n39.0\nNaN\n\n\n16036\n216692\nS. Torrico\n42\nhttps://cdn.sofifa.net/players/216/692/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n72\n72\nSan Lorenzo de Almagro\nhttps://cdn.sofifa.net/teams/1013/30.png\n...\nNo\n&lt;span class=\"pos pos0\"&gt;GK\nApr 25, 2013\nNaN\n2022\n183cm\n84kg\n€375K\n12.0\nNaN\n\n\n16311\n254196\n21 L. Fernández\n42\nhttps://cdn.sofifa.net/players/254/196/21_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n61\n61\nSociedad Deportiva Aucas\nhttps://cdn.sofifa.net/teams/110987/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJan 29, 2018\nNaN\n2024\n187cm\n82kg\n€75K\n1.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17360\n261023\n21 H. Broun\n16\nhttps://cdn.sofifa.net/players/261/023/21_60.png\nScotland\nhttps://cdn.sofifa.net/flags/gb-sct.png\n52\n72\nKilmarnock\nhttps://cdn.sofifa.net/teams/82/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nSep 17, 2020\nNaN\n2022\n182cm\n70kg\n€523K\n40.0\nNaN\n\n\n15536\n263639\n22 M. Pavel\n16\nhttps://cdn.sofifa.net/players/263/639/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n51\n69\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2021\nNaN\n2023\n178cm\n66kg\n€277K\n77.0\nNaN\n\n\n11398\n256405\n21 W. Essanoussi\n16\nhttps://cdn.sofifa.net/players/256/405/21_60.png\nNetherlands\nhttps://cdn.sofifa.net/flags/nl.png\n59\n75\nVVV-Venlo\nhttps://cdn.sofifa.net/teams/100651/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJul 1, 2019\nNaN\n2022\n178cm\n70kg\n€1.1M\n24.0\nNaN\n\n\n15030\n270594\nT. Walczak\n16\nhttps://cdn.sofifa.net/players/270/594/23_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n54\n68\nWisła Płock\nhttps://cdn.sofifa.net/teams/1569/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nSep 7, 2021\nNaN\n2023\n191cm\n88kg\n€494K\n99.0\nNaN\n\n\n17636\n263636\n22 D. Oncescu\n15\nhttps://cdn.sofifa.net/players/263/636/22_60.png\nRomania\nhttps://cdn.sofifa.net/flags/ro.png\n50\n72\nFC Dinamo 1948 Bucureşti\nhttps://cdn.sofifa.net/teams/100757/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 1, 2021\nNaN\n2025\n190cm\n77kg\n€306K\n34.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- 예시3: 능력치가 좋은 순서대로 정렬\n\ndf.sort_values('Overall',ascending=False)\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n41\n188545\nR. Lewandowski\n33\nhttps://cdn.sofifa.net/players/188/545/23_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n91\n91\nFC Barcelona\nhttps://cdn.sofifa.net/teams/241/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 18, 2022\nNaN\n2025\n185cm\n81kg\n€172.2M\n9.0\nNaN\n\n\n124\n165153\nK. Benzema\n34\nhttps://cdn.sofifa.net/players/165/153/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n91\nReal Madrid CF\nhttps://cdn.sofifa.net/teams/243/30.png\n...\nYes\n&lt;span class=\"pos pos21\"&gt;CF\nJul 9, 2009\nNaN\n2023\n185cm\n81kg\n€131.2M\n9.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n56\n158023\nL. Messi\n35\nhttps://cdn.sofifa.net/players/158/023/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n91\n91\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos23\"&gt;RW\nAug 10, 2021\nNaN\n2023\n169cm\n67kg\n€99.9M\n30.0\nNaN\n\n\n75\n231747\nK. Mbappé\n23\nhttps://cdn.sofifa.net/players/231/747/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n95\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 1, 2018\nNaN\n2025\n182cm\n73kg\n€366.7M\n7.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15513\n266751\n22 Jung Ho Yeon\n20\nhttps://cdn.sofifa.net/players/266/751/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n45\n53\nGwangJu FC\nhttps://cdn.sofifa.net/teams/112258/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 20, 2022\nNaN\n2026\n180cm\n73kg\n€145K\n23.0\nNaN\n\n\n16215\n268279\n22 J. Looschen\n24\nhttps://cdn.sofifa.net/players/268/279/22_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n44\n47\nSV Meppen\nhttps://cdn.sofifa.net/teams/110597/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMar 19, 2022\nNaN\n2026\n178cm\n78kg\n€92K\n42.0\nNaN\n\n\n16042\n255283\n20 Kim Yeong Geun\n22\nhttps://cdn.sofifa.net/players/255/283/20_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n44\n49\nGyeongnam FC\nhttps://cdn.sofifa.net/teams/111588/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 9, 2020\nNaN\n2020\n174cm\n71kg\n€53K\n43.0\nNaN\n\n\n14634\n269038\n22 Zhang Wenxuan\n16\nhttps://cdn.sofifa.net/players/269/038/22_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n44\n59\nGuangzhou FC\nhttps://cdn.sofifa.net/teams/111839/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nMay 1, 2022\nNaN\n2022\n175cm\n70kg\n€239K\n29.0\nNaN\n\n\n17618\n168933\n07 I. Paskov\n33\nhttps://cdn.sofifa.net/players/168/933/07_60.png\nBulgaria\nhttps://cdn.sofifa.net/flags/bg.png\n43\n42\nNaN\nhttps://cdn.sofifa.net/flags/bg.png\n...\nNaN\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\nNaN\nNaN\n184cm\n79kg\nNaN\n24.0\nNaN\n\n\n\n\n17660 rows × 29 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#f.-df.info",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#f.-df.info",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "F. df.info()",
    "text": "F. df.info()\n각 컬럼에 들어있는 데이터 타입 혹은 missing에 대한 정보를 한꺼번에 알고싶다면 df.info() 를 사용하면 된다.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17660 entries, 0 to 17659\nData columns (total 29 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   ID                        17660 non-null  int64  \n 1   Name                      17660 non-null  object \n 2   Age                       17660 non-null  int64  \n 3   Photo                     17660 non-null  object \n 4   Nationality               17660 non-null  object \n 5   Flag                      17660 non-null  object \n 6   Overall                   17660 non-null  int64  \n 7   Potential                 17660 non-null  int64  \n 8   Club                      17449 non-null  object \n 9   Club Logo                 17660 non-null  object \n 10  Value                     17660 non-null  object \n 11  Wage                      17660 non-null  object \n 12  Special                   17660 non-null  int64  \n 13  Preferred Foot            17660 non-null  object \n 14  International Reputation  17660 non-null  float64\n 15  Weak Foot                 17660 non-null  float64\n 16  Skill Moves               17660 non-null  float64\n 17  Work Rate                 17660 non-null  object \n 18  Body Type                 17622 non-null  object \n 19  Real Face                 17622 non-null  object \n 20  Position                  17625 non-null  object \n 21  Joined                    16562 non-null  object \n 22  Loaned From               694 non-null    object \n 23  Contract Valid Until      17299 non-null  object \n 24  Height                    17660 non-null  object \n 25  Weight                    17660 non-null  object \n 26  Release Clause            16509 non-null  object \n 27  Kit Number                17625 non-null  float64\n 28  Best Overall Rating       21 non-null     object \ndtypes: float64(4), int64(5), object(20)\nmemory usage: 3.9+ MB\n\n\n\ndf['Best Overall Rating']\n\n0        NaN\n1        NaN\n2        NaN\n3        NaN\n4        NaN\n        ... \n17655    NaN\n17656    NaN\n17657    NaN\n17658    NaN\n17659    NaN\nName: Best Overall Rating, Length: 17660, dtype: object"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#g.-df.isna",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#g.-df.isna",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "G. df.isna()",
    "text": "G. df.isna()\n- 예시1: 열별로 결측치 count\n\ndf.isna() # 결측치가 있으면 True, 그렇지 않으면 False\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17656\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17657\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17658\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17659\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n17660 rows × 29 columns\n\n\n\n\narr = np.array([[True, False], [True, False], [False, True]])\narr\n\narray([[ True, False],\n       [ True, False],\n       [False,  True]])\n\n\n\narr.shape\n\n(3, 2)\n\n\n\narr.sum(axis=0)\n\narray([2, 1])\n\n\n\ndf.isna().sum(axis=0)\ndf.isna().sum()\n\nID                              0\nName                            0\nAge                             0\nPhoto                           0\nNationality                     0\nFlag                            0\nOverall                         0\nPotential                       0\nClub                          211\nClub Logo                       0\nValue                           0\nWage                            0\nSpecial                         0\nPreferred Foot                  0\nInternational Reputation        0\nWeak Foot                       0\nSkill Moves                     0\nWork Rate                       0\nBody Type                      38\nReal Face                      38\nPosition                       35\nJoined                       1098\nLoaned From                 16966\nContract Valid Until          361\nHeight                          0\nWeight                          0\nRelease Clause               1151\nKit Number                     35\nBest Overall Rating         17639\ndtype: int64\n\n\n- 예시2: 결측치가 50% 이상인 열 출력\n\ndf.loc[:,(df.isna().sum() / len(df)) &gt; 0.5]\n\n\n\n\n\n\n\n\nLoaned From\nBest Overall Rating\n\n\n\n\n0\nNaN\nNaN\n\n\n1\nNaN\nNaN\n\n\n2\nNaN\nNaN\n\n\n3\nNaN\nNaN\n\n\n4\nNaN\nNaN\n\n\n...\n...\n...\n\n\n17655\nNaN\nNaN\n\n\n17656\nNaN\nNaN\n\n\n17657\nNaN\nNaN\n\n\n17658\nNaN\nNaN\n\n\n17659\nNaN\nNaN\n\n\n\n\n17660 rows × 2 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#h.-df.drop",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#h.-df.drop",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "H. df.drop()",
    "text": "H. df.drop()\n- 예시1: [0,1,2,3] 행을 drop\n\n# df.drop([0,1,2,3])\ndf.drop([0,1,2,3],axis=0)\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n5\n212622\nJ. Kimmich\n27\nhttps://cdn.sofifa.net/players/212/622/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n89\n90\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos9\"&gt;RDM\nJul 1, 2015\nNaN\n2025\n177cm\n75kg\n€182M\n6.0\nNaN\n\n\n6\n197445\nD. Alaba\n30\nhttps://cdn.sofifa.net/players/197/445/23_60.png\nAustria\nhttps://cdn.sofifa.net/flags/at.png\n86\n86\nReal Madrid CF\nhttps://cdn.sofifa.net/teams/243/30.png\n...\nYes\n&lt;span class=\"pos pos6\"&gt;LCB\nJul 1, 2021\nNaN\n2026\n180cm\n78kg\n€113.8M\n4.0\nNaN\n\n\n7\n187961\n22 Paulinho\n32\nhttps://cdn.sofifa.net/players/187/961/22_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n83\n83\nAl Ahli\nhttps://cdn.sofifa.net/teams/112387/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJul 22, 2021\nNaN\n2024\n183cm\n80kg\n€48.5M\n15.0\nNaN\n\n\n8\n208333\nE. Can\n28\nhttps://cdn.sofifa.net/players/208/333/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n82\n82\nBorussia Dortmund\nhttps://cdn.sofifa.net/teams/22/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nFeb 18, 2020\nNaN\n2024\n186cm\n86kg\n€51.9M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17656 rows × 29 columns\n\n\n\n- 예시2: [‘Name’, ‘Age’] 열을 drop\n\ndf.drop(['Name','Age'],axis=1)\n# df.drop(columns=['Name','Age'])\n\n\n\n\n\n\n\n\nID\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\nValue\nWage\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n€91M\n€115K\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n€78.5M\n€190K\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n€46.5M\n€46K\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n€107.5M\n€350K\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n€89.5M\n€110K\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n€100K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n€100K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n€70K\n€2K\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n€90K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n€90K\n€500\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17660 rows × 27 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#gh-에-대한-연습문제",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#gh-에-대한-연습문제",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "# G~H 에 대한 연습문제",
    "text": "# G~H 에 대한 연습문제\n# 예제: 결측치가 50퍼 이상인 열을 제외하라.\n- (풀이1)\n\ndf.isna().mean(axis=0) &gt; 0.5\n\nID                          False\nName                        False\nAge                         False\nPhoto                       False\nNationality                 False\nFlag                        False\nOverall                     False\nPotential                   False\nClub                        False\nClub Logo                   False\nValue                       False\nWage                        False\nSpecial                     False\nPreferred Foot              False\nInternational Reputation    False\nWeak Foot                   False\nSkill Moves                 False\nWork Rate                   False\nBody Type                   False\nReal Face                   False\nPosition                    False\nJoined                      False\nLoaned From                  True\nContract Valid Until        False\nHeight                      False\nWeight                      False\nRelease Clause              False\nKit Number                  False\nBest Overall Rating          True\ndtype: bool\n\n\n\ndf.drop(columns=['Loaned From','Best Overall Rating'])\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nWork Rate\nBody Type\nReal Face\nPosition\nJoined\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nHigh/ Medium\nUnique\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\n2026\n189cm\n82kg\n€157M\n8.0\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\n2026\n179cm\n69kg\n€155M\n8.0\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nHigh/ High\nStocky (170-185)\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\n2024\n172cm\n69kg\n€97.7M\n19.0\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\n2025\n181cm\n70kg\n€198.9M\n17.0\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nHigh/ High\nNormal (170-)\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\n2026\n172cm\n68kg\n€154.4M\n23.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\n2027\n190cm\n78kg\n€218K\n35.0\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\n2026\n195cm\n84kg\n€188K\n21.0\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\n2023\n190cm\n82kg\n€142K\n12.0\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\n2021\n187cm\n79kg\n€214K\n40.0\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\n2021\n186cm\n78kg\n€131K\n30.0\n\n\n\n\n17660 rows × 27 columns\n\n\n\n- (풀이2)\n\ndf.loc[:,df.isna().mean() &lt; 0.5] # False인 것을 제외하고 선택!\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nWork Rate\nBody Type\nReal Face\nPosition\nJoined\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nHigh/ Medium\nUnique\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\n2026\n189cm\n82kg\n€157M\n8.0\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\n2026\n179cm\n69kg\n€155M\n8.0\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nHigh/ High\nStocky (170-185)\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\n2024\n172cm\n69kg\n€97.7M\n19.0\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nHigh/ High\nUnique\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\n2025\n181cm\n70kg\n€198.9M\n17.0\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nHigh/ High\nNormal (170-)\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\n2026\n172cm\n68kg\n€154.4M\n23.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\n2027\n190cm\n78kg\n€218K\n35.0\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\n2026\n195cm\n84kg\n€188K\n21.0\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nMedium/ Medium\nLean (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\n2023\n190cm\n82kg\n€142K\n12.0\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\n2021\n187cm\n79kg\n€214K\n40.0\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nMedium/ Medium\nNormal (185+)\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\n2021\n186cm\n78kg\n€131K\n30.0\n\n\n\n\n17660 rows × 27 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#a.-numpy",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#a.-numpy",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "A. Numpy",
    "text": "A. Numpy\n- 발생: np.nan\n\nnp.nan\n\nnan\n\n\n\n[1,2,3,np.nan]\n\n[1, 2, 3, nan]\n\n\n\narr = np.array([1,2,3,np.nan])\narr\n\narray([ 1.,  2.,  3., nan])\n\n\n- np.array에 nan이 있으면 연산결과도 nan\n\narr.mean()\n\nnan\n\n\n- type\n\ntype(np.nan) \n\nfloat\n\n\n\ntype(arr[0])\n\nnumpy.float64\n\n\n\ntype(arr[-1])\n\nnumpy.float64"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#b.-pandas",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#b.-pandas",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "B. Pandas",
    "text": "B. Pandas\n\nPandas Sereis로 바꾸게 되면? NAN값을 빈칸으로 생각.\n\n- 발생: np.nan, pd.NA\n\npd.Series([np.nan,1,2,3])\n\n0    NaN\n1    1.0\n2    2.0\n3    3.0\ndtype: float64\n\n\n\npd.Series([pd.NA,1,2,3])\n\n0    &lt;NA&gt;\n1       1\n2       2\n3       3\ndtype: object\n\n\n- pd.Series에 NaN 혹은 &lt;NA&gt; 가 있다면 연산할때 제외함\n\npd.Series([np.nan,1,2,3]).mean()\n\n2.0\n\n\n\npd.Series([pd.NA,1,2,3]).mean()\n\n2.0\n\n\n- type\n\ns1 = pd.Series([np.nan,1,2,3])\ntype(s1[0])\n\nnumpy.float64\n\n\n\ns2 = pd.Series([pd.NA,1,2,3])\ntype(s2[0])\n\npandas._libs.missing.NAType\n\n\n\nPandas 내부적으로 숨겨져 있는데 NAType.\n\n- 검출 (\\(\\star\\))\n\ns1.isna()\n\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\n\n\ns2.isna()\n\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\n\n\nnan도 missing, na도 missing 이라고 판단한다.\n\n\npd.isna(s1[0]), pd.isnull(s1[0])\n\n(True, True)\n\n\n\npd.isna(s2[0]), pd.isnull(s2[0])\n\n(True, True)\n\n\n\nid(pd.isna), id(pd.isnull) # 같은함수 (같은주소에 있는 서로 다른 이름.)\n\n(140118877856240, 140118877856240)\n\n\n\n똑같은지 아닌지 의심스러우면 id로 확인하면 된다."
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#a.-기본-query",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#a.-기본-query",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "A. 기본 query",
    "text": "A. 기본 query\n- 예시1: A&gt;0 and B&lt;0\n\nts.query('A&gt;0 and B&lt;0')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-31\n1.437765\n-1.100252\n0.062147\n-0.642809\nA\n\n\n2023-01-04\n1.611089\n-1.187192\n-0.247400\n0.446779\nA\n\n\n2023-01-10\n0.576171\n-1.193324\n-0.119947\n-0.147450\nB\n\n\n2023-01-11\n1.034063\n-2.751738\n-0.257993\n-0.747487\nB\n\n\n\n\n\n\n\n- 예시2: A&lt;B&lt;C\n\nts.query('A&lt;B&lt;C')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-28\n-0.402852\n-0.342033\n0.810158\n0.289628\nA\n\n\n2023-01-08\n0.569372\n0.652307\n0.852527\n0.590511\nB\n\n\n\n\n\n\n\n- 예시3: (A+B)/2 &gt; 0\n\nts.query('(A+B)&gt;0')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n1.138250\n2.569641\n0.684869\n1.397645\nA\n\n\n2022-12-31\n1.437765\n-1.100252\n0.062147\n-0.642809\nA\n\n\n2023-01-01\n2.897278\n0.132532\n1.248877\n0.451774\nA\n\n\n2023-01-04\n1.611089\n-1.187192\n-0.247400\n0.446779\nA\n\n\n2023-01-05\n-0.440344\n1.053794\n0.678581\n-1.113279\nB\n\n\n2023-01-07\n-0.349422\n0.717378\n-0.662365\n-0.447297\nB\n\n\n2023-01-08\n0.569372\n0.652307\n0.852527\n0.590511\nB\n\n\n2023-01-09\n0.282654\n1.453329\n-0.682983\n-0.276578\nB\n\n\n\n\n\n\n\n- 예시4: (A+B/2) &gt; 0 and E=='A'\n\nts.query('(A+B)&gt;0 and E==\"A\"')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n1.138250\n2.569641\n0.684869\n1.397645\nA\n\n\n2022-12-31\n1.437765\n-1.100252\n0.062147\n-0.642809\nA\n\n\n2023-01-01\n2.897278\n0.132532\n1.248877\n0.451774\nA\n\n\n2023-01-04\n1.611089\n-1.187192\n-0.247400\n0.446779\nA\n\n\n\n\n\n\n\n\nts.query(\"(A+B)&gt;0 and E=='A'\")\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n1.138250\n2.569641\n0.684869\n1.397645\nA\n\n\n2022-12-31\n1.437765\n-1.100252\n0.062147\n-0.642809\nA\n\n\n2023-01-01\n2.897278\n0.132532\n1.248877\n0.451774\nA\n\n\n2023-01-04\n1.611089\n-1.187192\n-0.247400\n0.446779\nA"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#b.-외부변수를-이용",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#b.-외부변수를-이용",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "B. 외부변수를 이용",
    "text": "B. 외부변수를 이용\n- 예시1: A &gt; mean(A)\n\nts[ts.A &gt; ts.A.mean()]\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n1.138250\n2.569641\n0.684869\n1.397645\nA\n\n\n2022-12-31\n1.437765\n-1.100252\n0.062147\n-0.642809\nA\n\n\n2023-01-01\n2.897278\n0.132532\n1.248877\n0.451774\nA\n\n\n2023-01-04\n1.611089\n-1.187192\n-0.247400\n0.446779\nA\n\n\n2023-01-08\n0.569372\n0.652307\n0.852527\n0.590511\nB\n\n\n2023-01-09\n0.282654\n1.453329\n-0.682983\n-0.276578\nB\n\n\n2023-01-10\n0.576171\n-1.193324\n-0.119947\n-0.147450\nB\n\n\n2023-01-11\n1.034063\n-2.751738\n-0.257993\n-0.747487\nB\n\n\n\n\n\n\n\n\n# ts.query('A &gt; A.mean()') # 이게되기는 함.\n# ts.query('A &gt; np.mean(A)\") # 이것은 불가능함.\n# ts.query('A &gt; -0.0009126500034361273') # 이것이 가능한데\n\n\nmean = ts.A.mean()\nmean\n\n-0.0009126500034361273\n\n\n\n# ts.query('A &gt; mean') # 데이터프레임 내부에 mean이라는 컬럼이 없어서 query가 헷갈려한다. &gt;&gt; 에러\n\n\nts.query('A &gt; @mean') # 외부에 있는 변수를 참조하겠다. &gt;&gt; \"@\"\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n1.138250\n2.569641\n0.684869\n1.397645\nA\n\n\n2022-12-31\n1.437765\n-1.100252\n0.062147\n-0.642809\nA\n\n\n2023-01-01\n2.897278\n0.132532\n1.248877\n0.451774\nA\n\n\n2023-01-04\n1.611089\n-1.187192\n-0.247400\n0.446779\nA\n\n\n2023-01-08\n0.569372\n0.652307\n0.852527\n0.590511\nB\n\n\n2023-01-09\n0.282654\n1.453329\n-0.682983\n-0.276578\nB\n\n\n2023-01-10\n0.576171\n-1.193324\n-0.119947\n-0.147450\nB\n\n\n2023-01-11\n1.034063\n-2.751738\n-0.257993\n-0.747487\nB\n\n\n\n\n\n\n\n\nts.query('A &gt; A.mean()') # 이것도 가능은 함.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-27\n1.138250\n2.569641\n0.684869\n1.397645\nA\n\n\n2022-12-31\n1.437765\n-1.100252\n0.062147\n-0.642809\nA\n\n\n2023-01-01\n2.897278\n0.132532\n1.248877\n0.451774\nA\n\n\n2023-01-04\n1.611089\n-1.187192\n-0.247400\n0.446779\nA\n\n\n2023-01-08\n0.569372\n0.652307\n0.852527\n0.590511\nB\n\n\n2023-01-09\n0.282654\n1.453329\n-0.682983\n-0.276578\nB\n\n\n2023-01-10\n0.576171\n-1.193324\n-0.119947\n-0.147450\nB\n\n\n2023-01-11\n1.034063\n-2.751738\n-0.257993\n-0.747487\nB"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#c.-index로-query",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#c.-index로-query",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "C. Index로 query",
    "text": "C. Index로 query\n- 예시: (2022년 12월30일 보다 이전 날짜) \\(\\cup\\) (2023년 1월10일)\n\nts.query('index &lt;= \"2022-12-30\" or index == \"2023-01-10\"')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n2022-12-26\n-0.290124\n-0.396444\n0.560434\n-1.443280\nA\n\n\n2022-12-27\n1.138250\n2.569641\n0.684869\n1.397645\nA\n\n\n2022-12-28\n-0.402852\n-0.342033\n0.810158\n0.289628\nA\n\n\n2022-12-29\n-2.158452\n-0.307801\n-0.747125\n1.191654\nA\n\n\n2022-12-30\n-1.524006\n0.157796\n-1.288993\n0.974777\nA\n\n\n2023-01-10\n0.576171\n-1.193324\n-0.119947\n-0.147450\nB\n\n\n\n\n\n\n\n\n공백이 있는 변수들은 사용이 안되었었는데 지금은 된다."
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#d.-열의-이름에-공백이-있을-경우",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#d.-열의-이름에-공백이-있을-경우",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "D. 열의 이름에 공백이 있을 경우",
    "text": "D. 열의 이름에 공백이 있을 경우\n- 예시1: 열의 이름에 공백이 있으면 백틱(``)을 이용하면 된다.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ndf.head()\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n\ndf['Skill Moves'].describe()\n\ncount    17660.000000\nmean         2.297169\nstd          0.754264\nmin          1.000000\n25%          2.000000\n50%          2.000000\n75%          3.000000\nmax          5.000000\nName: Skill Moves, dtype: float64\n\n\n\ndf.query(\"`Skill Moves` &gt; 4\") # 백틱을 이용해서 공백을 포함한 변수이름을 감싸면 됨.\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n19\n193082\nJ. Cuadrado\n34\nhttps://cdn.sofifa.net/players/193/082/23_60.png\nColombia\nhttps://cdn.sofifa.net/flags/co.png\n83\n83\nJuventus\nhttps://cdn.sofifa.net/teams/45/30.png\n...\nYes\n&lt;span class=\"pos pos3\"&gt;RB\nJul 1, 2017\nNaN\n2023\n179cm\n72kg\n€23M\n11.0\nNaN\n\n\n27\n189509\nThiago\n31\nhttps://cdn.sofifa.net/players/189/509/23_60.png\nSpain\nhttps://cdn.sofifa.net/flags/es.png\n86\n86\nLiverpool\nhttps://cdn.sofifa.net/teams/9/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nSep 18, 2020\nNaN\n2024\n174cm\n70kg\n€102.7M\n6.0\nNaN\n\n\n44\n232411\nC. Nkunku\n24\nhttps://cdn.sofifa.net/players/232/411/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n86\n89\nRB Leipzig\nhttps://cdn.sofifa.net/teams/112172/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\nNaN\nNaN\n175cm\n73kg\n€166.9M\n12.0\nNaN\n\n\n62\n233927\nLucas Paquetá\n24\nhttps://cdn.sofifa.net/players/233/927/23_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n82\n87\nOlympique Lyonnais\nhttps://cdn.sofifa.net/teams/66/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nOct 1, 2020\nNaN\n2025\n180cm\n72kg\n€90.9M\n10.0\nNaN\n\n\n75\n231747\nK. Mbappé\n23\nhttps://cdn.sofifa.net/players/231/747/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n91\n95\nParis Saint-Germain\nhttps://cdn.sofifa.net/teams/73/30.png\n...\nYes\n&lt;span class=\"pos pos25\"&gt;ST\nJul 1, 2018\nNaN\n2025\n182cm\n73kg\n€366.7M\n7.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4516\n253755\nTalles Magno\n20\nhttps://cdn.sofifa.net/players/253/755/23_60.png\nBrazil\nhttps://cdn.sofifa.net/flags/br.png\n71\n83\nNew York City FC\nhttps://cdn.sofifa.net/teams/112828/30.png\n...\nNo\n&lt;span class=\"pos pos16\"&gt;LM\nMay 18, 2021\nNaN\n2026\n186cm\n70kg\n€7.7M\n43.0\nNaN\n\n\n4643\n246548\nO. Sahraoui\n21\nhttps://cdn.sofifa.net/players/246/548/23_60.png\nNorway\nhttps://cdn.sofifa.net/flags/no.png\n67\n78\nVålerenga Fotball\nhttps://cdn.sofifa.net/teams/920/30.png\n...\nNo\n&lt;span class=\"pos pos27\"&gt;LW\nMay 15, 2019\nNaN\n2023\n170cm\n65kg\n€3.3M\n10.0\nNaN\n\n\n4872\n251570\nR. Cherki\n18\nhttps://cdn.sofifa.net/players/251/570/23_60.png\nFrance\nhttps://cdn.sofifa.net/flags/fr.png\n73\n88\nOlympique Lyonnais\nhttps://cdn.sofifa.net/teams/66/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 7, 2019\nNaN\n2023\n176cm\n71kg\n€17.7M\n18.0\nNaN\n\n\n5361\n225712\nD. Bahamboula\n27\nhttps://cdn.sofifa.net/players/225/712/23_60.png\nCongo\nhttps://cdn.sofifa.net/flags/cg.png\n63\n63\nLivingston FC\nhttps://cdn.sofifa.net/teams/621/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 9, 2022\nNaN\n2024\n185cm\n70kg\n€875K\n7.0\nNaN\n\n\n10452\n212455\n17 H. Mastour\n18\nhttps://cdn.sofifa.net/players/212/455/17_60.png\nMorocco\nhttps://cdn.sofifa.net/flags/ma.png\n65\n76\nPEC Zwolle\nhttps://cdn.sofifa.net/teams/1914/30.png\n...\nNo\n&lt;span class=\"pos pos28\"&gt;SUB\nNaN\n&lt;a href=\"/team/47/ac-milan/\"&gt;AC Milan&lt;/a&gt;\nJun 30, 2017\n175cm\n63kg\nNaN\n98.0\nNaN\n\n\n\n\n65 rows × 29 columns"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#a.-df.assign-추천star",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#a.-df.assign-추천star",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "A. df.assign() – 추천(\\(\\star\\))",
    "text": "A. df.assign() – 추천(\\(\\star\\))\n- 예시: total = att*0.1 + rep*0.2 + mid*0.35 + fin*0.35 를 계산하여 할당\n\n_total = df.att*0.1 + df.rep*0.2 + df.mid*0.35 + df.fin*0.35\ndf.assign(total = _total).head() # 원본을 손상시키지 않음.\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n49.00\n\n\n1\n95\n100\n50\n80\n75.00\n\n\n2\n65\n90\n60\n30\n56.00\n\n\n3\n55\n80\n75\n80\n75.75\n\n\n4\n80\n30\n30\n100\n59.50\n\n\n\n\n\n\n\n\n# df.assign(total = df.att*0.1 + df.rep*0.2 + df.mid*0.35 + df.fin*0.35) \n\n\ndf.head()\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n0\n65\n55\n50\n40\n\n\n1\n95\n100\n50\n80\n\n\n2\n65\n90\n60\n30\n\n\n3\n55\n80\n75\n80\n\n\n4\n80\n30\n30\n100\n\n\n\n\n\n\n\n\nNote: 이 방법은 df를 일시적으로 변화시킴"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#b.-df.eval",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#b.-df.eval",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "B. df.eval()",
    "text": "B. df.eval()\n- 예시: total = att*0.1 + rep*0.2 + mid*0.35 + fin*0.35 를 계산하여 할당\n\ndf.eval('total = att*0.1 + rep*0.2 + mid*0.3 + fin*0.4') \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n48.5\n\n\n1\n95\n100\n50\n80\n76.5\n\n\n2\n65\n90\n60\n30\n54.5\n\n\n3\n55\n80\n75\n80\n76.0\n\n\n4\n80\n30\n30\n100\n63.0\n\n\n5\n75\n40\n100\n15\n51.5\n\n\n6\n65\n45\n45\n90\n65.0\n\n\n7\n60\n60\n25\n0\n25.5\n\n\n8\n95\n65\n20\n10\n32.5\n\n\n9\n90\n80\n80\n20\n57.0\n\n\n10\n55\n75\n35\n25\n41.0\n\n\n11\n95\n95\n45\n0\n42.0\n\n\n12\n95\n55\n15\n35\n39.0\n\n\n13\n50\n80\n40\n30\n45.0\n\n\n14\n50\n55\n15\n85\n54.5\n\n\n15\n95\n30\n30\n95\n62.5\n\n\n16\n50\n50\n45\n10\n32.5\n\n\n17\n65\n55\n15\n45\n40.0\n\n\n18\n70\n70\n40\n35\n47.0\n\n\n19\n90\n90\n80\n90\n87.0\n\n\n\n\n\n\n\n\n복잡한 경우 잘 적용이 안되는 경우가 있음.\n\n\n이 방법은 df를 일시적으로 변화시킴"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#c.-dfcolname-xxx",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#c.-dfcolname-xxx",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "C. df[colname] = xxx",
    "text": "C. df[colname] = xxx\n\ndct = {'A':[1,2,3], 'B':[3,4,5]}\ndct\n\n{'A': [1, 2, 3], 'B': [3, 4, 5]}\n\n\n\ndct['C'] = [4,5,6]\ndct\n\n{'A': [1, 2, 3], 'B': [3, 4, 5], 'C': [4, 5, 6]}\n\n\n- 예시: total = att*0.1 + rep*0.2 + mid*0.35 + fin*0.35 를 계산하여 할당\n\ndf['total'] = df.att*0.1 + df.rep*0.2 + df.mid*0.3 + df.fin*0.4\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\ntotal\n\n\n\n\n0\n65\n55\n50\n40\n48.5\n\n\n1\n95\n100\n50\n80\n76.5\n\n\n2\n65\n90\n60\n30\n54.5\n\n\n3\n55\n80\n75\n80\n76.0\n\n\n4\n80\n30\n30\n100\n63.0\n\n\n5\n75\n40\n100\n15\n51.5\n\n\n6\n65\n45\n45\n90\n65.0\n\n\n7\n60\n60\n25\n0\n25.5\n\n\n8\n95\n65\n20\n10\n32.5\n\n\n9\n90\n80\n80\n20\n57.0\n\n\n10\n55\n75\n35\n25\n41.0\n\n\n11\n95\n95\n45\n0\n42.0\n\n\n12\n95\n55\n15\n35\n39.0\n\n\n13\n50\n80\n40\n30\n45.0\n\n\n14\n50\n55\n15\n85\n54.5\n\n\n15\n95\n30\n30\n95\n62.5\n\n\n16\n50\n50\n45\n10\n32.5\n\n\n17\n65\n55\n15\n45\n40.0\n\n\n18\n70\n70\n40\n35\n47.0\n\n\n19\n90\n90\n80\n90\n87.0\n\n\n\n\n\n\n\n\n싫어하는 이유는 원래 데이터프레임을 손상시킨다.\n\n\n이 방법은 df를 영구적으로 변화시킴"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#a.-lambda",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#a.-lambda",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "A. lambda",
    "text": "A. lambda\n- 예시1: \\(x \\to x+2\\)\n\n(lambda x: x+2)(1)\n\n3\n\n\n\n(lambda x: x+2) 통째로 함수역할을 한다.\n\n\nf = lambda x: x+2 \nf(1)\n\n3\n\n\n- 예시2: \\(x,y \\to x+y\\)\n\nlambda x,y: x+y # 통째로 함수\n\n&lt;function __main__.&lt;lambda&gt;(x, y)&gt;\n\n\n\n(lambda x,y: x+y)(3,5)\n\n8\n\n\n\nf = lambda x,y: x+y \nf(3,5)\n\n8\n\n\n- 예시3: ‘2023-09’ \\(\\to\\) 9\n\nx = '2023-09'\n\n\nint(x[-2:])\n\n9\n\n\n\nlambda x: int(x[-2:]) # 통째로 함수\n\n&lt;function __main__.&lt;lambda&gt;(x)&gt;\n\n\n\n(lambda x: int(x[-2:]))('2023-09')\n\n9\n\n\n\nf = lambda x: int(x[-2:])\nf('2023-09')\n\n9\n\n\n\nf('2022-10')\n\n10\n\n\n- 예시4: ‘2023-09’ \\(\\to\\) (2023,9)\n\nf = lambda x: (int(x[:4]),int(x[-2:]))\nf('2023-09')\n\n(2023, 9)\n\n\n\nf('2023-12')\n\n(2023, 12)\n\n\n- 예시5: 문자열이 ‘cat’이면 1 ’dog’ 이면 0 // ’cat이면 1 ’cat’이 아니면 0\n\ndef f(x):\n    return 1 if x=='cat' else 0\n\n\nf('cat'), f('dog')\n\n(1, 0)\n\n\n\n(lambda x: 1 if x=='cat' else 0)('dog'), (lambda x: 1 if x=='cat' else 0)('cat')\n\n(0, 1)\n\n\n\nf = lambda x: 1 if x=='cat' else 0 \n\n\nf('cat'),f('dog')\n\n(1, 0)\n\n\n- Note: f로 이름을 정하지 않고 직접 사용 가능\n\n(lambda x: x+1)(1)\n\n2\n\n\n\n(lambda x,y: x+y)(2,3)\n\n5"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#b.-map",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#b.-map",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "B. map",
    "text": "B. map\n- 개념: map(f,[x1,x2,...xn])=[f(x1),f(x2),...,f(xn)]\n- 예시1: x-&gt;x+1을 [1,2,3]에 적용\nmap(함수비슷한것(callable), 리스트비슷한것(iterable))\n\nf = lambda x: x+1 \nmap(f,[1,2,3]) # 뭔가 나옴. --&gt; 리스트화 시켜서 보자.\n\n&lt;map at 0x7f6fdffb95e0&gt;\n\n\n\nf = lambda x: x+1 \nlist(map(f,[1,2,3]))\n\n[2, 3, 4]\n\n\n- 예시2 df.Height열 변환하기\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height[:5]\ns\n\n0    189cm\n1    179cm\n2    172cm\n3    181cm\n4    172cm\nName: Height, dtype: object\n\n\n\nx = s[0]\nx\n\n'189cm'\n\n\n\nlist(map(lambda x: int(x.replace('cm','')), s))\n\n[189, 179, 172, 181, 172]\n\n\n\nlist(map(lambda x: int(x[:-2]),s))\n\n[189, 179, 172, 181, 172]\n\n\n# 예시3 df.Height열 변환하기 + 변환된 열 할당하기\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\n\n- (풀이1): df.assign(Height=새롭게 잘 변환한 리스트 같은 것.)\n\ndf.assign(Height = list(map(lambda x: int(x.replace('cm','')), df.Height))).info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17660 entries, 0 to 17659\nData columns (total 29 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   ID                        17660 non-null  int64  \n 1   Name                      17660 non-null  object \n 2   Age                       17660 non-null  int64  \n 3   Photo                     17660 non-null  object \n 4   Nationality               17660 non-null  object \n 5   Flag                      17660 non-null  object \n 6   Overall                   17660 non-null  int64  \n 7   Potential                 17660 non-null  int64  \n 8   Club                      17449 non-null  object \n 9   Club Logo                 17660 non-null  object \n 10  Value                     17660 non-null  object \n 11  Wage                      17660 non-null  object \n 12  Special                   17660 non-null  int64  \n 13  Preferred Foot            17660 non-null  object \n 14  International Reputation  17660 non-null  float64\n 15  Weak Foot                 17660 non-null  float64\n 16  Skill Moves               17660 non-null  float64\n 17  Work Rate                 17660 non-null  object \n 18  Body Type                 17622 non-null  object \n 19  Real Face                 17622 non-null  object \n 20  Position                  17625 non-null  object \n 21  Joined                    16562 non-null  object \n 22  Loaned From               694 non-null    object \n 23  Contract Valid Until      17299 non-null  object \n 24  Height                    17660 non-null  int64  \n 25  Weight                    17660 non-null  object \n 26  Release Clause            16509 non-null  object \n 27  Kit Number                17625 non-null  float64\n 28  Best Overall Rating       21 non-null     object \ndtypes: float64(4), int64(6), object(19)\nmemory usage: 3.9+ MB\n\n\n\nHeight가 int로 바뀐 것을 확인할 수 있다.\n\n- (풀이2) – 사실 수틀리면 컴프리헨션 쓰면 된다.\n\ndf.assign(Height = [int(s.replace('cm', '')) for s in df.Height])\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\n&lt;span class=\"pos pos28\"&gt;SUB\nJul 1, 2018\nNaN\n2026\n189\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\n&lt;span class=\"pos pos15\"&gt;LCM\nJan 30, 2020\nNaN\n2026\n179\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\n&lt;span class=\"pos pos7\"&gt;LB\nSep 14, 2020\nNaN\n2024\n172\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nAug 30, 2015\nNaN\n2025\n181\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\n&lt;span class=\"pos pos13\"&gt;RCM\nSep 1, 2020\nNaN\n2026\n172\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nApr 11, 2022\nNaN\n2027\n190\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2022\nNaN\n2026\n195\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJun 6, 2021\nNaN\n2023\n190\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 1, 2020\nNaN\n2021\n187\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\n&lt;span class=\"pos pos29\"&gt;RES\nJan 8, 2020\nNaN\n2021\n186\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n#\n# 예시4 – df.Position 열에 아래와 같은 변환을 수행하고, 변환된 열을 할당하라.\n\n\n\nbefore\nafter\n\n\n\n\n&lt;span class=\"pos pos28\"&gt;SUB\nSUB\n\n\n&lt;span class=\"pos pos15\"&gt;LCM\nLCM\n\n\n&lt;span class=\"pos pos7\"&gt;LB\nLB\n\n\n&lt;span class=\"pos pos13\"&gt;RCM\nRCM\n\n\n&lt;span class=\"pos pos13\"&gt;RCM\nRCM\n\n\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\n\n- (풀이1)\n\nx = df.Position[0]\nx\n\n'&lt;span class=\"pos pos28\"&gt;SUB'\n\n\n\ndf.Position.isna().sum() # missing이 float형으로 되어있음.\n\n35\n\n\n\nlist((lambda x: x.split('&gt;')[-1], df.Position))\n\n[&lt;function __main__.&lt;lambda&gt;(x)&gt;,\n 0        &lt;span class=\"pos pos28\"&gt;SUB\n 1        &lt;span class=\"pos pos15\"&gt;LCM\n 2          &lt;span class=\"pos pos7\"&gt;LB\n 3        &lt;span class=\"pos pos13\"&gt;RCM\n 4        &lt;span class=\"pos pos13\"&gt;RCM\n                     ...             \n 17655    &lt;span class=\"pos pos29\"&gt;RES\n 17656    &lt;span class=\"pos pos29\"&gt;RES\n 17657    &lt;span class=\"pos pos29\"&gt;RES\n 17658    &lt;span class=\"pos pos29\"&gt;RES\n 17659    &lt;span class=\"pos pos29\"&gt;RES\n Name: Position, Length: 17660, dtype: object]\n\n\n\nlist(map(lambda x: x.split('&gt;')[-1] if not pd.isna(x) else 'NA', df.Position))[:10]\n\n['SUB', 'LCM', 'LB', 'RCM', 'RCM', 'RDM', 'LCB', 'LCM', 'SUB', 'LB']\n\n\n\ndf.assign(Position = list(map(lambda x: x.split(\"&gt;\")[-1] if not pd.isna(x) else 'None', df.Position)))\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\nSUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\nLCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\nLB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\nRCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\nRCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\nRES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\nRES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\nRES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\nRES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\nRES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n- (풀이2) – 리스트컴프리헨션\n먼저 함수를 f라고 받아놓자.\n\nf = lambda x: x.split(\"&gt;\")[-1] if not pd.isna(x) else 'None' \n\n\ndf.assign(Position = [f(x) for x in df.Position])\n\n\n\n\n\n\n\n\nID\nName\nAge\nPhoto\nNationality\nFlag\nOverall\nPotential\nClub\nClub Logo\n...\nReal Face\nPosition\nJoined\nLoaned From\nContract Valid Until\nHeight\nWeight\nRelease Clause\nKit Number\nBest Overall Rating\n\n\n\n\n0\n209658\nL. Goretzka\n27\nhttps://cdn.sofifa.net/players/209/658/23_60.png\nGermany\nhttps://cdn.sofifa.net/flags/de.png\n87\n88\nFC Bayern München\nhttps://cdn.sofifa.net/teams/21/30.png\n...\nYes\nSUB\nJul 1, 2018\nNaN\n2026\n189cm\n82kg\n€157M\n8.0\nNaN\n\n\n1\n212198\nBruno Fernandes\n27\nhttps://cdn.sofifa.net/players/212/198/23_60.png\nPortugal\nhttps://cdn.sofifa.net/flags/pt.png\n86\n87\nManchester United\nhttps://cdn.sofifa.net/teams/11/30.png\n...\nYes\nLCM\nJan 30, 2020\nNaN\n2026\n179cm\n69kg\n€155M\n8.0\nNaN\n\n\n2\n224334\nM. Acuña\n30\nhttps://cdn.sofifa.net/players/224/334/23_60.png\nArgentina\nhttps://cdn.sofifa.net/flags/ar.png\n85\n85\nSevilla FC\nhttps://cdn.sofifa.net/teams/481/30.png\n...\nNo\nLB\nSep 14, 2020\nNaN\n2024\n172cm\n69kg\n€97.7M\n19.0\nNaN\n\n\n3\n192985\nK. De Bruyne\n31\nhttps://cdn.sofifa.net/players/192/985/23_60.png\nBelgium\nhttps://cdn.sofifa.net/flags/be.png\n91\n91\nManchester City\nhttps://cdn.sofifa.net/teams/10/30.png\n...\nYes\nRCM\nAug 30, 2015\nNaN\n2025\n181cm\n70kg\n€198.9M\n17.0\nNaN\n\n\n4\n224232\nN. Barella\n25\nhttps://cdn.sofifa.net/players/224/232/23_60.png\nItaly\nhttps://cdn.sofifa.net/flags/it.png\n86\n89\nInter\nhttps://cdn.sofifa.net/teams/44/30.png\n...\nYes\nRCM\nSep 1, 2020\nNaN\n2026\n172cm\n68kg\n€154.4M\n23.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17655\n269526\nDeng Xiongtao\n19\nhttps://cdn.sofifa.net/players/269/526/23_60.png\nChina PR\nhttps://cdn.sofifa.net/flags/cn.png\n48\n61\nMeizhou Hakka\nhttps://cdn.sofifa.net/teams/114628/30.png\n...\nNo\nRES\nApr 11, 2022\nNaN\n2027\n190cm\n78kg\n€218K\n35.0\nNaN\n\n\n17656\n267946\n22 Lim Jun Sub\n17\nhttps://cdn.sofifa.net/players/267/946/22_60.png\nKorea Republic\nhttps://cdn.sofifa.net/flags/kr.png\n48\n64\nJeju United FC\nhttps://cdn.sofifa.net/teams/1478/30.png\n...\nNo\nRES\nJan 1, 2022\nNaN\n2026\n195cm\n84kg\n€188K\n21.0\nNaN\n\n\n17657\n270567\nA. Demir\n25\nhttps://cdn.sofifa.net/players/270/567/23_60.png\nTurkey\nhttps://cdn.sofifa.net/flags/tr.png\n51\n56\nÜmraniyespor\nhttps://cdn.sofifa.net/teams/113796/30.png\n...\nNo\nRES\nJun 6, 2021\nNaN\n2023\n190cm\n82kg\n€142K\n12.0\nNaN\n\n\n17658\n256624\n21 S. Czajor\n18\nhttps://cdn.sofifa.net/players/256/624/21_60.png\nPoland\nhttps://cdn.sofifa.net/flags/pl.png\n50\n65\nFleetwood Town\nhttps://cdn.sofifa.net/teams/112260/30.png\n...\nNo\nRES\nJan 1, 2020\nNaN\n2021\n187cm\n79kg\n€214K\n40.0\nNaN\n\n\n17659\n256376\n21 F. Jakobsson\n20\nhttps://cdn.sofifa.net/players/256/376/21_60.png\nSweden\nhttps://cdn.sofifa.net/flags/se.png\n50\n61\nIFK Norrköping\nhttps://cdn.sofifa.net/teams/702/30.png\n...\nNo\nRES\nJan 8, 2020\nNaN\n2021\n186cm\n78kg\n€131K\n30.0\nNaN\n\n\n\n\n17660 rows × 29 columns\n\n\n\n\n[x.split('&gt;')[-1] for x in df.Position if not pd.isna(x)][:10] # missing일 경우에는 아예 빠지게 된다.\n\n['SUB', 'LCM', 'LB', 'RCM', 'RCM', 'RDM', 'LCB', 'LCM', 'SUB', 'LB']\n\n\n\ndf.assign(Position = [f(s) for s in df.Position])\n\n\n개인의견: 이미 람다로 함수를 만들어야 해서 그냥 map을 쓰는게 자연스러움"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#c.-s.apply변환함수",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#c.-s.apply변환함수",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "C. s.apply(변환함수)",
    "text": "C. s.apply(변환함수)\n- 예시: 원소별로 처음3개의 숫자만 출력\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height\n\n\ns\n\n0        189cm\n1        179cm\n2        172cm\n3        181cm\n4        172cm\n         ...  \n17655    190cm\n17656    195cm\n17657    190cm\n17658    187cm\n17659    186cm\nName: Height, Length: 17660, dtype: object\n\n\n\npd.Series(map(lambda x: int(x[:-2]), s))\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nLength: 17660, dtype: int64\n\n\n\n#pd.Series(map(lambda x: int(x[:-2]),s))\ns.apply(lambda x: int(x[:-2])) # 편한가?\n\n\ns.apply(lambda x: int(x[:-2]))\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: int64"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#d.-s.str",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#d.-s.str",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "D. s.str",
    "text": "D. s.str\n- 예시1: 원소별로 처음 3개의 숫자만 출력\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height\n\n\ns.str[:3]\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: object\n\n\n- 예시2: 원소별로 isupper를 수행\n\ns = pd.Series(['A','B','C','d','e','F'])\ns\n\n0    A\n1    B\n2    C\n3    d\n4    e\n5    F\ndtype: object\n\n\n\ns.str.isupper()\n\n0     True\n1     True\n2     True\n3    False\n4    False\n5     True\ndtype: bool"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-25-04wk-1-t.html#e.-s.astype",
    "href": "posts/2_DV2022/2023-09-25-04wk-1-t.html#e.-s.astype",
    "title": "[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column",
    "section": "E. s.astype()",
    "text": "E. s.astype()\n- 예시1: 원소의 타입을 모두 int형으로 변경\n\ns = pd.Series(list('12345'))\ns\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: object\n\n\n\ns.astype(np.int64)\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\n- 예시2: 원소별로 (1) 처음3개의 문자열만 취하고 (2) 원소의 타입을 모두 인트형으로 변경\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height\n\n\ns.str[:3].astype('int')\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: int64"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-16-07wk-1-t.html#a.-lambda-df-with-indexer",
    "href": "posts/2_DV2022/2023-10-16-07wk-1-t.html#a.-lambda-df-with-indexer",
    "title": "[DV2023] 07wk-1: Pandas – lambda df:의 활용, MultiIndex의 이해, tidydata의 이해, melt/stack",
    "section": "A. lambda df: with indexer",
    "text": "A. lambda df: with indexer\n- ref: https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-callable\n예시1 : 아래는 같은 문법이다.\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n(표현1)\n\ndf[df.A.isna()] # A컬럼에 접근해서 NAN이 있는지?\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n\ndf.A.isna()\n\n0    False\n1     True\n2    False\n3    False\nName: A, dtype: bool\n\n\n\n(lambda df: pd.Series([False, True, False, False]))('a') # 뭘 넣어도 F,T,F,F\n\n0    False\n1     True\n2    False\n3    False\ndtype: bool\n\n\n(표현2)\n\ndf[(lambda _df: _df.A.isna())(df)]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n(표현3)\n\ndf[lambda _df: _df.A.isna()] \n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n\n하나의 아규먼트 라는 것은 하나의 입력이라는 뜻.\ndf.loc[함수비슷한것:] , df.loc[:,함수비슷한것] , df.iloc[:, 함수비슷한것], df.iloc[함수비슷한것,:] 모두 가능하다.\n\n#\n- 예시2: loc, iloc 에서도 가능함\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n\ndf.loc[lambda _df: _df.A.isna(), :]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n\ndf.iloc[lambda _df: list(_df.A.isna()), :]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n\nf = lambda _df: _df.A.isna()\n# f(df)\npd.Series([False, True, False, False])\n\n0    False\n1     True\n2    False\n3    False\ndtype: bool\n\n\n\npd.iloc[pd.Series([False, True, False, False]),:] # 에러가나네?\n\nAttributeError: module 'pandas' has no attribute 'iloc'\n\n\n\ndf.iloc[list(pd.Series([False, True, False, False])),:] # iloc을 쓰려면 리스트로 만들어주면된다. (pandas Series는 에러가 남)\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n1\nNaN\n3.0\n4.0\n\n\n\n\n\n\n\n\niloc은 True, False 형태로 이루어진 pd.Series가 들어올 경우 인덱싱이 불가능하므로 리스트로 바꿔줘야했었음..\n\n예시3: 왜 이런 문법이 있을까? 연속적으로 dataFrame을 변화시킬 경우 유리한 테크닉\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\nstep1: D=A+B+C를 계산\n\n_df = df.assign(D = df.A+df.B+df.C)\n_df\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n\n\n1\nNaN\n3.0\n4.0\nNaN\n\n\n2\n1.0\nNaN\n5.0\nNaN\n\n\n3\n1.0\n4.0\n6.0\n11.0\n\n\n\n\n\n\n\n\ndf.eval('D=A+B+C')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n\n\n1\nNaN\n3.0\n4.0\nNaN\n\n\n2\n1.0\nNaN\n5.0\nNaN\n\n\n3\n1.0\n4.0\n6.0\n11.0\n\n\n\n\n\n\n\n- row별로 결측값 개수를 세서 E라는 변수를 만들자.\n1번 row에서는 결측치 2개, 2번 row에서는 결측치가 2개, 3번 row에서는 결측치가 2개, 4번 row에는 결측치가 0개…\n\ndf.isna().sum(axis=1)\n\n0    1\n1    1\n2    1\n3    0\ndtype: int64\n\n\n\ndf.eval('D=A+B+C').assign(E = df.isna().sum(axis=1)) # 이건 df에 연산을 건 것.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n1\n\n\n1\nNaN\n3.0\n4.0\nNaN\n1\n\n\n2\n1.0\nNaN\n5.0\nNaN\n1\n\n\n3\n1.0\n4.0\n6.0\n11.0\n0\n\n\n\n\n\n\n\n\ndf.eval('D=A+B+C').assign(E = lambda _df : _df.isna().sum(axis=1)) # lamba df\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n2\n\n\n1\nNaN\n3.0\n4.0\nNaN\n2\n\n\n2\n1.0\nNaN\n5.0\nNaN\n2\n\n\n3\n1.0\n4.0\n6.0\n11.0\n0\n\n\n\n\n\n\n\nstep2: 여기에서 결측치의 값이 50%가 넘는 열만 고르고 싶다면?\n\n_df.loc[:,_df.isna().mean() &gt; 0.5] # D열만 뽑히게 됨.\n\n\n\n\n\n\n\n\nD\n\n\n\n\n0\nNaN\n\n\n1\nNaN\n\n\n2\nNaN\n\n\n3\n11.0\n\n\n\n\n\n\n\n\ndf.assign(D=df.A+df.B+df.C).loc[:,lambda _df: _df.isna().mean()&gt;0.5]\n\n\n\n\n\n\n\n\nD\n\n\n\n\n0\nNaN\n\n\n1\nNaN\n\n\n2\nNaN\n\n\n3\n11.0\n\n\n\n\n\n\n\n\ndf.eval('D=A+B+C').isna().mean()\n\nA    0.25\nB    0.25\nC    0.25\nD    0.75\ndtype: float64\n\n\n\ndf.eval('D=A+B+C').loc[:,lambda _df: _df.isna().mean()&gt;0.5]\n\n\n\n\n\n\n\n\nD\n\n\n\n\n0\nNaN\n\n\n1\nNaN\n\n\n2\nNaN\n\n\n3\n11.0\n\n\n\n\n\n\n\n#"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-16-07wk-1-t.html#b.-lambda-df-with-assign",
    "href": "posts/2_DV2022/2023-10-16-07wk-1-t.html#b.-lambda-df-with-assign",
    "title": "[DV2023] 07wk-1: Pandas – lambda df:의 활용, MultiIndex의 이해, tidydata의 이해, melt/stack",
    "section": "B. lambda df: with assign",
    "text": "B. lambda df: with assign\n예시1\n\ndf = pd.DataFrame({'A':[-1,np.nan,1,1],'B':[2,3,np.nan,4],'C':[np.nan,4,5,6]})\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.0\n2.0\nNaN\n\n\n1\nNaN\n3.0\n4.0\n\n\n2\n1.0\nNaN\n5.0\n\n\n3\n1.0\n4.0\n6.0\n\n\n\n\n\n\n\n\ndf.eval('D=A+B+C')\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n\n\n1\nNaN\n3.0\n4.0\nNaN\n\n\n2\n1.0\nNaN\n5.0\nNaN\n\n\n3\n1.0\n4.0\n6.0\n11.0\n\n\n\n\n\n\n\n여기에서 결측치의 값을 row-wise하게 count하여 새로운열 E에 할당하고 싶다면?\n\n1번 row에서는 결측치 2개, 2번 row에서는 결측칠가 2개, 3번 row에서는 결측치가 2개, 4번 row에는 결측치가 0개…\n\n\ndf.eval('D=A+B+C').isna().sum(axis=1)\n# _df = df.eval('D=A+B+C')\n# _df.isna().sum(axis=1)\n\n0    2\n1    2\n2    2\n3    0\ndtype: int64\n\n\n\ndf.eval('D=A+B+C').assign(E=lambda _df: _df.isna().sum(axis=1))\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\n0\n-1.0\n2.0\nNaN\nNaN\n2\n\n\n1\nNaN\n3.0\n4.0\nNaN\n2\n\n\n2\n1.0\nNaN\n5.0\nNaN\n2\n\n\n3\n1.0\n4.0\n6.0\n11.0\n0\n\n\n\n\n\n\n\n#\n예시2 – 원본데이터를 손상시키지 않으며 데이터를 변형하고 싶을때..\n\nnp.random.seed(43052)\ndf = pd.DataFrame({'A':[12,234,3456,12345,654222]})\ndf\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\n12\n\n\n1\n234\n\n\n2\n3456\n\n\n3\n12345\n\n\n4\n654222\n\n\n\n\n\n\n\n(풀이1) – 복사본생성 (실패)\n\ndf2 = df \ndf2\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\n12\n\n\n1\n234\n\n\n2\n3456\n\n\n3\n12345\n\n\n4\n654222\n\n\n\n\n\n\n\n\ndf2['B'] = np.log(df.A)\ndf2['C'] = (df2.B - df2.B.mean())/df2.B.std()\n\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350\n\n\n\n\n\n\n\n\ndf # 니가 왜 여기서 나와?\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350\n\n\n\n\n\n\n\n\n이게 아닌데???\n이게 이렇게 되는 이유는 깊은복사 얕은복사 강의 참고.\n\n(풀이2) – 복사본생성 (성공)\n\nnp.random.seed(43052)\ndf = pd.DataFrame({'A':[12,234,3456,12345,654222]})\ndf\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\n12\n\n\n1\n234\n\n\n2\n3456\n\n\n3\n12345\n\n\n4\n654222\n\n\n\n\n\n\n\n\ndf2 = df.copy()\n\n\ndf2['B'] = np.log(df.A)\ndf2['C'] = (df2.B - df2.B.mean())/df2.B.std()\n\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350\n\n\n\n\n\n\n\n\ndf\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\n12\n\n\n1\n234\n\n\n2\n3456\n\n\n3\n12345\n\n\n4\n654222\n\n\n\n\n\n\n\n(풀이3) – assign + lambda df: 이용\n\ndf.assign(B = lambda df: np.log(df.A)).assign(C = lambda df: (df.B - df.B.mean())/df.B.std())\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350\n\n\n\n\n\n\n\n(풀이4) – eval 이용\n\ndf.eval('B=log(A)').eval('C=(B-B.mean())/B.std()') # 오? lambda df 안해도 되네?\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n12\n2.484907\n-1.286574\n\n\n1\n234\n5.455321\n-0.564847\n\n\n2\n3456\n8.147867\n0.089367\n\n\n3\n12345\n9.421006\n0.398704\n\n\n4\n654222\n13.391202\n1.363350\n\n\n\n\n\n\n\n그렇지만 eval expression에 지원하는 함수는 한계가 있다. (\\(\\sin\\),\\(\\cos\\)은 지원되는데 \\(\\tan\\)은 안된다든가..)\n뭐가 될지 안될지 모름…\n\n_df = pd.DataFrame({'A':np.linspace(-1.5,1.5,100)})\n_df\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\n-1.500000\n\n\n1\n-1.469697\n\n\n2\n-1.439394\n\n\n3\n-1.409091\n\n\n4\n-1.378788\n\n\n...\n...\n\n\n95\n1.378788\n\n\n96\n1.409091\n\n\n97\n1.439394\n\n\n98\n1.469697\n\n\n99\n1.500000\n\n\n\n\n100 rows × 1 columns\n\n\n\n\n_df.eval('B=sin(A)').plot(y='B')\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\n_df.eval('B=tan(A)').plot(y='B') # tan는 안되네?\n\nValueError: \"tan\" is not a supported function\n\n\n\n안정성이 떨어진다…\n\n- tan함수를 외부에 선언하는 방법이 있긴 함.\n\nf = np.tan \n_df.eval('B=@f(A)').plot(y='B') # 그래서 이게 정석임..\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\n이것보다는 assign이나 lambda를 쓰는 것을 추천한다."
  },
  {
    "objectID": "posts/2_DV2022/2023-10-16-07wk-1-t.html#a.-원래-df-s는-딕셔너리-계열임",
    "href": "posts/2_DV2022/2023-10-16-07wk-1-t.html#a.-원래-df-s는-딕셔너리-계열임",
    "title": "[DV2023] 07wk-1: Pandas – lambda df:의 활용, MultiIndex의 이해, tidydata의 이해, melt/stack",
    "section": "A. 원래 df, s는 딕셔너리 계열임",
    "text": "A. 원래 df, s는 딕셔너리 계열임\n- 예시1: df는 dct에서 만들수 있음\n\ndct = {'A': [1,2,3],'B': [2,3,4]}  # 딕셔너리\ndf = pd.DataFrame(dct) # 데이터프레임으로 변신\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\n사실 판다스 데이터프레임이 딕셔너리로부터 나온거야… 이걸 알면 편함.\n\n\ndf['A'] # 딕셔너리에서 key로 value에 접근하듯이...\n\n0    1\n1    2\n2    3\nName: A, dtype: int64\n\n\n- 예시2: s도 dct에서 만들수 있음\n\ndct = {'43052': 80, '43053': 90, '43054': 50}\ns = pd.Series(dct)\ns\n\n43052    80\n43053    90\n43054    50\ndtype: int64\n\n\n\ns['43052']\n\n80\n\n\n- 예시3: dict의 키로 올수 있는것들?\n\nref: https://guebin.github.io/PP2023/posts/01_PythonBasic/2023-03-29-4wk-2.html\n\n튜플로 dct를 만든다면? (key가 튜플로 옴)\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ns\n\n43052  4    80\n43053  1    90\n43054  2    50\ndtype: int64\n\n\n\ndct\n\n{('43052', 4): 80, ('43053', 1): 90, ('43054', 2): 50}\n\n\n\n# dict\ndct[('43052',4)]\n\n80\n\n\n\n# series\ns[('43052',4)]\n\n80\n\n\n\n# series\ns[('43053',1)]\n\n90\n\n\n\ns.index\n\nMultiIndex([('43052', 4),\n            ('43053', 1),\n            ('43054', 2)],\n           )"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-16-07wk-1-t.html#b.-.index-혹은-.columns에-name이-있는-경우",
    "href": "posts/2_DV2022/2023-10-16-07wk-1-t.html#b.-.index-혹은-.columns에-name이-있는-경우",
    "title": "[DV2023] 07wk-1: Pandas – lambda df:의 활용, MultiIndex의 이해, tidydata의 이해, melt/stack",
    "section": "B. .index 혹은 .columns에 name이 있는 경우",
    "text": "B. .index 혹은 .columns에 name이 있는 경우\n예시1: index에 이름이 있는 경우 ['id']\n\ndct = {'43052': 80, '43053': 90, '43054': 50}\ns = pd.Series(dct)\ns\n\n43052    80\n43053    90\n43054    50\ndtype: int64\n\n\n\ns.rename_axis(['id'])\n\nid\n43052    80\n43053    90\n43054    50\ndtype: int64\n\n\n\ns.index, s.rename_axis(['id']).index,\n\n(Index(['43052', '43053', '43054'], dtype='object'),\n Index(['43052', '43053', '43054'], dtype='object', name='id'))\n\n\n\ns.index\n\nIndex(['43052', '43053', '43054'], dtype='object')\n\n\n\ns.rename_axis(['id']).index\n\nIndex(['43052', '43053', '43054'], dtype='object', name='id')\n\n\n\n인덱스를 뽑을 때 인덱스를 통칭하는 타이틀을 설정할 수도 있다. (ex. name=‘id’)\n\n#\n예시2: index에 이름이 있는 경우 ['id','year']\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ns.rename_axis(['id','year'])\n\nid     year\n43052  4       80\n43053  1       90\n43054  2       50\ndtype: int64\n\n\n\ns.rename_axis(['id','year']).index # 리스트형태로 인덱스의 타이틀이 설정되어 있다.\n\nMultiIndex([('43052', 4),\n            ('43053', 1),\n            ('43054', 2)],\n           names=['id', 'year'])\n\n\n#\n예시3: 예시2가 데이터프레임이라면 이렇게 보인다\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ndf = pd.DataFrame(s.rename_axis(['id','year']))\ndf\n\n\n\n\n\n\n\n\n\n0\n\n\nid\nyear\n\n\n\n\n\n43052\n4\n80\n\n\n43053\n1\n90\n\n\n43054\n2\n50\n\n\n\n\n\n\n\n\nlist(df.columns)  # column name\n\n[0]\n\n\n\ndf.index # row에 대한 각각 인덱스 값, 각 인덱스의 이름.\n\nMultiIndex([('43052', 4),\n            ('43053', 1),\n            ('43054', 2)],\n           names=['id', 'year'])\n\n\n\n2개의 중첩되어있는 인덱스를 가지고 있구나!\n\n\npd.DataFrame(s) # 볼드체는 인덱스, 인덱스의 타이틀은 없는 상태, 컬럼이름은 0\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n43052\n4\n80\n\n\n43053\n1\n90\n\n\n43054\n2\n50\n\n\n\n\n\n\n\n#\n예시4: 심슨의 역설 – 전체\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index='gender', columns='result', values='count', aggfunc=sum)\ndf\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\n\n\n\n\n\n\nfemale\n1063\n772\n\n\nmale\n1291\n1400\n\n\n\n\n\n\n\n\nfemale, male은 인덱스, gender는 인덱스의 타이틀, fail, pass는 컬럼이름. result는 컬럼 이름에 대한 타이틀.\n\n\ndf.index,df.columns\n\n(Index(['female', 'male'], dtype='object', name='gender'),\n Index(['fail', 'pass'], dtype='object', name='result'))\n\n\n1열과 2열을 더하고 싶다면? 단순히 아래와 같이 하면 된다. (여기에서 gender,result는 각각 index 의 이름, columns의 이름일 뿐이므로 신경쓸 필요 없음)\n\ndf['fail']+df['pass']\n\ngender\nfemale    1835\nmale      2691\ndtype: int64\n\n\n\ndf.sum(axis=1)\n\ngender\nfemale    1835\nmale      2691\ndtype: int64\n\n\n\ndf\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\n\n\n\n\n\n\nfemale\n1063\n772\n\n\nmale\n1291\n1400\n\n\n\n\n\n\n\n\npd.DataFrame({'fail':[1063, 1291 ], 'pass':[772, 1400]})\n\n\n\n\n\n\n\n\nfail\npass\n\n\n\n\n0\n1063\n772\n\n\n1\n1291\n1400\n\n\n\n\n\n\n\n\n그냥 이거임.\n\n#\n예시5: 심슨의 역설 – 학과별\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index=['gender','department'], columns='result',values='count',aggfunc=sum)\ndf \n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\ndepartment\n\n\n\n\n\n\nfemale\nA\n19\n89\n\n\nB\n7\n18\n\n\nC\n391\n202\n\n\nD\n244\n131\n\n\nE\n299\n94\n\n\nF\n103\n238\n\n\nmale\nA\n314\n511\n\n\nB\n208\n352\n\n\nC\n204\n121\n\n\nD\n279\n138\n\n\nE\n137\n54\n\n\nF\n149\n224\n\n\n\n\n\n\n\n\ndf.index, df.columns\n\n(MultiIndex([('female', 'A'),\n             ('female', 'B'),\n             ('female', 'C'),\n             ('female', 'D'),\n             ('female', 'E'),\n             ('female', 'F'),\n             (  'male', 'A'),\n             (  'male', 'B'),\n             (  'male', 'C'),\n             (  'male', 'D'),\n             (  'male', 'E'),\n             (  'male', 'F')],\n            names=['gender', 'department']),\n Index(['fail', 'pass'], dtype='object', name='result'))\n\n\n학과별 합격률을 알고 싶다면?\n\ndf.assign(rate = df['pass']/df.sum(axis=1))\n\n\n\n\n\n\n\n\nresult\nfail\npass\nrate\n\n\ngender\ndepartment\n\n\n\n\n\n\n\nfemale\nA\n19\n89\n0.824074\n\n\nB\n7\n18\n0.720000\n\n\nC\n391\n202\n0.340641\n\n\nD\n244\n131\n0.349333\n\n\nE\n299\n94\n0.239186\n\n\nF\n103\n238\n0.697947\n\n\nmale\nA\n314\n511\n0.619394\n\n\nB\n208\n352\n0.628571\n\n\nC\n204\n121\n0.372308\n\n\nD\n279\n138\n0.330935\n\n\nE\n137\n54\n0.282723\n\n\nF\n149\n224\n0.600536\n\n\n\n\n\n\n\n\ndf.sum(axis=1)\n\ngender  department\nfemale  A             108\n        B              25\n        C             593\n        D             375\n        E             393\n        F             341\nmale    A             825\n        B             560\n        C             325\n        D             417\n        E             191\n        F             373\ndtype: int64\n\n\n#"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-16-07wk-1-t.html#a.-tidydata의-개념",
    "href": "posts/2_DV2022/2023-10-16-07wk-1-t.html#a.-tidydata의-개념",
    "title": "[DV2023] 07wk-1: Pandas – lambda df:의 활용, MultiIndex의 이해, tidydata의 이해, melt/stack",
    "section": "A. tidydata의 개념",
    "text": "A. tidydata의 개념\n- 아래의 자료는 불리하다. (뭐가??)\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index=['gender','department'], columns='result',values='count',aggfunc=sum)\ndf \n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\ndepartment\n\n\n\n\n\n\nfemale\nA\n19\n89\n\n\nB\n7\n18\n\n\nC\n391\n202\n\n\nD\n244\n131\n\n\nE\n299\n94\n\n\nF\n103\n238\n\n\nmale\nA\n314\n511\n\n\nB\n208\n352\n\n\nC\n204\n121\n\n\nD\n279\n138\n\n\nE\n137\n54\n\n\nF\n149\n224\n\n\n\n\n\n\n\n\n이건 다루기 쉬운 형태의 데이터가 아니다.\n\n\ndf.stack().reset_index().query('department==\"A\"')\n\n\n\n\n\n\n\n\ngender\ndepartment\nresult\n0\n\n\n\n\n0\nfemale\nA\nfail\n19\n\n\n1\nfemale\nA\npass\n89\n\n\n12\nmale\nA\nfail\n314\n\n\n13\nmale\nA\npass\n511\n\n\n\n\n\n\n\n- 가정1: 만약에 A학과에 해당하는 결과만 뽑고 싶다면? –&gt; departmet가 column으로 있어야함..\n- 가정2: 이 데이터를 바탕으로 합격한사람만 bar plot을 그리고 싶다면? –&gt; department, gender, pass 가 column으로 있어야함..\n\n#\n\n- tidydata 정의: https://r4ds.had.co.nz/tidy-data.html\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nmulti-index가 있으면 무조건 tidydata가 아니다."
  },
  {
    "objectID": "posts/2_DV2022/2023-10-16-07wk-1-t.html#b.-tidydata가-아닌-예시",
    "href": "posts/2_DV2022/2023-10-16-07wk-1-t.html#b.-tidydata가-아닌-예시",
    "title": "[DV2023] 07wk-1: Pandas – lambda df:의 활용, MultiIndex의 이해, tidydata의 이해, melt/stack",
    "section": "B. tidydata가 아닌 예시",
    "text": "B. tidydata가 아닌 예시\n예시1 – MultiIndex 구조를 가지면 무조건 tidydata가 아니다.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index=['gender','department'], columns='result',values='count',aggfunc=sum)\ndf\n\n\n\n\n\n\n\n\nresult\nfail\npass\n\n\ngender\ndepartment\n\n\n\n\n\n\nfemale\nA\n19\n89\n\n\nB\n7\n18\n\n\nC\n391\n202\n\n\nD\n244\n131\n\n\nE\n299\n94\n\n\nF\n103\n238\n\n\nmale\nA\n314\n511\n\n\nB\n208\n352\n\n\nC\n204\n121\n\n\nD\n279\n138\n\n\nE\n137\n54\n\n\nF\n149\n224\n\n\n\n\n\n\n\n\n이건 tidydata가 아니고 // 단순하게 생각하면, 멀티인덱스가 있으니까 tidydata가 아니야.\n\n\ntidydata = df.stack().reset_index().rename({0:'applicant_count'},axis=1)\ntidydata\n\n\n\n\n\n\n\n\ngender\ndepartment\nresult\napplicant_count\n\n\n\n\n0\nfemale\nA\nfail\n19\n\n\n1\nfemale\nA\npass\n89\n\n\n2\nfemale\nB\nfail\n7\n\n\n3\nfemale\nB\npass\n18\n\n\n4\nfemale\nC\nfail\n391\n\n\n5\nfemale\nC\npass\n202\n\n\n6\nfemale\nD\nfail\n244\n\n\n7\nfemale\nD\npass\n131\n\n\n8\nfemale\nE\nfail\n299\n\n\n9\nfemale\nE\npass\n94\n\n\n10\nfemale\nF\nfail\n103\n\n\n11\nfemale\nF\npass\n238\n\n\n12\nmale\nA\nfail\n314\n\n\n13\nmale\nA\npass\n511\n\n\n14\nmale\nB\nfail\n208\n\n\n15\nmale\nB\npass\n352\n\n\n16\nmale\nC\nfail\n204\n\n\n17\nmale\nC\npass\n121\n\n\n18\nmale\nD\nfail\n279\n\n\n19\nmale\nD\npass\n138\n\n\n20\nmale\nE\nfail\n137\n\n\n21\nmale\nE\npass\n54\n\n\n22\nmale\nF\nfail\n149\n\n\n23\nmale\nF\npass\n224\n\n\n\n\n\n\n\n\ntidydata.columns\n\nIndex(['gender', 'department', 'result', 'applicant_count'], dtype='object')\n\n\n\n이것이 tidydata\n\n- 구분하는 방법1: 직관에 의한 설명\n\nquery쓰기 불편: 남성지원자만 뽑고 싶다면?, 학과A만 뽑고싶다면? 탈락한지원자만 뽑고싶다면? 학과A에서 탈락한 지원자만 뽑고싶다면??\n시각화하기 불편:\n하여튼 다루기 불편해..\n\n- 구분하는 방법2: 정의에 의한 설명\n\ndf는 원칙 1에 위배된다. (왜냐하면 gender, department,result,applicant_count에 해당하는 변수는 하나의 컬럼을 차지하지 못함)\ndf는 원칙 2에 위배된다. (왜냐하면 하나의 행에 2개의 applicant_count observation이 존재함)\n\n# 우리가 다루기 편리한 자료의 형태를 tidydata라고 생각하자. (query를 쓰기 편하고, 시각화하기 편한 데이터…)\n예시2 – 아래의 자료는 tidydata가 아니다.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1).pivot_table(index='gender', columns='result', values='count', aggfunc=sum)\\\n.assign(pass_fail = lambda df: list(map(lambda x,y: (y,x),df['fail'],df['pass']))).drop(['fail','pass'],axis=1).reset_index()\ndf\n\n\n\n\n\n\n\nresult\ngender\npass_fail\n\n\n\n\n0\nfemale\n(772, 1063)\n\n\n1\nmale\n(1400, 1291)\n\n\n\n\n\n\n\n\n이 df는 원칙 3에 위배된다.\n\n#\n# 예시3 – wide df (넓은형태의 df)\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\n이건 tidydata 가 아니고\n\n- long form\n\ntidydata = df.melt(id_vars='Date').assign(Date = lambda _df: _df.Date.apply(pd.to_datetime))\ntidydata\n\n\n\n\n\n\n\n\nDate\nvariable\nvalue\n\n\n\n\n0\n2019-10-01\nSamsung\n461\n\n\n1\n2019-11-01\nSamsung\n461\n\n\n2\n2019-12-01\nSamsung\n426\n\n\n3\n2020-01-01\nSamsung\n677\n\n\n4\n2020-02-01\nSamsung\n593\n\n\n...\n...\n...\n...\n\n\n203\n2020-06-01\nAsus\n16\n\n\n204\n2020-07-01\nAsus\n12\n\n\n205\n2020-08-01\nAsus\n20\n\n\n206\n2020-09-01\nAsus\n15\n\n\n207\n2020-10-01\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n\n이건 tidydata 이다.\n\n- df를 가지고 아래와 같은 그림을 그릴 수 있겠어?\n\n#\n\n–&gt; 그럼 멀티인덱스가 있으면 멀티인덱스를 깨야하고, 와이드폼으로 되어있으면 롱 폼으로 바꿔야겠지?"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-16-07wk-1-t.html#a.-reset_index",
    "href": "posts/2_DV2022/2023-10-16-07wk-1-t.html#a.-reset_index",
    "title": "[DV2023] 07wk-1: Pandas – lambda df:의 활용, MultiIndex의 이해, tidydata의 이해, melt/stack",
    "section": "A. reset_index()",
    "text": "A. reset_index()\n\n중첩구조를 가지는 series일 경우 .reset_index()를 사용하면 쉽게 tidydata를 얻을 수 있다.\n\n- 예시1\n\ndct = {'43052': 80, '43053': 90, '43054': 50}\ns = pd.Series(dct)\ns\n\n43052    80\n43053    90\n43054    50\ndtype: int64\n\n\n\ns.reset_index() # reset_index() : 인덱스를 깨라!\n\n\n\n\n\n\n\n\nindex\n0\n\n\n\n\n0\n43052\n80\n\n\n1\n43053\n90\n\n\n2\n43054\n50\n\n\n\n\n\n\n\n- 예시2\n\ndct = {('43052',4): 80, ('43053',1): 90, ('43054',2): 50} # (학번,학년)\ns = pd.Series(dct)\ns\n\n43052  4    80\n43053  1    90\n43054  2    50\ndtype: int64\n\n\n\ns.reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\n0\n\n\n\n\n0\n43052\n4\n80\n\n\n1\n43053\n1\n90\n\n\n2\n43054\n2\n50\n\n\n\n\n\n\n\n- 예시3\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).stack().stack()\ndf\n\nA  fail  female     19\n         male      314\n   pass  female     89\n         male      511\nB  fail  female      7\n         male      208\n   pass  female     18\n         male      352\nC  fail  female    391\n         male      204\n   pass  female    202\n         male      121\nD  fail  female    244\n         male      279\n   pass  female    131\n         male      138\nE  fail  female    299\n         male      137\n   pass  female     94\n         male       54\nF  fail  female    103\n         male      149\n   pass  female    238\n         male      224\ndtype: int64\n\n\n\n인덱스가 3개가 중첩된 멀티인덱스\n\n\ndf.reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nlevel_2\n0\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n- 예시4 – .reset_index() 는 말그대로 index를 reset 하는 명령어, 꼭 pd.Series에만 쓰는건 아니다.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1]).stack()\ndf\n\n\n\n\n\n\n\n\n\nfemale\nmale\n\n\n\n\nA\nfail\n19\n314\n\n\npass\n89\n511\n\n\nB\nfail\n7\n208\n\n\npass\n18\n352\n\n\nC\nfail\n391\n204\n\n\npass\n202\n121\n\n\nD\nfail\n244\n279\n\n\npass\n131\n138\n\n\nE\nfail\n299\n137\n\n\npass\n94\n54\n\n\nF\nfail\n103\n149\n\n\npass\n238\n224\n\n\n\n\n\n\n\n\ndf.reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nfemale\nmale\n\n\n\n\n0\nA\nfail\n19\n314\n\n\n1\nA\npass\n89\n511\n\n\n2\nB\nfail\n7\n208\n\n\n3\nB\npass\n18\n352\n\n\n4\nC\nfail\n391\n204\n\n\n5\nC\npass\n202\n121\n\n\n6\nD\nfail\n244\n279\n\n\n7\nD\npass\n131\n138\n\n\n8\nE\nfail\n299\n137\n\n\n9\nE\npass\n94\n54\n\n\n10\nF\nfail\n103\n149\n\n\n11\nF\npass\n238\n224\n\n\n\n\n\n\n\n\n어떤 시리즈나 데이터프레임에 reset_index()를 취하면 이런식으로 된다."
  },
  {
    "objectID": "posts/2_DV2022/2023-10-16-07wk-1-t.html#b.-melt",
    "href": "posts/2_DV2022/2023-10-16-07wk-1-t.html#b.-melt",
    "title": "[DV2023] 07wk-1: Pandas – lambda df:의 활용, MultiIndex의 이해, tidydata의 이해, melt/stack",
    "section": "B. melt()",
    "text": "B. melt()\nwide data를 다루는 데 원툴.\n# 예시1: 아래의 자료(wide form)를 tidydata로 만들라.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n(풀이1) .melt() – 실패\n\ndf.melt() # 실패\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nDate\n2019-10\n\n\n1\nDate\n2019-11\n\n\n2\nDate\n2019-12\n\n\n3\nDate\n2020-01\n\n\n4\nDate\n2020-02\n\n\n...\n...\n...\n\n\n216\nAsus\n16\n\n\n217\nAsus\n12\n\n\n218\nAsus\n20\n\n\n219\nAsus\n15\n\n\n220\nAsus\n21\n\n\n\n\n221 rows × 2 columns\n\n\n\n(풀이2) .melt(id_vars=) – 성공\n\ndf.melt(id_vars='Date') # 성공\n\n\n\n\n\n\n\n\nDate\nvariable\nvalue\n\n\n\n\n0\n2019-10\nSamsung\n461\n\n\n1\n2019-11\nSamsung\n461\n\n\n2\n2019-12\nSamsung\n426\n\n\n3\n2020-01\nSamsung\n677\n\n\n4\n2020-02\nSamsung\n593\n\n\n...\n...\n...\n...\n\n\n203\n2020-06\nAsus\n16\n\n\n204\n2020-07\nAsus\n12\n\n\n205\n2020-08\nAsus\n20\n\n\n206\n2020-09\nAsus\n15\n\n\n207\n2020-10\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n\n컬럼들을 녹여서 밑으로 쫙 흘러내리게!\n\n#"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-16-07wk-1-t.html#c.-stack-reset_index",
    "href": "posts/2_DV2022/2023-10-16-07wk-1-t.html#c.-stack-reset_index",
    "title": "[DV2023] 07wk-1: Pandas – lambda df:의 활용, MultiIndex의 이해, tidydata의 이해, melt/stack",
    "section": "C. stack() + reset_index()",
    "text": "C. stack() + reset_index()\n\n제 최애테크닉: DataFrame을 MultiIndex를 가지는 Series로 “일부러” 변환하고 reset_index()를 시킴\n\n# 예시1: 아래의 자료를 tidydata로 만들라.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n461\n324\n136\n109\n76\n81\n43\n37\n135\n28\n39\n14\n22\n17\n20\n17\n\n\n1\n2019-11\n461\n358\n167\n141\n86\n61\n29\n36\n141\n27\n29\n20\n23\n10\n19\n27\n\n\n2\n2019-12\n426\n383\n143\n105\n53\n45\n51\n48\n129\n30\n20\n26\n28\n18\n18\n19\n\n\n3\n2020-01\n677\n494\n212\n187\n110\n79\n65\n49\n158\n23\n13\n19\n19\n22\n27\n22\n\n\n4\n2020-02\n593\n520\n217\n195\n112\n67\n62\n71\n157\n25\n18\n16\n24\n18\n23\n20\n\n\n5\n2020-03\n637\n537\n246\n187\n92\n66\n59\n67\n145\n21\n16\n24\n18\n31\n22\n14\n\n\n6\n2020-04\n647\n583\n222\n154\n98\n59\n48\n64\n113\n20\n23\n25\n19\n19\n23\n21\n\n\n7\n2020-05\n629\n518\n192\n176\n91\n87\n50\n66\n150\n43\n27\n15\n18\n19\n19\n13\n\n\n8\n2020-06\n663\n552\n209\n185\n93\n69\n54\n60\n140\n39\n16\n16\n17\n29\n25\n16\n\n\n9\n2020-07\n599\n471\n214\n193\n89\n78\n65\n59\n130\n40\n27\n25\n21\n18\n18\n12\n\n\n10\n2020-08\n615\n567\n204\n182\n105\n82\n62\n42\n129\n47\n16\n23\n21\n27\n23\n20\n\n\n11\n2020-09\n621\n481\n230\n220\n102\n88\n56\n49\n143\n54\n14\n15\n17\n15\n19\n15\n\n\n12\n2020-10\n637\n555\n232\n203\n90\n52\n63\n49\n140\n33\n17\n20\n22\n9\n22\n21\n\n\n\n\n\n\n\n\n# 중첩된 구조로 내려오는데? \n# 첫번째 인덱스에는 Data라는 이름이 있고, 두번쨰 인덱스에는 이름은 없네?\ndf.set_index('Date').stack() \n\nDate            \n2019-10  Samsung    461\n         Apple      324\n         Huawei     136\n         Xiaomi     109\n         Oppo        76\n                   ... \n2020-10  Nokia       20\n         Lenovo      22\n         OnePlus      9\n         Sony        22\n         Asus        21\nLength: 208, dtype: int64\n\n\n\ndf.set_index('Date').stack().reset_index()\n\n\n\n\n\n\n\n\nDate\nlevel_1\n0\n\n\n\n\n0\n2019-10\nSamsung\n461\n\n\n1\n2019-10\nApple\n324\n\n\n2\n2019-10\nHuawei\n136\n\n\n3\n2019-10\nXiaomi\n109\n\n\n4\n2019-10\nOppo\n76\n\n\n...\n...\n...\n...\n\n\n203\n2020-10\nNokia\n20\n\n\n204\n2020-10\nLenovo\n22\n\n\n205\n2020-10\nOnePlus\n9\n\n\n206\n2020-10\nSony\n22\n\n\n207\n2020-10\nAsus\n21\n\n\n\n\n208 rows × 3 columns\n\n\n\n#\n# 예시2: 아래의 자료를 tidydata로 만들어라.\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\ndf\n\n\n\n\n\n\n\n\nmale\nfemale\n\n\n\nfail\npass\nfail\npass\n\n\n\n\nA\n314\n511\n19\n89\n\n\nB\n208\n352\n7\n18\n\n\nC\n204\n121\n391\n202\n\n\nD\n279\n138\n244\n131\n\n\nE\n137\n54\n299\n94\n\n\nF\n149\n224\n103\n238\n\n\n\n\n\n\n\n\nA,B,C,D,E,F가 인덱스, 컬럼이 중첩구조로 되어있따.\n\n\ndf.columns # 남자+떨어진사람, 남자+붙은사람, 여자+떨어진사람, 여자+붙은사람 (2겹)\n\nMultiIndex([(  'male', 'fail'),\n            (  'male', 'pass'),\n            ('female', 'fail'),\n            ('female', 'pass')],\n           )\n\n\n\ndf.index # (1겹)\n\nIndex(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object')\n\n\n\ndf.stack() # 인덱스 2겹, 컬럼 1겹\n\n\n\n\n\n\n\n\n\nfemale\nmale\n\n\n\n\nA\nfail\n19\n314\n\n\npass\n89\n511\n\n\nB\nfail\n7\n208\n\n\npass\n18\n352\n\n\nC\nfail\n391\n204\n\n\npass\n202\n121\n\n\nD\nfail\n244\n279\n\n\npass\n131\n138\n\n\nE\nfail\n299\n137\n\n\npass\n94\n54\n\n\nF\nfail\n103\n149\n\n\npass\n238\n224\n\n\n\n\n\n\n\n\ndf.stack().stack() # 인덱스 중첩3겹, 컬럼 중첩 0겹 // pandas series\n\nA  fail  female     19\n         male      314\n   pass  female     89\n         male      511\nB  fail  female      7\n         male      208\n   pass  female     18\n         male      352\nC  fail  female    391\n         male      204\n   pass  female    202\n         male      121\nD  fail  female    244\n         male      279\n   pass  female    131\n         male      138\nE  fail  female    299\n         male      137\n   pass  female     94\n         male       54\nF  fail  female    103\n         male      149\n   pass  female    238\n         male      224\ndtype: int64\n\n\n\ndf.stack().stack().reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nlevel_2\n0\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n\n컬럼이름만 바꿔주면 되겠는데?\n\n#"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-16-07wk-1-t.html#d.-unstack-reset_index",
    "href": "posts/2_DV2022/2023-10-16-07wk-1-t.html#d.-unstack-reset_index",
    "title": "[DV2023] 07wk-1: Pandas – lambda df:의 활용, MultiIndex의 이해, tidydata의 이해, melt/stack",
    "section": "D. unstack() + reset_index()",
    "text": "D. unstack() + reset_index()\n\ndf.stack().unstack().unstack() # 인덱스에 있는 중첩구조를 컬럼으로 옮긴다. (인덱스 중첩 0겹, 컬럼3겹의 시리즈 리턴)\n\nfemale  fail  A     19\n              B      7\n              C    391\n              D    244\n              E    299\n              F    103\n        pass  A     89\n              B     18\n              C    202\n              D    131\n              E     94\n              F    238\nmale    fail  A    314\n              B    208\n              C    204\n              D    279\n              E    137\n              F    149\n        pass  A    511\n              B    352\n              C    121\n              D    138\n              E     54\n              F    224\ndtype: int64\n\n\n# 예시1 – .stack()과 .unstack()은 반대연산\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv')\\\n.groupby([\"AIRLINE\",\"WEEKDAY\"]).agg({\"CANCELLED\":[np.mean,\"count\"],\"DIVERTED\":[np.mean,\"count\"]})\ndf\n\n\n\n\n\n\n\n\n\nCANCELLED\nDIVERTED\n\n\n\n\nmean\ncount\nmean\ncount\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\n\n\nAA\n1\n0.032106\n1277\n0.004699\n1277\n\n\n2\n0.007341\n1226\n0.001631\n1226\n\n\n3\n0.011949\n1339\n0.001494\n1339\n\n\n4\n0.015004\n1333\n0.003751\n1333\n\n\n5\n0.014151\n1272\n0.000786\n1272\n\n\n...\n...\n...\n...\n...\n...\n\n\nWN\n3\n0.014118\n1275\n0.001569\n1275\n\n\n4\n0.007911\n1264\n0.003165\n1264\n\n\n5\n0.005828\n1201\n0.000000\n1201\n\n\n6\n0.010132\n987\n0.003040\n987\n\n\n7\n0.006066\n1154\n0.002600\n1154\n\n\n\n\n98 rows × 4 columns\n\n\n\n\ndf.stack()\n\n\n\n\n\n\n\n\n\n\nCANCELLED\nDIVERTED\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\n\nAA\n1\nmean\n0.032106\n0.004699\n\n\ncount\n1277.000000\n1277.000000\n\n\n2\nmean\n0.007341\n0.001631\n\n\ncount\n1226.000000\n1226.000000\n\n\n3\nmean\n0.011949\n0.001494\n\n\n...\n...\n...\n...\n...\n\n\nWN\n5\ncount\n1201.000000\n1201.000000\n\n\n6\nmean\n0.010132\n0.003040\n\n\ncount\n987.000000\n987.000000\n\n\n7\nmean\n0.006066\n0.002600\n\n\ncount\n1154.000000\n1154.000000\n\n\n\n\n196 rows × 2 columns\n\n\n\n\ndf.unstack()\n\n\n\n\n\n\n\n\nCANCELLED\n...\nDIVERTED\n\n\n\nmean\ncount\n...\nmean\ncount\n\n\nWEEKDAY\n1\n2\n3\n4\n5\n6\n7\n1\n2\n3\n...\n5\n6\n7\n1\n2\n3\n4\n5\n6\n7\n\n\nAIRLINE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAA\n0.032106\n0.007341\n0.011949\n0.015004\n0.014151\n0.018667\n0.021837\n1277\n1226\n1339\n...\n0.000786\n0.008000\n0.000753\n1277\n1226\n1339\n1333\n1272\n1125\n1328\n\n\nAS\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n118\n106\n115\n...\n0.000000\n0.000000\n0.000000\n118\n106\n115\n110\n109\n103\n107\n\n\nB6\n0.000000\n0.012658\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n70\n79\n78\n...\n0.000000\n0.016667\n0.011364\n70\n79\n78\n84\n84\n60\n88\n\n\nDL\n0.006068\n0.005208\n0.005131\n0.001940\n0.001982\n0.003195\n0.001294\n1648\n1536\n1559\n...\n0.001982\n0.003195\n0.002587\n1648\n1536\n1559\n1546\n1514\n1252\n1546\n\n\nEV\n0.034130\n0.023918\n0.022910\n0.026895\n0.013111\n0.022504\n0.030233\n879\n878\n873\n...\n0.002384\n0.001406\n0.002326\n879\n878\n873\n818\n839\n711\n860\n\n\nF9\n0.016129\n0.005376\n0.000000\n0.000000\n0.005155\n0.011050\n0.015625\n186\n186\n192\n...\n0.000000\n0.000000\n0.005208\n186\n186\n192\n186\n194\n181\n192\n\n\nHA\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n19\n19\n19\n...\n0.000000\n0.000000\n0.000000\n19\n19\n19\n15\n11\n12\n17\n\n\nMQ\n0.086785\n0.032819\n0.025145\n0.039146\n0.028000\n0.038356\n0.055777\n507\n518\n517\n...\n0.000000\n0.000000\n0.000000\n507\n518\n517\n562\n500\n365\n502\n\n\nNK\n0.035354\n0.013158\n0.013953\n0.013216\n0.012821\n0.019900\n0.009390\n198\n228\n215\n...\n0.000000\n0.004975\n0.004695\n198\n228\n215\n227\n234\n201\n213\n\n\nOO\n0.030581\n0.011156\n0.014478\n0.013627\n0.026399\n0.024125\n0.031385\n981\n986\n967\n...\n0.004224\n0.002413\n0.003247\n981\n986\n967\n954\n947\n829\n924\n\n\nUA\n0.018914\n0.017498\n0.007799\n0.007719\n0.010195\n0.006682\n0.013487\n1216\n1143\n1154\n...\n0.001699\n0.002227\n0.003854\n1216\n1143\n1154\n1166\n1177\n898\n1038\n\n\nUS\n0.026906\n0.018018\n0.004545\n0.003953\n0.009050\n0.004695\n0.022814\n223\n222\n220\n...\n0.000000\n0.000000\n0.000000\n223\n222\n220\n253\n221\n213\n263\n\n\nVX\n0.007194\n0.000000\n0.000000\n0.014184\n0.006667\n0.000000\n0.014815\n139\n130\n183\n...\n0.000000\n0.000000\n0.007407\n139\n130\n183\n141\n150\n115\n135\n\n\nWN\n0.012708\n0.019562\n0.014118\n0.007911\n0.005828\n0.010132\n0.006066\n1259\n1278\n1275\n...\n0.000000\n0.003040\n0.002600\n1259\n1278\n1275\n1264\n1201\n987\n1154\n\n\n\n\n14 rows × 28 columns\n\n\n\n\ndf.unstack().stack()\n\n\n\n\n\n\n\n\n\nCANCELLED\nDIVERTED\n\n\n\n\nmean\ncount\nmean\ncount\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\n\n\nAA\n1\n0.032106\n1277\n0.004699\n1277\n\n\n2\n0.007341\n1226\n0.001631\n1226\n\n\n3\n0.011949\n1339\n0.001494\n1339\n\n\n4\n0.015004\n1333\n0.003751\n1333\n\n\n5\n0.014151\n1272\n0.000786\n1272\n\n\n...\n...\n...\n...\n...\n...\n\n\nWN\n3\n0.014118\n1275\n0.001569\n1275\n\n\n4\n0.007911\n1264\n0.003165\n1264\n\n\n5\n0.005828\n1201\n0.000000\n1201\n\n\n6\n0.010132\n987\n0.003040\n987\n\n\n7\n0.006066\n1154\n0.002600\n1154\n\n\n\n\n98 rows × 4 columns\n\n\n\n\ndf.stack().unstack()\n\n\n\n\n\n\n\n\n\nCANCELLED\nDIVERTED\n\n\n\n\nmean\ncount\nmean\ncount\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\n\n\nAA\n1\n0.032106\n1277.0\n0.004699\n1277.0\n\n\n2\n0.007341\n1226.0\n0.001631\n1226.0\n\n\n3\n0.011949\n1339.0\n0.001494\n1339.0\n\n\n4\n0.015004\n1333.0\n0.003751\n1333.0\n\n\n5\n0.014151\n1272.0\n0.000786\n1272.0\n\n\n...\n...\n...\n...\n...\n...\n\n\nWN\n3\n0.014118\n1275.0\n0.001569\n1275.0\n\n\n4\n0.007911\n1264.0\n0.003165\n1264.0\n\n\n5\n0.005828\n1201.0\n0.000000\n1201.0\n\n\n6\n0.010132\n987.0\n0.003040\n987.0\n\n\n7\n0.006066\n1154.0\n0.002600\n1154.0\n\n\n\n\n98 rows × 4 columns\n\n\n\n#\n# 예시2 – 아래의 자료를 tidydata로 만들라.\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv')\\\n.groupby([\"AIRLINE\",\"WEEKDAY\"]).agg({\"CANCELLED\":[np.mean,\"count\"],\"DIVERTED\":[np.mean,\"count\"]})\ndf\n\n\n\n\n\n\n\n\n\nCANCELLED\nDIVERTED\n\n\n\n\nmean\ncount\nmean\ncount\n\n\nAIRLINE\nWEEKDAY\n\n\n\n\n\n\n\n\nAA\n1\n0.032106\n1277\n0.004699\n1277\n\n\n2\n0.007341\n1226\n0.001631\n1226\n\n\n3\n0.011949\n1339\n0.001494\n1339\n\n\n4\n0.015004\n1333\n0.003751\n1333\n\n\n5\n0.014151\n1272\n0.000786\n1272\n\n\n...\n...\n...\n...\n...\n...\n\n\nWN\n3\n0.014118\n1275\n0.001569\n1275\n\n\n4\n0.007911\n1264\n0.003165\n1264\n\n\n5\n0.005828\n1201\n0.000000\n1201\n\n\n6\n0.010132\n987\n0.003040\n987\n\n\n7\n0.006066\n1154\n0.002600\n1154\n\n\n\n\n98 rows × 4 columns\n\n\n\n(풀이1)\n\ndf.stack().stack().reset_index()\n\n\n\n\n\n\n\n\nAIRLINE\nWEEKDAY\nlevel_2\nlevel_3\n0\n\n\n\n\n0\nAA\n1\nmean\nCANCELLED\n0.032106\n\n\n1\nAA\n1\nmean\nDIVERTED\n0.004699\n\n\n2\nAA\n1\ncount\nCANCELLED\n1277.000000\n\n\n3\nAA\n1\ncount\nDIVERTED\n1277.000000\n\n\n4\nAA\n2\nmean\nCANCELLED\n0.007341\n\n\n...\n...\n...\n...\n...\n...\n\n\n387\nWN\n6\ncount\nDIVERTED\n987.000000\n\n\n388\nWN\n7\nmean\nCANCELLED\n0.006066\n\n\n389\nWN\n7\nmean\nDIVERTED\n0.002600\n\n\n390\nWN\n7\ncount\nCANCELLED\n1154.000000\n\n\n391\nWN\n7\ncount\nDIVERTED\n1154.000000\n\n\n\n\n392 rows × 5 columns\n\n\n\n(풀이2)\n\ndf.unstack().unstack().reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nWEEKDAY\nAIRLINE\n0\n\n\n\n\n0\nCANCELLED\nmean\n1\nAA\n0.032106\n\n\n1\nCANCELLED\nmean\n1\nAS\n0.000000\n\n\n2\nCANCELLED\nmean\n1\nB6\n0.000000\n\n\n3\nCANCELLED\nmean\n1\nDL\n0.006068\n\n\n4\nCANCELLED\nmean\n1\nEV\n0.034130\n\n\n...\n...\n...\n...\n...\n...\n\n\n387\nDIVERTED\ncount\n7\nOO\n924.000000\n\n\n388\nDIVERTED\ncount\n7\nUA\n1038.000000\n\n\n389\nDIVERTED\ncount\n7\nUS\n263.000000\n\n\n390\nDIVERTED\ncount\n7\nVX\n135.000000\n\n\n391\nDIVERTED\ncount\n7\nWN\n1154.000000\n\n\n\n\n392 rows × 5 columns\n\n\n\n예시3 – 아래의 자료를 tidydata로 만들어라.\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\ndf\n\n\n\n\n\n\n\n\nmale\nfemale\n\n\n\nfail\npass\nfail\npass\n\n\n\n\nA\n314\n511\n19\n89\n\n\nB\n208\n352\n7\n18\n\n\nC\n204\n121\n391\n202\n\n\nD\n279\n138\n244\n131\n\n\nE\n137\n54\n299\n94\n\n\nF\n149\n224\n103\n238\n\n\n\n\n\n\n\n(풀이1)\n\ndf.stack().stack().reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nlevel_2\n0\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224\n\n\n\n\n\n\n\n(풀이2)\n\ndf.unstack().reset_index()\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\nlevel_2\n0\n\n\n\n\n0\nmale\nfail\nA\n314\n\n\n1\nmale\nfail\nB\n208\n\n\n2\nmale\nfail\nC\n204\n\n\n3\nmale\nfail\nD\n279\n\n\n4\nmale\nfail\nE\n137\n\n\n5\nmale\nfail\nF\n149\n\n\n6\nmale\npass\nA\n511\n\n\n7\nmale\npass\nB\n352\n\n\n8\nmale\npass\nC\n121\n\n\n9\nmale\npass\nD\n138\n\n\n10\nmale\npass\nE\n54\n\n\n11\nmale\npass\nF\n224\n\n\n12\nfemale\nfail\nA\n19\n\n\n13\nfemale\nfail\nB\n7\n\n\n14\nfemale\nfail\nC\n391\n\n\n15\nfemale\nfail\nD\n244\n\n\n16\nfemale\nfail\nE\n299\n\n\n17\nfemale\nfail\nF\n103\n\n\n18\nfemale\npass\nA\n89\n\n\n19\nfemale\npass\nB\n18\n\n\n20\nfemale\npass\nC\n202\n\n\n21\nfemale\npass\nD\n131\n\n\n22\nfemale\npass\nE\n94\n\n\n23\nfemale\npass\nF\n238"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-04-05wk-2-t.html",
    "href": "posts/2_DV2022/2023-10-04-05wk-2-t.html",
    "title": "[DV2023] 05wk-2: Pandas – transform column (꿀팁)",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-wVp_fe2YXiERTc0ZEh9Li_&si=rr31LPWCeN_negav"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-04-05wk-2-t.html#a.-lambda",
    "href": "posts/2_DV2022/2023-10-04-05wk-2-t.html#a.-lambda",
    "title": "[DV2023] 05wk-2: Pandas – transform column (꿀팁)",
    "section": "A. lambda",
    "text": "A. lambda\n저번시간에 했음"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-04-05wk-2-t.html#b.-map",
    "href": "posts/2_DV2022/2023-10-04-05wk-2-t.html#b.-map",
    "title": "[DV2023] 05wk-2: Pandas – transform column (꿀팁)",
    "section": "B. map",
    "text": "B. map\n저번시간에 했음"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-04-05wk-2-t.html#c.-s.apply변환함수",
    "href": "posts/2_DV2022/2023-10-04-05wk-2-t.html#c.-s.apply변환함수",
    "title": "[DV2023] 05wk-2: Pandas – transform column (꿀팁)",
    "section": "C. s.apply(변환함수)",
    "text": "C. s.apply(변환함수)\n\n변환함수: 스칼라 \\(\\to\\) 스칼라 // 벡터 \\(\\to\\) 벡터 // 매트릭스 \\(\\to\\) 매트릭스 (예: x\\(\\to x+1\\): [1,2] \\(\\to\\) [2,3])\n집계함수: 벡터 \\(\\to\\) 스칼라 (예: 평균 [1,2,3,4] \\(\\to\\) 2.5)\n\n- 예시1: 원소별로 처음3개의 숫자만 출력\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height # pandas series\n\n\ns\n\n0        189cm\n1        179cm\n2        172cm\n3        181cm\n4        172cm\n         ...  \n17655    190cm\n17656    195cm\n17657    190cm\n17658    187cm\n17659    186cm\nName: Height, Length: 17660, dtype: object\n\n\n\ns.apply(lambda x: x[:3])\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: object\n\n\n\n숫자가 아닌 오브젝트형!\n\n\ns_ = pd.Series([1,2,3])\ns_.apply(lambda x: np.mean(x)) # 우리가 원하는 [1,2,3]의 평균인 2가 나오지 않음!\n\n0    1.0\n1    2.0\n2    3.0\ndtype: float64\n\n\n::: {.callout-note}\nseries의 apply함수에 집계함수를 쓰면 우리가 생각하는 대로 동작하지 않는다. (후에 데이터프레임과 혼용하여 쓸 때 헷갈림 주의) :::\n- 예시2: 원소별로 처음3개의 문자만 출력 \\(\\to\\) str자료형을 int자료형으로 변환\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height\n\n(방법1) – 한번에\n\ns.apply(lambda x: int(x[:3])) # 타입이 int로 잘 바뀜\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: int64\n\n\n(방법2) – 연쇄적으로..\n\ns.apply(lambda x: x[:3]).apply(int)"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-04-05wk-2-t.html#d.-s.str-idx.str",
    "href": "posts/2_DV2022/2023-10-04-05wk-2-t.html#d.-s.str-idx.str",
    "title": "[DV2023] 05wk-2: Pandas – transform column (꿀팁)",
    "section": "D. s.str, idx.str",
    "text": "D. s.str, idx.str\n- 예시1: 원소별로 처음 3개의 숫자만 출력\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\ns = df.Height\n\n\n'190cm'[:3] # 브로드 캐스팅이 됐으면 좋을 것 같은데?\n\n'190'\n\n\n\ns.str\n\n&lt;pandas.core.strings.accessor.StringMethods at 0x7fcfb304a310&gt;\n\n\n\ns.str[:3] # 문자열들의 수열버전\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: object\n\n\n\ns.str.replace('cm','') #  이것도 이제 잘됨.\n\n0        189\n1        179\n2        172\n3        181\n4        172\n        ... \n17655    190\n17656    195\n17657    190\n17658    187\n17659    186\nName: Height, Length: 17660, dtype: object\n\n\n- 예시2: 원소별로 isupper를 수행\n\ns = pd.Series(['A','B','C','d','e','F'])\ns\n\n0    A\n1    B\n2    C\n3    d\n4    e\n5    F\ndtype: object\n\n\n\ns.isupper() # 이렇게 하면 오류남!\n\nAttributeError: 'Series' object has no attribute 'isupper'\n\n\n\ns.str.isupper() \n\n0     True\n1     True\n2     True\n3    False\n4    False\n5     True\ndtype: bool\n\n\n- 예시3: 원소별로 공백제거 (pd.Series 뿐만 아니라 pd.Index 자료형에도 사용가능)\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/FIFA23_official_data.csv')\nidx = df.columns\n\n\nidx\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club Logo', 'Value', 'Wage', 'Special',\n       'Preferred Foot', 'International Reputation', 'Weak Foot',\n       'Skill Moves', 'Work Rate', 'Body Type', 'Real Face', 'Position',\n       'Joined', 'Loaned From', 'Contract Valid Until', 'Height', 'Weight',\n       'Release Clause', 'Kit Number', 'Best Overall Rating'],\n      dtype='object')\n\n\n\n# 공백삭제\nidx.str.replace(' ', '')\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'ClubLogo', 'Value', 'Wage', 'Special',\n       'PreferredFoot', 'InternationalReputation', 'WeakFoot', 'SkillMoves',\n       'WorkRate', 'BodyType', 'RealFace', 'Position', 'Joined', 'LoanedFrom',\n       'ContractValidUntil', 'Height', 'Weight', 'ReleaseClause', 'KitNumber',\n       'BestOverallRating'],\n      dtype='object')\n\n\n\n# 공백 언더바\nidx.str.replace(' ', '_')\n\nIndex(['ID', 'Name', 'Age', 'Photo', 'Nationality', 'Flag', 'Overall',\n       'Potential', 'Club', 'Club_Logo', 'Value', 'Wage', 'Special',\n       'Preferred_Foot', 'International_Reputation', 'Weak_Foot',\n       'Skill_Moves', 'Work_Rate', 'Body_Type', 'Real_Face', 'Position',\n       'Joined', 'Loaned_From', 'Contract_Valid_Until', 'Height', 'Weight',\n       'Release_Clause', 'Kit_Number', 'Best_Overall_Rating'],\n      dtype='object')\n\n\n\n.str 을 이용하면, 문자열을 편하게 쓸 수 있다."
  },
  {
    "objectID": "posts/2_DV2022/2023-10-04-05wk-2-t.html#e.-s.astype",
    "href": "posts/2_DV2022/2023-10-04-05wk-2-t.html#e.-s.astype",
    "title": "[DV2023] 05wk-2: Pandas – transform column (꿀팁)",
    "section": "E. s.astype()",
    "text": "E. s.astype()\n- 예시1: 원소의 타입을 모두 int형으로 변경\n\ns = pd.Series(list('12345'))\ns\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: object\n\n\n\nint(s)\n\nTypeError: cannot convert the series to &lt;class 'int'&gt;\n\n\n\ns.astype(int) # 이게 더 좋은 방법..\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\n\ns.apply(int) # 이렇게 해도 됨. (코드가 직관적이지 않음)\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\n- 예시2: 원소의 타입을 변환한 이후 브로드캐스팅 (int)\n\ns1 = pd.Series(list('12345')) # object type\ns2 = pd.Series([-1,-2,-3,-4,-5]) # int type\n\n\n# s1 + s2 ## 에러\n\n\ns1.astype(int)+s2\n\n0    0\n1    0\n2    0\n3    0\n4    0\ndtype: int64\n\n\n# 예시3: 원소의 타입을 변환한 이후 브로드캐스팅 (str)\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n위의 자료에서 Embarked 열과 Pclass열을 이용하여 아래와 같은 New Feature를 만들어라.\n\n\n\nEmbarked\nPclass\nNew Feature\n\n\n\n\n‘S’\n3\n‘S3’\n\n\n‘C’\n1\n‘C1’\n\n\n‘S’\n3\n‘S3’\n\n\n‘S’\n1\n‘S1’\n\n\n‘S’\n3\n‘S3’\n\n\n\n(풀이)\n\n# df.Embarked + df.Pclass ## 에러남\n\n\ndf.Embarked + df.Pclass.astype(str)\n\n0    S3\n1    C1\n2    S3\n3    S1\n4    S3\ndtype: object\n\n\n#"
  },
  {
    "objectID": "posts/2_DV2022/2023-10-04-05wk-2-t.html#f.-컴프리헨션-lambdamap을-무시하지-말-것",
    "href": "posts/2_DV2022/2023-10-04-05wk-2-t.html#f.-컴프리헨션-lambdamap을-무시하지-말-것",
    "title": "[DV2023] 05wk-2: Pandas – transform column (꿀팁)",
    "section": "F. 컴프리헨션, lambda+map을 무시하지 말 것",
    "text": "F. 컴프리헨션, lambda+map을 무시하지 말 것\n# 예시1\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n위의 자료에서 아래와 같은 변환을 하고 싶다면?\n\\[\nf(\\text{sex}, \\text{sibsp}) =\n\\begin{cases}\n0.7 + 0.25 \\times \\text{sibsp} & \\text{if } \\text{sex} = \\text{'female'} \\\\\n0.1 + 0.15 \\times \\text{sibsp} & \\text{otherwise}\n\\end{cases}\n\\]\n\nmap(입력이2개인 함수, 입력1의 리스트, 입력2의 리스트)\n\n\ndf.assign(Prob= list(map(lambda sex,sibsp: 0.7 + sibsp*0.25 if sex=='female' else 0.2 + sibsp*0.15, df.Sex, df.SibSp)))\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\nProb\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n0.35\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n0.95\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n0.70\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n0.95\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n0.20\n\n\n\n\n\n\n\n# 예시2\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2023/main/posts/titanic.csv\")[:5]\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nlogFare\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n1.981001\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n4.266662\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n2.070022\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n3.972177\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n2.085672\n\n\n\n\n\n\n\n위의 자료에서 Name열을 아래와 같이 분리하는 작업을 수행하라.\n\n\n\n\ntitle\nName\n\n\n\n\n0\nMr\nOwen Harris Braund\n\n\n1\nMrs\nJohn Bradley (Florence Briggs Thayer) Cumings\n\n\n2\nMiss\nLaina Heikkinen\n\n\n3\nMrs\nJacques Heath (Lily May Peel) Futrelle\n\n\n4\nMr\nWilliam Henry Allen\n\n\n\n\ndf.Name\n\n0                              Braund, Mr. Owen Harris\n1    Cumings, Mrs. John Bradley (Florence Briggs Th...\n2                               Heikkinen, Miss. Laina\n3         Futrelle, Mrs. Jacques Heath (Lily May Peel)\n4                             Allen, Mr. William Henry\nName: Name, dtype: object\n\n\n\n[i for i in df.Name.str.replace(', ','/').str.replace('. ','/').str.split('/')]\n\n[['Braund', 'Mr', 'Owen Harris'],\n ['Cumings', 'Mrs', 'John Bradley (Florence Briggs Thayer)'],\n ['Heikkinen', 'Miss', 'Laina'],\n ['Futrelle', 'Mrs', 'Jacques Heath (Lily May Peel)'],\n ['Allen', 'Mr', 'William Henry']]\n\n\n\n[[last_name,title,first_name] for last_name,title,first_name in df.Name.str.replace(', ','/').str.replace('. ','/').str.split('/')]\n\n[['Braund', 'Mr', 'Owen Harris'],\n ['Cumings', 'Mrs', 'John Bradley (Florence Briggs Thayer)'],\n ['Heikkinen', 'Miss', 'Laina'],\n ['Futrelle', 'Mrs', 'Jacques Heath (Lily May Peel)'],\n ['Allen', 'Mr', 'William Henry']]\n\n\n\n# title, 이름+성\n[[title,first_name+ ' ' + last_name] for last_name,title,first_name in df.Name.str.replace(', ','/').str.replace('. ','/').str.split('/')]\n\n[['Mr', 'Owen Harris Braund'],\n ['Mrs', 'John Bradley (Florence Briggs Thayer) Cumings'],\n ['Miss', 'Laina Heikkinen'],\n ['Mrs', 'Jacques Heath (Lily May Peel) Futrelle'],\n ['Mr', 'William Henry Allen']]\n\n\n\n# fstring 이용\n[[title,f'{first_name} {last_name}'] for last_name,title,first_name in df.Name.str.replace(', ','/').str.replace('. ','/').str.split('/')]\n\n[['Mr', 'Owen Harris Braund'],\n ['Mrs', 'John Bradley (Florence Briggs Thayer) Cumings'],\n ['Miss', 'Laina Heikkinen'],\n ['Mrs', 'Jacques Heath (Lily May Peel) Futrelle'],\n ['Mr', 'William Henry Allen']]\n\n\n(풀이)\n\n_lst = [[title,f'{first_name} {last_name}'] for last_name, title, first_name in df.Name.str.replace(', ','/').str.replace('. ','/').str.split('/')]\npd.DataFrame(_lst, columns=['title','Name2'])\n\n\n\n\n\n\n\n\ntitle\nName2\n\n\n\n\n0\nMr\nOwen Harris Braund\n\n\n1\nMrs\nJohn Bradley (Florence Briggs Thayer) Cumings\n\n\n2\nMiss\nLaina Heikkinen\n\n\n3\nMrs\nJacques Heath (Lily May Peel) Futrelle\n\n\n4\nMr\nWilliam Henry Allen\n\n\n\n\n\n\n\n참고: “Mr,Mrs,Miss” 만 뽑아내는 코드 (대면수업에서 하려다가 실패한 코드)\n\ndf.Name.str.split(', ').str[-1].str.split('. ').str[0]\n\n0      Mr\n1     Mrs\n2    Miss\n3     Mrs\n4      Mr\nName: Name, dtype: object"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "",
    "text": "Pandas 행과열의 선택"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html#a.-열의-선택",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html#a.-열의-선택",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "A. 열의 선택",
    "text": "A. 열의 선택\n- 방법1: df.? + str\n\n\n\n\n\n\n방법1 (df.?)\n\n\n\nclass의 instance 느낌으로 df를 하나의 오브젝트라고 보면, 안에있는 어트리뷰터 중 하나로 접근하는 느낌.\n\n\n\ndf.X1\n\n0    65\n1    95\n2    65\n3    55\n4    80\nName: X1, dtype: int64\n\n\n\n# df.X1?\n\n- 방법2: df[?] + str, [str,str]\n\n\n\n\n\n\n방법2 (df.?)\n\n\n\ndictionary에서 value를 뽑으려면 key로 접근하듯이~ (기원은 딕셔너리)\nDataFrame은 dictionary에서 플러스 알파느낌\n\n\n\ndct # 딕셔너리\n\n{'date': ['12/30', '12/31', '01/01', '01/02', '01/03'],\n 'X1': [65, 95, 65, 55, 80],\n 'X2': [55, 100, 90, 80, 30],\n 'X3': [50, 50, 60, 75, 30],\n 'X4': [40, 80, 30, 80, 100]}\n\n\n\ndct.keys() # dictinoary keys = column name\n\ndict_keys(['date', 'X1', 'X2', 'X3', 'X4'])\n\n\n\ndf['X1'] # df를 딕셔너리처럼 보고 있음. (key를 X1으로 갖고 있는 컬럼을 뽑는 것)\n\n0    65\n1    95\n2    65\n3    55\n4    80\nName: X1, dtype: int64\n\n\n\ntype(df['X1']) # 1차원 array\n\npandas.core.series.Series\n\n\n\ndf[['X1']] # 프레임 형태로 (column vec형태)\n\n\n\n\n\n\n\n\nX1\n\n\n\n\n0\n65\n\n\n1\n95\n\n\n2\n65\n\n\n3\n55\n\n\n4\n80\n\n\n\n\n\n\n\n\ndct[['X1']] # 이런건 없음.\n\nTypeError: unhashable type: 'list'\n\n\n\ndf[['X1', 'X3']]\n\n\n\n\n\n\n\n\nX1\nX3\n\n\n\n\n0\n65\n50\n\n\n1\n95\n50\n\n\n2\n65\n60\n\n\n3\n55\n75\n\n\n4\n80\n30\n\n\n\n\n\n\n\n\ndct[['X1','X3']] # 당연히 안됨.\n\nTypeError: unhashable type: 'list'\n\n\n\ndataframe은 dictionary에서 플러스 알파느낌이라는 것을 기억하자.\n\n- df.X는 변수명에 _ 언더바가 있으면 안된다.\n\ndf.rename({'X1':'X_1'}, axis=1)\n\n\n\n\n\n\n\n\ndate\nX_1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ndf.X_1  # 에러남!\n\nAttributeError: 'DataFrame' object has no attribute 'X_1'\n\n\n\n# df['X1'] # str \n# df[['X1']] # [str]\n# df[['X1','X3']] # [str,str]\n\n- 방법3: df.iloc[:,?] + int, int:int, [int,int], [bool,bool], range\n\ndf.iloc # 통쨰로 numpy array라고 생각.\n\n&lt;pandas.core.indexing._iLocIndexer at 0x1f1173cb540&gt;\n\n\n\ndf[:,0] # 이건 안되지만\n\nInvalidIndexError: (slice(None, None, None), 0)\n\n\n\ndf.iloc[:,0] # 이건 된다!\n\n0    12/30\n1    12/31\n2    01/01\n3    01/02\n4    01/03\nName: date, dtype: object\n\n\n\n# df.iloc[:,0] # int\n# df.iloc[:,-2:] # int:int - 슬라이싱\n# df.iloc[:,1::2] # int:int - 스트라이딩\n# df.iloc[:,[0]] # [int]\n# df.iloc[:,[0,1]] # [int,int]\n# df.iloc[:,[True,True,False,False,False]] # bool의 list \n# df.iloc[:,range(2)] # range\n\n\n\n\n\n\n\nNote\n\n\n\ndf.iloc이 통째로 numpy array이다\n\n\n- 방법4: df.loc[:,?] + str, ‘str:str’, [str,str], [bool,bool]\n\n컨셉: 편해보이는 건 다 가져다 쓰겠다.\n\n\ndf.loc[:,:] # 모든 행과 컬럼을 다 뽑겠다.\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ndf.loc[:,['X1','X3']]\n\n\n\n\n\n\n\n\nX1\nX3\n\n\n\n\n0\n65\n50\n\n\n1\n95\n50\n\n\n2\n65\n60\n\n\n3\n55\n75\n\n\n4\n80\n30\n\n\n\n\n\n\n\n\ndf.loc[:,'date':'X3'] # 문자열로 슬라이싱도 됨.\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\n\n\n\n\n0\n12/30\n65\n55\n50\n\n\n1\n12/31\n95\n100\n50\n\n\n2\n01/01\n65\n90\n60\n\n\n3\n01/02\n55\n80\n75\n\n\n4\n01/03\n80\n30\n30\n\n\n\n\n\n\n\n\ndf.loc[:,'date'::2] # 2칸씩 건너서 뽑을수도 있어.\n\n\n\n\n\n\n\n\ndate\nX2\nX4\n\n\n\n\n0\n12/30\n55\n40\n\n\n1\n12/31\n100\n80\n\n\n2\n01/01\n90\n30\n\n\n3\n01/02\n80\n80\n\n\n4\n01/03\n30\n100\n\n\n\n\n\n\n\n\ndf.loc[:,[True, False, False, True, True]] # true만 뽑을 수도~\n\n\n\n\n\n\n\n\ndate\nX3\nX4\n\n\n\n\n0\n12/30\n50\n40\n\n\n1\n12/31\n50\n80\n\n\n2\n01/01\n60\n30\n\n\n3\n01/02\n75\n80\n\n\n4\n01/03\n30\n100\n\n\n\n\n\n\n\n\ndf.loc[[0,2,4], [True,False,True, False, True]] # row는 숫자로도 뽑힘.\n\n\n\n\n\n\n\n\ndate\nX2\nX4\n\n\n\n\n0\n12/30\n55\n40\n\n\n2\n01/01\n90\n30\n\n\n4\n01/03\n30\n100\n\n\n\n\n\n\n\n\nrow이름이 숫자로 되어있어서 가능한 거임. (인덱싱의 편의상 숫자가 좋음)\n\n\n# df.loc[:,'X1'] # str\n# df.loc[:,'X1':'X3'] # 'str':'str' -- 칼럼이름으로 슬라이싱 **\n# df.loc[:,'X1'::2] # 'str':'str' -- 칼럼이름으로 스트라이딩 ** \n# df.loc[:,['X1']] # [str]\n# df.loc[:,['X1','X4']] # [str,str]\n# df.loc[:,[True,False,False,True]] # bool의 list"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html#b.-행의-선택",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html#b.-행의-선택",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "B. 행의 선택",
    "text": "B. 행의 선택\n- 방법1: df[] + int:int, str:str, [bool,bool], pd.Series([bool,bool]) – \\((\\star\\star\\star\\star\\star)\\)\n\ndf[0:3] # 딕셔너리 타입인데 숫자로도 되네? (예외사항)\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n\n\n\n\n\n\ndf[:2]\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n\n\n\n\n\n\ndf[::2] # 스트라이딩\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\nts\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n12/30\n65\n55\n50\n40\n\n\n12/31\n95\n100\n50\n80\n\n\n01/01\n65\n90\n60\n30\n\n\n01/02\n55\n80\n75\n80\n\n\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\nts['12/30':'01/01'] # 이것도 됨!\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n12/30\n65\n55\n50\n40\n\n\n12/31\n95\n100\n50\n80\n\n\n01/01\n65\n90\n60\n30\n\n\n\n\n\n\n\n\nts['12/31'::2] # 12/31부터 2칸씩 점프를 하면서 뽑아줘.\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n12/31\n95\n100\n50\n80\n\n\n01/02\n55\n80\n75\n80\n\n\n\n\n\n\n\n\ndf[[True, False, False, False, False]]\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf[['12' in date for date in df.date]]\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n\n\n\n\n\n\ndf[df.X1 &lt; 70] # pd.Series([bool,bool])\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n\n\n\n\n\n\n# df[:2] # int:int -- 슬라이싱 // df.iloc[:2,:], df.iloc[:2] 와 같음\n# df[::2] # int:int -- 스트라이딩 \n# ts['12/30':'01/02'] # str:str -- 슬라이싱\n# ts['12/31'::2] # str:str -- 스트라이딩\n# df[['12' in date for date in df.date]] # [bool,bool]\n# df[df.X1 &lt; 70] # pd.Series([bool,bool])\n\n- 방법2: df.iloc[], df.iloc[,:] + int, int:int, [int,int], [bool,bool], range\n명시적으로 쓴다면 df.iloc[,:] 지만 생략해도 같은 코드이긴 함.\n\ndf.iloc[:,:] # default\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ndf.iloc[[0],:] # dataframe 형식이 유지되면서 뽑힘.\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.iloc[0] # numpy에서 되니까 되는거임.\n\ndate    12/30\nX1         65\nX2         55\nX3         50\nX4         40\nName: 0, dtype: object\n\n\n\nlst = [[1,2,3], [3,4,5]]\nlst[0]\n\n[1, 2, 3]\n\n\n\nnp.array(lst) # 중첩된 리스트로 보면?\n\narray([[1, 2, 3],\n       [3, 4, 5]])\n\n\n\nnp.array(lst)[0]\n\narray([1, 2, 3])\n\n\n\npd.DataFrame(np.array(lst), columns=list('ABC')).iloc[0] # 첫번째 로우가 뽑힘.\n\nA    1\nB    2\nC    3\nName: 0, dtype: int32\n\n\n\n# df.iloc[0] # int \n# df.iloc[-2:] # int:int -- 슬라이싱\n# df.iloc[1::2] # int:int -- 스트라이딩\n# df.iloc[[0]] # [int]\n# df.iloc[[0,1]] # [int,int]\n# df.iloc[['12' in date for date in df.date]] # [bool,bool]\n# df.iloc[range(2)] # range\n\n\n# df.iloc[0,:] # int \n# df.iloc[-2:,:] # int:int -- 슬라이싱\n# df.iloc[1::2,:] # int:int -- 스트라이딩\n# df.iloc[[0],:] # [int]\n# df.iloc[[0,1],:] # [int,int]\n# df.iloc[['12' in date for date in df.date],:] # [bool,bool]\n# df.iloc[range(2),:] # range\n\n- 방법3: df.loc[], df.loc[,:] + int, str, int:int, str:str, [int,int], [str,str], [bool,bool], pd.Series([bool,bool])\n\nts.loc['12/30']\n\nX1    65\nX2    55\nX3    50\nX4    40\nName: 12/30, dtype: int64\n\n\n\nts.loc[['12/30','01/01']]\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n12/30\n65\n55\n50\n40\n\n\n01/01\n65\n90\n60\n30\n\n\n\n\n\n\n\n차이점 (끝점의 포함여부)\n\ndf.iloc[:2,:]  # 끝점이 포함되지 X\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n\n\n\n\n\n\ndf.loc[:2,:] # 끝점이 포함 O\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n\n\n\n\n\n\ndf.loc[df.X1&gt;70] # 이건 되는데\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ntype(df.X1&gt;70)\n\npandas.core.series.Series\n\n\n\ndf.iloc[df.X1&gt;70] # 이건 안되네? (series는 안됨)\n\nNotImplementedError: iLocation based boolean indexing on an integer type is not available\n\n\n\ndf.iloc[list(df.X1&gt;70)] # list로 바꾸면 된다!\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n\n# df.loc[0] # int \n# ts.loc['12/30'] # str \n# df.loc[:2] # int:int \n# ts.loc[:'01/02'] # str:str \n# df.loc[[0,1]] # [int,int]\n# ts.loc[['12/30','01/01']] # [str,str]\n# df.loc[['12' in date for date in df.date]] # [bool,bool]\n# df.loc[df.X1&gt;70] # pd.Series([bool,bool])"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html#d.-제-스타일",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html#d.-제-스타일",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "D. 제 스타일",
    "text": "D. 제 스타일\n- 가장 안전한 코드\n\ndf.loc[:,:]\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n4\n01/03\n80\n30\n30\n100\n\n\n\n\n\n\n\n- 상황1: 하나의 col을 뽑으려 할때 좋은 코드\n\n# df.X1 # 최애 \n# df['X1'] # 차애 \n# df[['X1']] # 차애\n\n\ndf.X1이 왜 좋냐면 변수가 많을 때 .찍고 탭누르면 뜨는 변수이름들 중에 고르면 되기 때문에 매우 편리함.\n\n- 상황2: row 슬라이싱을 할때 좋은 코드 \\((\\star\\star\\star)\\)\n\n# df[:5] # 최애 \n# ts[:'01/02'] # 시계열인 경우 \n\n\nts[:'01/02']\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n12/30\n65\n55\n50\n40\n\n\n12/31\n95\n100\n50\n80\n\n\n01/01\n65\n90\n60\n30\n\n\n01/02\n55\n80\n75\n80\n\n\n\n\n\n\n\n- 상황3: 조건에 맞는 row를 뽑을때 좋은 코드\n\n# df[df.X1&lt;60] # 최애\n# df.loc[['12' in date for date in df.date]] # 차애\n\n\ndf.loc[['12' in date for date in df.date],:]\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n1\n12/31\n95\n100\n50\n80\n\n\n\n\n\n\n\n- 상황4: 하나의 row를 뽑으려 할때 좋은 코드\n\n# df.iloc[0] # 최애 \n# df.loc[0] # 차애\n\n\ndf.iloc[0]\n\ndate    12/30\nX1         65\nX2         55\nX3         50\nX4         40\nName: 0, dtype: object\n\n\n\ndf.loc[0]\n\ndate    12/30\nX1         65\nX2         55\nX3         50\nX4         40\nName: 0, dtype: object\n\n\n\nts.loc['12/31',:]\n\nX1     95\nX2    100\nX3     50\nX4     80\nName: 12/31, dtype: int64\n\n\n\n# ts.loc[0,:] # 이건 안됨.\n\n- 상황5: (row,col)을 뽑으려 할때 좋은 코드\n\ndf.loc[2,'X2']\n\n90\n\n\n\ndf.X2[2]\n\n90\n\n\n\ndf.X2[:2] # 이것도 편함.\n\n0     55\n1    100\nName: X2, dtype: int64\n\n\n\n# 최애: pd.Series를 뽑고 -&gt; 인덱스로접근\n# df.X1[0]\n# df['X1'][0]\n\n# 차애: iloc, loc 으로 한번에 뽑기\n# df.iloc[0,0]\n# df.loc[0,'X1']\n\n위의 상황이외에는 df.loc[:,:]를 사용하는것이 유리하다\n- 상황6: column 슬라이싱을 할때\n\n# df.loc[:,'X1':'X3'] # 끝점포함\n\n- 상황7: row + column 슬라이싱을 하는 가장 좋은 코드\n\ndf.loc[::2,'X1':'X2']\n\n\n\n\n\n\n\n\nX1\nX2\n\n\n\n\n0\n65\n55\n\n\n2\n65\n90\n\n\n4\n80\n30\n\n\n\n\n\n\n\n- 상황8: 조건에 맞는 col을 뽑기에 가장 좋은 코드\n\n# df.loc[:,[len(colname)&gt;2 for colname in df.columns]]\n\n- 상황9: 조건에 맞는 row, col을 뽑기에 가장 좋은 코드\n\n# df.loc[df.X1&gt;70,[len(colname)&gt;2 for colname in df.columns]]"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html#d.-제-스타일-x",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html#d.-제-스타일-x",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "D. 제 스타일 X",
    "text": "D. 제 스타일 X\n- 제가 안쓰는 코드1\n\ndf[:1]\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n\n\n\n\n\n이러면 내 입장에서는 마치 아래가 동작할 것 같잖아..\n\ndf[0] \n\nKeyError: 0\n\n\n- 제가 안쓰는 코드2: bool의 list를 사용할때 iloc은 가급적 쓰지마세요\n\ndf.iloc[list(df['X1']&lt;80),:]\n\n\n\n\n\n\n\n\ndate\nX1\nX2\nX3\nX4\n\n\n\n\n0\n12/30\n65\n55\n50\n40\n\n\n2\n01/01\n65\n90\n60\n30\n\n\n3\n01/02\n55\n80\n75\n80\n\n\n\n\n\n\n\n이러면 마치 아래도 동작할 것 같잖아..\n\ndf.iloc[df['X1']&lt;80,:]\n\nNotImplementedError: iLocation based boolean indexing on an integer type is not available"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html#e.-요약",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html#e.-요약",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "E. 요약",
    "text": "E. 요약\n- 알아두면 좋은 규칙\n\n.iloc[] 와 .iloc[,:]는 완전히 동등하다.\n.loc[] 와 .loc[,:]는 완전히 동등하다.\n결과를 pd.Series 형태가 아닌 pd.DataFrame 형태로 얻고 싶다면 [[?]]를 사용하면 된다.\n\n- 정리\n\nROW\n\n\n\n\n\n\n\n\n\n\n\ntype of indexer\n.\n[]\n.iloc\n.loc\n내가 쓴다면?\n\n\n\n\nint\nX\nX\nO\n\\(\\Delta\\)\ndf.iloc[3,:]\n\n\nint:int\nX\nO\nO\n\\(\\Delta\\)\ndf[3:5]\n\n\n[int,int]\nX\nX\nO\n\\(\\Delta\\)\ndf.iloc[idx,:]\n\n\nstr\nX\nX\nX\nO\nts.loc['time1',:]\n\n\nstr:str\nX\nO\nX\nO\nts.loc['time1':'time2',:]\n\n\n[str,str]\nX\nX\nX\nO\n안할 듯\n\n\n[bool,bool]\nX\nO\nO\nO\ndf[filtered_idx]\n\n\npd.Series([bool,bool])\nX\nO\nX\nO\ndf[df.X1&gt;20]\n\n\n\n\n\nCOL\n\n\n\n\n\n\n\n\n\n\n\n\ntype of indexer\ntarget\n.\n[]\n.iloc\n.loc\n내가 쓴다면?\n\n\n\n\nint\ncol\nX\nX\nO\nX\ndf.iloc[:,0]\n\n\nint:int\ncol\nX\nX\nO\nX\ndf.iloc[:,0:2]\n\n\n[int,int]\ncol\nX\nX\nO\nX\ndf.iloc[:,idx]\n\n\nstr\ncol\nO\nO\nX\nO\ndf.loc[:,'X1']\n\n\nstr:str\ncol\nX\nX\nX\nO\ndf.loc[:,'X1':'X4']\n\n\n[str,str]\ncol\nX\nO\nX\nO\ndf.loc[:,colname_list]\n\n\n[bool,bool]\ncol\nX\nX\nO\nO\ndf.loc[:,bool_list]"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html#a.-기본-query",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html#a.-기본-query",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "A. 기본 query",
    "text": "A. 기본 query\n- 예시1: A&gt;0 and B&lt;0\n\ndf.query('A&gt;0 and B&lt;0')\n\n- 예시2: A&lt;B&lt;C\n\ndf.query('A&lt;B&lt;C')\n\n- 예시3: (A+B/2) &gt; 0\n\ndf.query('(A+B)&gt;0')\n\n- 예시4: (A+B/2) &gt; 0 and E=='A'\n\ndf.query('(A+B)&gt;0 and E==\"A\"')\n\n\ndf.query(\"(A+B)&gt;0 and E=='A'\")"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html#b.-외부변수를-이용",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html#b.-외부변수를-이용",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "B. 외부변수를 이용",
    "text": "B. 외부변수를 이용\n- 예시\n\nmean = df.A.mean()\nmean\n\n\ndf.query('A &gt; @mean')"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html#c.-index로-query",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html#c.-index로-query",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "C. Index로 query",
    "text": "C. Index로 query\n- 예시\n\ndf.query('index &lt;= \"2022-12-30\" or index == \"2023-01-10\"')"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html#a.-df.assign",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html#a.-df.assign",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "A. df.assign()",
    "text": "A. df.assign()\n- 방법1: assign을 이용한 추가\n\ndf.assign(total = df.att*0.1 + df.rep*0.2 + df.mid*0.35 + df.fin*0.35) \n\n\n이 방법은 df를 일시적으로 변화시킴"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html#b.-df.eval",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html#b.-df.eval",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "B. df.eval()",
    "text": "B. df.eval()\n- 방법2: eval을 이용한 추가\n\ndf.eval('total = att*0.1 + rep*0.2 + mid*0.3 + fin*0.4') \n\n\n이 방법은 df를 일시적으로 변화시킴"
  },
  {
    "objectID": "posts/2_DV2022/2023-09-20-03wk-2-t.html#c.-dfcolname-xxx",
    "href": "posts/2_DV2022/2023-09-20-03wk-2-t.html#c.-dfcolname-xxx",
    "title": "[DV2023] 03wk-2: Pandas",
    "section": "C. df[colname] = xxx",
    "text": "C. df[colname] = xxx\n- 방법3: df['total'] 을 이용한 할당\n\ndf['total'] = df.att*0.1 + df.rep*0.2 + df.mid*0.3 + df.fin*0.4\ndf\n\n\n이 방법은 df를 영구적 으로 변화시킴"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html",
    "title": "05wk-2",
    "section": "",
    "text": "훌륭한 시각화, mpg 데이터 소개, plotnine(p9)을 이용한 고차원 산점도"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#애드워드-터프티",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#애드워드-터프티",
    "title": "05wk-2",
    "section": "애드워드 터프티",
    "text": "애드워드 터프티\n- 데이터 시각화계의 거장\n- 터프티의 이론중 백미: 엄격한 미니멀리즘\n\n최소한의 잉크로 많은 정보를 전달할 수 있다면 그것이 바로 좋은 그래프이다.\n작은 지면 내에서 잉크를 최대한 적게 써서 짧은 시간 안에 많은 영감을 주어야 한다.\n\n- 데이터-잉크비: 데이터를 표현하는데 들아가는 잉크의 양 / 그래픽을 인쇄하는데 들어가는 잉크의 총량\n- 차트정크 (나이젤홈즈의 그래프)\n\n\n“Lurking behind chartjunk is contempt both for information and for the audience. Chartjunk promoters imagine that numbers and details are boring, dull, and tedious, requiring ornament to enliven. Cosmetic decoration, which frequently distorts the data, will never salvage an underlying lack of content. If the numbers are boring, then you’ve got the wrong numbers (…) Worse is contempt for our audience, designing as if readers were obtuse and uncaring. In fact, consumers of graphics are often more intelligent about the information at hand than those who fabricate the data decoration (…) The operating moral premise of information design should be that our readers are alert and caring; they may be busy, eager to get on with it, but they are not stupid.”\n\n\n차트정크 = 대중을 멸시 + 데이터에 대한 모독\n차트정크 옹호가는 숫자와 데이터가 지루하여 활기가 필요하다고 생각하는 모양이다..\n\n- 별로인 그래프 (왼쪽) / 우수한 그래프 오른쪽\n\n- 별로인 그래프 (왼쪽) / 우수한 그래프 오른쪽\n\n- 별로인 그래프 (왼쪽) / 우수한 그래프 오른쪽\n\n- 제 생각: 글쎄…"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#찰스미나드의-도표",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#찰스미나드의-도표",
    "title": "05wk-2",
    "section": "찰스미나드의 도표",
    "text": "찰스미나드의 도표\n\n인류역사상 가장 훌륭한 시각화\n\n\n- 터프티의 평\n\n지금까지 그려진 최고의 통계 그래픽일지도 모른다.\n여기에서는 군대의 크기, 2차원 평면상의 위치, 군대의 이동방향, 모스코바에서 퇴각하는 동안의 여러날짜, 온도 \\(\\to\\) 6차원의 변수\n백만번에 한번 이런 그림을 그릴수는 있겠지만 이러한 멋진 그래픽을 만드는 방법에 대한 원칙은 없다. \\(\\to\\) 미니멀리즘..\n\n- 왜 우수한 그래프일까?\n\n자료를 파악하는 기법은 최근까지도 산점도, 막대그래프, 라인플랏에 의존\n이러한 플랏의 단점은 고차원의 자료를 분석하기 어렵다는 것임\n미나드는 여러그램을 그리는 방법 대신에 한 그림에서 패널을 늘리는 방법을 선택함."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#미나드처럼-그리는게-왜-어려운가",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#미나드처럼-그리는게-왜-어려운가",
    "title": "05wk-2",
    "section": "미나드처럼 그리는게 왜 어려운가?",
    "text": "미나드처럼 그리는게 왜 어려운가?\n- 몸무게, 키, 성별, 국적\n\ndf1=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/male1.csv')\ndf2=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/male2.csv')  \ndf3=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/female.csv') \ndf4=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/foreign.csv')\n\n- 미나드의 접근방법\n\n_df = pd.concat([pd.concat([df1,df2],axis=1).assign(g='m'),df3.assign(g='f')])\ndf = pd.concat([_df.assign(g2='korea'),df4.assign(g2='foreign')]).reset_index(drop=True)\ndf\n\n\n\n\n\n\n\n\nw\nh\ng\ng2\n\n\n\n\n0\n72.788217\n183.486773\nm\nkorea\n\n\n1\n66.606430\n173.599877\nm\nkorea\n\n\n2\n69.806324\n173.237903\nm\nkorea\n\n\n3\n67.449439\n173.223805\nm\nkorea\n\n\n4\n70.463183\n174.931946\nm\nkorea\n\n\n...\n...\n...\n...\n...\n\n\n1525\n78.154632\n188.324350\nm\nforeign\n\n\n1526\n74.754308\n183.017979\nf\nforeign\n\n\n1527\n91.196208\n190.100456\nm\nforeign\n\n\n1528\n87.770394\n187.987255\nm\nforeign\n\n\n1529\n88.021995\n193.456798\nm\nforeign\n\n\n\n\n1530 rows × 4 columns\n\n\n\n\nsns.scatterplot(data=df,x='w',y='h',hue='g',style='g2')\n\n&lt;AxesSubplot:xlabel='w', ylabel='h'&gt;\n\n\n\n\n\n- 어려운점: (1) 센스가 없어서 hue/style을 이용하여 그룹을 구분할 생각을 못함 (2) long df (=tidy data) 형태로 데이터를 정리할 생각을 못함 (3) long df 형태로 데이터를 변형하는 코드를 모름\n\n\n기획력부족 -&gt; 훌륭한 시각화를 많이 볼 것\n\n\n데이터프레임에 대한 이해부족 -&gt; tidydata에 대한 개념\n\n\n프로그래밍 능력 -&gt; 코딩공부열심히 (pandas를 엄청 잘해야함)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#방법1-rpy2-코랩-아닌경우-실습금지",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#방법1-rpy2-코랩-아닌경우-실습금지",
    "title": "05wk-2",
    "section": "방법1: rpy2 (코랩 아닌경우 실습금지)",
    "text": "방법1: rpy2 (코랩 아닌경우 실습금지)\n\nimport rpy2\n%load_ext rpy2.ipython\n\n\n%%R \n### 여기는 R처럼 쓸 수 있다. \na &lt;- c(1,2,3) \na+1\n\n[1] 2 3 4\n\n\n\na\n\nNameError: name 'a' is not defined\n\n\n\n%%R \nlibrary(tidyverse)\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# … with 224 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nmpg\n\nNameError: name 'mpg' is not defined\n\n\n\n%R -o mpg # R에 있는 자료가 파이썬으로 넘어옴\n\n\nmpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n1\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n2\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n4\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n5\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n234\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#방법2-저장된-csv파일을-통하여-데이터를-확보",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#방법2-저장된-csv파일을-통하여-데이터를-확보",
    "title": "05wk-2",
    "section": "방법2: 저장된 csv파일을 통하여 데이터를 확보",
    "text": "방법2: 저장된 csv파일을 통하여 데이터를 확보\n\nmpg.to_csv(\"mpg.csv\",index=False)\n\n\npd.read_csv(\"mpg.csv\")\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#방법3-github등에-공개된-csv를-읽어오기",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#방법3-github등에-공개된-csv를-읽어오기",
    "title": "05wk-2",
    "section": "방법3: github등에 공개된 csv를 읽어오기",
    "text": "방법3: github등에 공개된 csv를 읽어오기\n\npd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/mpg.csv')\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns\n\n\n\n- 깃허브 저장소에 아예 데이터만 따로 모아서 관리하는 것도 좋은 방법입니다."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#data-설명",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#data-설명",
    "title": "05wk-2",
    "section": "data 설명",
    "text": "data 설명\n- displ: 자동차의 엔진크기\n- hwy: 연료의 효율, 동일한 연료로 얼마나 멀리 가느냐?\n- 자세한 설명은 R에서 ?mpg를 이용해 스스로 찾아볼 것"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#python에서-plotnine을-이용한-산점도",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#python에서-plotnine을-이용한-산점도",
    "title": "05wk-2",
    "section": "python에서: plotnine을 이용한 산점도",
    "text": "python에서: plotnine을 이용한 산점도\n\nggplot(data=mpg) + geom_point(mapping=aes(x='displ',y='hwy')) ## plotnine\n\n\n\n\n\n산점도 해석: 엔진크기가 클수록 효율이 낮음.\n\n- 빠르게 그리기: data=와 mapping=은 생략가능함\n\nggplot(mpg) + geom_point(aes(x='displ',y='hwy')) ## plotnine"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#r에서-ggplot2를-이용한-산점도",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#r에서-ggplot2를-이용한-산점도",
    "title": "05wk-2",
    "section": "R에서: ggplot2를 이용한 산점도",
    "text": "R에서: ggplot2를 이용한 산점도\n- R에서도 거의 똑같은 문법으로 그릴 수 있음 (데이터프레임 혹은 티블에 저장된 column 이름을 사용할때 따옴표만 제거하면 된다!)\n\n%%R -w 800\nggplot(mpg) + geom_point(aes(x=displ,y=hwy)) ## plotnine"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#python에서-객체지향적인-느낌으로-산점도-그리기",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#python에서-객체지향적인-느낌으로-산점도-그리기",
    "title": "05wk-2",
    "section": "python에서: 객체지향적인 느낌으로 산점도 그리기",
    "text": "python에서: 객체지향적인 느낌으로 산점도 그리기\nstep1: 도화지를 준비한다.\n\nfig = ggplot(data=mpg)\nfig\n\n\n\n\nstep2 변수와 에스테틱사이의 맵핑을 설정한다.\n\na1= aes(x='displ',y='hwy')\na1\n\n{'x': 'displ', 'y': 'hwy'}\n\n\nstep3 점들의 집합을 만든다. 즉 포인트 지옴을 만든다.\n\npoint1=geom_point(mapping=a1)\n\n\ngeom_point(): 점들을 그려! 어떻게?\na1에서 설정된 표를 보고\n\nstep4 도화지와 지옴을 합친다.\n\nfig+point1"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경",
    "title": "05wk-2",
    "section": "산점도 + 점크기변경",
    "text": "산점도 + 점크기변경\n\nggplot(data=mpg) + geom_point(mapping = aes(x='displ',y='hwy',size='class'))\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-투명도변경",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-투명도변경",
    "title": "05wk-2",
    "section": "산점도 + 투명도변경",
    "text": "산점도 + 투명도변경\n\nggplot(data=mpg) + geom_point(mapping = aes(x='displ',y='hwy',alpha='class'))\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_alpha.py:70: PlotnineWarning: Using alpha for a discrete variable is not advised."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-투명도점크기를-동시에-적용",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-투명도점크기를-동시에-적용",
    "title": "05wk-2",
    "section": "산점도 + 투명도/점크기를 동시에 적용",
    "text": "산점도 + 투명도/점크기를 동시에 적용\n\nggplot(data=mpg) + geom_point(mapping = aes(x='displ',y='hwy',alpha='class',size='class'))\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_alpha.py:70: PlotnineWarning: Using alpha for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-형태",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-형태",
    "title": "05wk-2",
    "section": "산점도 + 형태",
    "text": "산점도 + 형태\n\nggplot(data=mpg) + geom_point(mapping = aes(x='displ',y='hwy',shape='class'))"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-색깔",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-색깔",
    "title": "05wk-2",
    "section": "산점도 + 색깔",
    "text": "산점도 + 색깔\n\nggplot(data=mpg) + geom_point(mapping = aes(x='displ',y='hwy',color='class'))"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#객체지향적-느낌으로",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#객체지향적-느낌으로",
    "title": "05wk-2",
    "section": "객체지향적 느낌으로?",
    "text": "객체지향적 느낌으로?\n\na2 = aes(x='displ', y='hwy', color='class') \n\n\na1,a2\n\n({'x': 'displ', 'y': 'hwy'}, {'x': 'displ', 'y': 'hwy', 'color': 'class'})\n\n\n\npoint2=geom_point(a2)\n\n\nfig+point2"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-색깔-적합선",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-색깔-적합선",
    "title": "05wk-2",
    "section": "산점도 + 색깔 + 적합선",
    "text": "산점도 + 색깔 + 적합선\n- 일단 색깔이 없는 포인트 지옴부터 연습\n\nfig+point1\n\n\n\n\n\nline1 = geom_smooth(a1)\n\n\nfig+point1+line1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- point1(색깔없는 포인트 지옴)을 point2(색깔있는 포인트 지옴)으로 언제든지 바꿔치기 가능!\n\nfig+point2+line1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 명령어로 한번에 그리기\n\nggplot(data=mpg) + \\\ngeom_point(mapping=aes(x='displ',y='hwy',color='class')) + \\\ngeom_smooth(mapping=aes(x='displ',y='hwy'))\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 공통적인 맵핑규칙은 ggplot()쪽으로 빼기도 한다. (figure를 선언하는 곳에서 공통으로 선언함)\n\nggplot(data=mpg,mapping=aes(x='displ',y='hwy')) + \\\ngeom_point(mapping=aes(color='class')) + \\\ngeom_smooth()\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- R에서는 confidence interval도 geom_smooth()를 이용하여 확인할 수 있다.\n\n%%R -w 800\nggplot(data=mpg,mapping=aes(x=displ,y=hwy)) + geom_point(mapping=aes(color=class)) + geom_smooth()\n\nR[write to console]: `geom_smooth()` using method = 'loess' and formula 'y ~ x'"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔",
    "title": "05wk-2",
    "section": "산점도 + 점크기변경 + 색깔",
    "text": "산점도 + 점크기변경 + 색깔\n- drv (전륜, 후륜, 4륜 구동)에 따라서 데이터를 시각화 하고 싶다.\n\nggplot(data=mpg, mapping=aes(x='displ',y='hwy')) + geom_point(mapping=aes(size='class',color='drv'),alpha=0.3)\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n모든 \\(x\\)에 대하여 붉은색 점들이 대부분 초록색과 보라색 점들에 비하여 아래쪽에 있음 \\(\\to\\) 4륜구동방식이 연비가 좋지 않음"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-객체지향버전",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-객체지향버전",
    "title": "05wk-2",
    "section": "산점도 + 점크기변경 + 색깔 (객체지향버전)",
    "text": "산점도 + 점크기변경 + 색깔 (객체지향버전)\n- 맵핑규칙\n\na1,a2\n\n({'x': 'displ', 'y': 'hwy'}, {'x': 'displ', 'y': 'hwy', 'color': 'class'})\n\n\n\na3 = a2.copy() \n\n\na3['color'] = 'drv'\na3['size'] = 'class'\na3\n\n{'x': 'displ', 'y': 'hwy', 'color': 'drv', 'size': 'class'}\n\n\n\n아래와 같이 선언해도 괜찮음\n\na3= aes(x='displ',y='hwy',color='drv',size='class')\n\npoint3=geom_point(a3)\n\n\nfig+point3\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n그림의 전체적인 투명도를 조절하면 좋겠음\n\n\npoint3=geom_point(a3,alpha=0.2)\nfig+point3\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-선추가",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-선추가",
    "title": "05wk-2",
    "section": "산점도 + 점크기변경 + 색깔 + 선추가",
    "text": "산점도 + 점크기변경 + 색깔 + 선추가\n\nfig+point3+line1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-drv별로-선추가",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-drv별로-선추가",
    "title": "05wk-2",
    "section": "산점도 + 점크기변경 + 색깔 + drv별로 선추가",
    "text": "산점도 + 점크기변경 + 색깔 + drv별로 선추가\n- 맵핑규칙\n\na1,a2,a3\n\n({'x': 'displ', 'y': 'hwy'},\n {'x': 'displ', 'y': 'hwy', 'color': 'class'},\n {'x': 'displ', 'y': 'hwy', 'color': 'drv', 'size': 'class'})\n\n\n\na4 = a2.copy() \na4['color']='drv'\na4\n\n{'x': 'displ', 'y': 'hwy', 'color': 'drv'}\n\n\n\nline2 = geom_smooth(a4)\n\n\nfig + point3 +line2\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 선의 색깔을 동일하게 하고 선의 타입을 변경하여 drv를 표시하고 싶다면?\n\na1,a2,a3,a4\n\n({'x': 'displ', 'y': 'hwy'},\n {'x': 'displ', 'y': 'hwy', 'color': 'class'},\n {'x': 'displ', 'y': 'hwy', 'color': 'drv', 'size': 'class'},\n {'x': 'displ', 'y': 'hwy', 'color': 'drv'})\n\n\n\na5=a1.copy()\na5['linetype']='drv' \na5\n\n{'x': 'displ', 'y': 'hwy', 'linetype': 'drv'}\n\n\n\nline3 = geom_smooth(a5,size=0.5,color='gray')\n\n\nfig+point3+line3\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 전체적인 추세선도 추가하고 싶다면?\n\nfig+point3+line3+line1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 그려보니까 역시 drv별로 그려지는 추세선은 색깔별로 구분하는게 좋겠음.\n\nline2 = geom_smooth(a4,size=0.5,linetype='dashed')\nfig+point3+line2+line1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 고차원을 변수를 표현할 수 있는 무기는 다양하다.\n\n산점도(포인트지옴): 점의크기, 점의형태, 점의색깔, 점의투명도\n라인플랏(스무스지옴,라인지옴): 선의형태, 선의색깔, 선의굵기"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "",
    "text": "이번주 선형대수 // 다음주 확률 // optimization"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#notations",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#notations",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Notations",
    "text": "Notations\n- Scalar\nSingle number. (real number or natural number)\n- Vector\n숫자들이 순서대로 배열된 1차원 배열.\n- Matrix\n2차원 배열. (벡터를 쌓아놓은 것)\n\\[{\\bf A} = \\begin{bmatrix}A_{1,1} & A_{1,2} \\\\ A_{2,1} & A_{2,2} \\end{bmatrix}\\]\n- Tensor\n3차원 이상의 배열."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#transpose-multiplication",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#transpose-multiplication",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Transpose & Multiplication",
    "text": "Transpose & Multiplication\n- 1. Transpose\n\\({\\bf A}_{i,j}^\\top = {\\bf A}_{j,i}^\\top\\)\n- 2. Addition\n\n\\({\\bf A} + {\\bf B} = {\\bf C}\\) where \\(C_{i,j} = A_{i,j} + B_{i,j}\\)\n\\(a\\cdot {\\bf A} + c = {\\bf G}\\) where \\(G_{i,j} = a\\cdot A_{i,j}+c\\)\n\n\\(\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}=\\begin{bmatrix} a_{11} + b_1 & a_{12} + b_2 \\\\ a_{21} + b_1 & a_{22} + b_2 \\end{bmatrix}\\)\n\ninner product는 차원을 줄이는 것이라면 outer product는 차원을 늘리는 것.\n\n- 3. Multiplication\nMultiplication\nmatrices \\({\\bf A} \\in \\mathbb{R}^{a_1\\times b}\\) and \\({\\bf B} \\in \\mathbb{R}^{b\\times a_2}\\)의 곱은 아래와 같다.\n\\[{\\bf C} = {\\bf A}{\\bf B}\\]\nwhere \\(C_{i,j} = \\sum_{l}A_{i,l}B_{l,j} (i,j) = \\{1,\\dots,a_1\\}\\times \\{1,\\dots, a_2\\}\\).\nHadamard product\nHadamard product (element-wise product) of \\(A \\in \\mathbb{R}^{a\\times b}\\) and \\(B^{a\\times b}\\)\n\\[{\\bf C} = {\\bf A} \\odot {\\bf B}\\]\nwhere \\(C_{i,j} = A_{i,j}B_{i,j}.\\)\n아다마르 곱(또는 요소별 곱셈, Hadamard product)은 다양한 분야에서 사용되며 주로 두 개의 행렬 또는 벡터 간의 요소별 연산을 나타냅니다. 아다마르 곱은 다음과 같은 상황에서 쓰입니다:\n\n신호 처리: 아다마르 곱은 디지털 신호 처리에서 자주 사용됩니다. 예를 들어, 두 시계열 데이터를 아다마르 곱하여 두 신호 간의 상관 관계를 계산하거나 신호를 필터링하는 데 사용될 수 있습니다.\n이미지 처리: 컴퓨터 비전 및 이미지 처리에서 두 이미지나 이미지와 마스크(필터) 사이의 요소별 곱셈은 특정 이미지 처리 작업에 사용됩니다. 예를 들어, 이미지를 선명하게 만들거나 특정 부분을 강조하는 데 유용합니다.\n뉴럴 네트워크: 인공 신경망에서 아다마르 곱은 활성화 함수와 가중치 간의 요소별 연산에 사용됩니다. 이를 통해 네트워크의 비선형성을 증가시키고 특정 기능을 강조할 수 있습니다.\n행렬 연산: 다른 행렬 연산과 결합하여 특정 형태의 계산을 수행할 때 아다마르 곱이 사용될 수 있습니다. 예를 들어, 고유값 분해(Eigendecomposition)와 같은 행렬 분해 기술에서도 사용됩니다.\n요소별 연산: 두 개의 행렬 또는 벡터 사이의 각 요소를 독립적으로 처리하고 싶을 때 아다마르 곱을 사용합니다. 이렇게 하면 각 요소 간의 관계를 보존하면서 연산을 수행할 수 있습니다.\n\n아다마르 곱은 행렬 곱셈과는 다르며, 두 개의 행렬 또는 벡터의 크기가 동일해야 합니다. 따라서 요소별 연산을 수행하려면 같은 크기의 입력이 필요합니다.\n4. Properties of multiplication\n5. Linear equation\n\\({\\bf A}{\\bf x} = {\\bf b}\\) where \\({\\bf A} \\in \\mathbb{R}^{m\\times n}, {\\bf b} \\in \\mathbb{R}^{m\\times 1}\\)"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#identity-and-inverse-matrix",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#identity-and-inverse-matrix",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Identity and Inverse Matrix",
    "text": "Identity and Inverse Matrix\nIdentity matrix: all diagonal terms are one, others are zero such that \\({\\bf I}_n \\in \\mathbb{R}^{n\\times n}\\)\n- 해가 엄청나게 많은 상황\n- 해를 못구하는 상황"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#norms-and-special-kinds-of-matrices-and-vectors",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#norms-and-special-kinds-of-matrices-and-vectors",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Norms and Special Kinds of Matrices and Vectors",
    "text": "Norms and Special Kinds of Matrices and Vectors\n\n1. Norm\n- Definition of \\(L^p\\) norm\n\\[||{\\bf x}||_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\\]\nNorm function \\(f\\) 는 다음을 만족한다.\n\n\\(f(x) = 0 \\Rightarrow {\\bf x} = 0\\)\n\\(f(x+y) \\leq f(x) + f(y)\\) (triangle inequality)\n\\(\\forall \\alpha \\in \\mathbb{R}, f(\\alpha {\\bf x}) = |\\alpha|f(x)\\)\n\n- \\(L^1\\) and \\(L^0\\) norm are considered to address the above.\n\\[||{\\bf x}||_1 = \\sum_i |x_i|\\]\n\\[||{\\bf x}||_0 = \\sum_i \\mathbb{I} (x_i \\neq 0)\\]\nhigh dimension에서는 \\(L^0\\)나 \\(L^1\\)을 많이 쓴다.\n- \\(L^2\\) norm은 \\(0\\)과 \\(0\\)이 아닌 기계학습 구별에서 너무 큰 값을 제공한다.\n- Other norms such as max norm and Frobenius norm\n\\[||{\\bf x}||_{\\infty} = \\max|x_i|\\]\n\\[||{\\bf A}||_F = \\sqrt{\\sum_{i,j}{\\bf A}^2_{i,j}}\\]\n- Definition of an angle between two vectors.\n\\(\\cos \\theta = \\frac{x^\\top y}{||x|| ||y||}\\)\n\\(w_1(1,0,0) + w_1(0,1,0) + w_3(0,0,1) = (w_1, w_2, w_3)\\)"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#linear-dependence-and-span",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#linear-dependence-and-span",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Linear Dependence and Span",
    "text": "Linear Dependence and Span"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#decompositions",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#decompositions",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Decompositions",
    "text": "Decompositions\n\n1. Eigendecomposition\n\\[{\\bf A} = {\\bf V}\\text{diag}({\\lambda}){\\bf V}^{-1}\\]\n\n데이터의 correaltion이 강할때 오른쪽 그림과 같다.\n\n\n2. Singular value decomposition\nDecomposition for a non-square matrix \\({\\bf A} \\in \\mathbb{R}^{n\\times m}\\)\n\\[{\\bf A} = {\\bf U}{\\bf D}{\\bf V}^\\top\\]\nwhere \\({\\bf U} \\in \\mathbb{R}^{n\\times n}, {\\bf D} \\in \\mathbb{R}^{n\\times m}\\), and \\({\\bf V}\\in \\mathbb{R}^{m\\times m}\\)."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#the-moore-penrose-pseudoinverse-trace-and-determinant",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#the-moore-penrose-pseudoinverse-trace-and-determinant",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "The Moore-Penrose Pseudoinverse, Trace, and Determinant",
    "text": "The Moore-Penrose Pseudoinverse, Trace, and Determinant\n\nlinear equation의 해가 너무 많아서 못푸는 문제 같은 경우는 대안이 있지 않을까?\n\n변환시킨 경우 \\(n\\)차원 중 어떤 한 축이 0이면 \\(n-1\\) 차원. 그럼 볼륨이 0이된다."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#example-pca",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#example-pca",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Example: PCA",
    "text": "Example: PCA\n통계: 분산이 가장 큰 축을 찾는다.\ninput data는 엄청 큰데 작은 차원으로 줄어들면서 다시 커지는 과정을 PCA가 하고 있다. 단, 제약이 있음.\n인코딩 디코딩 개념 알아두자.\n\n\\(f(x)\\): 인코딩 (관측을 하는 큰 차원에서 관측하지 못하는 작은 차원으로)\n\\(g(f(x))\\): 디코딩 (관측하지 못하는 작은 차원에서 관측을 하는 큰차원으로)"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-04-DL-1wk.html",
    "href": "posts/4_DL2023/2023-09-04-DL-1wk.html",
    "title": "딥러닝",
    "section": "",
    "text": "중간 : 지필\n기말 : 프로젝트 (조편성? / 3,4명)\n과제 : 2주에 한번\n\n- 문제1 : non-linear \\(\\to\\) Stochastic descent 어떻게 쓰지?\nlinear일 때는 연산이 되는데 non-linear일 때는 어떻게 하지? (복잡하다…)\n역전파 등장\n- 문제2: 왜 bounded function을 쓸까?\n딥네트워크 뉴런개수 많아. linear sum 할 때 예를들어 10만개가 들어와? 그런데 바운디드가 안되어있으면, 오버플러우 문제 발생. (값을 일정하게 안정화시키는 것이 중요했다. \\(\\to\\) 가능하면 bounded function사용. (시그모이드 사용한 이유)\n- 문제3: ReLU를 쓰면 그레디언트 터지는거 아냐?\n뉴런의 수가 많아지고 층이 깊어지면 그레디언트 터지는거 아니야?\n0과 양수가 섞여있는데 0부분이 있어서 폭발을 막아준다.\n노드들을 연결하고 있는게 엣지라고 하는데 ReLU를 적용시키면 0이되는 (쓸모없는) 에지들이 있다. 혹은 뉴런이 있다. 극단적인 경우 10%만 쓸모있고 나머지는 날려도 좋다…\nlinear summation \\(\\to\\) non linear activation \\(\\to\\) linear summation \\(\\to\\) non linear activation \\(\\to\\) \\(\\cdots\\)\nactivation 과정에서 일부 날라가게 되겠지\n- 볼츠만 머신\n지금 데이터를 가지고 새로운 피처를 만드는데 뉴럴넷 구조로 하자. (unsupervised learning)\n학습이 잘되는 피처를 만들겠다.\n2009 딥볼츠만머신까지는 피처의 변환까지만 (회귀/분류 이런건 안했음) –&gt; 알렉스넷부터…\n- Pretrained Model\n특정 문제만 풀수있다? \\(\\to\\) informative 하다고 할 수 없지만\n다른 데이터로 학습을 했지만 “이미지” 라는 도메인에서 정보를 추출하는 방법으로 학습을 하기 때문에 다른 이미지가 들어와도 잘 학습한다.\nFeature Extractor가 들어온 것이 큰 변화다. (Representation Power)\n\nRepresentation Power\n레이어를 쌓아가면서 참조를 하니까 성능이 올라가게 된다.\n이미지에서 로컬한 부분먼저 본다. 레이어가 쌓아가면서 뭉쳐서 본다. \\(\\to\\) 전체적인 개형이 나온다. But 전체적인 개형이 원래의 이미지와 같을 수는 없다. (어떤 부분은 강조가 되고, 어떤 부분은 사라지고)\n\n- Depth\n\n계산\n레이어 개수 (check!) : 베이직하게 레이터를 몇층 쌓느냐라고 생각하면 된다.\n\n아키텍처를 크게 만들어서 1,2를 분류할 때 일부분만, 2,3을 예측할때 다른 일부분만,… 이런식으로 학습하는 방법도 있다."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_03_21_(3주차)_3월21일.html",
    "href": "posts/3_STBDA2022/2022_03_21_(3주차)_3월21일.html",
    "title": "[STBDA] 3wk. 텐서플로우 intro2 (tf.GradientTape())",
    "section": "",
    "text": "(3주차) 3월21일\n\n강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-yCZH5zqsORTEkCZ082SCYc\n\n\n\nimports\n\nimport tensorflow as tf\nimport numpy as np\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\n\n지난강의 보충\n- max, min, sum, mean\n\na= tf.constant([1.0,2.0,3.0,4.0])\na\n\n&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt;\n\n\n\ntf.reduce_mean(a)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.5&gt;\n\n\n\nconcat, stack\n- 예제: (2,3,4,5) stack (2,3,4,5) -&gt; (?,?,?,?,?)\n\na = tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5))\nb = -a \n\n\nTip: 총 5가지 case가 있다. case1 (1,2,3,4,5) stack (1,2,3,4,5) –&gt; (2,2,3,4,5) # axis=0  case2 (2,1,3,4,5) stack (2,1,3,4,5) –&gt; (2,2,3,4,5) # axis=1  case3 (2,3,1,4,5) stack (2,3,1,4,5) –&gt; (2,3,2,4,5) # axis=2  case4 (2,3,4,1,5) stack (2,3,4,1,5) –&gt; (2,3,4,2,5) # axis=3  case5 (2,3,4,5,1) stack (2,3,4,5,1) –&gt; (2,3,4,5,2) # axis=4\n\ncase1 (1,2,3,4,5) stack (1,2,3,4,5) –&gt; (2,2,3,4,5) # axis=0\n\ntf.stack([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]]],\n\n\n\n       [[[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase2 (2,1,3,4,5) stack (2,1,3,4,5) –&gt; (2,2,3,4,5) # axis=1\n\ntf.stack([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase3 (2,3,1,4,5) stack (2,3,1,4,5) –&gt; (2,3,2,4,5) # axis=2\n\ntf.stack([a,b],axis=2)\n\n&lt;tf.Tensor: shape=(2, 3, 2, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase4 (2,3,4,1,5) stack (2,3,4,1,5) –&gt; (2,3,4,2,5) # axis=3\n\ntf.stack([a,b],axis=-2).shape\n\nTensorShape([2, 3, 4, 2, 5])\n\n\n\ntf.stack([a,b],axis=-2)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 2, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   0,   -1,   -2,   -3,   -4]],\n\n         [[   5,    6,    7,    8,    9],\n          [  -5,   -6,   -7,   -8,   -9]],\n\n         [[  10,   11,   12,   13,   14],\n          [ -10,  -11,  -12,  -13,  -14]],\n\n         [[  15,   16,   17,   18,   19],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [ -20,  -21,  -22,  -23,  -24]],\n\n         [[  25,   26,   27,   28,   29],\n          [ -25,  -26,  -27,  -28,  -29]],\n\n         [[  30,   31,   32,   33,   34],\n          [ -30,  -31,  -32,  -33,  -34]],\n\n         [[  35,   36,   37,   38,   39],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [ -40,  -41,  -42,  -43,  -44]],\n\n         [[  45,   46,   47,   48,   49],\n          [ -45,  -46,  -47,  -48,  -49]],\n\n         [[  50,   51,   52,   53,   54],\n          [ -50,  -51,  -52,  -53,  -54]],\n\n         [[  55,   56,   57,   58,   59],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [ -60,  -61,  -62,  -63,  -64]],\n\n         [[  65,   66,   67,   68,   69],\n          [ -65,  -66,  -67,  -68,  -69]],\n\n         [[  70,   71,   72,   73,   74],\n          [ -70,  -71,  -72,  -73,  -74]],\n\n         [[  75,   76,   77,   78,   79],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [ -80,  -81,  -82,  -83,  -84]],\n\n         [[  85,   86,   87,   88,   89],\n          [ -85,  -86,  -87,  -88,  -89]],\n\n         [[  90,   91,   92,   93,   94],\n          [ -90,  -91,  -92,  -93,  -94]],\n\n         [[  95,   96,   97,   98,   99],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [-100, -101, -102, -103, -104]],\n\n         [[ 105,  106,  107,  108,  109],\n          [-105, -106, -107, -108, -109]],\n\n         [[ 110,  111,  112,  113,  114],\n          [-110, -111, -112, -113, -114]],\n\n         [[ 115,  116,  117,  118,  119],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase5 (2,3,4,5,1) stack (2,3,4,5,1) –&gt; (2,3,4,5,2) # axis=4\n\ntf.stack([a,b],axis=-1).shape\n\nTensorShape([2, 3, 4, 5, 2])\n\n\n\ntf.stack([a,b],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 5, 2), dtype=int32, numpy=\narray([[[[[   0,    0],\n          [   1,   -1],\n          [   2,   -2],\n          [   3,   -3],\n          [   4,   -4]],\n\n         [[   5,   -5],\n          [   6,   -6],\n          [   7,   -7],\n          [   8,   -8],\n          [   9,   -9]],\n\n         [[  10,  -10],\n          [  11,  -11],\n          [  12,  -12],\n          [  13,  -13],\n          [  14,  -14]],\n\n         [[  15,  -15],\n          [  16,  -16],\n          [  17,  -17],\n          [  18,  -18],\n          [  19,  -19]]],\n\n\n        [[[  20,  -20],\n          [  21,  -21],\n          [  22,  -22],\n          [  23,  -23],\n          [  24,  -24]],\n\n         [[  25,  -25],\n          [  26,  -26],\n          [  27,  -27],\n          [  28,  -28],\n          [  29,  -29]],\n\n         [[  30,  -30],\n          [  31,  -31],\n          [  32,  -32],\n          [  33,  -33],\n          [  34,  -34]],\n\n         [[  35,  -35],\n          [  36,  -36],\n          [  37,  -37],\n          [  38,  -38],\n          [  39,  -39]]],\n\n\n        [[[  40,  -40],\n          [  41,  -41],\n          [  42,  -42],\n          [  43,  -43],\n          [  44,  -44]],\n\n         [[  45,  -45],\n          [  46,  -46],\n          [  47,  -47],\n          [  48,  -48],\n          [  49,  -49]],\n\n         [[  50,  -50],\n          [  51,  -51],\n          [  52,  -52],\n          [  53,  -53],\n          [  54,  -54]],\n\n         [[  55,  -55],\n          [  56,  -56],\n          [  57,  -57],\n          [  58,  -58],\n          [  59,  -59]]]],\n\n\n\n       [[[[  60,  -60],\n          [  61,  -61],\n          [  62,  -62],\n          [  63,  -63],\n          [  64,  -64]],\n\n         [[  65,  -65],\n          [  66,  -66],\n          [  67,  -67],\n          [  68,  -68],\n          [  69,  -69]],\n\n         [[  70,  -70],\n          [  71,  -71],\n          [  72,  -72],\n          [  73,  -73],\n          [  74,  -74]],\n\n         [[  75,  -75],\n          [  76,  -76],\n          [  77,  -77],\n          [  78,  -78],\n          [  79,  -79]]],\n\n\n        [[[  80,  -80],\n          [  81,  -81],\n          [  82,  -82],\n          [  83,  -83],\n          [  84,  -84]],\n\n         [[  85,  -85],\n          [  86,  -86],\n          [  87,  -87],\n          [  88,  -88],\n          [  89,  -89]],\n\n         [[  90,  -90],\n          [  91,  -91],\n          [  92,  -92],\n          [  93,  -93],\n          [  94,  -94]],\n\n         [[  95,  -95],\n          [  96,  -96],\n          [  97,  -97],\n          [  98,  -98],\n          [  99,  -99]]],\n\n\n        [[[ 100, -100],\n          [ 101, -101],\n          [ 102, -102],\n          [ 103, -103],\n          [ 104, -104]],\n\n         [[ 105, -105],\n          [ 106, -106],\n          [ 107, -107],\n          [ 108, -108],\n          [ 109, -109]],\n\n         [[ 110, -110],\n          [ 111, -111],\n          [ 112, -112],\n          [ 113, -113],\n          [ 114, -114]],\n\n         [[ 115, -115],\n          [ 116, -116],\n          [ 117, -117],\n          [ 118, -118],\n          [ 119, -119]]]]], dtype=int32)&gt;\n\n\n- 예제: (2,3,4), (2,3,4), (2,3,4)\n\na= tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb= -a \nc= 2*a\n\n(예시1) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (6,3,4)\n\ntf.concat([a,b,c],axis=0)\n\n&lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)&gt;\n\n\n(예시2) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,9,4)\n\ntf.concat([a,b,c],axis=1)\n\n&lt;tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11],\n        [  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23],\n        [ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)&gt;\n\n\n(예시3) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,12)\n\ntf.concat([a,b,c],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy=\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3,   0,   2,   4,   6],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7,   8,  10,  12,  14],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11,  16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15,  24,  26,  28,  30],\n        [ 16,  17,  18,  19, -16, -17, -18, -19,  32,  34,  36,  38],\n        [ 20,  21,  22,  23, -20, -21, -22, -23,  40,  42,  44,  46]]],\n      dtype=int32)&gt;\n\n\n(예시4) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (3,2,3,4)\n\n축이 늘어난 경우\n\n\ntf.stack([a,b,c],axis=0)\n\n&lt;tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]]],\n\n\n       [[[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]]],\n\n\n       [[[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)&gt;\n\n\n(예시5) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=1).shape\n\nTensorShape([2, 3, 3, 4])\n\n\n\ntf.stack([a,b,c],axis=1)\n\n&lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)&gt;\n\n\n(예시6) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=2)\n\n&lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  0,  -1,  -2,  -3],\n         [  0,   2,   4,   6]],\n\n        [[  4,   5,   6,   7],\n         [ -4,  -5,  -6,  -7],\n         [  8,  10,  12,  14]],\n\n        [[  8,   9,  10,  11],\n         [ -8,  -9, -10, -11],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [-12, -13, -14, -15],\n         [ 24,  26,  28,  30]],\n\n        [[ 16,  17,  18,  19],\n         [-16, -17, -18, -19],\n         [ 32,  34,  36,  38]],\n\n        [[ 20,  21,  22,  23],\n         [-20, -21, -22, -23],\n         [ 40,  42,  44,  46]]]], dtype=int32)&gt;\n\n\n(예시7) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,4,3)\n\ntf.stack([a,b,c],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy=\narray([[[[  0,   0,   0],\n         [  1,  -1,   2],\n         [  2,  -2,   4],\n         [  3,  -3,   6]],\n\n        [[  4,  -4,   8],\n         [  5,  -5,  10],\n         [  6,  -6,  12],\n         [  7,  -7,  14]],\n\n        [[  8,  -8,  16],\n         [  9,  -9,  18],\n         [ 10, -10,  20],\n         [ 11, -11,  22]]],\n\n\n       [[[ 12, -12,  24],\n         [ 13, -13,  26],\n         [ 14, -14,  28],\n         [ 15, -15,  30]],\n\n        [[ 16, -16,  32],\n         [ 17, -17,  34],\n         [ 18, -18,  36],\n         [ 19, -19,  38]],\n\n        [[ 20, -20,  40],\n         [ 21, -21,  42],\n         [ 22, -22,  44],\n         [ 23, -23,  46]]]], dtype=int32)&gt;\n\n\n- 예제: (2,3,4) (4,3,4) \\(\\to\\) (6,3,4)\n\na=tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4))\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[-24, -25, -26, -27],\n        [-28, -29, -30, -31],\n        [-32, -33, -34, -35]],\n\n       [[-36, -37, -38, -39],\n        [-40, -41, -42, -43],\n        [-44, -45, -46, -47]]], dtype=int32)&gt;\n\n\n\ntf.concat([a,b],axis=1) # dimension이 달라 오류!\n\nInvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n\ntf.concat([a,b],axis=2)\n\nInvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n- (2,2) @ (2,) 의 연산?\nnumpy\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88])\n\narray([ 77, -88])\n\n\n\n차원이 안맞는데 계산이 된다?\n\n\nnp.array([77,-88]) @ np.array([[1,0],[0,1]])\n\narray([ 77, -88])\n\n\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88]).reshape(2,1) # dimension 명시\n\narray([[ 77],\n       [-88]])\n\n\n\nnp.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) # 잘못된 걸 명시해주니까 계산 안됨!\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1)\n\n\n\nnp.array([77,-88]).reshape(1,2) @ np.array([[1,0],[0,1]]) \n\narray([[ 77, -88]])\n\n\n–&gt; 요약: numpy에서 길이가 2인 벡터는 매트릭스를 곱할 때 알아서 계산이 되서 결과가 나옴.\ntensorflow\n\nI = tf.constant([[1.0,0.0],[0.0,1.0]]) \nx = tf.constant([77.0,-88.0]) \n\n\nI @ x \n\nInvalidArgumentError: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul]\n\n\n\nx @ I\n\nInvalidArgumentError: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul]\n\n\n\nI @ tf.reshape(x,(2,1))\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[ 77.],\n       [-88.]], dtype=float32)&gt;\n\n\n\ntf.reshape(x,(1,2)) @ I \n\n&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)&gt;\n\n\n\n\n\n\ntf.Variable\n되게 쓸모없어 보이는데 쉽고, 중요합니다.\n\n선언\n- tf.Variable()로 선언\n\ntf.Variable([1,2,3,4])\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n\ntf.Variable([1.0,2.0,3.0,4.0])\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt;\n\n\n- tf.constant() 선언후 변환\n\na_ = tf.Variable(tf.constant([1,2,3,4]))\ntype(a_)\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\ntype이 ResourceVariable\n\n\ntf.Variable(tf.constant([1,2,3,4])) # type이렇게 바로 변환 가능\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n- np 등으로 선언후 변환\n\ntf.Variable(np.array([1,2,3,4]))\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int64, numpy=array([1, 2, 3, 4])&gt;\n\n\n\n\n타입\n\ntype(tf.Variable([1,2,3,4]))\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\n\n인덱싱\n\na=tf.Variable([1,2,3,4])\na\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n\ntype(a)\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\na[:2] # 처음 2개의 원소.\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;\n\n\n\ntype(a[:2])\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n연산하는 순간 type이 EagerTensor로 바뀜\n\n\n\n연산가능\n\na=tf.Variable([1,2,3,4])\nb=tf.Variable([-1,-2,-3,-4])\n\n\na+b\n\n&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)&gt;\n\n\ntf.Variable로 열심히 만들어도 연산하는 순간 tf.constant로 바뀜.. (자료형이 깨짐)\n\n\ntf.Variable도 쓰기 불편함\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14]) ## 에러\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n\n\n\n\ntnp의 은총도 일부만 가능\n\nimport tensorflow.experimental.numpy as tnp \ntnp.experimental_enable_numpy_behavior() \n\n- 알아서 형 변환\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14])\n\n&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])&gt;\n\n\n- .reshape 메소드\n\ntf.Variable([1,2,3,4]).reshape(2,2)\n\nAttributeError: 'ResourceVariable' object has no attribute 'reshape'\n\n\n\ntf.constant는 되는데 tf.Variable은 또 안됨…\n\n\n\n대부분의 동작은 tf.constant랑 큰 차이를 모르겠음\n- tf.concat\n\na= tf.Variable([[1,2],[3,4]]) \nb= tf.Variable([[-1,-2],[-3,-4]]) \ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1,  2],\n       [ 3,  4],\n       [-1, -2],\n       [-3, -4]], dtype=int32)&gt;\n\n\n- tf.stack\n\na= tf.Variable([[1,2],[3,4]]) \nb= tf.Variable([[-1,-2],[-3,-4]]) \ntf.stack([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\narray([[[ 1,  2],\n        [ 3,  4]],\n\n       [[-1, -2],\n        [-3, -4]]], dtype=int32)&gt;\n\n\n\n이건 비슷한데?\n\n\n\n변수값변경가능(?)\n\na = 1\nid(a)\n\n7585472\n\n\n\na = 456\nid(a)\n\n140177070372624\n\n\n\n새로만드는 것은 되는데 수정은 안됨. (즉, 재할당밖에 안됨. 수정은 안돼)\n근데 가변형으로 만들어 주는것이 좋은데.. (불변형을 사용하려면 메모리가 커야해..)\n그래서 편집 가능한 변수로 선언하는 것이 의미가 있다.\n보통 딥러닝 학습할 때 Data는 RAM에 올리고 파라미터는 GPU에 올린다. (GPU에 올리면 미분계산(선형연산)이 빨라짐)\n\n\na= tf.Variable([1,2,3,4])\nid(a)\n\n140177068844272\n\n\n\na.assign_add([-1,-2,-3,-4]) # 편집기능.\nid(a)\n\n140177068844272\n\n\n\n주소값이 똑같으니까 편집!\n\n\n\n요약\n- tf.Variable()로 만들어야 하는 뚜렷한 차이는 모르겠음.\n- 애써 tf.Variable()로 만들어도 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐.\n\n\n\n미분\n\n모티브\n- 예제: 컴퓨터를 이용하여 \\(x=2\\)에서 \\(y=3x^2\\)의 접선의 기울기를 구해보자.\n(손풀이)\n\\[\\frac{dy}{dx}=6x\\]\n이므로 \\(x=2\\)를 대입하면 12이다.\n(컴퓨터를 이용한 풀이)\n단계1\n\nx1=2 \ny1= 3*x1**2 \n\n\nx2=2+0.000000001\ny2= 3*x2**2\n\n\n(y2-y1)/(x2-x1)\n\n12.0\n\n\n단계2\n\ndef f(x):\n    return(3*x**2)\n\n\nf(3)\n\n27\n\n\n\ndef d(f,x):\n    return (f(x+0.000000001)-f(x))/0.000000001\n\n\nd(f,2)\n\n12.000000992884452\n\n\n단계3\n\nd(lambda x: 3*x**2 ,2)\n\n12.000000992884452\n\n\n\nd(lambda x: x**2 ,0)\n\n1e-09\n\n\n단계4\n\\[f(x,y)= x^2 +3y\\]\n\ndef f(x,y):\n    return(x**2 +3*y)\n\n\nd(f,(2,3))\n\nTypeError: can only concatenate tuple (not \"float\") to tuple\n\n\n\n\ntf.GradientTape() 사용방법\n- 예제1: \\(x=2\\)에서 \\(y=3x^2\\)의 도함수값을 구하라.\n\nx=tf.Variable(2.0) # 미분 기울기를 구하고 싶은 지점을 tf.Variable로 선언\na=tf.constant(3.0) # 숫자로 선언하고 싶은 것.\n\n\ntf.GradientTape()     # Tape: 뭔가 기록할 수 있는 것.\n\n&lt;tensorflow.python.eager.backprop.GradientTape at 0x7f7d84347bb0&gt;\n\n\n\n실행결과 오브젝트로 나왔는데 ,그것이 0x7f7d4c6cfdf0 이 메모리 주소 안에 있다.\n\n\ndir(mytape) # 여기서 __enter__와 __exit__에 주목!\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__enter__',\n '__eq__',\n '__exit__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_ensure_recording',\n '_persistent',\n '_pop_tape',\n '_push_tape',\n '_recording',\n '_tape',\n '_tf_api_names',\n '_tf_api_names_v1',\n '_watch_accessed_variables',\n '_watched_variables',\n 'batch_jacobian',\n 'gradient',\n 'jacobian',\n 'reset',\n 'stop_recording',\n 'watch',\n 'watched_variables']\n\n\n\nmytape=tf.GradientTape() \nmytape.__enter__() # 기록 시작 \ny=a*x**2 # y=ax^2 = 3x^2 (기록하고 싶은 것)\nmytape.__exit__(None,None,None) # 기록 끝 \n\n\n그럼 mytape에는 뭔가가 기록되어 있을 것이다.\n위의 코드에서 __enter__()와 __exit__()는 고정이라고 생각\n그 사이에는 수식쓰기\n\n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt;\n\n\n\n미분 결과 12\n\n- 예제2: 조금 다른예제\n\nx=tf.Variable(2.0)\n#a=tf.constant(3.0)\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \na=(x/2)*3 ## a=(3/2)x \ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝 \n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt;\n\n\n\n실행은 되는데 결과가 틀림! 왜 18이지?\n\n\\[a=\\frac{3}{2}x\\] \\[y=ax^2=\\frac{3}{2}x^3\\]\n\\[\\frac{dy}{dx}=\\frac{3}{2} 3x^2\\]\n\n3/2*3*4\n\n18.0\n\n\n- 테이프의 개념 (\\(\\star\\))\n(상황)\n우리가 어려운 미분계산을 컴퓨터에게 부탁하는 상황임. (예를들면 \\(y=3x^2\\)) 컴퓨터에게 부탁을 하기 위해서는 연습장(=테이프)에 \\(y=3x^2\\)이라는 수식을 써서 보여줘야하는데 이때 컴퓨터에게 target이 무엇인지 그리고 무엇으로 미분하고 싶은 것인지를 명시해야함.\n\nmytape = tf.GradientTape(): tf.GradientTape()는 연습장을 만드는 명령어, 만들어진 연습장을 mytape라고 이름을 붙인다.\nmytape.__enter__(): 만들어진 공책을 연다 (=기록할수 있는 상태로 만든다)\na=x/2*3; y=a*x**2: 컴퓨터에게 전달할 수식을 쓴다\nmytape.__exit__(None,None,None): 공책을 닫는다.\nmytape.gradient(y,x): \\(y\\)를 \\(x\\)로 미분하라는 메모를 남기고 컴퓨터에게 전달한다.\n\n- 예제3: 연습장을 언제 열고 닫을지 결정하는건 중요하다.\n\nx=tf.Variable(2.0)\na=(x/2)*3 ## a=(3/2)x\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝 \n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt;\n\n\n- 예제4: with문과 함께 쓰는 tf.GradientTape()\n열고 &gt; 쓰고 &gt; 닫고 &gt; 컴퓨터에 전달 –&gt;&gt; 이 과정을 간략하게 매크로화 시키자.\n\nx=tf.Variable(2.0)\na=(x/2)*3 \n\n\nwith tf.GradientTape() as mytape:\n    ## with문 시작 \n    y=a*x**2 \n    ## with문 끝 \n\n\nmytape.gradient(y,x) # y를 x로 미분하라.\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt;\n\n\n(문법해설)\n아래와 같이 쓴다.\nwith expression as myname:\n    ## with문 시작: myname.__enter__() \n    blabla ~ \n    yadiyadi !! \n    ## with문 끝: myname.__exit__()\n\nexpression 의 실행결과 오브젝트가 생성, 생성된 오브젝트는 myname라고 이름붙임. 이 오브젝트는 .__enter__()와 .__exit__()를 숨겨진 기능으로 포함해야 한다.\nwith문이 시작되면서 myname.__enter__()이 실행된다.\n블라블라와 야디야디가 실행된다.\nwith문이 종료되면서 myname.__exit__()이 실행된다.\n\n- 예제5: 예제2를 with문과 함께 구현\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt;\n\n\n- 예제6: persistent = True\n(관찰1)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상 실행해서 에러를 관측하라\n\nRuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n\n\n(관찰2)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상실행해도 에러가 나지않음 \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt;\n\n\n- 예제7: watch\n(관찰1)\n\nx=tf.constant(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰2)\n\nx=tf.constant(2.0)\nwith tf.GradientTape(persistent=True) as mytape:\n    mytape.watch(x) # 수동감시\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰3)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제 \n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰4)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제\n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰5)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True) as mytape: \n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n- 예제9: 카페예제로 돌아오자.\n- 예제10: 카페예제의 매트릭스 버전\n- 예제11: 위의 예제에서 이론적인 \\(\\boldsymbol{\\beta}\\)의 최적값을 찾아보고 (즉 \\(\\hat{\\boldsymbol{\\beta}}\\)을 찾고) 그곳에서 loss의 미분을 구하라. 구한결과가 \\(\\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}\\) 임을 확인하라."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_03_14_(2주차)_3월14일.html",
    "href": "posts/3_STBDA2022/2022_03_14_(2주차)_3월14일.html",
    "title": "[STBDA] 2wk. 텐서플로우 intro1 (tf.constant선언, tnp사용법)",
    "section": "",
    "text": "(2주차) 3월14일\n\n강의노트\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-z8oR8bQZHR0mpy_9OcsWOz\n\n\n\nimport\n\nimport tensorflow as tf\nimport numpy as np\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[]\n\n\n\n\ntf.constant\n\n예비학습: 중첩리스트\n- 리스트\n\nlst = [1,2,4,5,6]\nlst \n\n[1, 2, 4, 5, 6]\n\n\n\nlst[1] # 두번쨰원소 \n\n2\n\n\n\nlst[-1] # 마지막원소 \n\n6\n\n\n- (2,2) matrix 느낌의 list\n\nlst= [[1,2],[3,4]]\nlst\n\n[[1, 2], [3, 4]]\n\n\n위를 아래와 같은 매트릭스로 생각할수 있다.\n1 2 \n3 4 \n\nprint(lst[0][0]) # (1,1) \nprint(lst[0][1]) # (1,2) \nprint(lst[1][0]) # (2,1) \nprint(lst[1][1]) # (2,2) \n\n1\n2\n3\n4\n\n\n- (4,1) matrix 느낌의 list\n\nlst=[[1],[2],[3],[4]] # (4,1) matrix = 길이가 4인 col-vector\nlst\n\n[[1], [2], [3], [4]]\n\n\n\nnp.array(lst), np.array(lst).shape\n\n(array([[1],\n        [2],\n        [3],\n        [4]]),\n (4, 1))\n\n\n- (1,4) matrix 느낌의 list\n\nlst=[[1,2,3,4]] # (1,4) matrix = 길이가 4인 row-vector \nlst\n\n[[1, 2, 3, 4]]\n\n\n\nnp.array(lst), np.array(lst).shape\n\n(array([[1, 2, 3, 4]]), (1, 4))\n\n\n\n\n선언 (변수 선언하는 법?)\n\n텐서플로우를 쓰려면 텐서로 바꿔줘야 한다.\n\n- 스칼라\n\ntf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt;\n\n\n\ntf.constant(3.14)+tf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=6.28&gt;\n\n\n- 벡터\n\ntype(1),type([1,2,3]), type(_vector) # int 혹은 list 타입 --&gt; EagerTensor 타입으로 바꾸라는 뜻.\n\n(int, list, tensorflow.python.framework.ops.EagerTensor)\n\n\n\n_vector=tf.constant([1,2,3])\n\n\n_vector[-1]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt;\n\n\n- 매트릭스\n\n_matrix= tf.constant([[1,0],[0,1]])\n_matrix\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 1]], dtype=int32)&gt;\n\n\n- array\n\ntf.constant([[[0,1,1],[1,2,-1]],[[0,1,2],[1,2,-1]]])\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 0,  1,  1],\n        [ 1,  2, -1]],\n\n       [[ 0,  1,  2],\n        [ 1,  2, -1]]], dtype=int32)&gt;\n\n\n\n\n타입\n\ntype(tf.constant(3.14))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\nEagerTensor 타입이라는 것만 기억!\n\n\n\n인덱싱\n\n_matrix = tf.constant([[1,2],[3,4]])\n_matrix\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n\n_matrix[0][0]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;\n\n\n\n_matrix[0]\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;\n\n\n\n_matrix[0,:]\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;\n\n\n\n_matrix[:,0]\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)&gt;\n\n\n\n\ntf.constant는 불편하다.\n- 불편한점 1. 모든 원소가 같은 dtype을 가지고 있어야함. 2. 원소 수정이 불가능함. 3. 묵시적 형변환이 불가능하다.\n- 원소수정이 불가능함\n\na=tf.constant([1,22,33])\na\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)&gt;\n\n\n\na[0]=11 \n\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment\n\n\n- 묵시적 형변환이 불가능하다\n\n1 + 3.14 # 1(int), 3.14(float) --&gt; 4.14(float)\n\n4.140000000000001\n\n\n\n1을 float으로 암묵적으로 바꿔서 계산함.\n\n\ntf.constant(1)+tf.constant(3.14) ## 에러!\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n\n\n\nint와 float이라 에러나는 것\n\n\ntf.constant(1.0)+tf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.1400003&gt;\n\n\n- 같은 float도 안되는 경우가 있음\n\ntf.constant(1.0,dtype=tf.float64)\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=1.0&gt;\n\n\n\ntf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt;\n\n\n\ntf.constant(1.0,dtype=tf.float64)+tf.constant(3.14)\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2]\n\n\n\n조금만 틀리면 에러남..\n\n\n\ntf.constant \\(\\to\\) 넘파이\n\nnp.array(tf.constant(1)) # 방법1\n\narray(1, dtype=int32)\n\n\n\nnumpy로 연산을 다 해놓고 tensor로 바꾼다..\n\n\na=tf.constant([3.14,-3.14])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na.numpy() # numpy라는 메서드가 있음.\n\narray([ 3.14, -3.14], dtype=float32)\n\n\n\n\n연산\n- 더하기\n\na=tf.constant([1,2])\nb=tf.constant([3,4])\na+b\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt;\n\n\n처음에 int로 선언했으면 나머지도 모두 int로 선언해야해..\n\ntf.add(a,b)\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt;\n\n\n\n결과는 동일\n\n- 곱하기\n\na=tf.constant([[1,2],[3,4]])\nb=tf.constant([[5,6],[7,8]])\na*b # elementwise 하게 곱한하고 한다.\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)&gt;\n\n\n\ntf.multiply(a,b) # 이게 먼저 나왔음.\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)&gt;\n\n\n- 매트릭스의곱\n\na=tf.constant([[1,0],[0,1]]) # (2,2)\nb=tf.constant([[5],[7]]) # (2,1) \na@b\n\n&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)&gt;\n\n\n\ntf.matmul(a,b)\n\n&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)&gt;\n\n\n- 역행렬\n\na=tf.constant([[1,0],[0,2]])\na\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 2]], dtype=int32)&gt;\n\n\n\ntf.linalg.inv(a)\n\nInvalidArgumentError: Value for attr 'T' of int32 is not in the list of allowed values: double, float, half, complex64, complex128\n    ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse]\n\n\n\n1/2을 계산하려면 애초에 float형이어야 했음.\n\n\na=tf.constant([[1.0,0.0],[0.0,2.0]])\ntf.linalg.inv(a)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1. , 0. ],\n       [0. , 0.5]], dtype=float32)&gt;\n\n\n- tf.linalg. + tab을 누르면 좋아보이는 연산들 많음\n\na=tf.constant([[1.0,2.0],[3.0,4.0]])\nprint(a)\ntf.linalg.det(a)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt;\n\n\n\ntf.linalg.trace(a)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;\n\n\n\n\n형태변환\n- 기본: tf.reshape() 를 이용\n\na=tf.constant([1,2,3,4]) #  길이가 4인 vector\na\n\n&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,1)) # column-vec로 변환.\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,1))\n\n&lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\narray([[[1],\n        [2]],\n\n       [[3],\n        [4]]], dtype=int32)&gt;\n\n\n- 다차원\n\na=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12]) # 길이가 12인 vector\na\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,3)) # 2*2*3\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,3)) # 4*3\n\n&lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)&gt;\n\n\n- tf.reshape\n\na=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12])\na\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,-1)) # 원소 어짜피 12개 있으니까 4만 주면 나머지 3은 알아서 맞춰줌.\n\n&lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,-1))\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\nb=tf.reshape(a,(2,2,-1))\nb\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\ntf.reshape(b,-1) # 길이가 12인 vector로 바꿔줌.\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\n\n선언고급\n뭔가를 초기화할 때 필요한 기능을 정리한 것입니다.\n- 다른 자료형 (리스트나 넘파이)로 만들고 바꾸는것도 좋다.\n\nnp.diag([1,2,3,4])\n\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])\n\n\n\ntf.constant(np.diag([1,2,3,4]))\n\n&lt;tf.Tensor: shape=(4, 4), dtype=int64, numpy=\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])&gt;\n\n\n- tf.ones, tf.zeros\n\ntf.zeros([3,3])\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)&gt;\n\n\n\ntf.reshape(tf.constant([0]*9),(3,3)) # tensor로 바꾸고 shape을 바꿔주는 방법도 있다.\n\n&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0]], dtype=int32)&gt;\n\n\n- range(10)\n\na=range(0,12)\ntype(a) # type이 range임!\n\nrange\n\n\n\nlist(a) # 타입을 list로 바꿀 수 있음.\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\n\n\na=range(0,12)\ntf.constant(a)\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32)&gt;\n\n\n\ntf.constant(range(1,20,3)) \n\n&lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([ 1,  4,  7, 10, 13, 16, 19], dtype=int32)&gt;\n\n\n- tf.linspace\n\ntf.linspace(0,1,10) # 0부터시작해서 1까지 총 10개 // 여기서는 1(마지막 숫자)이 포함됨\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])&gt;\n\n\n\n\ntf.concat\n- (2,1) concat (2,1) =&gt; (2,2) - 두번째 축이 바뀌었다. =&gt; axis=1\n\na=tf.constant([[1],[2]])\nb=tf.constant([[3],[4]])\na,b # 2개의 col-vec\n\n(&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=1) # 2col-vec -&gt; matrix\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 3],\n       [2, 4]], dtype=int32)&gt;\n\n\n- (2,1) concat (2,1) =&gt; (4,1) - 첫번째 축이 바뀌었다. =&gt; axis=0\n\na=tf.constant([[1],[2]])\nb=tf.constant([[3],[4]])\na,b\n\n(&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)&gt;\n\n\n- (1,2) concat (1,2) =&gt; (2,2) - 첫번째 // axis=0\n\na=tf.constant([[1,2]])\nb=tf.constant([[3,4]])\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n- (1,2) concat (1,2) =&gt; (1,4) - 두번째 // axis=1\n- (2,3,4,5) concat (2,3,4,5) =&gt; (4,3,4,5) - 첫번째 // axis=0\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(4, 3, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]]],\n\n\n       [[[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,6,4,5) - 두번째 // axis=1\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(2, 6, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]],\n\n        [[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]],\n\n        [[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,8,5) - 세번째 // axis=2\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=2)\n\n&lt;tf.Tensor: shape=(2, 3, 8, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19],\n         [   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39],\n         [ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59],\n         [ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79],\n         [ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99],\n         [ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119],\n         [-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,4,10) - 네번째 // axis=3 # 0,1,2,3 // -4 -3 -2 -1\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4,    0,   -1,   -2,   -3,   -4],\n         [   5,    6,    7,    8,    9,   -5,   -6,   -7,   -8,   -9],\n         [  10,   11,   12,   13,   14,  -10,  -11,  -12,  -13,  -14],\n         [  15,   16,   17,   18,   19,  -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24,  -20,  -21,  -22,  -23,  -24],\n         [  25,   26,   27,   28,   29,  -25,  -26,  -27,  -28,  -29],\n         [  30,   31,   32,   33,   34,  -30,  -31,  -32,  -33,  -34],\n         [  35,   36,   37,   38,   39,  -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44,  -40,  -41,  -42,  -43,  -44],\n         [  45,   46,   47,   48,   49,  -45,  -46,  -47,  -48,  -49],\n         [  50,   51,   52,   53,   54,  -50,  -51,  -52,  -53,  -54],\n         [  55,   56,   57,   58,   59,  -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64,  -60,  -61,  -62,  -63,  -64],\n         [  65,   66,   67,   68,   69,  -65,  -66,  -67,  -68,  -69],\n         [  70,   71,   72,   73,   74,  -70,  -71,  -72,  -73,  -74],\n         [  75,   76,   77,   78,   79,  -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84,  -80,  -81,  -82,  -83,  -84],\n         [  85,   86,   87,   88,   89,  -85,  -86,  -87,  -88,  -89],\n         [  90,   91,   92,   93,   94,  -90,  -91,  -92,  -93,  -94],\n         [  95,   96,   97,   98,   99,  -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104, -100, -101, -102, -103, -104],\n         [ 105,  106,  107,  108,  109, -105, -106, -107, -108, -109],\n         [ 110,  111,  112,  113,  114, -110, -111, -112, -113, -114],\n         [ 115,  116,  117,  118,  119, -115, -116, -117, -118, -119]]]],\n      dtype=int32)&gt;\n\n\n- (4,) concat (4,) =&gt; (8,) - 첫번째축? // axis=0\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1,  2,  3,  4, -1, -2, -3, -4], dtype=int32)&gt;\n\n\n- (4,) concat (4,) =&gt; (4,2) - 두번째축? // axis=1 ==&gt; 이런거없다..\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=1)\n\nInvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat\n\n\n\n에러남! 이럴때는 tf.stack을 쓰면 된다.\n\n\n\ntf.stack\n\nstack은 차원이 늘어남!\n\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.stack([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[ 1,  2,  3,  4],\n       [-1, -2, -3, -4]], dtype=int32)&gt;\n\n\n\ntf.stack([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3],\n       [ 4, -4]], dtype=int32)&gt;\n\n\n\n\n\ntnp\n- tf는 넘파이에 비하여 텐서만들기가 너무힘듬\n\nnp.diag([1,2,3])\n\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n\n\n\nnp.diag([1,2,3]).reshape(-1)\n\narray([1, 0, 0, 0, 2, 0, 0, 0, 3])\n\n\n\n넘파이는 이런식으로 np.diag()도 쓸수 있고 reshape을 메소드로 쓸 수도 있는데…\n\n\ntnp 사용방법 (불만해결방법)\ntensorflow에서 numpy처럼 동작하도록 할 수 있는 모듈이라고 생각 즉, np의 모방버전으로 생각하면 된다.\n\nimport tensorflow.experimental.numpy as tnp\ntnp.experimental_enable_numpy_behavior()\n\n\ntype(tnp.array([1,2,3]))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n이렇게 만들어도 된다.\n\n- int와 float을 더할 수 있음\n\ntnp.array([1,2,3])+tnp.array([1.0,2.0,3.0])\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt;\n\n\n\ntf.constant([1,2,3])+tf.constant([1.0,2.0,3.0]) # 이게 원래 에러났었음!\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt;\n\n\n\ntnp 모듈을 불러오는 순간 tf로 선언하는 모든 것들도 우리가 알고있는 넘파이처럼 동작합니다.\n\n\ntnp.array(1)+tnp.array([1.0,2.0,3.0])\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])&gt;\n\n\n\ntnp.diag([1,2,3])\n\n&lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy=\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])&gt;\n\n\n\na=tnp.diag([1,2,3])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na=tf.constant([1,2,3])\na.reshape(3,1)\n\n&lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3]], dtype=int32)&gt;\n\n\n\n\n선언고급\n\nnp.random.randn(5) # random module\n\narray([1.12411749, 1.43127059, 0.61763568, 0.43586944, 0.33208259])\n\n\n\ntnp.random.randn(5) # 넘파이가 되면 나도 된다.\n\n&lt;tf.Tensor: shape=(5,), dtype=float64, numpy=array([-0.64055227, -1.33606436, -0.71424816,  1.61243245, -2.07980232])&gt;\n\n\n\n\n타입\n\ntype(tnp.random.randn(5))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n\ntf.contant로 만들어도 마치 넘파이인듯 쓰는 기능들\n- 묵시적형변환이 가능\nint랑 float을 계산할 수 있다는 것\n\ntf.constant([1,1])+tf.constant([2.2,3.3])\n\n&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([3.20000005, 4.29999995])&gt;\n\n\n- 메소드를 쓸수 있음.\n사용가능한 메소드가 많아짐..\n\na= tnp.array([[1,2,3,4]])\na.T\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int64, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]])&gt;\n\n\n\n\n그렇지만 np.array는 아님\n- 원소를 할당하는것은 불가능\n\na=tf.constant([1,2,3])\na\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;\n\n\n\na[0]=11\n\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment\n\n\n\n그냥 새로 만들어서 할당해야 함. (그래도 많이 개선된 것@)"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-zvc1QWi-Li-2GAeW91oicn"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#강의영상",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#강의영상",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-zvc1QWi-Li-2GAeW91oicn"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#imports",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#imports",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "imports",
    "text": "imports\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\ntf.config.experimental.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#softmax-function",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#softmax-function",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "softmax function",
    "text": "softmax function\n\n로지스틱 모형 (1): 활성화함수로 sigmoid 선택\n- 기본버전은 아래와 같다\n\\[y_i \\approx \\text{sigmoid}(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})= \\frac{\\exp(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})}{1+\\exp(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})}\\]\n- 벡터버전은 아래와 같다.\n\\[{\\boldsymbol y} \\approx \\text{sigmoid}({\\bf X}{\\bf W} + b) = \\frac{\\exp({\\bf XW} +b)}{1+\\exp({\\bf XW} +b)}\\]\n- 벡터버전에 익숙해지도록 하자. 벡터버전에 사용된 차원 및 연산을 정리하면 아래와 같다.\n\n\\({\\bf X}\\): (n,784) matrix\n\\({\\boldsymbol y}\\): (n,1) matrix\n\\({\\bf W}\\): (784,1) matrix\n\\(b\\): (1,1) matrix\n+, exp 는 브로드캐스팅\n\n\n\n로지스틱 모형 (2): 활성화함수로 softmax 선택\n- \\(y_i=0 \\text{ or } 1\\) 대신에 \\(\\boldsymbol{y}_i=[y_{i1},y_{i2}]= [1,0] \\text { or } [0,1]\\)와 같이 코딩하면 어떠할까? (즉 원핫인코딩을 한다면?)\n- 활성화 함수를 취하기 전의 버전은 아래와 같이 볼 수 있다.\n\\[[{\\boldsymbol y}_1 ~ {\\boldsymbol y}_2] \\propto  [ {\\bf X}{\\bf W}_1  ~ {\\bf X}{\\bf W}_2] + [b_1 ~ b_2]= {\\bf X} [{\\bf W}_1 {\\bf W}_2] + [b_1 ~ b_2]= {\\bf X}{\\bf W} + {\\boldsymbol b}\\]\n여기에서 매트릭스 및 연산의 차원을 정리하면 아래와 같다.\n\n\\({\\bf X}\\): (n,784) matrix\n\\({\\boldsymbol y}_1,{\\boldsymbol y}_2\\): (n,1) matrix\n\\({\\boldsymbol y}:=[{\\boldsymbol y}_1~ {\\boldsymbol y}_2]\\): (n,2) matrix\n\\({\\bf W}_1\\), \\({\\bf W}_2\\): (784,1) matrix\n\\({\\bf W}:=[{\\bf W}_1~ {\\bf W}_2]\\): (784,2) matrix\n\\(b_1,b_2\\): (1,1) matrix\n$:= [b_1 ~b_2] $: (1,2) matrix\n+ 는 브로드캐스팅\n\n- 즉 로지스틱 모형 (1)의 형태를 겹쳐놓은 형태로 해석할 수 있음. 따라서 \\({\\bf X} {\\bf W}_1 + b_1\\)와 \\({\\bf X} {\\bf W}_2 + b_2\\)의 row값이 클수록 \\({\\boldsymbol y}_1\\)와 \\({\\boldsymbol y}_2\\)의 row값이 1이어야 함\n\n\\({\\boldsymbol y}_1 \\propto {\\bf X} {\\bf W}_1 + b_1\\) \\(\\to\\) \\({\\bf X} {\\bf W}_1 + b_1\\)의 row값이 클수록 \\(\\boldsymbol{y}_1\\)의 row 값이 1이라면 모형계수를 잘 추정한것\n\\({\\boldsymbol y}_2 \\propto {\\bf X} {\\bf W}_2 + b_2\\) \\(\\to\\) \\({\\bf X} {\\bf W}_2 + b_2\\)의 row값이 클수록 \\(\\boldsymbol{y}_2\\)의 row 값을 1이라면 모형계수를 잘 추정한것\n\n- (문제) \\({\\bf X}{\\bf W}_1 +b_1\\)의 값이 500, \\({\\bf X}{\\bf W}_2 +b_2\\)의 값이 200 인 row가 있다고 하자. 대응하는 \\(\\boldsymbol{y}_1, \\boldsymbol{y}_2\\)의 row값은 얼마로 적합되어야 하는가?\n\n\\([0,0]\\)\n\\([0,1]\\)\n\\([1,0]\\) &lt;– 이게 답이다!\n\\([1,1]\\)\n\n\nnote: 둘다 0 혹은 둘다 1로 적합할수는 없으니까 (1), (4)는 제외한다. \\({\\bf X}{\\bf W}_1 +b_1\\)의 값이 \\({\\bf X}{\\bf W}_2 +b_2\\)의 값보다 크므로 (3)번이 합리적임\n\n- 목표: 위와 같은 문제의 답을 유도해주는 활성화함수를 설계하자. 즉 합리적인 \\(\\hat{\\boldsymbol{y}}_1,\\hat{\\boldsymbol{y}}_2\\)를 구해주는 활성화 함수를 설계해보자. 이를 위해서는 아래의 사항들이 충족되어야 한다.\n\n\\(\\hat{\\boldsymbol{y}}_1\\), \\(\\hat{\\boldsymbol{y}}_2\\)의 각 원소는 0보다 크고 1보다 작아야 한다. (확률을 의미해야 하니까)\n\\(\\hat{\\boldsymbol{y}}_1+\\hat{\\boldsymbol{y}}_2={\\bf 1}\\) 이어야 한다. (확률의 총합은 1이니까!)\n\\(\\hat{\\boldsymbol{y}}_1\\)와 \\(\\hat{\\boldsymbol{y}}_2\\)를 각각 따로해석하면 로지스틱처럼 되면 좋겠다.\n\n- 아래와 같은 활성화 함수를 도입하면 어떨까?\n\\[\\hat{\\boldsymbol{y}}=[\\hat{\\boldsymbol y}_1 ~ \\hat{\\boldsymbol y}_2] =  \\big[ \\frac{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  ~~ \\frac{\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  \\big]\\]\n- (1),(2)는 만족하는 듯 하다. (3)은 바로 이해되지는 않는다\n\n\\(\\hat{\\boldsymbol{y}}_1\\), \\(\\hat{\\boldsymbol{y}}_2\\)의 각 원소는 0보다 크고 1보다 작아야 한다. –&gt; OK!\n\\(\\hat{\\boldsymbol{y}}_1+\\hat{\\boldsymbol{y}}_2={\\bf 1}\\) 이어야 한다. –&gt; OK!\n\\(\\hat{\\boldsymbol{y}}_1\\)와 \\(\\hat{\\boldsymbol{y}}_2\\)를 각각 따로해석하면 로지스틱처럼 되면 좋겠다. –&gt; ???\n\n- 그런데 조금 따져보면 (3)도 만족된다는 것을 알 수 있다. (sigmoid, softmax Section 참고)\n- 위와 같은 함수를 softmax라고 하자. 즉 아래와 같이 정의하자.\n\\[\n\\hat{\\boldsymbol y} = \\text{softmax}({\\bf X}\\hat{\\bf W} + {\\boldsymbol b})\n= \\big[ \\frac{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  ~~ \\frac{\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  \\big]\n\\]\n\n\nsigmoid, softmax\n\nsoftmax는 sigmoid의 확장형\n- 아래의 수식을 관찰하자. \\[\\frac{\\exp(\\beta_0+\\beta_1 x_i)}{1+\\exp(\\beta_0+\\beta_1x_i)}=\\frac{\\exp(\\beta_0+\\beta_1 x_i)}{e^0+\\exp(\\beta_0+\\beta_1x_i)}\\]\n- 1을 \\(e^0\\)로 해석하면 모형2의 해석을 아래와 같이 모형1의 해석으로 적용할수 있다. - 모형2: \\({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1\\) 와 \\({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2\\) 의 크기를 비교하고 확률 결정 - 모형1: \\({\\bf X}\\hat{\\bf W} +\\hat{b}\\) 와 \\(0\\)의 크기를 비교하고 확률 결정 = \\({\\bf X}\\hat{\\bf W} +\\hat{b}\\)의 row값이 양수이면 1로 예측하고 음수이면 0으로 예측\n- 이항분포를 차원이 2인 다항분포로 해석가능한 것처럼 sigmoid는 차원이 2인 softmax로 해석가능하다. 즉 다항분포가 이항분포의 확장형으로 해석가능한 것처럼 softmax도 sigmoid의 확장형으로 해석가능하다.\n\n\n클래스의 수가 2인 경우 softmax vs sigmoid\n- 언뜻 생각하면 클래스가 2인 경우에도 sigmoid 대신 softmax로 활성화함수를 이용해도 될 듯 하다. 즉 \\(y=0 \\text{ or } 1\\)와 같이 정리하지 않고 \\(y=[0,1] \\text{ or } [1,0]\\) 와 같이 정리해도 무방할 듯 하다.\n- 하지만 sigmoid가 좀 더 좋은 선택이다. 즉 \\(y= 0 \\text{ or } 1\\)로 데이터를 정리하는 것이 더 좋은 선택이다. 왜냐하면 sigmoid는 softmax와 비교하여 파라메터의 수가 적지만 표현력은 동등하기 때문이다.\n- 표현력이 동등한 이유? 아래 수식을 관찰하자.\n\\[\\big(\\frac{e^{300}}{e^{300}+e^{500}},\\frac{e^{500}}{e^{300}+e^{500}}\\big) =\\big( \\frac{e^{0}}{e^{0}+e^{200}}, \\frac{e^{200}}{e^{0}+e^{200}}\\big)\\]\n\n\\(\\big(\\frac{e^{300}}{e^{300}+e^{500}},\\frac{e^{500}}{e^{300}+e^{500}}\\big)\\)를 표현하기 위해서 300, 500 이라는 2개의 숫자가 필요한것이 아니고 따지고보면 200이라는 하나의 숫자만 필요하다.\n\\((\\hat{\\boldsymbol{y}}_1,\\hat{\\boldsymbol{y}}_2)\\)의 표현에서도 \\({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1\\) 와 \\({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2\\) 라는 숫자 각각이 필요한 것이 아니고 \\(({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1)-({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2)\\)의 값만 알면 된다.\n\n- 클래스의 수가 2개일 경우는 softmax가 sigmoid에 비하여 장점이 없다. 하지만 softmax는 클래스의 수가 3개 이상일 경우로 쉽게 확장할 수 있다는 점에서 매력적인 활성화 함수이다.\n\n\n\n분류할 클래스가 3개 이상일 경우 신경망 모형의 설계\n- y의 모양: [0 1 0 0 0 0 0 0 0 0]\n- 활성화함수의 선택: softmax\n- 손실함수의 선택: cross entropy\n\n\nFashion_MNIST 여러클래스의 분류 (softmax의 실습)\n- 데이터정리\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= x_train.reshape(-1,784)\ny= tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,784)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 시도1: 간단한 신경망\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -&gt; \"node1\"\n    \"x2\" -&gt; \"node1\"\n    \"..\" -&gt; \"node1\"\n\n    \"x784\" -&gt; \"node1\"\n    \"x1\" -&gt; \"node2\"\n    \"x2\" -&gt; \"node2\"\n    \"..\" -&gt; \"node2\"\n    \"x784\" -&gt; \"node2\"\n\n    \"x1\" -&gt; \"...\"\n    \"x2\" -&gt; \"...\"\n    \"..\" -&gt; \"...\"\n    \"x784\" -&gt; \"...\"\n\n    \"x1\" -&gt; \"node30\"\n    \"x2\" -&gt; \"node30\"\n    \"..\" -&gt; \"node30\"\n    \"x784\" -&gt; \"node30\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -&gt; \"y10\"\n    \"node2\" -&gt; \"y10\"\n    \"...\" -&gt; \"y10\"\n    \"node30\" -&gt; \"y10\"\n\n    \"node1\" -&gt; \"y1\"\n    \"node2\" -&gt; \"y1\"\n    \"...\" -&gt; \"y1\"\n    \"node30\" -&gt; \"y1\"\n\n    \"node1\" -&gt; \".\"\n    \"node2\" -&gt; \".\"\n    \"...\" -&gt; \".\"\n    \"node30\" -&gt; \".\"\n\n    label = \"Layer 2: softmax\"\n}\n''')\n\n\n\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 1s 485us/step - loss: 2.2998 - accuracy: 0.4233\nEpoch 2/5\n1875/1875 [==============================] - 1s 475us/step - loss: 1.2198 - accuracy: 0.5444\nEpoch 3/5\n1875/1875 [==============================] - 1s 498us/step - loss: 1.0499 - accuracy: 0.5938\nEpoch 4/5\n1875/1875 [==============================] - 1s 484us/step - loss: 0.9243 - accuracy: 0.6309\nEpoch 5/5\n1875/1875 [==============================] - 1s 484us/step - loss: 0.8560 - accuracy: 0.6506\n\n\n&lt;keras.callbacks.History at 0x7f904d4618b0&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 374us/step - loss: 0.8689 - accuracy: 0.6363\n\n\n[0.8688836693763733, 0.6363000273704529]\n\n\n\nnet.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 30)                23550     \n                                                                 \n dense_1 (Dense)             (None, 10)                310       \n                                                                 \n=================================================================\nTotal params: 23,860\nTrainable params: 23,860\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 시도2: 더 깊은 신경망\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -&gt; \"node1\"\n    \"x2\" -&gt; \"node1\"\n    \"..\" -&gt; \"node1\"\n\n    \"x784\" -&gt; \"node1\"\n    \"x1\" -&gt; \"node2\"\n    \"x2\" -&gt; \"node2\"\n    \"..\" -&gt; \"node2\"\n    \"x784\" -&gt; \"node2\"\n\n    \"x1\" -&gt; \"...\"\n    \"x2\" -&gt; \"...\"\n    \"..\" -&gt; \"...\"\n    \"x784\" -&gt; \"...\"\n\n    \"x1\" -&gt; \"node500\"\n    \"x2\" -&gt; \"node500\"\n    \"..\" -&gt; \"node500\"\n    \"x784\" -&gt; \"node500\"\n\n\n    label = \"Layer 1: relu\"\n}\n\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -&gt; \"node1(2)\"\n    \"node2\" -&gt; \"node1(2)\"\n    \"...\" -&gt; \"node1(2)\"\n    \"node500\" -&gt; \"node1(2)\"\n\n    \"node1\" -&gt; \"node2(2)\"\n    \"node2\" -&gt; \"node2(2)\"\n    \"...\" -&gt; \"node2(2)\"\n    \"node500\" -&gt; \"node2(2)\"\n\n    \"node1\" -&gt; \"....\"\n    \"node2\" -&gt; \"....\"\n    \"...\" -&gt; \"....\"\n    \"node500\" -&gt; \"....\"\n\n    \"node1\" -&gt; \"node500(2)\"\n    \"node2\" -&gt; \"node500(2)\"\n    \"...\" -&gt; \"node500(2)\"\n    \"node500\" -&gt; \"node500(2)\"\n\n\n    label = \"Layer 2: relu\"\n}\n\nsubgraph cluster_4{\n    style=filled;\n    color=lightgrey;\n\n    \"node1(2)\" -&gt; \"y10\"\n    \"node2(2)\" -&gt; \"y10\"\n    \"....\" -&gt; \"y10\"\n    \"node500(2)\" -&gt; \"y10\"\n\n    \"node1(2)\" -&gt; \"y1\"\n    \"node2(2)\" -&gt; \"y1\"\n    \"....\" -&gt; \"y1\"\n    \"node500(2)\" -&gt; \"y1\"\n\n    \"node1(2)\" -&gt; \".\"\n    \"node2(2)\" -&gt; \".\"\n    \"....\" -&gt; \".\"\n    \"node500(2)\" -&gt; \".\"\n\n    label = \"Layer 3: softmax\"\n}\n''')\n\n\n\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.3745 - accuracy: 0.7530\nEpoch 2/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.6109 - accuracy: 0.7965\nEpoch 3/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5170 - accuracy: 0.8190\nEpoch 4/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4650 - accuracy: 0.8356\nEpoch 5/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4269 - accuracy: 0.8481\n\n\n&lt;keras.callbacks.History at 0x7f8fa06980d0&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 714us/step - loss: 0.4323 - accuracy: 0.8456\n\n\n[0.4322887063026428, 0.8456000089645386]\n\n\n\nnet.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 500)               392500    \n                                                                 \n dense_3 (Dense)             (None, 500)               250500    \n                                                                 \n dense_4 (Dense)             (None, 10)                5010      \n                                                                 \n=================================================================\nTotal params: 648,010\nTrainable params: 648,010\nNon-trainable params: 0\n_________________________________________________________________"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#평가지표",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#평가지표",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "평가지표",
    "text": "평가지표\n\n다양한 평가지표들\n- 의문: 왜 다양한 평가지표가 필요한가? (accuray면 끝나는거 아닌가? 더 이상 뭐가 필요해?)\n- 여러가지 평가지표들: https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values - 이걸 다 암기하는건 불가능함. - 몇 개만 뽑아서 암기하고 왜 쓰는지만 생각해보고 넘어가자!\n\n\nconfusion matrix의 이해\n- 표1\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP\nFN\n\n\n안나감(실제)\nFP\nTN\n\n\n\n- 표2 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\n$(y,)= $ (O,O)\n$(y,)= $(O,X)\n\n\n안나감(실제)\n$(y,)= $(X,O)\n$(y,)= $(X,X)\n\n\n\n- 표3 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\#O/X\\)\n\n\n안나감(실제)\nFP, \\(\\#X/O\\)\nTN, \\(\\#X/X\\)\n\n\n\n\n암기법, (1) 두번째 글자를 그대로 쓴다 (2) 첫글자가 T이면 분류를 제대로한것, 첫글자가 F이면 분류를 잘못한것\n\n- 표4 (위키등에 있음)\n\n\n\n\n\n\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\# O/X\\)\nSensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\#O/O}{\\# O/O+ \\#O/X}\\)\n\n\n안나감(실제)\nFP, \\(\\# X/O\\)\nTN, \\(\\# X/X\\)\n\n\n\n\nPrecision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\)\n\nAccuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+\\# X/X}{total}\\)\n\n\n\n\n\n상황극\n- 최규빈은 입사하여 “퇴사자 예측시스템”의 개발에 들어갔다.\n- 자료의 특성상 대부분의 사람이 퇴사하지 않고 회사에 잘 다닌다. 즉 1000명이 있으면 10명정도 퇴사한다.\n\n\nAccuracy\n- 정의: Accuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+ \\#X/X}{total}\\) - 한국말로는 정확도, 정분류율이라고 한다. - 한국말이 헷갈리므로 그냥 영어를 외우는게 좋다. (어차피 Keras에서 옵션도 영어로 넣음)\n- (상확극 시점1) 왜 애큐러시는 불충분한가? - 회사: 퇴사자예측프로그램 개발해 - 최규빈: 귀찮은데 다 안나간다고 하자! -&gt; 99퍼의 accuracy\n\n모델에 사용한 파라메터 = 0. 그런데 애큐러시 = 99! 이거 엄청 좋은 모형이다?\n\n\n\nSensitivity(민감도), Recall(재현율), True Positive Rate(TPR)\n- 정의: Sensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# O/X}\\) - 분모: 실제 O인 관측치 수 - 분자: 실제 O를 O라고 예측한 관측치 수 - 뜻: 실제 O를 O라고 예측한 비율\n- (상황극 시점2) recall을 봐야하는 이유 - 인사팀: 실제 퇴사자를 퇴사자로 예측해야 의미가 있음! 우리는 퇴사할것 같은 10명을 찍어달란 의미였어요! (그래야 면담을 하든 할거아냐!) - 최규빈: 가볍고(=파라메터 적고) 잘 맞추는 모형 만들어 달라면서요?\n\n인사팀: (고민중..) 사실 생각해보니까 이 경우는 애큐러시는 의미가 없네. 실제 나간 사람 중 최규빈이 나간다고 한 사람이 몇인지 카운트 하는게 더 의미가 있겠다. 우리는 앞으로 리컬(혹은 민감도)를 보겠다!\n\n\n예시1: 실제로 퇴사한 10명중 최규빈이 나간다고 찍은 사람이 5명이면 리컬이 50%\n\n\n예시2: 최규빈이 아무도 나가지 않는다고 예측해버린다? 실제 10명중에서 최규빈이 나간다고 적중시킨사람은 0명이므로 이 경우 리컬은 0%\n\n\n결론: 우리가 필요한건 recall이니까 앞으로 recall을 가져와! accuracy는 큰 의미없어. (그래도 명색이 모델인데 accuracy가 90은 되면 좋겠다)\n\n\n\nPrecision\n- 정의: Precision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\) - 분모: O라고 예측한 관측치 - 분자: O라고 예측한 관측치중 진짜 O인 관측치 - 뜻: O라고 예측한 관측치중 진짜 O인 비율\n- (상황극 시점3) recall 만으로 불충분한 이유\n\n최규빈: 에휴.. 귀찮은데 그냥 좀만 수틀리면 다 나갈것 같다고 해야겠다. -&gt; 한 100명 나간다고 했음 -&gt; 실제로 최규빈이 찍은 100명중에 10명이 다 나감!\n\n\n이 경우 애큐러시는 91%, 리컬은 100% (퇴사자 10명을 일단은 다 맞췄으므로).\n\n\n인사팀: (화가 많이 남) 멀쩡한 사람까지 다 퇴사할 것 같다고 하면 어떡해요? 최규빈 연구원이 나간다고 한 100명중에 실제로 10명만 나갔어요.\n인사팀: 마치 총으로 과녁중앙에 맞춰 달라고 했더니 기관총을 가져와서 한번 긁은것이랑 뭐가 달라요? 맞추는게 문제가 아니고 precision이 너무 낮아요.\n최규빈: accuracy 90% 이상, recall은 높을수록 좋다는게 주문 아니었나요?\n인사팀: (고민중..) 앞으로는 recall과 함께 precision도 같이 제출하세요. precision은 당신이 나간다고 한 사람중에 실제 나간사람의 비율을 의미해요. 이 경우는 \\(\\frac{10}{100}\\)이니까 precision이 10%입니다. (속마음: recall 올리겠다고 무작정 너무 많이 예측하지 말란 말이야!)\n\n\n\nF1 score\n- 정의: recall과 precision의 조화평균\n- (상황극 시점4) recall, precision을 모두 고려\n\n최규빈: recall/precision을 같이 내는건 좋은데요, 둘은 trade off의 관계에 있습니다. 물론 둘다 올리는 모형이 있다면 좋지만 그게 쉽지는 않아요. 보통은 precision을 올리려면 recall이 희생되는 면이 있고요, recall을 올리려고 하면 precision이 다소 떨어집니다.\n최규빈: 평가기준이 애매하다는 의미입니다. 모형1,2가 있는데 모형1은 모형2보다 precision이 약간 좋고 대신 recall이 떨어진다면 모형1이 좋은것입니까? 아니면 모형2가 좋은것입니까?\n인사팀: 그렇다면 둘을 평균내서 F1score를 계산해서 제출해주세요.\n\n\n\nSpecificity(특이도), False Positive Rate(FPR)\n- 정의:\n\nSpecificity(특이도)=\\(\\frac{TN}{FP+TN}\\)=\\(\\frac{\\# X/X}{\\# X/O+\\# X/X}\\)\nFalse Positive Rate (FPR) = 1-Specificity(특이도) = \\(\\frac{FP}{FP+TN}\\)=\\(\\frac{\\# X/O}{\\# X/O+\\# X/X}\\)\n\n- 의미: FPR = 오해해서 미안해, recall(=TPR)을 올리려고 보니 어쩔 수 없었어 ㅠㅠ - specificity는 안나간 사람을 안나갔다고 찾아낸 비율인데 별로 안중요하다. - FPR은 recall을 올리기 위해서 “실제로는 회사 잘 다니고 있는 사람 중 최규빈이 나갈것 같다고 찍은 사람들” 의 비율이다.\n\n즉 생사람잡은 비율.. 오해해서 미안한 사람의 비율..\n\n\n\nROC curve\n- 정의: \\(x\\)축=FPR, \\(y\\)축=TPR 을 그린 커브\n- 의미: - 결국 “오해해서 미안해 vs recall”을 그린 곡선이 ROC커브이다. - 생각해보면 오해하는 사람이 많을수록 당연히 recall은 올라간다. 따라서 우상향하는 곡선이다. - 오해한 사람이 매우 적은데 recall이 우수하면 매우 좋은 모형이다. 그래서 초반부터 ROC값이 급격하게 올라가면 좋은 모형이다.\n\n\nFashion MNIST 다양한 평가지표활용\n- data\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= x_train.reshape(-1,784)\ny= tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,784)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 다양한 평가지표를 넣는 방법 (1)\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy','Recall'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.1689 - accuracy: 0.7489 - recall: 0.7085\nEpoch 2/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6779 - accuracy: 0.7775 - recall: 0.7104\nEpoch 3/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6277 - accuracy: 0.7802 - recall: 0.7084\nEpoch 4/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5648 - accuracy: 0.7937 - recall: 0.7370\nEpoch 5/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5308 - accuracy: 0.8032 - recall: 0.7394\n\n\n&lt;keras.callbacks.History at 0x7f8fa054c2b0&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 592us/step - loss: 0.6554 - accuracy: 0.7603 - recall: 0.7150\n\n\n[0.6554250717163086, 0.7602999806404114, 0.7149999737739563]\n\n\n- 다양한 평가지표를 넣는 방법 (2)\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 1.9477 - categorical_accuracy: 0.7461 - recall: 0.7014\nEpoch 2/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.6755 - categorical_accuracy: 0.7792 - recall: 0.7217\nEpoch 3/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5605 - categorical_accuracy: 0.8143 - recall: 0.7659\nEpoch 4/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4762 - categorical_accuracy: 0.8356 - recall: 0.7929\nEpoch 5/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4460 - categorical_accuracy: 0.8422 - recall: 0.8033\n\n\n&lt;keras.callbacks.History at 0x7f904e1603d0&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 590us/step - loss: 0.4699 - categorical_accuracy: 0.8353 - recall: 0.7885\n\n\n[0.46987518668174744, 0.8353000283241272, 0.7885000109672546]"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#flatten-layer",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#flatten-layer",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "flatten layer",
    "text": "flatten layer\n- 이미지 데이터를 분류하기 좋은 형태로 자료를 재정리하자.\n\nx_train.shape\n\n(60000, 28, 28)\n\n\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nX.shape,XX.shape,y.shape,yy.shape\n\n(TensorShape([60000, 28, 28, 1]),\n TensorShape([10000, 28, 28, 1]),\n (60000, 10),\n (10000, 10))\n\n\n- 일반적인 이미지 분석 모형을 적용하기 용이한 데이터 형태로 정리했다. -&gt; 그런데 모형에 넣고 돌릴려면 다시 차원을 펼쳐야 하지 않을까?\n- 안펼치고 하고싶다.\n\nflttn = tf.keras.layers.Flatten()\n\n\nset(dir(flttn)) & {'__call__'}\n\n{'__call__'}\n\n\n\nX.shape, flttn(X).shape, X.reshape(-1,784).shape\n\n(TensorShape([60000, 28, 28, 1]),\n TensorShape([60000, 784]),\n TensorShape([60000, 784]))\n\n\n\n같은 기능\n다른건? \\(\\to\\) layers (layer로 넣을 수 있는 무기들 중 Flatten이 하나 추가됨!) \\(\\to\\) 네트워크에 넣을 수 있겠다!\n\n- flttn\n\\(X \\to \\text{Dense}(500, relu) \\to \\text{Dense}(500, relu) \\to \\text{Dense}(10,\\text{softmax}):=y\\)\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten()) ## dimension을 맞춰주는 것을 네트워크 안으로 넣은 것.\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.5320 - categorical_accuracy: 0.7491 - recall_1: 0.7126\nEpoch 2/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.6705 - categorical_accuracy: 0.7844 - recall_1: 0.7238\nEpoch 3/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5829 - categorical_accuracy: 0.8096 - recall_1: 0.7708\nEpoch 4/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4954 - categorical_accuracy: 0.8318 - recall_1: 0.7970\nEpoch 5/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4676 - categorical_accuracy: 0.8383 - recall_1: 0.8020\n\n\n&lt;keras.callbacks.History at 0x7f904e050940&gt;\n\n\n\nnet.layers\n\n[&lt;keras.layers.reshaping.flatten.Flatten at 0x7f904df61430&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f904df612e0&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f904deed700&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f904deed970&gt;]\n\n\n\nDense layer는 activation이 합쳐진 layer.\n\n\nprint(X.shape)\nprint(net.layers[0](X).shape)\nprint(net.layers[1](net.layers[0](X)).shape)\nprint(net.layers[2](net.layers[1](net.layers[0](X))).shape)\nprint(net.layers[3](net.layers[2](net.layers[1](net.layers[0](X)))).shape)\n\n(60000, 28, 28, 1)\n(60000, 784)\n(60000, 500)\n(60000, 500)\n(60000, 10)\n\n\n\nnet.layers[1]\n\n&lt;keras.layers.core.dense.Dense at 0x7f904df612e0&gt;\n\n\n- 좀 더 복잡한 네트워크 -&gt; 하지만 한계가 보인다 -&gt; 좀 더 나은 아키텍처는 없을까\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics='accuracy')\nnet.fit(X,y,epochs=10)\n\nEpoch 1/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 1.1550 - accuracy: 0.7911\nEpoch 2/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.4523 - accuracy: 0.8377\nEpoch 3/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.4129 - accuracy: 0.8532\nEpoch 4/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3926 - accuracy: 0.8610\nEpoch 5/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.3748 - accuracy: 0.8680\nEpoch 6/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3645 - accuracy: 0.8718\nEpoch 7/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3461 - accuracy: 0.8773\nEpoch 8/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3381 - accuracy: 0.8788\nEpoch 9/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3372 - accuracy: 0.8810\nEpoch 10/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.3240 - accuracy: 0.8844\n\n\n&lt;keras.callbacks.History at 0x7f904e1d9d00&gt;\n\n\n\nnet.summary()\n\nModel: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_3 (Flatten)         (None, 784)               0         \n                                                                 \n dense_20 (Dense)            (None, 500)               392500    \n                                                                 \n dense_21 (Dense)            (None, 500)               250500    \n                                                                 \n dense_22 (Dense)            (None, 500)               250500    \n                                                                 \n dense_23 (Dense)            (None, 500)               250500    \n                                                                 \n dense_24 (Dense)            (None, 10)                5010      \n                                                                 \n=================================================================\nTotal params: 1,149,010\nTrainable params: 1,149,010\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 752us/step - loss: 0.4136 - accuracy: 0.8566\n\n\n[0.4136269986629486, 0.8565999865531921]\n\n\n- layer중에 우리는 끽해야 Dense정도 쓰고있었음. \\(\\to\\) flatten과 같은 다른 layer도 많음. \\(\\to\\) 이런것도 써보자"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_03_28_(4주차)_3월28일.html",
    "href": "posts/3_STBDA2022/2022_03_28_(4주차)_3월28일.html",
    "title": "[STBDA] 4wk. 미분 / 경사하강법",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-xGxAT-3Sq_jpD-eBxpsT8L"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_03_28_(4주차)_3월28일.html#경사하강법",
    "href": "posts/3_STBDA2022/2022_03_28_(4주차)_3월28일.html#경사하강법",
    "title": "[STBDA] 4wk. 미분 / 경사하강법",
    "section": "경사하강법",
    "text": "경사하강법\n\n최적화문제\n- \\(loss=(\\frac{1}{2}\\beta-1)^2\\)를 최소하는 \\(\\beta\\)를 컴퓨터를 활용하여 구하는 문제를 생각해보자. - 답은 \\(\\beta=2\\)임을 알고 있다.\n\n\n방법1: grid search\n\n알고리즘\n\nbeta = [-10.00,-9.99,…,10.00] 와 같은 리스트를 만든다.\n(1)의 리스트의 각원소에 해당하는 loss를 구한다.\n(2)에서 구한 loss를 제일 작게 만드는 beta를 찾는다.\n\n\n\n구현코드\n\nbeta = np.linspace(-10,10,100) \nloss = (beta/2 -1)**2 \n\n\n# argmin()\ntnp.argmin([1,2,-3,3,4]) # 가장작은 인덱스 리턴\n\n&lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt;\n\n\n\ntnp.argmin([1,2,3,-3,4])\n\n&lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt;\n\n\n\ntnp.argmin(loss)\n\n&lt;tf.Tensor: shape=(), dtype=int64, numpy=59&gt;\n\n\n\nbeta[59]\n\n1.9191919191919187\n\n\n\n\n그리드서치의 문제점\n- 비판1: [-10,10]이외에 해가 존재하면? - 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 - 하지만 임의의 고정된 \\(x,y\\)에 대하여 \\(loss(\\beta)=(x\\beta-y)^2\\) 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 - 해결책: 더 넓게 많은 범위를 탐색하자?\n- 비판2: 효율적이지 않음 - 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 - \\(\\to\\) 생각해보니까 \\(\\beta=2\\)인 순간 \\(loss=(\\frac{1}{2}\\beta-1)^2=0\\)이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) - \\(\\to\\) 따라서 \\(\\beta=2\\) 이후로는 탐색할 필요가 없다\n\n\n\n방법2: gradient descent\n\\(loss=(\\frac{1}{2}\\beta-1)^2\\)\n\n알고리즘!\n\nbeta = -5 로 셋팅한다.\n\n\n(-5/2-1)**2 # 아무거나 던지면 됨..\n\n12.25\n\n\n\nbeta=-5 근처에서 조금씩 이동하여 loss를 조사해본다.\n\n\n(-4.99/2-1)**2 ## 오른쪽으로 0.01 이동하고 loss조사\n\n12.215025\n\n\n\n(-5.01/2-1)**2 ## 왼쪽으로 0.01 이동하고 loss조사\n\n12.285025\n\n\n\n(2)의 결과를 잘 해석하고 더 유리한 쪽으로 이동\n위의 과정을 반복하고 왼쪽, 오른쪽 어느쪽으로 움직여도 이득이 없다면 멈춘다.\n\n\n\n알고리즘 분석\n- (2)-(3)의 과정은 beta=-5 에서 미분계수를 구하고 미분계수가 양수이면 왼쪽으로 움직이고 음수이면 오른쪽으로 움직인다고 해석가능. 아래그림을 보면 더 잘 이해가 된다.\n\nplt.plot(beta,loss)\n\n\n\n\n\n\n왼쪽/오른쪽중에 어디로 갈지 어떻게 판단하는 과정을 수식화?\n- 아래와 같이 해석가능\n\n오른쪽으로 0.01 간다 = beta_old에 0.01을 더함. (if, 미분계수가 음수)\n왼쪽으로 0.01 간다. = beta_old에 0.01을 뺀다. (if, 미분계수가 양수)\n\n- 그렇다면 $_{new} =\n\\[\\begin{cases}\n\\beta_{old} + 0.01, & loss'(\\beta_{old})&lt; 0  \\\\\n\\beta_{old} - 0.01, & loss'(\\beta_{old})&gt; 0\n\\end{cases}\\]\n$\n\n\n혹시 알고리즘을 좀 개선할수 있을까?\n- 항상 0.01씩 움직여야 하는가?\n\nplt.plot(beta,loss)\n\n\n\n\n- \\(\\beta=-10\\) 일 경우의 접선의 기울기? \\(\\beta=-4\\) 일때 접선의 기울기?\n\n\\(\\beta=-10\\) =&gt; 기울기는 -6\n\\(\\beta=-4\\) =&gt; 기울기는 -3\n\n\n(-10/2-1), (-4/2-1)\n\n(-6.0, -3.0)\n\n\n- 실제로 6,3씩 이동할순 없으니 적당한 \\(\\alpha\\) (예를들면 \\(\\alpha=0.01\\)) 를 잡아서 곱한만큼 이동하자.\n- 수식화하면\n\n\\(\\beta_{new} = \\beta_{old} - \\alpha~ loss'(\\beta_{old})\\)\n\\(\\beta_{new} = \\beta_{old} - \\alpha~ \\left[\\frac{\\partial}{\\partial \\beta }loss(\\beta)\\right]_{\\beta=\\beta_{old}}\\)\n\n- \\(\\alpha\\)의 의미 - \\(\\alpha\\)가 크면 크게크게 움직이고 작으면 작게작게 움직인다. - \\(\\alpha&gt;0\\) 이어야 한다.\n\n\n구현코드\n- iter 1\n\\(\\beta=-10\\)이라고 하자.\n\nbeta = tf.Variable(-10.0) \n\n\nwith tf.GradientTape(persistent=True) as tape: \n    loss = (beta/2-1)**2 \n\n\ntape.gradient(loss,beta)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt;\n\n\n\\(\\beta = -10\\) 에서 0.01만큼 움직이고 싶음\n\nalpha= 0.01/6\n\n\nalpha * tape.gradient(loss,beta)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.01&gt;\n\n\n\nbeta.assign_sub(alpha * tape.gradient(loss,beta))\n\n&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n\n한번 더 움직이자.\n\n- iter2\n\nwith tf.GradientTape(persistent=True) as tape: \n    loss = (beta/2-1)**2 \n\n\nbeta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.980008&gt;\n\n\n\n더 나은 \\(\\beta\\)가 되었음! 이렇게 가다보면 \\(2\\)까지 잘 갈 수 있을 것 같다!\n\n- for 문을 이용하자.\n(강의용)\n\nbeta = tf.Variable(-10.0) \n\n\nfor k in range(10000): \n    with tf.GradientTape(persistent=True) as tape: \n        loss = (beta/2-1)**2 \n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251&gt;\n\n\n\n잘 찾았네!\n\n(시도1) beta 다시 초기화 해서 iter 100번 돌려보자.\n\nbeta = tf.Variable(-10.0) \n\n\nfor k in range(100): \n    with tf.GradientTape(persistent=True) as tape: \n        loss = (beta/2-1)**2 \n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-8.157076&gt;\n\n\n(시도2) beta 다시 초기화 해서 iter 1000번 돌려보자.\n\nbeta = tf.Variable(-10.0)  # 초기화\n\n\nfor k in range(1000): \n    with tf.GradientTape(persistent=True) as tape: \n        loss = (beta/2-1)**2 \n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-3.2133684&gt;\n\n\n\n10000번 돌렸을 때는 답과 근사했는데 iter 수를 줄였더니 잘 못맞추네… 그럼 이건 알고리즘상 문제가 아니라 보폭이 너무 작아서..느린 것 뿐..\n\n- 너무 느린 것 같다? \\(\\to\\) \\(\\alpha\\)를 키워보자!\n\n\n학습률\n- 목표: \\(\\alpha\\)에 따라서 수렴과정이 어떻게 달라지는 시각화해보자.\n\n[시각화 코드 예비학습]\n\nfig = plt.figure() # 도화지가 만들어지고 fig라는 이름을 붙인다. \n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nax = fig.add_subplot() # fig는 ax라는 물체를 만든다. \n\n\nid(fig.axes[0])\n\n140572441475776\n\n\n\nid(ax)\n\n140572441475776\n\n\n\npnts, = ax.plot([1,2,3],[4,5,6],'or')\npnts\n\n&lt;matplotlib.lines.Line2D at 0x7fd9926c52e0&gt;\n\n\n\npnts.get_xdata()\n\narray([1, 2, 3])\n\n\n\npnts.get_ydata()\n\narray([4, 5, 6])\n\n\n\nfig\n\n\n\n\n\npnts.set_ydata([5,5,5])\n\n\npnts.get_ydata() # 값이 수정됨!\n\n[5, 5, 5]\n\n\n\nfig\n\n\n\n\n- 응용\n\nplt.rcParams[\"animation.html\"]=\"jshtml\"\nfrom matplotlib import animation \n\n\ndef animate(i): \n    if i%2 == 0: \n        pnts.set_ydata([4,5,6]) # 4,5,6으로 셋팅\n    else: \n        pnts.set_ydata([5,5,5]) # 5,5,5로 셋팅\n\n\nani = animation.FuncAnimation(fig,animate,frames=10)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n예비학습 끝\n- beta_lst=[-10,-9,-8] 로 이동한다고 하자.\n\nbeta_lst = [-10,-9,-8]\nloss_lst = [(-10/2-1)**2,(-9/2-1)**2,(-8/2-1)**2] \n\n\nfig = plt.figure() \n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nax= fig.add_subplot()\n\n\n_beta = np.linspace(-15,19,100) \n\n\nax.plot(_beta,(_beta/2-1)**2)\n\n\nfig\n\n\n\n\n\npnts, = ax.plot(beta_lst[0],loss_lst[0],'ro')\nfig\n\n\n\n\n\ndef animate(i): \n    pnts.set_xdata(beta_lst[:(i+1)])\n    pnts.set_ydata(loss_lst[:(i+1)])\n\n\nani =animation.FuncAnimation(fig, animate, frames=3)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 최종아웃풋\n\nbeta = tf.Variable(-10.0) \nalpha = 0.01/6\n\n\nbeta_lst=[]\nloss_lst=[]\n\n\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nwith tf.GradientTape(persistent=True) as tape: \n    tape.watch(beta)  # beta에 대해 미분.\n    loss = (beta/2-1)**2\n\n\nbeta.assign_sub(tape.gradient(loss,beta)*alpha) \n\n&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nbeta_lst, loss_lst\n\n([-10.0, -9.99], [36.0, 35.94002362785341])\n\n\n\nbeta값과 그에 대응되는 loss값\n\n- for\n\nbeta = tf.Variable(-10.0) \nalpha = 0.01/6\nbeta_lst=[]\nloss_lst=[]\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\nfor k in range(100): \n    with tf.GradientTape(persistent=True) as tape: \n        tape.watch(beta) \n        loss = (beta/2-1)**2\n    beta.assign_sub(tape.gradient(loss,beta)*alpha)  # beta update\n    beta_lst.append(beta.numpy()) # update된 beta 추가\n    loss_lst.append((beta.numpy()/2-1)**2) # loss 계산\n\n\nfig = plt.figure() \nax = fig.add_subplot() \nax.plot(_beta,(_beta/2-1)**2) \npnts, = ax.plot(beta_lst[0],loss_lst[0],'or')\n\n\n\n\n\nani = animation.FuncAnimation(fig,animate,frames=100) \nani \n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n너무 느려… 왜 \\(\\alpha\\) 를 바꿔야할지 알듯하다..\n\n\n\n\n\n숙제\n\\(y=(x-1)^2\\)를 최소화 하는 \\(x\\)를 경사하강법을 이용하여 찾아라. 수렴과정을 animation으로 시각화하라.\n- x의 초기값은 -3으로 설정한다. - 적당한 \\(\\alpha\\)를 골라서 100번의 반복안에 수렴하도록 하라."
  },
  {
    "objectID": "posts/3_STBDA2022/2023-01-01.html",
    "href": "posts/3_STBDA2022/2023-01-01.html",
    "title": "Jupyter",
    "section": "",
    "text": "ref: STBDA 10주차 (2번째 동영상)\n- 파이썬 / 주피터 설치\nconda create -n test python=3.9\n\nconda env list\n\nconda activate test\n\n# conda라는 앱스토어에서 jupyterlab을 깐다. (conda-forge라는 커뮤니티에서 검증이 된 (추천목록) 패키지들을 깔겠다..)\n# 여러 솔루션 중 하나\nconda install -c conda-forge jupyterlab \n\njupyter lab\n\n주피터 설치 완료\n\n- 주피터에서 슬라이드 쇼\n\n## test 환경 활성화\nconda activte test\n\n## jupyter notebook slide show\nconda install -c conda-forge rise\njupyter notebook"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_11_(6주차)_4월11일.html",
    "href": "posts/3_STBDA2022/2022_04_11_(6주차)_4월11일.html",
    "title": "[STBDA] 6wk. 회귀모형 적합 with keras",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-zueMdNhXiDTIMD-Dz5sbBD\n\n\n\nimports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\n\n\\(x \\to \\hat{y}\\) 가 되는 과정을 그림으로 그리기\n- 단순회귀분석의 예시 - \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i, \\quad i=1,2,\\dots,n\\)\n(표현1)\n\n#collapse\ngv(''' \n    \"1\" -&gt; \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xₙ\" -&gt; \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xₙ*β̂₁,    bias=False\" -&gt; \"ŷₙ\"[label=\"identity\"]\n\n    \".\" -&gt; \"....................................\"[label=\"* β̂₀\"]\n    \"..\" -&gt; \"....................................\"[label=\"* β̂₁\"]\n    \"....................................\" -&gt; \"...\"[label=\" \"]\n\n    \"1 \" -&gt; \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₂\" -&gt; \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₂*β̂₁,    bias=False\" -&gt; \"ŷ₂\"[label=\"identity\"]\n    \n    \"1  \" -&gt; \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₁\" -&gt; \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₁*β̂₁,    bias=False\" -&gt; \"ŷ₁\"[label=\"identity\"]\n''')\n\n\n\n\n- 표현1의 소감? - 교수님이 고생해서 만든것 같음 - 그런데 그냥 다 똑같은 그림의 반복이라 사실 고생한 의미가 없음.\n(표현2)\n- 그냥 아래와 같이 그리고 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다”고 하면 될것 같다.\n\n#collapse\ngv(''' \n    \"1\" -&gt; \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xᵢ\" -&gt; \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xᵢ*β̂₁,    bias=False\" -&gt; \"ŷᵢ\"[label=\"identity\"]\n\n''')\n\n\n\n\n(표현3)\n- 그런데 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다” 라는 언급자체도 반복할 필요가 없을 것 같다. (어차피 당연히 그럴테니까) 그래서 단순히 아래와 같이 그려도 무방할듯 하다.\n\ngv(''' \n    \"1\" -&gt; \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x\" -&gt; \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x*β̂₁,    bias=False\" -&gt; \"ŷ\"[label=\"identity\"]\n\n''')\n\n\n\n\n(표현4)\n- 위의 모델은 아래와 같이 쓸 수 있다. (\\(\\beta_0\\)를 바이어스로 표현)\n\n#collapse\ngv('''\n\"x\" -&gt; \"x*β̂₁,    bias=True\"[label=\"*β̂₁\"] ;\n\"x*β̂₁,    bias=True\" -&gt; \"ŷ\"[label=\"indentity\"] ''')\n\n\n\n\n\n실제로는 이 표현을 많이 사용함\n\n(표현5)\n- 벡터버전으로 표현하면 아래와 같다. 이 경우에는 \\({\\bf X}=[1,x]\\)에 포함된 1이 bias의 역할을 해주므로 bias = False 임.\n\n#collapse\ngv('''\n\"X\" -&gt; \"X@β̂,    bias=False\"[label=\"@β̂\"] ;\n\"X@β̂,    bias=False\" -&gt; \"ŷ\"[label=\"indentity\"] ''')\n\n\n\n\n\n저는 이걸 좋아해요\n\n(표현5)’\n- 딥러닝에서는 \\(\\hat{\\boldsymbol{\\beta}}\\) 대신에 \\(\\hat{{\\bf W}}\\)을 라고 표현한다.\n\n#collapse\ngv('''\n\"X\" -&gt; \"X@Ŵ,    bias=False\"[label=\"@Ŵ\"] ;\n\"X@Ŵ,    bias=False\" -&gt; \"ŷ\"[label=\"identity\"] ''')\n\n\n\n\n- 실제로는 표현4 혹은 표현5를 외우면 된다.\n\n\nLayer의 개념\n- (표현4) 혹은 (표현5)의 그림은 레이어로 설명할 수 있다.\n- 레이어는 항상 아래와 같은 규칙을 가진다. - 첫 동그라미는 레이어의 입력이다. - 첫번째 화살표는 선형변환을 의미한다. - 두번째 동그라미는 선형변환의 결과이다. (이때 bias가 false인지 true인지에 따라서 실제 수식이 조금 다름) - 두번째 화살표는 두번째 동그라미에 어떠한 함수 \\(f\\)를 취하는 과정을 의미한다. (우리의 그림에서는 \\(f(x)=x\\)) - 세번째 동그라미는 레이어의 최종출력이다.\n- 엄청 복잡한데, 결국 레이어를 만들때 위의 그림들을 의미하도록 하려면 아래의 4개의 요소만 필요하다. 1. 레이어의 입력차원 2. 선형변환의 결과로 얻어지는 차원 3. 선형변환에서 바이어스를 쓸지? 안쓸지? 4. 함수 \\(f\\)\n- 주목: 1,2가 결정되면 자동으로 \\(\\hat{{\\bf W}}\\)의 차원이 결정된다.\n(예시) - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=1: \\(\\hat{\\bf W}\\)는 (2,1) 매트릭스 - 레이어의 입력차원=20, 선형변환의 결과로 얻어지는 차원=5: \\(\\hat{\\bf W}\\)는 (20,5) 매트릭스 - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=50: \\(\\hat{\\bf W}\\)는 (2,50) 매트릭스\n- 주목2: 이중에서 절대 생략불가능 것은 “2. 선형변환의 결과로 얻어지는 차원” 이다. - 레이어의 입력차원: 실제 레이어에 데이터가 들어올 때 데이터의 입력차원을 컴퓨터 스스로 체크하여 \\(\\hat{\\bf W}\\)의 차원을 결정할 수 있음. - 바이어스를 쓸지? 안쓸지? 기본적으로 쓴다고 가정한다. - 함수 \\(f\\): 기본적으로 항등함수를 가정하면 된다.\n\n\nKeras를 이용한 풀이\n- 기본뼈대: net생성 \\(\\to\\) add(layer) \\(\\to\\) compile(opt,loss) \\(\\to\\) fit(data,epochs)\n- 데이터정리\n\\[{\\bf y}\\approx 2.5 +4*x\\]\n\ntnp.random.seed(43052)\nN= 200 \nx= tnp.linspace(0,1,N)\nepsilon= tnp.random.randn(N)*0.5 \ny= 2.5+4*x +epsilon\n\n\nX=tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\n\n\n풀이1: 스칼라버전\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1)  \n# 입력차원? 데이터를 넣어보고 결정, 바이어스=디폴드값을 쓰겠음 (use_bias=true), 함수도 디폴트값을 쓰겠음 (f(x)=x)\nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) \n\n\n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 \n\n&lt;keras.callbacks.History at 0x7fb265125b20&gt;\n\n\n\nbatch_size=N : 경사하강법\nbatch_size!=N : 확률적 경사하강법\n\n(결과확인)\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.9330256]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([2.583672], dtype=float32)&gt;]\n\n\n\n스칼라버전 끝!\n\n\n\n풀이2: 벡터버전\n(0단계) 데이터정리\n\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False) ## 주의!  use_bias=False\nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) \n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 \n\n&lt;keras.callbacks.History at 0x7fb2650ead00&gt;\n\n\n(결과확인)\n\nnet.weights\n\n[&lt;tf.Variable 'dense_1/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)&gt;]\n\n\n\n\n\n\n\n\n잠시문법정리\n\n\n\n- 잠깐 Dense layer를 만드는 코드를 정리해보자.\n\n아래는 모두 같은 코드이다.\n\n\ntf.keras.layers.Dense(1)\ntf.keras.layers.Dense(units=1)\ntf.keras.layers.Dense(units=1,activation=‘linear’) // identity 가 더 맞는것 같은데..\ntf.keras.layers.Dense(units=1,activation=‘linear’,use_bias=True)\n\n\n아래의 코드1,2는 (1)의 코드들과 살짝 다른코드이다. (코드1과 코드2는 같은코드임)\n\n\ntf.keras.layers.Dense(1,input_dim=2) # 코드1\ntf.keras.layers.Dense(1,input_shape=(2,)) # 코드2\n\n\n아래는 사용불가능한 코드이다.\n\n\ntf.keras.layers.Dense(1,input_dim=(2,)) # 코드1\ntf.keras.layers.Dense(1,input_shape=2) # 코드2\n\n\n\n- 왜 input_dim이 필요한가?\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Dense(1,use_bias=False)) \n\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Dense(1,use_bias=False,input_dim=2))\n\n\nnet1.weights\n\nValueError: Weights for model 'sequential_3' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`.\n\n\n\nnet2.weights\n\n[&lt;tf.Variable 'dense_4/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-1.4113412],\n        [-1.3562037]], dtype=float32)&gt;]\n\n\n\nnet1.summary()\n\nValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.\n\n\n\nnet2.summary()\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_4 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n풀이3: 스칼라버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,input_dim=1)\n\n\nnet.add(layer)\n\n\nnet.get_weights()\n\n[array([[-0.80770403]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nweight, bias 순으로 출력\n\n\n초기값을 설정\n\nnet.weights\n\n[&lt;tf.Variable 'dense_8/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-0.80770403]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_8/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]\n\n\n\nnet.get_weights()\n\n[array([[-0.80770403]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nweight, bias순으로 출력\n\n\nnet.set_weights?\n\n\nSignature: net.set_weights(weights)\nDocstring:\nSets the weights of the layer, from NumPy arrays.\nThe weights of a layer represent the state of the layer. This function\nsets the weight values from numpy arrays. The weight values should be\npassed in the order they are created by the layer. Note that the layer's\nweights must be instantiated before calling this function, by calling\nthe layer.\nFor example, a `Dense` layer returns a list of two values: the kernel\nmatrix and the bias vector. These can be used to set the weights of\nanother `Dense` layer:\n&gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1,\n...   kernel_initializer=tf.constant_initializer(1.))\n&gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n&gt;&gt;&gt; layer_a.get_weights()\n[array([[1.],\n       [1.],\n       [1.]], dtype=float32), array([0.], dtype=float32)]\n&gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1,\n...   kernel_initializer=tf.constant_initializer(2.))\n&gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n&gt;&gt;&gt; layer_b.get_weights()\n[array([[2.],\n       [2.],\n       [2.]], dtype=float32), array([0.], dtype=float32)]\n&gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights())\n&gt;&gt;&gt; layer_b.get_weights()\n[array([[1.],\n       [1.],\n       [1.]], dtype=float32), array([0.], dtype=float32)]\nArgs:\n  weights: a list of NumPy arrays. The number\n    of arrays and their shape must match\n    number of the dimensions of the weights\n    of the layer (i.e. it should match the\n    output of `get_weights`).\nRaises:\n  ValueError: If the provided weights list does not match the\n    layer's specifications.\nFile:      ~/anaconda3/envs/torch/lib/python3.8/site-packages/keras/engine/base_layer.py\nType:      method\n\n\n\n\nlayer_b.set_weights(layer_a.get_weights()) 와 같은방식으로 쓴다는 것이군?\n\n- 한번따라해보자.\n\n_w = net.get_weights()\n_w\n\n[array([[-0.80770403]], dtype=float32), array([0.], dtype=float32)]\n\n\n\n길이가 2인 리스트이고, 각 원소는 numpy array 임\n\n\nnet.set_weights(\n    [np.array([[10.0]],dtype=np.float32), # weight, β1_hat\n     np.array([-5.0],dtype=np.float32)] # bias, β0_hat \n)\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_8/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[10.]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_8/bias:0' shape=(1,) dtype=float32, numpy=array([-5.], dtype=float32)&gt;]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1),tf.losses.MSE) \n\n(4단계) net.fit()\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7fb26550b130&gt;\n\n\n결과확인\n\nnet.weights\n\n[&lt;tf.Variable 'dense_8/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.933048]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_8/bias:0' shape=(1,) dtype=float32, numpy=array([2.58366], dtype=float32)&gt;]\n\n\n\n\n풀이4: 벡터버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False,input_dim=2) \n\n\nnet.add(layer)\n\n\nnet.summary()\n\nModel: \"sequential_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_9 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n초기값을 설정하자\n\nnet.set_weights([np.array([[-5.0],[10.0]], dtype=np.float32)])\n\n\nnet.get_weights()\n\n[array([[-5.],\n        [10.]], dtype=float32)]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.losses.MSE) # optimizer, loss 전달하여 compile \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) # verbose=0: 귀찮은 메시지X, batch_size=N: 경사하강법\n\n&lt;keras.callbacks.History at 0x7fb28c5bf550&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_9/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.58366 ],\n        [3.933048]], dtype=float32)&gt;]\n\n\n\n똑같이 잘 나옴.\n\n- 사실 실전에서는 초기값을 설정할 필요가 별로 없음.\n\n\n풀이5: 벡터버전 사용자정의 손실함수\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False)  # 출력1, bias사용X\n\n\nnet.add(layer)\n\n(3단계) net.compile()\n\nloss_fn = lambda y,yhat: (y-yhat).T @ (y-yhat) / N\n\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss_fn)\n\n\n사용자가 직접 loss function을 정의해서 컴파일해도 상관없다.\n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7fb26591f4c0&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_11/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)&gt;]\n\n\n\n잘 수렴한다…\n\n\n\n풀이6: 벡터버전, net.compile의 옵션으로 손실함수 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss='mse')  # 알아서 내장되어있는 mse 찾아서 컴파일해줌. \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7fb265183d60&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_13/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)&gt;]\n\n\n\n\n풀이7: 벡터버전, net.compile의 옵션으로 손실함수 지정 + 옵티마이저 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(optimizer='sgd', loss='mse') \n#net.optimizer.lr = tf.Variable(0.1,dtype=tf.float32)\n#net.optimizer.lr = 0.1\n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=5000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7fb2645af7c0&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_15/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5849245],\n        [3.9306912]], dtype=float32)&gt;]\n\n\n\n아까보다 에폭을 좀 더 늘리면 \\(2.58, 3.93\\)으로 수렴!\n\n\n\n\n여러가지 회귀모형의 적합과 학습과정의 모니터링\n\n예제1\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 x_i\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.random.randn(N) \nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*x +epsilon\n\n\nX= np.stack([np.ones(N),x],axis=1)\ny= y.reshape(N,1)\n\n\nplt.plot(x,y,'o') # 관측한 자료 \n\n\n\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\n\n\nyhat = X@beta_hat \n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-') \n\n\n\n\n더 좋은 적합선을 얻기위해서!\n\n\\(loss'(\\beta)=-2X'y +2X'X\\beta\\)\n\n\nslope = (2*X.T@X@beta_hat - 2*X.T@y)/ N \nbeta_hat2 = beta_hat - 0.1*slope  \nyhat2 = X@beta_hat2\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-') \nplt.plot(x,yhat2.reshape(-1),'-') \n\n\n\n\n초록색이 좀 더 나아보인다.\n\nbeta_hat = np.array([-3,-2]).reshape(2,1) \nbeta_hats = beta_hat # beta_hats = beta_hat.copy() 가 더 안전한 코드입니다. \nfor i in range(1,30):\n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat - 2*X.T@y) / N \n    beta_hat = beta_hat - 0.1*slope # 0.1은 적당, 0.3은 쪼금빠르지만 그래도 적당, 0.9는 너무 나간것같음, 1.0 은 수렴안함, 1.2 \n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) \n\n\n좀 안정적인 건 \\(0.1\\)\n\\(0.3, 0.9\\)는 수렴은 하는데 좀 더 안전한건 \\(0.1\\)\n\\(1\\sim\\) 는 수렴하는척하면서 터짐…\n\n\nbeta_hats\n\narray([[-3.        , -1.98776175, -1.16054651, -0.48448286,  0.06808892,\n         0.51975837,  0.88897604,  1.19081334,  1.43758253,  1.63934274,\n         1.80431301,  1.93920945,  2.04952049,  2.13973166,  2.2135091 ,\n         2.27384947,  2.32320238,  2.36357035,  2.39659058,  2.42360161,\n         2.44569792,  2.46377445,  2.47856304,  2.49066214,  2.50056123,\n         2.5086606 ,  2.51528766,  2.52071021,  2.52514732,  2.52877816],\n       [-2.        , -0.929175  , -0.05089843,  0.66940348,  1.26010705,\n         1.74449975,  2.14169103,  2.46736052,  2.73437247,  2.95328049,\n         3.13274182,  3.27985761,  3.40045219,  3.4993023 ,  3.58032529,\n         3.64673351,  3.70116104,  3.74576767,  3.78232418,  3.81228235,\n         3.83683236,  3.85694989,  3.87343472,  3.88694243,  3.89801039,\n         3.90707901,  3.91450928,  3.92059703,  3.92558472,  3.92967104]])\n\n\n\nbeta_hats[0]\n\narray([-3.        , -1.98776175, -1.16054651, -0.48448286,  0.06808892,\n        0.51975837,  0.88897604,  1.19081334,  1.43758253,  1.63934274,\n        1.80431301,  1.93920945,  2.04952049,  2.13973166,  2.2135091 ,\n        2.27384947,  2.32320238,  2.36357035,  2.39659058,  2.42360161,\n        2.44569792,  2.46377445,  2.47856304,  2.49066214,  2.50056123,\n        2.5086606 ,  2.51528766,  2.52071021,  2.52514732,  2.52877816])\n\n\n\nb0hats = beta_hats[0].tolist()\nb1hats = beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.5451404 ],\n       [3.94818596]])\n\n\n\nfrom matplotlib import animation \nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n&lt;Figure size 864x360 with 0 Axes&gt;\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 (data, 적합된 직선)\nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*x) \n# ax2: 오른쪽그림 (loss function)\nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*x)**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02) \nax2.scatter(2.5451404,3.94818596,loss_fn(2.5451404,3.94818596),s=200,marker='*') # loss값이 제일 작은 지점. \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*x) \n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 참고\n\nβ0,β1 = np.meshgrid(np.array([0,1,2]), np.array([4,5,6]), indexing='ij')\nβ0 = β0.reshape(-1)\nβ1 = β1.reshape(-1)\n\n\nβ0, β1\n\n(array([0, 0, 0, 1, 1, 1, 2, 2, 2]), array([4, 5, 6, 4, 5, 6, 4, 5, 6]))\n\n\n\n_a=list(map(lambda x,y: x*y, β0, β1))\n\n\n_a\n\n[0, 0, 0, 4, 5, 6, 8, 10, 12]\n\n\n\n\n예제2\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX= np.stack([np.ones(N),np.exp(-x)],axis=1)\ny= y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\nbeta_hats = beta_hat.copy() # shallow copy, deep copy &lt;--- 여름 방학 특강 \nfor i in range(1,30): \n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat - 2*X.T@y) /N \n    beta_hat = beta_hat - 0.05*slope\n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) \n\n\nbeta_hats\n\narray([[-3.        , -1.74671631, -0.82428979, -0.14453919,  0.35720029,\n         0.72834869,  1.0036803 ,  1.20869624,  1.36209751,  1.47759851,\n         1.56525696,  1.63244908,  1.68458472,  1.72563174,  1.75850062,\n         1.78532638,  1.80767543,  1.82669717,  1.84323521,  1.85790889,\n         1.8711731 ,  1.88336212,  1.89472176,  1.90543297,  1.91562909,\n         1.92540859,  1.93484428,  1.94399023,  1.9528867 ,  1.96156382],\n       [-2.        , -0.25663415,  1.01939241,  1.95275596,  2.63488171,\n         3.13281171,  3.49570765,  3.75961951,  3.95098231,  4.08918044,\n         4.18842797,  4.2591476 ,  4.30898175,  4.34353413,  4.36691339,\n         4.38213187,  4.39139801,  4.39633075,  4.39811673,  4.3976256 ,\n         4.3954946 ,  4.3921905 ,  4.38805511,  4.3833386 ,  4.37822393,\n         4.37284482,  4.36729887,  4.36165718,  4.35597148,  4.35027923]])\n\n\n\nb0hats= beta_hats[0].tolist()\nb1hats= beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X)@X.T@y\n\narray([2.46307644, 3.99681332])\n\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n&lt;Figure size 864x360 with 0 Axes&gt;\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x))\n# ax2: 오른쪽그림 \nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02) \nax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x))\n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n예제3\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) \ny=y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2,-1]).reshape(3,1) \nbeta_hats = beta_hat.copy()\nfor i in range(1,30):\n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat -2*X.T@y) /N \n    beta_hat = beta_hat - 0.1 * slope \n    beta_hats= np.concatenate([beta_hats,beta_hat],axis=1)\n\n\nbeta_hats\n\narray([[-3.        , -0.71767532,  0.36255782,  0.89072137,  1.16423101,\n         1.31925078,  1.41819551,  1.48974454,  1.54713983,  1.59655416,\n         1.64091846,  1.68167278,  1.71956758,  1.75503084,  1.78833646,\n         1.81968188,  1.84922398,  1.877096  ,  1.90341567,  1.92828934,\n         1.95181415,  1.97407943,  1.99516755,  2.01515463,  2.0341111 ,\n         2.05210214,  2.06918818,  2.08542523,  2.10086524,  2.11555643],\n       [-2.        ,  1.16947474,  2.64116513,  3.33411605,  3.66880042,\n         3.83768856,  3.92897389,  3.98315095,  4.01888831,  4.04486085,\n         4.06516144,  4.08177665,  4.09571971,  4.10754954,  4.1176088 ,\n         4.12613352,  4.13330391,  4.13926816,  4.14415391,  4.14807403,\n         4.15112966,  4.1534121 ,  4.15500404,  4.15598045,  4.15640936,\n         4.15635249,  4.15586584,  4.15500014,  4.15380139,  4.1523112 ],\n       [-1.        , -0.95492718, -0.66119313, -0.27681968,  0.12788212,\n         0.52254445,  0.89491388,  1.24088224,  1.55993978,  1.85310654,\n         2.12199631,  2.36839745,  2.59408948,  2.8007666 ,  2.99000967,\n         3.16327964,  3.32192026,  3.46716468,  3.60014318,  3.72189116,\n         3.83335689,  3.93540864,  4.02884144,  4.11438316,  4.19270026,\n         4.26440288,  4.33004965,  4.39015202,  4.44517824,  4.49555703]])\n\n\n\nb0hats,b1hats,b2hats = beta_hats # unpacking\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.46597526],\n       [4.00095138],\n       [5.04161877]])\n\n\n\ntrue가 잘 찾아짐.\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n&lt;Figure size 864x360 with 0 Axes&gt;\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x) + b2hats[0]*np.cos(5*x))\n# ax2: 오른쪽그림 \n# β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\n# β0=β0.reshape(-1)\n# β1=β1.reshape(-1)\n# loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\n# loss = list(map(loss_fn, β0,β1))\n# ax2.scatter(β0,β1,loss,alpha=0.02) \n# ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x) + b2hats[i]*np.cos(5*x))\n    # ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n예제3: 케라스로 해보자!\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) \ny=y.reshape(N,1)\n\n\nnet = tf.keras.Sequential() # 1: 네트워크 생성\nnet.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer \nnet.compile(tf.optimizers.SGD(0.1), loss='mse') # 3: compile\nnet.fit(X,y,epochs=30, batch_size=N) # 4: fit \n\nEpoch 1/30\n1/1 [==============================] - 0s 77ms/step - loss: 45.8620\nEpoch 2/30\n1/1 [==============================] - 0s 1ms/step - loss: 20.5386\nEpoch 3/30\n1/1 [==============================] - 0s 1ms/step - loss: 13.5033\nEpoch 4/30\n1/1 [==============================] - 0s 1ms/step - loss: 10.5736\nEpoch 5/30\n1/1 [==============================] - 0s 1ms/step - loss: 8.7382\nEpoch 6/30\n1/1 [==============================] - 0s 1ms/step - loss: 7.3295\nEpoch 7/30\n1/1 [==============================] - 0s 1ms/step - loss: 6.1758\nEpoch 8/30\n1/1 [==============================] - 0s 1ms/step - loss: 5.2144\nEpoch 9/30\n1/1 [==============================] - 0s 1ms/step - loss: 4.4098\nEpoch 10/30\n1/1 [==============================] - 0s 1ms/step - loss: 3.7355\nEpoch 11/30\n1/1 [==============================] - 0s 1ms/step - loss: 3.1704\nEpoch 12/30\n1/1 [==============================] - 0s 1ms/step - loss: 2.6967\nEpoch 13/30\n1/1 [==============================] - 0s 1ms/step - loss: 2.2996\nEpoch 14/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.9667\nEpoch 15/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.6877\nEpoch 16/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.4539\nEpoch 17/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.2578\nEpoch 18/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.0935\nEpoch 19/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.9557\nEpoch 20/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.8403\nEpoch 21/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.7435\nEpoch 22/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.6623\nEpoch 23/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.5943\nEpoch 24/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.5373\nEpoch 25/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.4895\nEpoch 26/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.4494\nEpoch 27/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.4158\nEpoch 28/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3877\nEpoch 29/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3641\nEpoch 30/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3443\n\n\n&lt;keras.callbacks.History at 0x7fb265509dc0&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_16/kernel:0' shape=(3, 1) dtype=float32, numpy=\n array([[2.4614503],\n        [3.925311 ],\n        [4.5953326]], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'o') \nplt.plot(x,(X@net.weights).reshape(-1),'--')\n\n\n\n\n\n\n\n숙제\n\n예제2: 케라스를 이용하여 아래를 만족하는 적절한 \\(\\beta_0\\)와 \\(\\beta_1\\)을 구하라. 적합결과를 시각화하라. (애니메이션 시각화 X)\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon\n\n\nX = np.stack([np.ones(N), np.exp(-x)],axis=1)\ny = y.reshape(N,1)\n\n\nX.shape, y.shape\n\n((100, 2), (100, 1))\n\n\n\nnet = tf.keras.Sequential() # 1. 네트워크 생성\nnet.add(tf.keras.layers.Dense(1,use_bias=False)) # 2. add layer\nnet.compile(tf.optimizers.SGD(0.1), loss='mse') # 3. compile\nnet.fit(X,y,epochs=100, batch_size=N, verbose=0) # 4. fit\n\n&lt;keras.callbacks.History at 0x7fb1ef048940&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_21/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5061626],\n        [3.966341 ]], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@net.weights).reshape(-1),'--')"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_04_(5주차)_4월4일.html",
    "href": "posts/3_STBDA2022/2022_04_04_(5주차)_4월4일.html",
    "title": "[STBDA] 5wk. optimizer를 이용한 최적화",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-wVWUAZ5xT35INvWbNOXpBx\n\n옵티마이저를 이용하면 이전의 그 수식들을 다 기억하고 있지 않아도 된다는 장점!\n\n\nimports\n\n#\n#!conda install -c conda-forge python-graphviz -y\n\n\nimport tensorflow as tf \nimport numpy as np\nimport matplotlib.pyplot as plt \n\n\nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior() \n\n\n\n최적화의 문제\n- \\(loss=(\\frac{1}{2}\\beta-1)^2\\)\n- 기존에 했던 방법은 수식을 알고 있어야 한다는 단점이 있음\n\n\ntf.keras.optimizers를 이용한 최적화방법\n\n방법1: opt.apply_gradients()를 이용\n\nalpha= 0.01/6 # 학습률설정\n\n\nbeta= tf.Variable(-10.0)  # 초깃값 설정\n\n\nopt = tf.keras.optimizers.SGD(alpha)\n\n- iter1\n\nwith tf.GradientTape() as tape:  # 미분을 위한 tape object 생성\n    tape.watch(beta) \n    loss=(beta/2-1)**2 \nslope = tape.gradient(loss,beta)\n\n\n# 얠 안하고 싶다는 거임.\n# beta.assign_sub(slope*alpha) # beta update\n# beta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.969999&gt;\n\n\n\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) \nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n\n\n\n\n\n\nWarning\n\n\n\nopt.apply_gradients()의 입력은 pair의 list\n\n\n- iter2\n\nwith tf.GradientTape() as tape: \n    tape.watch(beta) \n    loss=(beta/2-1)**2 \nslope = tape.gradient(loss,beta)\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) \nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.980008&gt;\n\n\n\n\\(-10 \\to -9.99 \\to -9.98\\to \\dots\\)\n\n- for문으로 정리\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0) \nopt = tf.keras.optimizers.SGD(alpha)\n\n\nfor epoc in range(10000): \n    with tf.GradientTape() as tape: \n        tape.watch(beta) \n        loss=(beta/2-1)**2 \n    slope = tape.gradient(loss,beta)\n    opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) \n    beta\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251&gt;\n\n\n\nopt.lr\n\n&lt;tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0016666667&gt;\n\n\n\nopt.learning_rate\n\n&lt;tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0016666667&gt;\n\n\n\nid(opt.lr), id(opt.learning_rate) # 똑같음...\n\n(140176929947312, 140176929947312)\n\n\n\nopt.apply_gradients()의 입력은 pair 의 list\n지난시간에 했던 것이 optimizer로 완벽히 구현되었다!\n\n\n\n방법2: opt.minimize()\n\n방법2는 GradientTape()를 안써도 된다.\n\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0)\nopt = tf.keras.optimizers.SGD(alpha)\n\n\nloss_fn = lambda: (beta/2-1)**2\n\n\nlambda x: x**2 &lt;=&gt; lambda(x)=x^2\nlambda x,y: x+y &lt;=&gt; lambda(x,y)=x+y\nlambda: y &lt;=&gt; lambda()=y, 입력이 없으며 출력은 항상 y인 함수\n\n\nloss_fn() # 입력은 없고 출력은 뭔가 계산되는 함수 \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=36.0&gt;\n\n\n- iter 1\n\nopt.minimize(loss_fn, [beta])\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n- iter2\n\nopt.minimize(loss_fn, [beta])\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.980008&gt;\n\n\n- for문으로 정리하면\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0) \nopt = tf.keras.optimizers.SGD(alpha)\nloss_fn = lambda: (beta/2-1)**2\nfor epoc in range(10000): \n    opt.minimize(loss_fn, [beta])\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251&gt;\n\n\n\n\n\n회귀분석 문제\n- \\({\\bf y} \\approx 2.5 + 4.0 {\\bf x}\\)\n\ntnp.random.seed(43052)\nN = 200\nx = tnp.linspace(0,1,N) \nepsilon = tnp.random.randn(N)*0.5 # 오차항.\ny = 2.5+4*x + epsilon\ny_true = 2.5+4*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_true,'r--')\n\n\n\n\n\n\n이론적 풀이\n\n풀이1: 스칼라버전\n- 포인트 - \\(S_{xx}=\\), \\(S_{xy}=\\) - \\(\\hat{\\beta}_0=\\), \\(\\hat{\\beta}_1=\\)\n- 풀이\n\nSxx = sum((x-x.mean())**2)\nSxy = sum((x-x.mean())*(y-y.mean()))\n\n\nbeta1_hat = Sxy/Sxx \nbeta1_hat # true: 4.0\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=3.9330345167331697&gt;\n\n\n\nbeta0_hat = y.mean() - x.mean()*beta1_hat\nbeta0_hat # true: 2.5\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867&gt;\n\n\n\n\n풀이2: 벡터버전\n- 포인트 - \\(\\hat{\\beta}=(X'X)^{-1}X'y\\)\n- 풀이\n\ny=y.reshape(N,1)\nX=tf.stack([tf.ones(N,dtype=tf.float64),x],axis=1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\ntf.linalg.inv(X.T @ X ) @ X.T @ y \n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366721],\n       [3.93303452]])&gt;\n\n\n\n\n풀이3: 벡터버전, 손실함수의 도함수이용\n- 포인트\n\n\\(loss'(\\beta)=-2X'y +2X'X\\beta\\)\n\\(\\beta_{new} = \\beta_{old} - \\alpha \\times loss'(\\beta_{old})\\)\n\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tnp.array([-5,10]).reshape(2,1)\nbeta_hat\n\n&lt;tf.Tensor: shape=(2, 1), dtype=int64, numpy=\narray([[-5],\n       [10]])&gt;\n\n\n\n# slope = (-2*X.T @ y + 2*X.T @ X @ beta_hat) / N # SSE\nslope = (-2*X.T @ y + 2*X.T @ X @ beta_hat) / N   # MSE \nslope\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-9.10036894],\n       [-3.52886113]])&gt;\n\n\n\n앞으로는 MSE 버전으로 할 것임. (sample 수가 커질때마다 alpha 값을 조정하기가 너무 귀찮으니까..)\n\n\nalpha= 0.1 \n\n\nstep = slope*alpha\nstep\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-0.91003689],\n       [-0.35288611]])&gt;\n\n\n\nfor epoc in range(1000): \n    slope = (-2*X.T @ y + 2*X.T @ X @ beta_hat)/N \n    beta_hat = beta_hat - alpha* slope\n\n\nbeta_hat\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n\nGradientTape를 이용\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드1: 그레디언트 테입  \nwith tf.GradientTape() as tape: \n    loss = \n## 포인트코드2: 미분 \nslope = tape.gradient(loss,beta_hat) \n## 포인트코드3: update \nbeta_hat.assign_sub(slope*alph) \n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\nalpha=0.1\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= X@beta_hat\n        loss= (y-yhat).T @ (y-yhat) / N\n    slope = tape.gradient(loss,beta_hat) \n    beta_hat.assign_sub(alpha*slope) \n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n위에서 구한것과 같음!\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 미분\nslope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat])\n- 풀이\n\ny=y.reshape(-1) # 길이가 200인 벡터.\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0)\n\n\nalpha=0.1\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= beta0_hat + x*beta1_hat \n        loss= tf.reduce_sum((y-yhat)**2)/N #loss= sum((y-yhat)**2)/N\n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) \n    beta0_hat.assign_sub(alpha*slope0)\n    beta1_hat.assign_sub(alpha*slope1)\n\n\nbeta0_hat,beta1_hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048&gt;)\n\n\n\nloss를 tf.reduce_sum((y-yhat)**2)/N 이렇게 해야 속도가 훨씬 빠름..\nloss= sum((y-yhat)**2)/N 얘와 같지만 속도는 다름!\n\n\n\n\nGradientTape + opt.apply_gradients\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드: 업데이트\nopt.apply_gradients([(slope,beta_hat)])  ## pair의 list가 입력 \n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\n# tf.keras.optimizers       ## tf.optimizers와 동일..\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= X@beta_hat\n        loss= (y-yhat).T @ (y-yhat) / N\n    slope = tape.gradient(loss,beta_hat)\n    opt.apply_gradients([(slope,beta_hat)])\n    #beta_hat.assign_sub(alpha*slope) \n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 업데이트 \nopt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) ## pair의 list가 입력 \n- 풀이\n\ny=y.reshape(-1)\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0)\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= beta0_hat + beta1_hat*x #X@beta_hat\n        loss= tf.reduce_sum((y-yhat)**2) / N\n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat])\n    opt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)])\n\n\npair의 list [(slope0,beta0_hat),(slope1,beta1_hat)]이런 형태가 필요함.\n미분하는 애들이 여러개 있을 때는 리스트로 전달함으로써 연산을 용이하게 함. (리스트로 전달하는 이유.)\n\n\nbeta0_hat,beta1_hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048&gt;)\n\n\n\n\n\nopt.minimize\n\n풀이1: 벡터버전, 사용자정의 손실함수 with lambda\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\n# 손실함수 정의\nloss_fn = lambda: (y-X@beta_hat).T @ (y-X@beta_hat) / N \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,[beta_hat])\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366721],\n       [3.93303452]])&gt;\n\n\n\n\\(\\hat{\\beta}\\)이 적절히 잘 추론됨.\n\n\n\n풀이2: 스칼라버전, 사용자정의 손실함수 with lambda\n- 포인트\n## 포인트코드: 미분 & 업데이트 = minimize \nopt.minimize(loss_fn,[beta0_hat,beta1_hat])\n- 풀이\n\ny=y.reshape(-1)\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0) \n\n\nloss_fn = lambda: tf.reduce_sum((y-beta0_hat-beta1_hat*x )**2) / N \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,[beta0_hat,beta1_hat])\n\n\nbeta0_hat,beta1_hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048&gt;)\n\n\n\n결과는 동일.\n\n\n\n풀이3: 벡터버전, 사용자정의 (짧은) 손실함수\n- 포인트\n## 포인트코드: 손실함수정의 \ndef loss_fn():\n    return ??\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\ndef loss_fn():\n    return (y-X@beta_hat).T @ (y-X@beta_hat) / N \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn, [beta_hat])\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n풀이4: 벡터버전, 사용자정의 (긴) 손실함수\n- 포인트\n## 포인트코드: 손실함수정의 \ndef loss_fn():\n    ??\n    ??\n    return ??\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = (y-yhat).T @ (y-yhat) / N # 컴퓨터한테 전달할 수식 2 \n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 \n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,[beta_hat])\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n풀이5: 벡터버전, 사용자정의 손실함수 &lt;- tf.losses.MSE\n- 포인트\n## 포인트코드: 미리구현되어있는 손실함수 이용 \ntf.losses.MSE(y,yhat)\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\n# 알아서 MSE 잘 구해줌.\ntf.keras.losses.MSE(tnp.array([0.0,0.0,0.0]),tnp.array([1.0,2.0,3.0]))\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=4.666666666666667&gt;\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = tf.keras.losses.MSE(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2 \n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 \n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,[beta_hat])\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n풀이6: 벡터버전, 사용자정의 손실함수 &lt;- tf.losses.MeaSquaredError\n- 포인트\n## 포인트코드: 클래스로부터 손실함수 오브젝트 생성 (함수를 찍어내는 클래스) \nmse_fn = tf.losses.MeanSquaredError()\nmse_fn(y,yhat)\n- 풀이\n\nmseloss_fn = tf.losses.MeanSquaredError()\n\n\nmseloss_fn = tf.keras.losses.MSE 라고 보면된다.\n\n\nmseloss_fn(tnp.array([0.0,0.0]), tnp.array([1.0,2.0]))\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=2.5&gt;\n\n\n\ndir(mseloss_fn) # callable object.\n\n['__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_allow_sum_over_batch_size',\n '_fn_kwargs',\n '_get_reduction',\n '_keras_api_names',\n '_keras_api_names_v1',\n '_name_scope',\n '_set_name_scope',\n 'call',\n 'fn',\n 'from_config',\n 'get_config',\n 'name',\n 'reduction']\n\n\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2 \n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,[beta_hat])\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n\ntf.keras.Sequential\n- \\(\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1x_i\\) 의 서로다른 표현\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\ngv(''' \n    \"1\" -&gt; \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta0_hat\"]\n    \"x\" -&gt; \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta1_hat\"]\n    \"beta0_hat + x*beta1_hat,    bias=False\" -&gt; \"yhat\"[label=\"indentity\"]\n    ''')\n\n\n\n\n\ngv('''\n\"x\" -&gt; \"x*beta1_hat,    bias=True\"[label=\"*beta1_hat\"] ;\n\"x*beta1_hat,    bias=True\" -&gt; \"yhat\"[label=\"indentity\"] ''')\n\n\n\n\n\ngv('''\n\"X=[1 x]\" -&gt; \"X@beta_hat,    bias=False\"[label=\"@beta_hat\"] ;\n\"X@beta_hat,    bias=False\" -&gt; \"yhat\"[label=\"indentity\"] ''')\n\n\n\n\n\n풀이1: 벡터버전, 사용자정의 손실함수\n- 포인트\n## 포인트코드1: 네트워크 생성 \nnet = tf.keras.Sequential()\n\n## 포인트코드2: 네트워크의 아키텍처 설계 \nnet.add(tf.keras.layers.Dense(1,input_shape=(2,),use_bias=False)) \n\n## 포인트코드3: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저\nnet.compile(opt,loss=loss_fn2)\n\n## 포인트코드4: 미분 & update \nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n- 레이어: 입력 -&gt; 레이어 -&gt; 출력\n- 네트워크: 레이어들의 집합\n- 풀이\n\nnet = tf.keras.Sequential() # 아무것도 없음..\n\n\nnetwork 안에 layer를 설계를 해야함.\n\n\nnet.add(tf.keras.layers.Dense(units=1,input_shape=(2,),use_bias=False)) ## yhat을 구하는 방법정의 = 아키텍처가 설계 \n\n\nunits는 layer의 출력의 차원, 이 경우는 yhat의 차원, yhat은 (200,1) 이므로 1임.\ninput_shape는 layer의 입력의 차원, 이 경우는 X의 차원, X는 (200,2) 이므로 2임.\n\n\nnet.summary() # 뭔가 만들어짐.\n\nModel: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n네트워크 안에 레이어가 만들어졌다.\n\n\nnet.weights # 뭔가 weight도 만들어져 있음!\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 1.2421309 ],\n        [-0.71216303]], dtype=float32)&gt;]\n\n\n\ndef loss_fn2(y,yhat):\n    return (y-yhat).T @ (y-yhat) / N \n\n\nalpha=0.1\nopt =tf.optimizers.SGD(alpha)\n\n\n[np.array([[-5.0],[10.0]],dtype=np.float32)]\n\n[array([[-5.],\n        [10.]], dtype=float32)]\n\n\n\nnet.set_weights([np.array([[-5.0],[10.0]],dtype=np.float32)])\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-5.],\n        [10.]], dtype=float32)&gt;]\n\n\n\nnet.compile(opt,loss=tf.losses.MSE)\n# 아키텍처 + 손실함수 + 옵티마이저 =&gt; 네트워크에 다 합치자 =&gt; 네트워크를 컴파일한다. \n\n\nnet.fit(X,y,epochs=1000,batch_size=N,verbose=0) # 미분 + 파라메터업데이트 = net.fit \n\n&lt;keras.callbacks.History at 0x7f7d0bd6cc70&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.58366 ],\n        [3.933048]], dtype=float32)&gt;]"
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-035.html",
    "href": "posts/5_STBDA2023/07wk-035.html",
    "title": "07wk-035: 아이스크림(이상치) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-weIAPJ89acBOc2V3_0HMcX&si=Lqw2fusURrQqchzH\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:100,3].to_numpy()\ntemp.sort()\nice_sales = 10 + temp * 0.5 + np.random.randn(100)\nice_sales[0] = 200\ndf_train = pd.DataFrame({'temp':temp,'ice_sales':ice_sales})\ndf_train\n\n\n\n\n\n\n\n\ntemp\nice_sales\n\n\n\n\n0\n-4.1\n200.000000\n\n\n1\n-3.7\n9.234175\n\n\n2\n-3.0\n9.642778\n\n\n3\n-1.3\n9.657894\n\n\n4\n-0.5\n9.987787\n\n\n...\n...\n...\n\n\n95\n12.4\n17.508688\n\n\n96\n13.4\n17.105376\n\n\n97\n14.7\n17.164930\n\n\n98\n15.0\n18.555388\n\n\n99\n15.2\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nplt.plot(df_train.temp,df_train.ice_sales,'o')\n\n\n\n\n\n\n4. 분석\n- 분석: 의사결정나무\n\n# step1\nX = df_train[['temp']]\ny = df_train['ice_sales']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3 \npredictr.fit(X,y)\n# step4 \ndf_train['ice_sales_hat'] = predictr.predict(X)\n\n\n# plt.plot(df_train.temp,df_train.ice_sales,'o')\n# plt.plot(df_train.temp,df_train.ice_sales_hat,'--')\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o')\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--')\n\n\n\n\n- 12.5~18 구간사이의 unseen data를 가상으로 만들고 예측값을 살펴보자.\n\nXX = df_test = pd.DataFrame({'temp': np.linspace(12.5, 18, 100)})\ndf_test['ice_sales_hat'] = predictr.predict(XX)\ndf_test\n\n\n\n\n\n\n\n\ntemp\nice_sales_hat\n\n\n\n\n0\n12.500000\n17.508688\n\n\n1\n12.555556\n17.508688\n\n\n2\n12.611111\n17.508688\n\n\n3\n12.666667\n17.508688\n\n\n4\n12.722222\n17.508688\n\n\n...\n...\n...\n\n\n95\n17.777778\n18.787014\n\n\n96\n17.833333\n18.787014\n\n\n97\n17.888889\n18.787014\n\n\n98\n17.944444\n18.787014\n\n\n99\n18.000000\n18.787014\n\n\n\n\n100 rows × 2 columns\n\n\n\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(12.5,18,100)})\n\n\ndf_test['ice_sales_hat'] = predictr.predict(XX)\n\n\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o',color='C0',alpha=0.5)\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--',color='C1',alpha=0.5)\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--',color='C2',linewidth=2)\n\n\n\n\n- -15~0 구간사이의 unseen data를 가상으로 만들고 예측값을 살펴보자.\n\nXX = df_test = pd.DataFrame({'temp':np.linspace(-15,0,100)})\n\n\ndf_test['ice_sales_hat'] = predictr.predict(XX)\n\n\nplt.plot(df_train.temp[1:],df_train.ice_sales[1:],'o',color='C0',alpha=0.5)\nplt.plot(df_train.temp[1:],df_train.ice_sales_hat[1:],'--',color='C1',alpha=0.5)\nplt.plot(df_test.temp,df_test.ice_sales_hat,'--',color='C2',alpha=0.5, linewidth=2)\n\n\n\n\n\n뭐 이 데이터에서는 최선이지 않을까?\n\n\n\n6. HW\n- 없어요. 다른과목 중간고사 준비 잘하세요!"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-015.html#a.-숫자형자료의-impute",
    "href": "posts/5_STBDA2023/04wk-015.html#a.-숫자형자료의-impute",
    "title": "04wk-015: 결측치 처리, sklearn.impute",
    "section": "A. 숫자형자료의 impute",
    "text": "A. 숫자형자료의 impute\n- 주어진자료\n\ndf = pd.DataFrame({'A':[2.1,1.9,2.2,np.nan,1.9], 'B':[0,0,np.nan,0,0]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n- 빈칸은 대충 아래와 같이 추정하면 되지 않을까?\n\ndf.loc[3, 'A'], df.loc[2, 'B']\n\n(nan, nan)\n\n\n\ndf.loc[3,'A'] = df.A.mean()\ndf.loc[2,'B'] = df.B.mean()\n\n\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.100\n0.0\n\n\n1\n1.900\n0.0\n\n\n2\n2.200\n0.0\n\n\n3\n2.025\n0.0\n\n\n4\n1.900\n0.0\n\n\n\n\n\n\n\n- 자동으로 하려면?\n\ndf = pd.DataFrame({'A':[2.1,1.9,2.2,np.nan,1.9], 'B':[0,0,np.nan,0,0]})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n(방법1)\n\nimputer = sklearn.impute.SimpleImputer()  # 인스턴스생성하고 imputer로 받아줌.\nimputer.fit(df)\nimputer.transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법2)\n\nimputer = sklearn.impute.SimpleImputer()\nimputer.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n- 다른방식으로 결측값 대체\n(방법1) – 평균으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='mean')\nimputer.fit_transform(df)\n\narray([[2.1  , 0.   ],\n       [1.9  , 0.   ],\n       [2.2  , 0.   ],\n       [2.025, 0.   ],\n       [1.9  , 0.   ]])\n\n\n(방법2) – 중앙값으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='median')\nimputer.fit_transform(df)\n\narray([[2.1, 0. ],\n       [1.9, 0. ],\n       [2.2, 0. ],\n       [2. , 0. ],\n       [1.9, 0. ]])\n\n\n(방법3) – 최빈값으로 대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimputer.fit_transform(df)\n\narray([[2.1, 0. ],\n       [1.9, 0. ],\n       [2.2, 0. ],\n       [1.9, 0. ],\n       [1.9, 0. ]])\n\n\n(방법4) – 상수대체\n\nimputer = sklearn.impute.SimpleImputer(strategy='constant',fill_value=-999)\nimputer.fit_transform(df)\n\narray([[   2.1,    0. ],\n       [   1.9,    0. ],\n       [   2.2, -999. ],\n       [-999. ,    0. ],\n       [   1.9,    0. ]])"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-015.html#b.-범주형자료의-impute",
    "href": "posts/5_STBDA2023/04wk-015.html#b.-범주형자료의-impute",
    "title": "04wk-015: 결측치 처리, sklearn.impute",
    "section": "B. 범주형자료의 impute",
    "text": "B. 범주형자료의 impute\n- 자료\n\ndf = pd.DataFrame({'A':['Y','N','Y','Y',np.nan], 'B':['stat','math',np.nan,'stat','bio']})\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n- 최빈값 혹은 상수대체만 가능\n(방법1) – 최빈값을 이용\n\nimptr = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimptr.fit_transform(df)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)\n\n\n(방법2) – 상수로 대체함\n\nimptr1 = sklearn.impute.SimpleImputer(strategy='constant',fill_value='Y')\nimptr1.fit_transform(df[['A']])\nimptr2 = sklearn.impute.SimpleImputer(strategy='constant',fill_value='math')\nimptr2.fit_transform(df[['B']])\n\narray([['stat'],\n       ['math'],\n       ['math'],\n       ['stat'],\n       ['bio']], dtype=object)\n\n\n\nnp.concatenate([imptr1.fit_transform(df[['A']]),imptr2.fit_transform(df[['B']])],axis=1)\n\narray([['Y', 'stat'],\n       ['N', 'math'],\n       ['Y', 'math'],\n       ['Y', 'stat'],\n       ['Y', 'bio']], dtype=object)"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-015.html#c.-혼합형자료의-impute-1-모두-최빈값으로-impute",
    "href": "posts/5_STBDA2023/04wk-015.html#c.-혼합형자료의-impute-1-모두-최빈값으로-impute",
    "title": "04wk-015: 결측치 처리, sklearn.impute",
    "section": "C. 혼합형자료의 impute – (1) 모두 최빈값으로 impute",
    "text": "C. 혼합형자료의 impute – (1) 모두 최빈값으로 impute\n# 예제: 아래의 df에서 결측치를 모두 최빈값으로 impute하라.\n\ndf = pd.DataFrame(\n    {'A':[2.1,1.9,2.2,np.nan,1.9],\n     'B':[0,0,np.nan,0,0],\n     'C':['Y','N','Y','Y',np.nan], \n     'D':['stat','math',np.nan,'stat','bio']}\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n(풀이)\n\nimptr = sklearn.impute.SimpleImputer(strategy='most_frequent')\nimptr.fit_transform(df)\n\narray([[2.1, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'N', 'math'],\n       [2.2, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'Y', 'stat'],\n       [1.9, 0.0, 'Y', 'bio']], dtype=object)"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-015.html#d.-혼합형자료의-impute-2-숫자형은-평균값으로-범주는-최빈값으로-impute",
    "href": "posts/5_STBDA2023/04wk-015.html#d.-혼합형자료의-impute-2-숫자형은-평균값으로-범주는-최빈값으로-impute",
    "title": "04wk-015: 결측치 처리, sklearn.impute",
    "section": "D. 혼합형자료의 impute – (2) 숫자형은 평균값으로, 범주는 최빈값으로 impute",
    "text": "D. 혼합형자료의 impute – (2) 숫자형은 평균값으로, 범주는 최빈값으로 impute\n# 예제: 아래의 df를 숫자형일 경우는 평균대치, 문자형일 경우는 최빈값으로 대치하라.\n\ndf = pd.DataFrame(\n    {'A':[2.1,1.9,2.2,np.nan,1.9],\n     'B':[0,0,np.nan,0,0],\n     'C':['Y','N','Y','Y',np.nan], \n     'D':['stat','math',np.nan,'stat','bio']}\n)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n(풀이)\n- step1: 복사본 생성\n\ndf_imputed = df.copy()\ndf_imputed\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.1\n0.0\nY\nstat\n\n\n1\n1.9\n0.0\nN\nmath\n\n\n2\n2.2\nNaN\nY\nNaN\n\n\n3\nNaN\n0.0\nY\nstat\n\n\n4\n1.9\n0.0\nNaN\nbio\n\n\n\n\n\n\n\n- step2: 데이터프레임 분리\nselect_dtypes 메소드 이용\n\ndf_num = df.select_dtypes(include=\"number\")\ndf_num\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2.1\n0.0\n\n\n1\n1.9\n0.0\n\n\n2\n2.2\nNaN\n\n\n3\nNaN\n0.0\n\n\n4\n1.9\n0.0\n\n\n\n\n\n\n\n\ndf_cat = df.select_dtypes(exclude=\"number\")\ndf_cat \n\n\n\n\n\n\n\n\nC\nD\n\n\n\n\n0\nY\nstat\n\n\n1\nN\nmath\n\n\n2\nY\nNaN\n\n\n3\nY\nstat\n\n\n4\nNaN\nbio\n\n\n\n\n\n\n\n- step3: impute\n\ndf_imputed[df_num.columns] = sklearn.impute.SimpleImputer(strategy='mean').fit_transform(df_num)\ndf_imputed[df_cat.columns] = sklearn.impute.SimpleImputer(strategy='most_frequent').fit_transform(df_cat)\n\n\ndf_imputed\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n2.100\n0.0\nY\nstat\n\n\n1\n1.900\n0.0\nN\nmath\n\n\n2\n2.200\n0.0\nY\nstat\n\n\n3\n2.025\n0.0\nY\nstat\n\n\n4\n1.900\n0.0\nY\nbio"
  },
  {
    "objectID": "posts/5_STBDA2023/2000-10-04.html",
    "href": "posts/5_STBDA2023/2000-10-04.html",
    "title": "Kaggle 리눅스 세팅",
    "section": "",
    "text": "step1 Kaggle 홈페이지에서 kaggle.json 다운로드\n다운로드 받은 파일 원하는 위치로 이동 (ex. Dropbox)\nstep2 터미널에서 확인\nls /home/jy/.kaggle.\n\nls: cannot access ‘/home/jy/.kaggle.’: No such file or directory\n\n현대 디렉토리에 .kaggle 이 없다?\nDropbox에서 ls로 파일 리스트를 확인해보면\nstep3 home/jy/ 위치에 다운로드 받은 kaggle.json 복사\ncp kaggle.json /home/jy/.kaggle\nls /home/jy/.kaggle\n# kaggle.json (복사된 kagglg.json)\n(참고)\n# for 보안\nchmod 600 /home/jy/.kaggle/kaggle.json\nstep4\n!kaggle competitions download -c titanic\n!unzip titanic.zip -d ./titanic\ndf_train = pd.read_csv('titanic/train.csv')\ndf_test = pd.read_csv('titanic/test.csv')\n!rm titanic.zip\n!rm -rf titanic/"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-014.html#a.-df.info",
    "href": "posts/5_STBDA2023/04wk-014.html#a.-df.info",
    "title": "04wk-014: 결측치 시각화, msno",
    "section": "A. df.info()",
    "text": "A. df.info()\n\ndf.shape\n\n(1000, 5)\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   A       668 non-null    float64\n 1   B       656 non-null    float64\n 2   C       608 non-null    float64\n 3   D       668 non-null    float64\n 4   E       660 non-null    float64\ndtypes: float64(5)\nmemory usage: 39.2 KB\n\n\n결측치가 30%~35% 정도 있다."
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-014.html#b.-msno.bar",
    "href": "posts/5_STBDA2023/04wk-014.html#b.-msno.bar",
    "title": "04wk-014: 결측치 시각화, msno",
    "section": "B. msno.bar()",
    "text": "B. msno.bar()\n\nmsno.bar(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-014.html#a.-msno.matrix",
    "href": "posts/5_STBDA2023/04wk-014.html#a.-msno.matrix",
    "title": "04wk-014: 결측치 시각화, msno",
    "section": "A. msno.matrix()",
    "text": "A. msno.matrix()\n\nmsno.matrix(df)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n결측치의 숫자는 비슷하지만 C의 경우 결측치가 랜덤하게 발생하였고, A,B 비슷한 패턴 // D,E 비슷한 패턴으로 결측치가 발생한 것을 볼 수 있다."
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-014.html#b.-msno.heatmap",
    "href": "posts/5_STBDA2023/04wk-014.html#b.-msno.heatmap",
    "title": "04wk-014: 결측치 시각화, msno",
    "section": "B. msno.heatmap()",
    "text": "B. msno.heatmap()\n\nmsno.heatmap(df)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n결측치의 정보가 A,B와 비슷 // D,E 비슷"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-014.html#c.-msno.dendrogram",
    "href": "posts/5_STBDA2023/04wk-014.html#c.-msno.dendrogram",
    "title": "04wk-014: 결측치 시각화, msno",
    "section": "C. msno.dendrogram()",
    "text": "C. msno.dendrogram()\n\nmsno.dendrogram(df)\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-040.html",
    "href": "posts/5_STBDA2023/11wk-040.html",
    "title": "11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-wy5HZfyF9HIZKvAXrdBwzO&si=vTJ3EiJLr487p2DT"
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-040.html#a.-기본시각화",
    "href": "posts/5_STBDA2023/11wk-040.html#a.-기본시각화",
    "title": "11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "A. 기본시각화",
    "text": "A. 기본시각화\n\nsklearn.tree.plot_tree(predictr);\n\n\n\n\n\n잘 안보임\n\n이런 시각화 옵션들을 몇 개 조정할 수 있음."
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-040.html#b.-max_depth-조정",
    "href": "posts/5_STBDA2023/11wk-040.html#b.-max_depth-조정",
    "title": "11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "B. max_depth 조정",
    "text": "B. max_depth 조정\n\nmax_depth를 지정하여 원하는 깊이만큼 볼 수 있다.\n\n\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth=0\n);"
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-040.html#c.-변수이름-추가",
    "href": "posts/5_STBDA2023/11wk-040.html#c.-변수이름-추가",
    "title": "11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "C. 변수이름 추가",
    "text": "C. 변수이름 추가\nx[5]&lt;=0.5에서 x[5]가 뭔지 모르겠는데?\n\nX.columns[5] # 매번 일일이 이렇게 찾기는 귀찮다.\n\n'smoker_no'\n\n\n\nsklearn.tree.plot_tree(predictr, max_depth=0, feature_names=X.columns.tolist());\n\n\n\n\n\nx[5]에 해당하는 column 이름이 보여진다."
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-040.html#d.-fig-오브젝트",
    "href": "posts/5_STBDA2023/11wk-040.html#d.-fig-오브젝트",
    "title": "11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "D. fig 오브젝트",
    "text": "D. fig 오브젝트\n- plt.gcf()를 이용하여 fig 오브젝트 추출\n\nsklearn.tree.plot_tree(\n    predictr,\n    max_depth=1,\n    feature_names=X.columns.tolist()\n);\nfig = plt.gcf()\n\n\n\n\n- fig.suptitle 을 이용하여 제목을 붙일 수도 있지 않을까?\n\nfig.suptitle(\"title??\")\n\nText(0.5, 0.98, 'title??')\n\n\n\nfig\n\n\n\n\n\n제목이 생겼다!\n\n- dpi 조정\n해상도 조정\n\nfig.set_dpi(250) # 크고 선명하게 보임.\nfig\n\n\n\n\n\n보험료를 책정할 때 흡연여부나 나이, bmi가 중요하다고 생각해볼 수 있음."
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-040.html#e.-matplotlib의-ax에-그리기",
    "href": "posts/5_STBDA2023/11wk-040.html#e.-matplotlib의-ax에-그리기",
    "title": "11wk-040: Medical Cost / 의사결정나무의 시각화",
    "section": "E. matplotlib의 ax에 그리기",
    "text": "E. matplotlib의 ax에 그리기\n\nfig = plt.figure() # matplotlib의 figure object 생성.\nax = fig.subplots(2,1)\nax[0].plot(y,y,'--')\nax[0].plot(y,predictr.predict(X),'o',alpha=0.1)\nsklearn.tree.plot_tree(predictr,feature_names=X.columns.tolist(),ax=ax[1],max_depth=0);\n\n\n\n\n\nmatplotlib으로 그림을 그려도 좋긴한데 해상도가 그렇게 좋지는 않다. 해상도를 억지로 올린다고 해도 한계가 있다."
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-016.html#a.-결측치-체크",
    "href": "posts/5_STBDA2023/04wk-016.html#a.-결측치-체크",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "A. 결측치 체크",
    "text": "A. 결측치 체크\n- 결측치확인\n\ndf_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   Ticket       891 non-null    object \n 7   Fare         891 non-null    float64\n 8   Cabin        204 non-null    object \n 9   Embarked     889 non-null    object \n 10  Fsize        891 non-null    int64  \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 76.7+ KB"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-016.html#b.-시각화",
    "href": "posts/5_STBDA2023/04wk-016.html#b.-시각화",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "B. 시각화",
    "text": "B. 시각화\n\nmsno.matrix(df_train) \n# msno.bar(df_train) # 큰 의미 X \n# msno.dendrogram(df_train) # 큰 의미 X \n# msno.heatmap(df_train) # 큰 의미 X \n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-016.html#c.-결측치-처리",
    "href": "posts/5_STBDA2023/04wk-016.html#c.-결측치-처리",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "C. 결측치 처리",
    "text": "C. 결측치 처리\n\ndf_train.select_dtypes(include=\"number\")\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nFare\nFsize\n\n\n\n\n0\n1\n0\n3\n22.0\n7.2500\n1\n\n\n1\n2\n1\n1\n38.0\n71.2833\n1\n\n\n2\n3\n1\n3\n26.0\n7.9250\n0\n\n\n3\n4\n1\n1\n35.0\n53.1000\n1\n\n\n4\n5\n0\n3\n35.0\n8.0500\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\n27.0\n13.0000\n0\n\n\n887\n888\n1\n1\n19.0\n30.0000\n0\n\n\n888\n889\n0\n3\nNaN\n23.4500\n3\n\n\n889\n890\n1\n1\n26.0\n30.0000\n0\n\n\n890\n891\n0\n3\n32.0\n7.7500\n0\n\n\n\n\n891 rows × 6 columns\n\n\n\n\ndf_train.select_dtypes(include=\"object\")\n\n\n\n\n\n\n\n\nName\nSex\nTicket\nCabin\nEmbarked\n\n\n\n\n0\nBraund, Mr. Owen Harris\nmale\nA/5 21171\nNaN\nS\n\n\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\nPC 17599\nC85\nC\n\n\n2\nHeikkinen, Miss. Laina\nfemale\nSTON/O2. 3101282\nNaN\nS\n\n\n3\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n113803\nC123\nS\n\n\n4\nAllen, Mr. William Henry\nmale\n373450\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n\n\n886\nMontvila, Rev. Juozas\nmale\n211536\nNaN\nS\n\n\n887\nGraham, Miss. Margaret Edith\nfemale\n112053\nB42\nS\n\n\n888\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nW./C. 6607\nNaN\nS\n\n\n889\nBehr, Mr. Karl Howell\nmale\n111369\nC148\nC\n\n\n890\nDooley, Mr. Patrick\nmale\n370376\nNaN\nQ\n\n\n\n\n891 rows × 5 columns\n\n\n\n\ndef impute_missing(df):\n    df_imputed = df.copy()\n    df_num = df.select_dtypes(include=\"number\")\n    df_cat = df.select_dtypes(exclude=\"number\")\n    df_imputed[df_num.columns] = sklearn.impute.SimpleImputer().fit_transform(df_num) # mean impute\n    df_imputed[df_cat.columns] = sklearn.impute.SimpleImputer(strategy='most_frequent').fit_transform(df_cat) \n    return df_imputed\n\n\nmsno.matrix(df_train)\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\nmsno.matrix(impute_missing(df_test))\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\nmsno.matrix(impute_missing(df_train))\n\n&lt;AxesSubplot: &gt;"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-016.html#a.-자료의-정리",
    "href": "posts/5_STBDA2023/04wk-016.html#a.-자료의-정리",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "A. 자료의 정리",
    "text": "A. 자료의 정리\n\n{c:len(set(df_train[c])) for c in df_train.select_dtypes(include=\"object\").columns}\n\n{'Name': 891, 'Sex': 2, 'Ticket': 681, 'Cabin': 148, 'Embarked': 4}\n\n\n\nSex, Embarked 정도가 유의미한 변수로 보임.\n\n\nX = pd.get_dummies(impute_missing(df_train).drop(['PassengerId','Survived','Name','Ticket','Cabin'],axis=1))\ny = impute_missing(df_train)[['Survived']]\n\n\nX\n\n\n\n\n\n\n\n\nPclass\nAge\nFare\nFsize\nSex_female\nSex_male\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n3.0\n22.000000\n7.2500\n1.0\n0\n1\n0\n0\n1\n\n\n1\n1.0\n38.000000\n71.2833\n1.0\n1\n0\n1\n0\n0\n\n\n2\n3.0\n26.000000\n7.9250\n0.0\n1\n0\n0\n0\n1\n\n\n3\n1.0\n35.000000\n53.1000\n1.0\n1\n0\n0\n0\n1\n\n\n4\n3.0\n35.000000\n8.0500\n0.0\n0\n1\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n2.0\n27.000000\n13.0000\n0.0\n0\n1\n0\n0\n1\n\n\n887\n1.0\n19.000000\n30.0000\n0.0\n1\n0\n0\n0\n1\n\n\n888\n3.0\n29.699118\n23.4500\n3.0\n1\n0\n0\n0\n1\n\n\n889\n1.0\n26.000000\n30.0000\n0.0\n0\n1\n1\n0\n0\n\n\n890\n3.0\n32.000000\n7.7500\n0.0\n0\n1\n0\n1\n0\n\n\n\n\n891 rows × 9 columns"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-016.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/04wk-016.html#b.-predictor-생성",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()\npredictr\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-016.html#c.-학습",
    "href": "posts/5_STBDA2023/04wk-016.html#c.-학습",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X, y)\n\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-016.html#d.-예측",
    "href": "posts/5_STBDA2023/04wk-016.html#d.-예측",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "D. 예측",
    "text": "D. 예측\n\npredictr.predict(X)\n\narray([0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n       0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n       1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n       0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n       1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n       1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n       0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n       1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n       1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n       1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n       1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n       0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n       1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n       1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n       0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n       0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n       0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n       0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n       0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n       1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n       0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n       1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n       0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n       1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n       1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n       0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n       1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n       1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n       1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n       0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n       0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n       0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n       0., 0., 0., 1., 0., 1., 0.])"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-016.html#e.-평가",
    "href": "posts/5_STBDA2023/04wk-016.html#e.-평가",
    "title": "04wk-016: 타이타닉, 결측치처리+로지스틱",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.8002244668911336"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-010.out.html",
    "href": "posts/5_STBDA2023/03wk-010.out.html",
    "title": "03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-yMATKoY2e3Ltd85lyvT-Ht&si=8Ke2fNuWswEvFReG"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-010.out.html#a.-데이터",
    "href": "posts/5_STBDA2023/03wk-010.out.html#a.-데이터",
    "title": "03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "A. 데이터",
    "text": "A. 데이터\n\nX = df[['temp','type']] # 독립변수, 설명변수, 피쳐\ny = df[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nX = X.assign(type = [type == 'choco' for type in X.type])\nX\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n-4.1\nTrue\n\n\n1\n-3.7\nTrue\n\n\n2\n-3.0\nTrue\n\n\n3\n-1.3\nTrue\n\n\n4\n-0.5\nTrue\n\n\n...\n...\n...\n\n\n95\n12.4\nFalse\n\n\n96\n13.4\nFalse\n\n\n97\n14.7\nFalse\n\n\n98\n15.0\nFalse\n\n\n99\n15.2\nFalse\n\n\n\n\n200 rows × 2 columns"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-010.out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/03wk-010.out.html#b.-predictor-생성",
    "title": "03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression() \npredictr \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-010.out.html#c.-학습-fit-learn",
    "href": "posts/5_STBDA2023/03wk-010.out.html#c.-학습-fit-learn",
    "title": "03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-010.out.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/03wk-010.out.html#d.-예측-predict",
    "title": "03wk-010: 아이스크림(초코/바닐라), 회귀분석",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n\nyhat = predictr.predict(X)\n\n\nplt.plot(df.temp,df.sales,'o',alpha=0.5) # true\nplt.plot(df.temp,yhat, 'x',alpha=0.5) # prediction\n\n\n\n\n- 어떻게 맞춘거지?\n\\[\\textup{아이스크림 판매량} = 40 + \\textup{아이스크림종류} \\times (-20) + \\textup{온도} \\times 2.5 + \\textup{오차(운)}\\]\n\npredictr.coef_, predictr.intercept_\n\n(array([[  2.52239574, -20.54021854]]), array([40.16877158]))\n\n\n- 온도가 -2이고, type이 초코라면? 예측값은?\n\nXnew = pd.DataFrame({'temp':[-2.0],'type':[1]})\nXnew\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n-2.0\n1\n\n\n\n\n\n\n\n\npredictr.predict(Xnew)\n\narray([[14.58376156]])\n\n\n- 온도가 -2이고, type이 바닐라라면? 예측값은?\n\nXnew = pd.DataFrame({'temp':[-2.0],'type':[0]})\nXnew\n\n\n\n\n\n\n\n\ntemp\ntype\n\n\n\n\n0\n-2.0\n0\n\n\n\n\n\n\n\n\npredictr.predict(Xnew)\n\narray([[35.1239801]])\n\n\n\nplt.plot(df.temp, df.sales, 'o')\nplt.plot(df.temp, predictr.predict(X), 'x')\nplt.plot(Xnew.temp, predictr.predict(Xnew), 'or')\n\n\n\n\nDummy variable을 도입해서 초코, 바닐라를 \\(0,1\\)로 코딩을 해서 넣게되면 범주형 변수가 있는 데이터에서도 linear regression을 잘 수행할 수 있다."
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-026.html",
    "href": "posts/5_STBDA2023/06wk-026.html",
    "title": "06wk-026: 취업+각종영어점수, LassoCV",
    "section": "",
    "text": "RidgeCV 와 마찬가지로 Lasso모델 에서 적절한 알파값을 선택해주는 것이 LassoCV.\n\n\n\n\n\n\n\nLasso\n\n\n\n\n라쏘를 이용하면 계수들이 꽤 합리적으로 잘 추정된다.\n다중공선성이 있는 모형일 경우에도 오버피팅문제를 막을 수 있다.\n\n\n\n\n1. 강의영상\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nimport matplotlib.pyplot as plt\n\n\n\n3. Data\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment_multicollinearity.csv\")\nnp.random.seed(43052)\ndf['employment_score'] = df.gpa * 1.0 + df.toeic* 1/100 + np.random.randn(500)\n\n\ndf\n\n\n\n\n\n\n\n\nemployment_score\ngpa\ntoeic\ntoeic0\ntoeic1\ntoeic2\ntoeic3\ntoeic4\ntoeic5\ntoeic6\n...\ntoeic490\ntoeic491\ntoeic492\ntoeic493\ntoeic494\ntoeic495\ntoeic496\ntoeic497\ntoeic498\ntoeic499\n\n\n\n\n0\n1.784955\n0.051535\n135\n129.566309\n133.078481\n121.678398\n113.457366\n133.564200\n136.026566\n141.793547\n...\n132.014696\n140.013265\n135.575816\n143.863346\n152.162740\n132.850033\n115.956496\n131.842126\n125.090801\n143.568527\n\n\n1\n10.789671\n0.355496\n935\n940.563187\n935.723570\n939.190519\n938.995672\n945.376482\n927.469901\n952.424087\n...\n942.251184\n923.241548\n939.924802\n921.912261\n953.250300\n931.743615\n940.205853\n930.575825\n941.530348\n934.221055\n\n\n2\n8.221213\n2.228435\n485\n493.671390\n493.909118\n475.500970\n480.363752\n478.868942\n493.321602\n490.059102\n...\n484.438233\n488.101275\n485.626742\n475.330715\n485.147363\n468.553780\n486.870976\n481.640957\n499.340808\n488.197332\n\n\n3\n2.137594\n1.179701\n65\n62.272565\n55.957257\n68.521468\n76.866765\n51.436321\n57.166824\n67.834920\n...\n67.653225\n65.710588\n64.146780\n76.662194\n66.837839\n82.379018\n69.174745\n64.475993\n52.647087\n59.493275\n\n\n4\n8.650144\n3.962356\n445\n449.280637\n438.895582\n433.598274\n444.081141\n437.005100\n434.761142\n443.135269\n...\n455.940348\n435.952854\n441.521145\n443.038886\n433.118847\n466.103355\n430.056944\n423.632873\n446.973484\n442.793633\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n9.057243\n4.288465\n280\n276.680902\n274.502675\n277.868536\n292.283300\n277.476630\n281.671647\n296.307373\n...\n269.541846\n278.220546\n278.484758\n284.901284\n272.451612\n265.784490\n275.795948\n280.465992\n268.528889\n283.638470\n\n\n496\n4.108020\n2.601212\n310\n296.940263\n301.545000\n306.725610\n314.811407\n311.935810\n309.695838\n301.979914\n...\n304.680578\n295.476836\n316.582100\n319.412132\n312.984039\n312.372112\n312.106944\n314.101927\n309.409533\n297.429968\n\n\n497\n2.430590\n0.042323\n225\n206.793217\n228.335345\n222.115146\n216.479498\n227.469560\n238.710310\n233.797065\n...\n233.469238\n235.160919\n228.517306\n228.349646\n224.153606\n230.860484\n218.683195\n232.949484\n236.951938\n227.997629\n\n\n498\n5.343171\n1.041416\n320\n327.461442\n323.019899\n329.589337\n313.312233\n315.645050\n324.448247\n314.271045\n...\n326.297700\n309.893822\n312.873223\n322.356584\n319.332809\n319.405283\n324.021917\n312.363694\n318.493866\n310.973930\n\n\n499\n6.505106\n3.626883\n375\n370.966595\n364.668477\n371.853566\n373.574930\n376.701708\n356.905085\n354.584022\n...\n382.278782\n379.460816\n371.031640\n370.272639\n375.618182\n369.252740\n376.925543\n391.863103\n368.735260\n368.520844\n\n\n\n\n500 rows × 503 columns\n\n\n\n\n\n4. LassoCV\n- LassoCV 클래스에서 모형을 선택해보자.\n\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.LassoCV(alphas= np.linspace(0,2,100))\n## step3\npredictr.fit(X,y)\n## step4 -- pass\n\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.256e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.652e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.606e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.694e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.950e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.379e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.839e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.182e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.368e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.121e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.062e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.033e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.667e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.202e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.410e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.363e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.107e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.023e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.036e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.050e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.043e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.022e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.211e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.432e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.682e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.873e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.890e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.955e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.278e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.196e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.619e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.615e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.663e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.716e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.874e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.076e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.567e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.629e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.070e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.461e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.806e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.742e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.590e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.535e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.552e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.512e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.746e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.771e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.253e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.761e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.898e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.657e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.876e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.096e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.101e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.186e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.907e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.861e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.073e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.116e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.124e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.071e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.424e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.659e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.496e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.021e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.055e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.085e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.212e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.339e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.503e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.684e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.803e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.897e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.991e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.252e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.602e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.907e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.098e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.392e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.630e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.790e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.793e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.675e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.340e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.623e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.670e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.597e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.611e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.417e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.999e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.284e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.148e+00, tolerance: 2.707e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.771e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.263e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.173e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.317e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.372e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.132e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.367e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.155e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.074e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.322e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.089e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.867e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.038e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.028e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.531e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.686e-01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.031e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.075e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.099e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.129e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.146e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.089e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.458e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.637e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.744e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.979e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.371e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.905e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.939e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.322e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.215e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.838e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.373e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.925e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.559e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.236e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.877e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.641e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.404e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.174e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.833e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.358e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.812e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.197e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.644e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.129e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.555e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.854e+00, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.064e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.141e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.198e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.247e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.240e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.183e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.156e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.116e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.078e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.186e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.308e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.426e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.755e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.950e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.141e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.279e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.344e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.413e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.430e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.501e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.526e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.518e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.807e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.050e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.138e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.158e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.185e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.238e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.341e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.402e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.280e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.104e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.067e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.970e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.733e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.357e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.054e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.661e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.076e+01, tolerance: 2.707e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.290e+00, tolerance: 2.707e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.021e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.739e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.749e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.104e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.254e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.181e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.324e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.057e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.330e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.041e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.544e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.969e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.370e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.501e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.239e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.585e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.942e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.750e-01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.008e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.097e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.320e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.642e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.913e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.018e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.223e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.570e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.774e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.292e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.959e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.804e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.961e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.097e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.040e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.837e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.006e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.174e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.087e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.304e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.659e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.153e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.229e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.118e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.808e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.491e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.042e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.231e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.454e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.437e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.412e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.513e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.981e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.463e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.113e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.770e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.355e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.757e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.891e+00, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.036e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.085e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.244e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.422e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.639e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.822e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.914e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.006e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.216e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.495e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.764e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.933e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.952e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.845e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.643e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.445e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.606e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.783e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.891e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.877e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.539e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.457e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.476e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.625e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.708e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.620e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.362e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.950e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.254e+01, tolerance: 2.670e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.393e+00, tolerance: 2.670e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.197e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.270e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.127e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.962e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.102e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.142e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.889e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.129e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.839e-01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.037e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.187e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.159e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.009e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.035e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.507e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.887e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.061e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.291e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.625e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.850e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.891e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.744e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.504e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.189e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.201e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.481e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.821e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.064e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.425e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.580e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.699e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.673e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.553e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.360e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.118e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.063e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.166e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.150e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.074e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.933e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.788e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.624e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.675e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.796e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.642e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.234e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.569e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.900e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.798e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.457e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.823e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.293e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.609e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.510e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.789e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.410e+00, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.038e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.209e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.365e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.482e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.639e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.782e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.862e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.911e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.929e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.003e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.056e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.040e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.035e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.009e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.973e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.105e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.305e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.590e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.849e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.026e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.989e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.921e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.077e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.289e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.330e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.570e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.744e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.730e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.588e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.491e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.287e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.947e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.513e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.394e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.414e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.192e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.801e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+01, tolerance: 2.721e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.565e+00, tolerance: 2.721e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.107e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.577e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.421e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.473e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.119e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.834e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.138e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.546e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.948e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.917e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.409e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.762e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.701e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.126e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.237e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.311e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.201e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.063e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.786e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.199e-01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.040e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.150e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.121e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.338e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.364e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.288e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.247e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.355e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.611e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.947e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.317e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.165e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.426e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.748e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.115e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.297e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.679e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.415e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.820e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.534e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.187e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.834e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.370e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.166e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.067e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.924e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.816e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.875e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.271e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.280e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.259e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.209e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.826e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.719e+00, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.120e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.270e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.457e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.620e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.863e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.935e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.963e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.933e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.607e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.400e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.298e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.439e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.569e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.714e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.869e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.773e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.654e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.988e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.395e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.979e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.277e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.566e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.866e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.129e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.319e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.439e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.399e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.270e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.128e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.926e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.656e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.321e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.010e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.703e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.331e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.767e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.078e+01, tolerance: 2.540e-01\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.059e+00, tolerance: 2.540e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.315e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\nLassoCV(alphas=array([0.        , 0.02020202, 0.04040404, 0.06060606, 0.08080808,\n       0.1010101 , 0.12121212, 0.14141414, 0.16161616, 0.18181818,\n       0.2020202 , 0.22222222, 0.24242424, 0.26262626, 0.28282828,\n       0.3030303 , 0.32323232, 0.34343434, 0.36363636, 0.38383838,\n       0.4040404 , 0.42424242, 0.44444444, 0.46464646, 0.48484848,\n       0.50505051, 0.52525253, 0.54545455, 0.56565657, 0.58585859,\n       0.60606061...\n       1.31313131, 1.33333333, 1.35353535, 1.37373737, 1.39393939,\n       1.41414141, 1.43434343, 1.45454545, 1.47474747, 1.49494949,\n       1.51515152, 1.53535354, 1.55555556, 1.57575758, 1.5959596 ,\n       1.61616162, 1.63636364, 1.65656566, 1.67676768, 1.6969697 ,\n       1.71717172, 1.73737374, 1.75757576, 1.77777778, 1.7979798 ,\n       1.81818182, 1.83838384, 1.85858586, 1.87878788, 1.8989899 ,\n       1.91919192, 1.93939394, 1.95959596, 1.97979798, 2.        ]))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoCVLassoCV(alphas=array([0.        , 0.02020202, 0.04040404, 0.06060606, 0.08080808,\n       0.1010101 , 0.12121212, 0.14141414, 0.16161616, 0.18181818,\n       0.2020202 , 0.22222222, 0.24242424, 0.26262626, 0.28282828,\n       0.3030303 , 0.32323232, 0.34343434, 0.36363636, 0.38383838,\n       0.4040404 , 0.42424242, 0.44444444, 0.46464646, 0.48484848,\n       0.50505051, 0.52525253, 0.54545455, 0.56565657, 0.58585859,\n       0.60606061...\n       1.31313131, 1.33333333, 1.35353535, 1.37373737, 1.39393939,\n       1.41414141, 1.43434343, 1.45454545, 1.47474747, 1.49494949,\n       1.51515152, 1.53535354, 1.55555556, 1.57575758, 1.5959596 ,\n       1.61616162, 1.63636364, 1.65656566, 1.67676768, 1.6969697 ,\n       1.71717172, 1.73737374, 1.75757576, 1.77777778, 1.7979798 ,\n       1.81818182, 1.83838384, 1.85858586, 1.87878788, 1.8989899 ,\n       1.91919192, 1.93939394, 1.95959596, 1.97979798, 2.        ]))\n\n\n\npredictr.score(X,y)\n\n0.9483494983570957\n\n\n\npredictr.score(XX,yy)\n\n0.8749691641081611\n\n\n\n약간 과적합되긴 했지만 오라클과 거의 비등하게 나온다. (라쏘를 이용하면 계수들이 꽤 합리적으로 잘 추정됨 / 다중공선성이 있는 모형일 경우에도 오버피팅문제를 막을 수 있다.\n\n\n\n5. HW\n- 현재 predictr 에 저장된1 alpha값을 조사하고, 그것을 바탕으로 아래의 코드를 수정하여 위와 동일한 train_score, test_score가 나오도록 하라.\n수정할코드\n## step1\ndf_train, df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nXX = df_test.loc[:,'gpa':'toeic499']\nyy = df_test.loc[:,'employment_score']\n## step2 \npredictr = sklearn.linear_model.Lasso(alpha=????)\n## step3\npredictr.fit(X,y)\n## step4 -- pass\n나와야할 결과\n\npredictr.score(X,y)\n\n0.9483494983570957\n\n\n\npredictr.score(XX,yy)\n\n0.8749691641081611\n\n\n(sol)\n\nbest_alpha = predictr.alpha_\nbest_alpha\n\n0.36363636363636365\n\n\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size=0.3, random_state=42)\nX = df_train.loc[:,'gpa':'toeic499']\nXX = df_test.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n\npredictr2 = sklearn.linear_model.Lasso(alpha=best_alpha)\npredictr2.fit(X,y)\n\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.315e+01, tolerance: 3.337e-01\n  model = cd_fast.enet_coordinate_descent(\n\n\nLasso(alpha=0.36363636363636365)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.36363636363636365)\n\n\n\npredictr2.score(X,y), predictr.score(XX,yy)\n\n(0.9483494983570957, 0.8749691641081611)\n\n\n\n\n\n\n\nFootnotes\n\n\nLassoCV클래스에서 생성된 predictor↩︎"
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html",
    "href": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "",
    "text": "- 왜 여자만 살까?\n\nGTP 답변\ngender_submission.csv파일은 단순히 모든 여성 승객이 생존한다고 가정한 예측을 담은 파일로, 실제 데이터에 기반한 예측이 아닙니다. 이 파일은 제출 파일의 형식과 구성을 보여주기 위한 예시입니다. 실제로는 성별이나 다른 특성에 따라 생존 여부가 결정되는 것이 아닌, 여러 다양한 변수들이 생존에 영향을 미치게 됩니다.\n실제로 타이타닉호에서는 여성들이 생존율이 높았습니다. 타이타닉호 침몰 사건 당시 여성과 아이들을 먼저 구조하고 구명보트에 태우는 등의 우선순위가 있었기 때문입니다. 따라서 “gender_submission.csv” 파일은 예시로서 제공되었고, 실제 예측 모델을 개발할 때에는 여러 다른 변수들도 고려하여 예측해야 합니다.\n\n\n\n\n- 항구는 승객의 배경과 사회적 상황을 반영할 수 있다.\n\nGPT 답변\n네, 항구별로 승객들의 배경과 사회적 상황이 다를 수 있습니다. 다양한 항구에서 탑승한 승객들은 지역적, 경제적, 문화적인 차이를 반영할 수 있습니다. 몇 가지 예시를 들어보겠습니다:\n\n경제적 차이: 서로 다른 항구에서 탑승한 승객들은 다른 경제적 배경을 가지고 있을 수 있습니다. 상류층 승객이나 부유한 승객들이 탑승한 항구일수록 1등급 티켓을 선택한 경우가 많을 것입니다.\n문화적 차이: 다른 항구에서 탑승한 승객들은 다른 문화적 배경을 가지고 있을 수 있습니다. 이로 인해 승객들의 습관, 언어, 관습 등이 다를 수 있습니다.\n지역적 차이: 항구의 위치와 지역적 특성에 따라 승객들의 배경이 달라질 수 있습니다. 해당 지역의 주요 산업이나 경제 활동에 따라 승객들의 직업이나 사회적 지위가 다를 수 있습니다.\n가족 관계: 특정 항구에서 가족들이 함께 탑승한 경우, 해당 지역의 가족 구조와 관련된 차이가 반영될 수 있습니다.\n\n이러한 다양한 차이를 분석하여 항구별로 생존율이나 다른 변수들과의 상관 관계를 파악할 수 있습니다. 이는 모델링 과정에서 추가적인 특성 엔지니어링이나 분석에 활용될 수 있습니다.\n\n- 항구는 국적에 대한 정보를 내포할 수도 있다.\n\nGPT 답변\n타이타닉호의 데이터에서는 승객들의 국적에 대한 직접적인 정보는 제공되지 않습니다. 따라서 승객들의 국적에 대한 정보를 확인하는 것은 어렵습니다. 그러나 항구별로 승객들의 출발지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수는 있습니다.\n예를 들어, Cherbourg 항구가 프랑스에 위치하며, Queenstown 항구가 아일랜드에 위치해 있습니다. 따라서 해당 항구에서 탑승한 승객들 중 일부는 해당 국가의 국적을 가질 가능성이 높을 것입니다. 그러나 국적은 항상 탑승 항구와 일치하지 않을 수 있으며, 탑승한 승객들의 다양한 출신지와 배경을 고려해야 합니다.\n요약하면, 타이타닉호 데이터에서는 직접적인 국적 정보는 제공되지 않지만, 항구별로 승객들의 출신지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수 있습니다.\n\n\n\n\n- 두 변수 sibsp와 parch를 합쳐 family_size라는 하나의 변수를 만들고, 이를 활용하여 y를 예측할 수 있다.\n\n데이터를 봐야 하겠지만 가족단위로 보트를 줬을 확률도 큼\n가족단위로 보트를 실제로 주었을 경우 family_size라는 변수는 매우 의미있는 해석이 됨\n\n\nGPT 답변\n가족 규모 특성 생성: ‘sibsp’와 ’parch’ 변수를 활용하여 승객의 가족 규모를 나타내는 새로운 변수를 만들 수 있습니다. 예를 들어, ‘sibsp’와 ’parch’ 값을 더한 후 1을 더해 ‘family_size’ 변수로 만들 수 있습니다. 이를 통해 가족 규모가 큰 승객이 생존 여부에 영향을 미칠 수 있는지 확인할 수 있습니다.\n\n\n\n\n- 경우에 따라서 두가지 범주를 하나도 합쳐서 만든 새로운 변수가 매우 의미있을 수 있음\n\n경우에 따라서 특정 국적 (프랑스)의 1등급 손님이 구명보트를 많이 얻었을 수도 있음.\n이럴경우 embarked와 class를 독립적으로 생각하는것보다 둘을 합친 변수가 훨씬 유의미한 변수임\n\n\nGPT 답변\n물론, 자세한 설명을 드리겠습니다. “항구와 클래스의 조합 특성 생성”이라는 개념은 ‘embarked’ (탑승 항구)와 ‘pclass’ (티켓 클래스) 두 변수를 활용하여 새로운 특성을 만들어 분석하는 것을 의미합니다.\n예를 들어, “embarked” 변수에는 ‘C’, ‘Q’, ‘S’와 같은 항구 정보가 있으며, “pclass” 변수에는 ’1’, ‘2’, ’3’과 같은 티켓 클래스 정보가 있습니다. 이 두 변수를 조합하여 새로운 변수를 만들어보겠습니다.\n생성할 새로운 변수의 예시:\n“embarked_pclass”: ‘C_1’, ‘C_2’, ‘C_3’, ‘Q_1’, ‘Q_2’, ‘Q_3’, ‘S_1’, ‘S_2’, ‘S_3’ 위와 같이 “embarked”와 “pclass”를 조합하여 새로운 변수 값을 만듭니다. 예를 들어 ’embarked’가 ’C’이고 ’pclass’가 ’1’인 경우, “embarked_pclass” 변수 값은 ’C_1’이 됩니다. 이렇게 조합된 변수는 해당 승객이 어떤 항구에서 탑승했고 어떤 티켓 클래스를 사용했는지를 동시에 나타내는 특성입니다.\n이렇게 생성된 “embarked_pclass” 변수를 사용하여 특정 항구와 클래스의 조합이 생존 여부에 어떤 영향을 미치는지를 분석할 수 있습니다. 예를 들어, ‘C_1’ 조합이 생존율과 관련이 있는지, ‘Q_3’ 조합이 생존 여부에 영향을 미치는지 등을 확인하여 해당 특성이 모델링에 유용한 정보를 제공하는지 알아볼 수 있습니다.\n\n\nGPT 답변 2 – 그냥 “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것과 무슨차이냐는 질문에 대한 답변\n네, “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것도 가능합니다. 그러나 새로운 변수를 만드는 것은 데이터를 조금 더 유용하게 활용하는 방법 중 하나입니다. 여기에 몇 가지 이유가 있습니다:\n상호작용 효과 강조: 새로운 변수를 만들면 “embarked”와 “pclass”의 조합이 생존 여부에 미치는 영향을 강조할 수 있습니다. 단순히 각각의 변수를 개별적으로 사용하는 것보다 조합된 변수를 통해 서로 간의 상호작용 효과를 쉽게 확인할 수 있습니다.\n더 풍부한 패턴 탐색: 새로운 변수를 만들면 서로 다른 항구와 클래스 조합들 간의 생존률 차이를 더 세부적으로 분석할 수 있습니다. 이를 통해 특정 항구와 클래스의 승객들이 생존 여부에 미치는 영향을 더 자세히 이해할 수 있습니다.\n다양한 조합 고려: 조합된 변수를 만들면 “embarked”와 “pclass”의 가능한 모든 조합을 커버할 수 있습니다. 이는 분석의 전체적인 시각을 확장하고, 어떤 조합이 중요한지를 더 효과적으로 파악할 수 있도록 도와줍니다.\n모델 학습에 더 적합한 데이터 생성: 몇몇 머신 러닝 알고리즘은 상호작용 변수를 활용하여 모델의 성능을 향상시킬 수 있습니다. 이러한 알고리즘들은 새로운 변수를 활용하여 더 복잡한 패턴을 학습할 수 있습니다.\n새로운 변수를 만들어 분석하는 것은 분석의 창의성과 도메인 지식을 활용하여 데이터로부터 더 많은 의미를 도출하려는 노력의 일부입니다. 분석의 목적에 따라 각 변수를 개별적으로 사용하거나 조합하여 새로운 변수를 만들 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#a.-gender_submission.csv",
    "href": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#a.-gender_submission.csv",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "",
    "text": "- 왜 여자만 살까?\n\nGTP 답변\ngender_submission.csv파일은 단순히 모든 여성 승객이 생존한다고 가정한 예측을 담은 파일로, 실제 데이터에 기반한 예측이 아닙니다. 이 파일은 제출 파일의 형식과 구성을 보여주기 위한 예시입니다. 실제로는 성별이나 다른 특성에 따라 생존 여부가 결정되는 것이 아닌, 여러 다양한 변수들이 생존에 영향을 미치게 됩니다.\n실제로 타이타닉호에서는 여성들이 생존율이 높았습니다. 타이타닉호 침몰 사건 당시 여성과 아이들을 먼저 구조하고 구명보트에 태우는 등의 우선순위가 있었기 때문입니다. 따라서 “gender_submission.csv” 파일은 예시로서 제공되었고, 실제 예측 모델을 개발할 때에는 여러 다른 변수들도 고려하여 예측해야 합니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#b.-embarked",
    "href": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#b.-embarked",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "",
    "text": "- 항구는 승객의 배경과 사회적 상황을 반영할 수 있다.\n\nGPT 답변\n네, 항구별로 승객들의 배경과 사회적 상황이 다를 수 있습니다. 다양한 항구에서 탑승한 승객들은 지역적, 경제적, 문화적인 차이를 반영할 수 있습니다. 몇 가지 예시를 들어보겠습니다:\n\n경제적 차이: 서로 다른 항구에서 탑승한 승객들은 다른 경제적 배경을 가지고 있을 수 있습니다. 상류층 승객이나 부유한 승객들이 탑승한 항구일수록 1등급 티켓을 선택한 경우가 많을 것입니다.\n문화적 차이: 다른 항구에서 탑승한 승객들은 다른 문화적 배경을 가지고 있을 수 있습니다. 이로 인해 승객들의 습관, 언어, 관습 등이 다를 수 있습니다.\n지역적 차이: 항구의 위치와 지역적 특성에 따라 승객들의 배경이 달라질 수 있습니다. 해당 지역의 주요 산업이나 경제 활동에 따라 승객들의 직업이나 사회적 지위가 다를 수 있습니다.\n가족 관계: 특정 항구에서 가족들이 함께 탑승한 경우, 해당 지역의 가족 구조와 관련된 차이가 반영될 수 있습니다.\n\n이러한 다양한 차이를 분석하여 항구별로 생존율이나 다른 변수들과의 상관 관계를 파악할 수 있습니다. 이는 모델링 과정에서 추가적인 특성 엔지니어링이나 분석에 활용될 수 있습니다.\n\n- 항구는 국적에 대한 정보를 내포할 수도 있다.\n\nGPT 답변\n타이타닉호의 데이터에서는 승객들의 국적에 대한 직접적인 정보는 제공되지 않습니다. 따라서 승객들의 국적에 대한 정보를 확인하는 것은 어렵습니다. 그러나 항구별로 승객들의 출발지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수는 있습니다.\n예를 들어, Cherbourg 항구가 프랑스에 위치하며, Queenstown 항구가 아일랜드에 위치해 있습니다. 따라서 해당 항구에서 탑승한 승객들 중 일부는 해당 국가의 국적을 가질 가능성이 높을 것입니다. 그러나 국적은 항상 탑승 항구와 일치하지 않을 수 있으며, 탑승한 승객들의 다양한 출신지와 배경을 고려해야 합니다.\n요약하면, 타이타닉호 데이터에서는 직접적인 국적 정보는 제공되지 않지만, 항구별로 승객들의 출신지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "href": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "",
    "text": "- 두 변수 sibsp와 parch를 합쳐 family_size라는 하나의 변수를 만들고, 이를 활용하여 y를 예측할 수 있다.\n\n데이터를 봐야 하겠지만 가족단위로 보트를 줬을 확률도 큼\n가족단위로 보트를 실제로 주었을 경우 family_size라는 변수는 매우 의미있는 해석이 됨\n\n\nGPT 답변\n가족 규모 특성 생성: ‘sibsp’와 ’parch’ 변수를 활용하여 승객의 가족 규모를 나타내는 새로운 변수를 만들 수 있습니다. 예를 들어, ‘sibsp’와 ’parch’ 값을 더한 후 1을 더해 ‘family_size’ 변수로 만들 수 있습니다. 이를 통해 가족 규모가 큰 승객이 생존 여부에 영향을 미칠 수 있는지 확인할 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "href": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "",
    "text": "- 경우에 따라서 두가지 범주를 하나도 합쳐서 만든 새로운 변수가 매우 의미있을 수 있음\n\n경우에 따라서 특정 국적 (프랑스)의 1등급 손님이 구명보트를 많이 얻었을 수도 있음.\n이럴경우 embarked와 class를 독립적으로 생각하는것보다 둘을 합친 변수가 훨씬 유의미한 변수임\n\n\nGPT 답변\n물론, 자세한 설명을 드리겠습니다. “항구와 클래스의 조합 특성 생성”이라는 개념은 ‘embarked’ (탑승 항구)와 ‘pclass’ (티켓 클래스) 두 변수를 활용하여 새로운 특성을 만들어 분석하는 것을 의미합니다.\n예를 들어, “embarked” 변수에는 ‘C’, ‘Q’, ‘S’와 같은 항구 정보가 있으며, “pclass” 변수에는 ’1’, ‘2’, ’3’과 같은 티켓 클래스 정보가 있습니다. 이 두 변수를 조합하여 새로운 변수를 만들어보겠습니다.\n생성할 새로운 변수의 예시:\n“embarked_pclass”: ‘C_1’, ‘C_2’, ‘C_3’, ‘Q_1’, ‘Q_2’, ‘Q_3’, ‘S_1’, ‘S_2’, ‘S_3’ 위와 같이 “embarked”와 “pclass”를 조합하여 새로운 변수 값을 만듭니다. 예를 들어 ’embarked’가 ’C’이고 ’pclass’가 ’1’인 경우, “embarked_pclass” 변수 값은 ’C_1’이 됩니다. 이렇게 조합된 변수는 해당 승객이 어떤 항구에서 탑승했고 어떤 티켓 클래스를 사용했는지를 동시에 나타내는 특성입니다.\n이렇게 생성된 “embarked_pclass” 변수를 사용하여 특정 항구와 클래스의 조합이 생존 여부에 어떤 영향을 미치는지를 분석할 수 있습니다. 예를 들어, ‘C_1’ 조합이 생존율과 관련이 있는지, ‘Q_3’ 조합이 생존 여부에 영향을 미치는지 등을 확인하여 해당 특성이 모델링에 유용한 정보를 제공하는지 알아볼 수 있습니다.\n\n\nGPT 답변 2 – 그냥 “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것과 무슨차이냐는 질문에 대한 답변\n네, “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것도 가능합니다. 그러나 새로운 변수를 만드는 것은 데이터를 조금 더 유용하게 활용하는 방법 중 하나입니다. 여기에 몇 가지 이유가 있습니다:\n상호작용 효과 강조: 새로운 변수를 만들면 “embarked”와 “pclass”의 조합이 생존 여부에 미치는 영향을 강조할 수 있습니다. 단순히 각각의 변수를 개별적으로 사용하는 것보다 조합된 변수를 통해 서로 간의 상호작용 효과를 쉽게 확인할 수 있습니다.\n더 풍부한 패턴 탐색: 새로운 변수를 만들면 서로 다른 항구와 클래스 조합들 간의 생존률 차이를 더 세부적으로 분석할 수 있습니다. 이를 통해 특정 항구와 클래스의 승객들이 생존 여부에 미치는 영향을 더 자세히 이해할 수 있습니다.\n다양한 조합 고려: 조합된 변수를 만들면 “embarked”와 “pclass”의 가능한 모든 조합을 커버할 수 있습니다. 이는 분석의 전체적인 시각을 확장하고, 어떤 조합이 중요한지를 더 효과적으로 파악할 수 있도록 도와줍니다.\n모델 학습에 더 적합한 데이터 생성: 몇몇 머신 러닝 알고리즘은 상호작용 변수를 활용하여 모델의 성능을 향상시킬 수 있습니다. 이러한 알고리즘들은 새로운 변수를 활용하여 더 복잡한 패턴을 학습할 수 있습니다.\n새로운 변수를 만들어 분석하는 것은 분석의 창의성과 도메인 지식을 활용하여 데이터로부터 더 많은 의미를 도출하려는 노력의 일부입니다. 분석의 목적에 따라 각 변수를 개별적으로 사용하거나 조합하여 새로운 변수를 만들 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html",
    "href": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html",
    "title": "[STBDA] Quarto Blog 만들기",
    "section": "",
    "text": "- 윈도우 컴퓨터인 경우 새로운 계정을 만들어 준다.\n\n한글계정명일 경우는 필수\n영어계정이더라도 계정을 분리하는게 관리에 유리\n\n- 최종적으로는\n\ngithub 로그인 + repository 생성완료\nanaconda 설치완료\ngit 설치완료\nquarto 설치완료\n\n이어야 한다.\n\n\n\ngithub 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리.\n\n\n\n\n\n아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨\n\n\n\n\n\n\n\ngit 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시\n\n\n\n\n\n\nquarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#a.-github",
    "href": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#a.-github",
    "title": "[STBDA] Quarto Blog 만들기",
    "section": "",
    "text": "github 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리."
  },
  {
    "objectID": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#b.-anaconda",
    "href": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#b.-anaconda",
    "title": "[STBDA] Quarto Blog 만들기",
    "section": "",
    "text": "아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨"
  },
  {
    "objectID": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#c.-git",
    "href": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#c.-git",
    "title": "[STBDA] Quarto Blog 만들기",
    "section": "",
    "text": "git 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#d.-quarto",
    "href": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#d.-quarto",
    "title": "[STBDA] Quarto Blog 만들기",
    "section": "",
    "text": "quarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/5_STBDA2023/08wk-supp.html",
    "href": "posts/5_STBDA2023/08wk-supp.html",
    "title": "08wk-supp: 중간점검",
    "section": "",
    "text": "1. 강의영상\n\n\n지금까지 했던 내용 추가해설.\n\n\n\n2. 타이타닉 데이터에 대한 이해\nref: https://guebin.github.io/STBDA2023/posts/01wk-002.html\n- 두 변수 sibsp와 parch를 합쳐 family_size라는 하나의 변수를 만들고, 이를 활용하여 y를 예측할 수 있다.\n\n\n\n\n\n\n다중공선성\n\n\n\nsibsp 와 parch 를 더하여 새로운 변수를 만든다면, 이후에는 sibsp와 parch 둘중 하나를 제거하나 둘다 제거하는 방법을 고려해야 한다.\n\n\n- embarked와 class에 대한 피처엔지니어링 아이디어\nembarked와 calss를 독립적으로 생각하는 것보다 둘을 합친 변수가 훨씬 유의미할 수 있을 수 있겠다는 아이디어. (경우에 따라 특정 국적의 1등급 손님이 구명보트를 많이 얻었을 수도 있으니까) –&gt; 쉽게말해 교호작용을 고려한다는 얘기.\n\n\n\n\n\n\n교호작용\n\n\n\n\n그냥 교호작용 고려한다는 소리죠? –&gt; 트리계열은 결과가 크게 좋아질 것이라 기대하기 어렵겠고, 선형모형계열로 적합한다면 결과가 좋아질 수도 있겠음.\n선형모델을 적합하는 상황을 가정하자. 교호작용의 효과가 있는 자료에서, 이를 무시하고 적합한다면 (“embarked” 변수와 “pclass” 변수만 넣고 적합한다면) 언더핏이 생김.\n\n\n\n\n\n3. 아이스크림/회귀분석\nref: https://guebin.github.io/STBDA2023/posts/03wk-009.html\n\n\n\n\n\n\n오버피팅\n\n\n\n오버피팅: 랜덤으로 뭐가 나올지 맞춘다는 것은 오차항을 적합하겠다는 의미임.\n\n\n\n오차항을 적합하는 것은 오버피팅. 적합된 것을 보고 적당 선에서 잘 끊어야 한다.\n\n\n4. 타이타닉/로지스틱\nref: https://guebin.github.io/STBDA2023/posts/03wk-013.html\n엄청 많은 범주가 있는 변수를 원핫인코딩 하게 되면 열이 엄청나게 많아지게 된다. 이러한 열들은 당연히 y와 직접적인 상관관계는 없게 나올것이다. 즉, 쓸모없는 변수들이 많아지는 상황이 되는데 이는 모듈21에서 소개한 변수들을 증가시킨 예제와 비슷한 상황이다.\n\n\n5. 취업(다중공선성) / 다중공선성의 개념\n\n\n\n\n\n\n오버피팅에 대한 내 개념\n\n\n\nPredictor 가 언더라잉이 아니라 오차항을 적합하고 있는 상황.\n\n규칙을 찾으면 안될 것 – 오차항\n규칙을 찾고 있음 – 오차항을 적합하고 있음.\n\n\n\n- 쓸모없는 변수란 느낌이 드는 경우?\n\n경우1: 진짜 쓸모없는 거.. (X1=부먹/찍먹, X2=민초/민초X) \\(\\to\\) 애초에 X1,X2를 보고 y를 맞출 생각이 들지 않아.\n경우2: 실제론 쓸모있는데, 대체자가 있는 경우. (X1=toeic, X2=유사toeic) \\(\\to\\) X1을 보고 y를 맞출 것 같은 생각이 들어, 그리고 X2를 보고 y를 맞출 것 같은 생각도 들어. 그런데 X1이랑 X2는 너무 비슷해.\n\n\n\n\n\n\n\n경우 1,2에 대한 추가설명\n\n\n\n\n경우1,2는 모듈28에서 소개한 것 처럼 히트맵을 그려서 파악할 수 있다.\n경우1은 corr(y,X1), corr(y,X2) 의 값이 낮게 나온다.\n경우2는 corr(y,X1), corr(y,X2) 의 값이 높게 나온다. 하지만 corr(X1,X2)의 값도 높게 나온다.\n\n\n\n\n\n6. 체중감량(교호작용) / 회귀분석\nref: https://guebin.github.io/STBDA2023/posts/07wk-029.html\n만약에 운동을 안하고, 약만먹을 경우 부작용이 생긴다면? (이것도 교호작용의 일종)\n\n이러한 경우 위의 모형으로 단순적합하기 어렵다. (위의 모형은 “운동O/약O”인 case에서 발생하는 효과만 고려되도록 설계되어 있음.)\n따라서 이럴 경우 차라리 (운동, 약)을 결합하여 새로운 범주형 변수를 만들고, 그 변수에서 원핫인코딩을 하는게 좋다. (마지막 더미변수는 제외하는 것이 좋지만, 파이썬에서는 제외하지 않아도 큰일나는 것은 아님.)\n사실 (운동, 약)을 결합하여 모든 새로운 범주를 만들고 이 중 필요없는 범주를 또 다시 제거해야하는 과정도 분석에 포함되어야 한다. (p-value를 보면서 뺄 수도 있고 다른 방법을 쓸 수도 있고..)\n그런데 범주형 변수가 3개라면? –&gt; 솔직히 이것저것 생각하기 귀찮으니까 이럴때는 “트리모형”계열을 사용하는게 속편하다. (아니면 교호작용이 없길 기도하거나..)"
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-022.html#a.-분석절차",
    "href": "posts/5_STBDA2023/06wk-022.html#a.-분석절차",
    "title": "06wk-022: 취업+각종영어점수, 다중공선성",
    "section": "A. 분석절차",
    "text": "A. 분석절차\n- step1: 데이터정리\n\ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\n\n\nX = df_train.loc[:,'gpa':'toeic499']\nXX = df_test.loc[:,'gpa':'toeic499']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n\n- step2: predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression()\n\n- step3: 학습\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- step4: 예측: 생략"
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-022.html#b.-계수해석-및-평가",
    "href": "posts/5_STBDA2023/06wk-022.html#b.-계수해석-및-평가",
    "title": "06wk-022: 취업+각종영어점수, 다중공선성",
    "section": "B. 계수해석 및 평가",
    "text": "B. 계수해석 및 평가\n- 계수해석\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns \ns\n\ngpa         0.035315\ntoeic       0.002680\ntoeic0      0.009333\ntoeic1     -0.017511\ntoeic2      0.005205\n              ...   \ntoeic495   -0.012811\ntoeic496   -0.007390\ntoeic497   -0.007487\ntoeic498    0.003379\ntoeic499   -0.002187\nLength: 502, dtype: float64\n\n\n\n실제계수값은 토익*1/100, GPA*1.0, 나머지 toeic0~toeic499 는 모두 계수값이 0임\n그러나 학습된 계수값은 그렇지 않음.\n\n- 평가: train/test score 계산\n\npredictr.score(X,y)\n\n1.0\n\n\n\ntrain 에서는 잘맞음 (퍼펙트) – 모의고사는 기가막히게 잘품\n\n\npredictr.score(XX,yy)\n\n0.117050782124969\n\n\n\ntest 에서는 잘 맞지 않음 – 수능을 보면 망한다."
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-022.html#a.-toeic과-gpa가-유의미한-변수라는걸-눈치챘다면-오라클..",
    "href": "posts/5_STBDA2023/06wk-022.html#a.-toeic과-gpa가-유의미한-변수라는걸-눈치챘다면-오라클..",
    "title": "06wk-022: 취업+각종영어점수, 다중공선성",
    "section": "A. toeic과 gpa가 유의미한 변수라는걸 눈치챘다면? (오라클..)",
    "text": "A. toeic과 gpa가 유의미한 변수라는걸 눈치챘다면? (오라클..)\n- 분석절차수행\n\n## step1: 데이터의 정리  \ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,'gpa':'toeic']\nXX = df_test.loc[:,'gpa':'toeic']\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n## step2: predictor 생성 \npredictr = sklearn.linear_model.LinearRegression()\n## step3: predictor.fit을 이용하여 predictor 학습\npredictr.fit(X,y)\n## step4: predictor.predict을 이용하여 예측 -- pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 계수해석\n\ns = pd.Series(predictr.coef_)\ns.index = X.columns\ns\n\ngpa      0.972163\ntoeic    0.010063\ndtype: float64\n\n\n\n실제계수값인 GPA*1.0, 토익*1/100이 잘 추정됨\n\n- score도 괜찮음\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.9133\ntest_score: 0.9127"
  },
  {
    "objectID": "posts/5_STBDA2023/06wk-022.html#b.-하다못해-toeic0과-gpa로-적합했다면",
    "href": "posts/5_STBDA2023/06wk-022.html#b.-하다못해-toeic0과-gpa로-적합했다면",
    "title": "06wk-022: 취업+각종영어점수, 다중공선성",
    "section": "B. 하다못해 toeic0과 gpa로 적합했다면?",
    "text": "B. 하다못해 toeic0과 gpa로 적합했다면?\n- 분석절차\n\n## step1: 데이터의 정리  \ndf_train,df_test = sklearn.model_selection.train_test_split(df,test_size=0.3,random_state=42)\nX = df_train.loc[:,['gpa','toeic0']]\nXX = df_test.loc[:,['gpa','toeic0']]\ny = df_train.loc[:,'employment_score']\nyy = df_test.loc[:,'employment_score']\n## step2: predictor 생성 \npredictr = sklearn.linear_model.LinearRegression()\n## step3: predictor.fit을 이용하여 predictor 학습\npredictr.fit(X,y)\n## step4: predictor.predict을 이용하여 예측 -- pass \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 계수해석\n\npredictr.coef_\n\narray([0.98130228, 0.0101011 ])\n\n\n\n합리적으로 추정된것 같음\n\n- 평가\n\nprint(f'train_score: {predictr.score(X,y):.4f}')\nprint(f'test_score: {predictr.score(XX,yy):.4f}')\n\ntrain_score: 0.9121\ntest_score: 0.9115\n\n\n\n오라클 만큼은 아니지만 이정도만 되어도 합리적임"
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-042.html",
    "href": "posts/5_STBDA2023/11wk-042.html",
    "title": "11wk-042: Weighted_Data / 의사결정나무 weights",
    "section": "",
    "text": "1. 강의영상\n\nhttps://youtu.be/playlist?list=PLQqh36zP38-yI1U6as_CdfTqS-6hPhXdr&si=W5npg8aaXMcs0WVz\n\n\n\n2. Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.tree\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n3. weights\n# 예제1 – 아래의 예제를 생각하자.\n\nX = np.array([1,2,3,4,5,6,7]).reshape(-1,1) # 7x1\ny = np.array([10,11,12,20,21,22,23])\n\n\nplt.plot(X,y,'o')\n\n\n\n\n- 어디에서 분기점을 나누는게 좋을까? 당연히 3.5 정도..\n\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\npredictr.fit(X,y)\nsklearn.tree.plot_tree(predictr);\n\n\n\n\n# 예제2 – 아래의 예제를 생각하자.\n\nX = np.array([1]*5000+[2]*5000+[3,4,5,6,7]).reshape(-1,1)\ny = np.array([10]*5000+[11]*5000+[12,20,21,22,23])\n\n\nplt.plot(X,y,'o',alpha=0.1)\n\n\n\n\n- 분기점은 어디에 나누는게 좋을까? 이 경우는 1.5근처에서 나누는게 합리적으로 보임\n\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\n\n\npredictr.fit(X,y)\n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n\nsklearn.tree.plot_tree(\n    predictr\n)\n\n[Text(0.5, 0.75, 'x[0] &lt;= 1.5\\nsquared_error = 0.299\\nsamples = 10005\\nvalue = 10.505'),\n Text(0.25, 0.25, 'squared_error = 0.0\\nsamples = 5000\\nvalue = 10.0'),\n Text(0.75, 0.25, 'squared_error = 0.089\\nsamples = 5005\\nvalue = 11.009')]\n\n\n\n\n\n# 예제3 – 다시 아래의 예제를 생각하자.\n\nX = np.array([1,2,3,4,5,6,7]).reshape(-1,1)\ny = np.array([10,11,12,20,21,22,23])\n\n\nplt.plot(X,y,'o')\nplt.plot(X[:2],y[:2],'o')\n\n\n\n\n\n주황색점들을 잘 맞추는 것이 파란색점들을 잘 맞추는 것보다 5000배정도 중요하다고 상상하자. –&gt; 주황색점에 똑같은 데이터 5000개가 뭉쳐있다고 생각.\n\n\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\npredictr.fit(X,y,sample_weight=[5000,5000,1,1,1,1,1])\nsklearn.tree.plot_tree(predictr);"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-x6USW3HM9Lm-B19o9qrm19&si=EFy8hdlgDJ-LUFHi"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#a.-데이터",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#a.-데이터",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"titanic/train.csv\")\ntst = TabularDataset(\"titanic/test.csv\")\n\n- 피처엔지니어링\n\n_tr = tr.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)\n_tst = tst.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#b.-predictor-생성",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230913_070925/\""
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#c.-적합fit",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#c.-적합fit",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(_tr,presets='best_quality') # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230913_070925/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.16\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   675.38 GB / 982.82 GB (68.7%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    35836.53 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.1s = Fit runtime\n    10 features in original data used to generate 27 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.15s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6308   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6364   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8418   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8496   = Validation score   (accuracy)\n    0.44s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8384   = Validation score   (accuracy)\n    0.36s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8238   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.853    = Validation score   (accuracy)\n    1.42s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8227   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.826    = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8519   = Validation score   (accuracy)\n    2.25s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.844    = Validation score   (accuracy)\n    0.68s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8418   = Validation score   (accuracy)\n    3.75s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.844    = Validation score   (accuracy)\n    1.0s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.853    = Validation score   (accuracy)\n    0.41s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 19.83s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230913_070925/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f0c4def4ee0&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.852974       0.031577  1.419052                0.031577           1.419052            1       True          7\n1       WeightedEnsemble_L2   0.852974       0.032645  1.832624                0.001068           0.413572            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.851852       0.063553  2.253746                0.063553           2.253746            1       True         10\n3           LightGBM_BAG_L1   0.849607       0.023809  0.435157                0.023809           0.435157            1       True          4\n4      LightGBMLarge_BAG_L1   0.843996       0.026434  1.002698                0.026434           1.002698            1       True         13\n5            XGBoost_BAG_L1   0.843996       0.033381  0.684330                0.033381           0.684330            1       True         11\n6         LightGBMXT_BAG_L1   0.841751       0.024597  0.384963                0.024597           0.384963            1       True          3\n7     NeuralNetTorch_BAG_L1   0.841751       0.077535  3.745817                0.077535           3.745817            1       True         12\n8   RandomForestGini_BAG_L1   0.838384       0.054624  0.364527                0.054624           0.364527            1       True          5\n9     ExtraTreesEntr_BAG_L1   0.826038       0.055092  0.248551                0.055092           0.248551            1       True          9\n10  RandomForestEntr_BAG_L1   0.823793       0.062297  0.314906                0.062297           0.314906            1       True          6\n11    ExtraTreesGini_BAG_L1   0.822671       0.056287  0.271098                0.056287           0.271098            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.004575  0.002916                0.004575           0.002916            1       True          2\n13    KNeighborsUnif_BAG_L1   0.630752       0.027986  0.002342                0.027986           0.002342            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.852974\n0.031577\n1.419052\n0.031577\n1.419052\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.852974\n0.032645\n1.832624\n0.001068\n0.413572\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.851852\n0.063553\n2.253746\n0.063553\n2.253746\n1\nTrue\n10\n\n\n3\nLightGBM_BAG_L1\n0.849607\n0.023809\n0.435157\n0.023809\n0.435157\n1\nTrue\n4\n\n\n4\nLightGBMLarge_BAG_L1\n0.843996\n0.026434\n1.002698\n0.026434\n1.002698\n1\nTrue\n13\n\n\n5\nXGBoost_BAG_L1\n0.843996\n0.033381\n0.684330\n0.033381\n0.684330\n1\nTrue\n11\n\n\n6\nLightGBMXT_BAG_L1\n0.841751\n0.024597\n0.384963\n0.024597\n0.384963\n1\nTrue\n3\n\n\n7\nNeuralNetTorch_BAG_L1\n0.841751\n0.077535\n3.745817\n0.077535\n3.745817\n1\nTrue\n12\n\n\n8\nRandomForestGini_BAG_L1\n0.838384\n0.054624\n0.364527\n0.054624\n0.364527\n1\nTrue\n5\n\n\n9\nExtraTreesEntr_BAG_L1\n0.826038\n0.055092\n0.248551\n0.055092\n0.248551\n1\nTrue\n9\n\n\n10\nRandomForestEntr_BAG_L1\n0.823793\n0.062297\n0.314906\n0.062297\n0.314906\n1\nTrue\n6\n\n\n11\nExtraTreesGini_BAG_L1\n0.822671\n0.056287\n0.271098\n0.056287\n0.271098\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.004575\n0.002916\n0.004575\n0.002916\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.630752\n0.027986\n0.002342\n0.027986\n0.002342\n1\nTrue\n1\n\n\n\n\n\n\n\n\nCatBoost_BAG_L1"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#d.-예측-predict",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(_tr)).mean()\n\n0.9113355780022446\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(_tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"./titanic_sub/autogluon(Fsize,Drop,best_quality)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#submission-result",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#submission-result",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "Submission Result",
    "text": "Submission Result\n\n\n\nSub and Description\nPublic Score\n\n\n\n\nautogulon_sub\n0.75358\n\n\nautogulon(Fsize)_sub\n0.77272\n\n\nautogulon(Fsize,Drop)\n0.78947\n\n\nautogulon(best_quality)\n0.80143"
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-034.html",
    "href": "posts/5_STBDA2023/07wk-034.html",
    "title": "07wk-034: 취업(오버피팅) / 의사결정나무",
    "section": "",
    "text": "1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-xYMQe_6GKus4q8E6c5RNIS&si=QtTWrQUAXDgwhxBp\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.model_selection\nimport sklearn.linear_model\nimport sklearn.tree\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n3. Data\n\ndef generating_df(n_balance):\n    df = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/employment.csv')\n    df_balance = pd.DataFrame((np.random.randn(500,n_balance)).reshape(500,n_balance)*1,columns = ['balance'+str(i) for i in range(n_balance)])\n    return pd.concat([df,df_balance],axis=1)\n\n\ndf = generating_df(10)\ndf\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\nbalance2\nbalance3\nbalance4\nbalance5\nbalance6\nbalance7\nbalance8\nbalance9\n\n\n\n\n0\n135\n0.051535\n0\n0.201954\n0.913711\n-0.589805\n0.316196\n1.285975\n0.094634\n1.781618\n-0.810583\n1.784278\n-0.604258\n\n\n1\n935\n0.355496\n0\n-0.290880\n-0.073337\n-0.570444\n-1.390016\n0.144310\n-0.247668\n1.736824\n-0.766542\n-0.279784\n-1.525845\n\n\n2\n485\n2.228435\n0\n1.333870\n0.997310\n1.500856\n0.774262\n-0.919978\n-0.202276\n1.419562\n-0.185264\n-0.782524\n1.766982\n\n\n3\n65\n1.179701\n0\n1.764506\n1.164117\n-0.967719\n-2.201651\n0.655650\n0.655106\n-1.416480\n-0.927737\n0.247814\n-1.238919\n\n\n4\n445\n3.962356\n1\n-0.413176\n0.988229\n0.564624\n0.885567\n0.560682\n-2.880517\n0.714108\n0.522645\n-1.822700\n0.794012\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n0.431759\n2.153168\n1.579232\n-1.154426\n0.208956\n-0.549120\n1.676330\n-0.015323\n1.859426\n-0.918818\n\n\n496\n310\n2.601212\n1\n0.716791\n-0.428856\n-0.770143\n-0.362971\n1.191776\n-0.763958\n-0.071902\n1.107869\n0.963393\n-0.996926\n\n\n497\n225\n0.042323\n0\n1.019516\n-0.765429\n1.868903\n-0.285379\n0.080620\n2.006618\n-1.155571\n-0.395149\n-0.430279\n0.712161\n\n\n498\n320\n1.041416\n0\n-1.195398\n0.470352\n-0.374949\n-0.397581\n1.556866\n0.281161\n-0.217248\n-1.649043\n0.252582\n0.817852\n\n\n499\n375\n3.626883\n1\n-0.445898\n0.390302\n-0.403406\n-0.528904\n1.745216\n1.194092\n0.627451\n0.583121\n1.058502\n-1.095658\n\n\n\n\n500 rows × 13 columns\n\n\n\n\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size=0.7, random_state=42)\n\n\nX,y = df_train.drop(['employment'],axis=1), df_train['employment']\nXX,yy = df_test.drop(['employment'],axis=1), df_test['employment']\n\n\n\n4. 분석\n- 분석1: 의사결정나무\n\n## step1 -- pass\n## step2 \npredictr = sklearn.tree.DecisionTreeClassifier(random_state=42)\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 1.0000\ntest_score = 0.7057\n\n\n- 분석2: 로지스틱 + Ridge\n:::{.callout-note} ## LogisticRegressionCV()에서 solver 선택하는 법\nWarning The choice of the algorithm depends on the penalty chosen. Supported penalties by solver: - lbfgs : [‘l2’]\n\nliblinear : [‘l1’, ‘l2’]\nnewton-cg : [‘l2’]\nnewton-cholesky : [‘l2’]\nsag : [‘l2’]\nsaga : [‘elasticnet’, ‘l1’, ‘l2’]\nref: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html :::‘l1’, ‘l2’]\n\n\n## step1 -- pass\n## step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(penalty='l2') ## logistic + Ridge\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 0.8733\ntest_score = 0.8743\n\n\n- 분석3: 로지스틱 + Lasso\n\n## step1 -- pass\n## step2 \npredictr = sklearn.linear_model.LogisticRegressionCV(penalty='l1', solver='liblinear')\n## step3 \npredictr.fit(X,y)\n## step4\ndf_train['employment'] = predictr.predict(X)\ndf_test['employment'] = predictr.predict(XX)\n#--#\nprint(f'train_score = {predictr.score(X,y):.4f}')\nprint(f'test_score = {predictr.score(XX,yy):.4f}')\n\ntrain_score = 0.8800\ntest_score = 0.8686\n\n\n\n\n5. 연구\n- Balance 변수들의 수가 커짐에 따라서 각 방법들(의사결정나무, 로지스틱+Ridge, 로지스틱+Lasso)의 train/test score는 어떻게 변화할까?\n- df, predictor -&gt; train_score, test_score 와 같은 함수를 만들자.\n\ndef anal(df,predictr):\n    df_train, df_test = sklearn.model_selection.train_test_split(df, test_size=0.7, random_state=42)\n    X,y = df_train.drop(['employment'],axis=1), df_train['employment']\n    XX,yy = df_test.drop(['employment'],axis=1), df_test['employment']\n    ## step1 -- pass\n    ## step2 -- pass \n    ## step3 \n    predictr.fit(X,y)\n    ## step4 -- pass \n    #--#\n    return predictr.score(X,y),predictr.score(XX,yy)\n\n\npredictr = sklearn.tree.DecisionTreeClassifier()\n\n\nanal(df,predictr)\n\n(1.0, 0.7085714285714285)\n\n\n- 실험해보자.\n\nn_balance_lst = range(0,5000,50)\n\n\npredictrs = [sklearn.tree.DecisionTreeClassifier(random_state=42), # dt\n             sklearn.linear_model.LogisticRegressionCV(penalty='l2'), # logitstic + ridge\n             sklearn.linear_model.LogisticRegressionCV(penalty='l1', solver='liblinear')] # logistic + lasso\n\n\n# 4분 ~ 5분정도 걸림.\nlst = [[anal(generating_df(n_balance),predictr) for predictr in predictrs] for n_balance in n_balance_lst]\n\n\nnp.array(lst).shape # 데이터셋의 개수 x 모델종류 x score(train / test)\n\n(100, 3, 2)\n\n\n- 실험결과 정리\n\narr = np.array(lst)\ntr = arr[:,:,0] # train score\ntst = arr[:,:,1] # test score\n\n\ndf1 = pd.DataFrame(tr, columns = ['tree','ridge','lasso']).eval('dataset = \"train\"').eval('n_balance=@n_balance_lst')\ndf1\n\n\n\n\n\n\n\n\ntree\nridge\nlasso\ndataset\nn_balance\n\n\n\n\n0\n1.0\n0.866667\n0.853333\ntrain\n0\n\n\n1\n1.0\n0.773333\n0.940000\ntrain\n50\n\n\n2\n1.0\n0.980000\n0.933333\ntrain\n100\n\n\n3\n1.0\n0.986667\n1.000000\ntrain\n150\n\n\n4\n1.0\n0.886667\n1.000000\ntrain\n200\n\n\n...\n...\n...\n...\n...\n...\n\n\n95\n1.0\n1.000000\n1.000000\ntrain\n4750\n\n\n96\n1.0\n1.000000\n1.000000\ntrain\n4800\n\n\n97\n1.0\n1.000000\n0.526667\ntrain\n4850\n\n\n98\n1.0\n1.000000\n1.000000\ntrain\n4900\n\n\n99\n1.0\n1.000000\n0.526667\ntrain\n4950\n\n\n\n\n100 rows × 5 columns\n\n\n\n\ndf2 = pd.DataFrame(tst, columns = ['tree','ridge','lasso']).eval('dataset = \"test\"').eval('n_balance = @n_balance_lst')\ndf2\n\n\n\n\n\n\n\n\ntree\nridge\nlasso\ndataset\nn_balance\n\n\n\n\n0\n0.754286\n0.877143\n0.868571\ntest\n0\n\n\n1\n0.762857\n0.725714\n0.857143\ntest\n50\n\n\n2\n0.754286\n0.805714\n0.825714\ntest\n100\n\n\n3\n0.720000\n0.748571\n0.831429\ntest\n150\n\n\n4\n0.731429\n0.714286\n0.748571\ntest\n200\n\n\n...\n...\n...\n...\n...\n...\n\n\n95\n0.674286\n0.640000\n0.622857\ntest\n4750\n\n\n96\n0.714286\n0.597143\n0.591429\ntest\n4800\n\n\n97\n0.637143\n0.602857\n0.505714\ntest\n4850\n\n\n98\n0.694286\n0.591429\n0.631429\ntest\n4900\n\n\n99\n0.705714\n0.608571\n0.505714\ntest\n4950\n\n\n\n\n100 rows × 5 columns\n\n\n\n\nridge랑 lasso는 score가 갈수록 현저히 떨어진다.\n\n\npd.concat([df1, df2]).set_index(['dataset','n_balance']).stack().reset_index().set_axis(['dataset','n_balance','method','score'], axis=1)\n\n\n\n\n\n\n\n\ndataset\nn_balance\nmethod\nscore\n\n\n\n\n0\ntrain\n0\ntree\n1.000000\n\n\n1\ntrain\n0\nridge\n0.866667\n\n\n2\ntrain\n0\nlasso\n0.853333\n\n\n3\ntrain\n50\ntree\n1.000000\n\n\n4\ntrain\n50\nridge\n0.773333\n\n\n...\n...\n...\n...\n...\n\n\n595\ntest\n4900\nridge\n0.591429\n\n\n596\ntest\n4900\nlasso\n0.631429\n\n\n597\ntest\n4950\ntree\n0.705714\n\n\n598\ntest\n4950\nridge\n0.608571\n\n\n599\ntest\n4950\nlasso\n0.505714\n\n\n\n\n600 rows × 4 columns\n\n\n\n\ndf1= pd.DataFrame(tr,columns=['tree','ridge','lasso']).eval('dataset = \"train\"').eval('n_balance = @n_balance_lst')\ndf2= pd.DataFrame(tst,columns=['tree','ridge','lasso']).eval('dataset = \"test\"').eval('n_balance = @n_balance_lst')\nresult_df = pd.concat([df1,df2]).set_index(['dataset','n_balance']).stack().reset_index().set_axis(['dataset','n_balance','method','score'],axis=1)\n\n\nsns.lineplot(result_df.query('dataset==\"test\"'),x='n_balance',y='score',hue='method')\n\n&lt;Axes: xlabel='n_balance', ylabel='score'&gt;\n\n\n\n\n\n\nresult_df.query('dataset==\"test\"')\n\n\n\n\n\n\n\n\ndataset\nn_balance\nmethod\nscore\n\n\n\n\n300\ntest\n0\ntree\n0.754286\n\n\n301\ntest\n0\nridge\n0.877143\n\n\n302\ntest\n0\nlasso\n0.868571\n\n\n303\ntest\n50\ntree\n0.762857\n\n\n304\ntest\n50\nridge\n0.725714\n\n\n...\n...\n...\n...\n...\n\n\n595\ntest\n4900\nridge\n0.591429\n\n\n596\ntest\n4900\nlasso\n0.631429\n\n\n597\ntest\n4950\ntree\n0.705714\n\n\n598\ntest\n4950\nridge\n0.608571\n\n\n599\ntest\n4950\nlasso\n0.505714\n\n\n\n\n300 rows × 4 columns\n\n\n\n\n\n5000/50\n\n100.0\n\n\n\ndf_ = np.array([[anal(generating_df(b),predictr) for predictr in predictrs] for b in range(0,100,50)])\ndf_\n\narray([[[1.        , 0.75428571],\n        [0.86666667, 0.87714286],\n        [0.85333333, 0.87142857]],\n\n       [[1.        , 0.74285714],\n        [0.93333333, 0.83142857],\n        [0.87333333, 0.87142857]]])\n\n\n\ndf_[:,:,0]\n\narray([[1.        , 0.86666667, 0.85333333],\n       [1.        , 0.93333333, 0.87333333]])\n\n\n\ndf_[:,:,1]\n\narray([[0.75428571, 0.87714286, 0.87142857],\n       [0.74285714, 0.83142857, 0.87142857]])\n\n\n\ngenerating_df(2)\n\n\n\n\n\n\n\n\ntoeic\ngpa\nemployment\nbalance0\nbalance1\n\n\n\n\n0\n135\n0.051535\n0\n-1.007441\n-0.522833\n\n\n1\n935\n0.355496\n0\n-0.441027\n0.367165\n\n\n2\n485\n2.228435\n0\n0.793840\n-1.796588\n\n\n3\n65\n1.179701\n0\n-0.023520\n-0.074258\n\n\n4\n445\n3.962356\n1\n-0.802179\n0.669743\n\n\n...\n...\n...\n...\n...\n...\n\n\n495\n280\n4.288465\n1\n1.048698\n0.464401\n\n\n496\n310\n2.601212\n1\n0.464867\n-0.826916\n\n\n497\n225\n0.042323\n0\n0.095497\n-0.008625\n\n\n498\n320\n1.041416\n0\n0.452554\n-0.639249\n\n\n499\n375\n3.626883\n1\n-0.998647\n-1.187848\n\n\n\n\n500 rows × 5 columns\n\n\n\n\n# arr = np.array(lst)\narr[:,:,0].shape, arr[:,:,1].shape\n\n((100, 3), (100, 3))"
  },
  {
    "objectID": "posts/5_STBDA2023/07wk-032.html",
    "href": "posts/5_STBDA2023/07wk-032.html",
    "title": "07wk-032: 아이스크림(교호작용) / 의사결정나무",
    "section": "",
    "text": "의사결정나무는 교호작용텀을 고려하지 않아도 알아서 잘 잡아준다.\n\n\n1. 강의영상\nhttps://youtu.be/playlist?list=PLQqh36zP38-xIhMfXInEIhFMvFPXeM3Tg&si=PFDKhFupDaWnuVsW\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport sklearn.linear_model \nimport sklearn.tree\n\n\n\n3. Data\n\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\nchoco = 40 + temp * 2.0 + np.random.randn(100)*3\nvanilla = 60 + temp * 5.0 + np.random.randn(100)*3\ndf1 = pd.DataFrame({'temp':temp,'sales':choco}).assign(type='choco')\ndf2 = pd.DataFrame({'temp':temp,'sales':vanilla}).assign(type='vanilla')\ndf_train = pd.concat([df1,df2])\ndf_train\n\n\n\n\n\n\n\n\ntemp\nsales\ntype\n\n\n\n\n0\n-4.1\n32.950261\nchoco\n\n\n1\n-3.7\n35.852524\nchoco\n\n\n2\n-3.0\n37.428335\nchoco\n\n\n3\n-1.3\n38.323681\nchoco\n\n\n4\n-0.5\n39.713362\nchoco\n\n\n...\n...\n...\n...\n\n\n95\n12.4\n119.708075\nvanilla\n\n\n96\n13.4\n129.300464\nvanilla\n\n\n97\n14.7\n136.596568\nvanilla\n\n\n98\n15.0\n136.213140\nvanilla\n\n\n99\n15.2\n135.595252\nvanilla\n\n\n\n\n200 rows × 3 columns\n\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',label='choco')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',label='vanilla')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f4deb06a680&gt;\n\n\n\n\n\n\n\n5. 분석\n- 분석1: 선형회귀\n\n# step1\nX = pd.get_dummies(df_train[['temp','type']],drop_first=True)\ny = df_train['sales']\n# step2 \npredictr = sklearn.linear_model.LinearRegression()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n#---#\nf'train score = {predictr.score(X,y):.4f}'\n\n'train score = 0.9250'\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',alpha=0.2,label='choco')\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales_hat[df_train.type=='choco'],'--',color='C0')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',alpha=0.2,label='vanilla')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales_hat[df_train.type=='vanilla'],'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f4deb0335e0&gt;\n\n\n\n\n\n–&gt; 타입에 따른 기울기 차이를 고려하지 못하고 있다. underlying function 자체를 잘 못맞추고 있어 –&gt; underfit\n- 분석2\n\n# step1\nX = pd.get_dummies(df_train[['temp','type']],drop_first=True)\ny = df_train['sales']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3\npredictr.fit(X,y)\n# step4 \ndf_train['sales_hat'] = predictr.predict(X)\n#---#\nf'train score = {predictr.score(X,y):.4f}'\n\n'train score = 0.9964'\n\n\n\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales[df_train.type=='choco'],'o',alpha=0.2,label='choco')\nplt.plot(df_train.temp[df_train.type=='choco'],df_train.sales_hat[df_train.type=='choco'],'--',color='C0')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales[df_train.type=='vanilla'],'o',alpha=0.2,label='vanilla')\nplt.plot(df_train.temp[df_train.type=='vanilla'],df_train.sales_hat[df_train.type=='vanilla'],'--',color='C1')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f4de2d99090&gt;\n\n\n\n\n\n– 적합하는 선보다 오차를 더 따라가려는 경향을 보임. –&gt; 오버핏\n* 오버피팅에 대한 제 개념: 통계에서 “관측치 = 언더라잉 + 랜덤” 으로 볼 수 있다. 모형이 설명해야할 영역은 “언더라잉” 이다. 만약에 모형이 언더라잉을 잘 설명하지 못한다면 언더피팅이고, 주어진 모형이 언더라잉을 넘어 오차항까지 설명하고 있다면 오버피팅이다.\n\n마음속의 underlying 을 간직한다 – 애매하죠?\n그 underlying 보다 잘 맞추면 오버피팅이다.\n내 마음속의 underlying 제대로 학습못하고 있다고 판단되면 모형미스 혹은 언더피팅이다.\n\n이러한 논리로 인하면 위의 의사결정나무로 적합된 결과는 오버피팅이다. (그렇지만 언더피팅보단 나을지도?)\n\n\n\n\n\n\nNote\n\n\n\n언더피팅이다 - 모형의 표현력이 부족하다 - 맞춰야할 것을 제대로 못맞춘다. - 데이터의 특징을 잘 찾아내지 못한다.\n오버피팅이다 - 모형의 표현력은 풍부하다 - 그런데 표현력이 너무 풍부해서 표현하지 말아야할 오차항도 표현한다. - 데이터의 특징만 잡는게 아니고 오차항의 특징까지 학습하고있다.\n결론은 regression tree를 사용하면 interaction term까지 잡아낸다."
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-043.html",
    "href": "posts/5_STBDA2023/11wk-043.html",
    "title": "11wk-043: 아이스크림 판매량 / 배깅",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-xFzPZja1CacdZIpcYBNrfO&si=4I4-hymlHlQjTWuy"
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-043.html#a.-원리",
    "href": "posts/5_STBDA2023/11wk-043.html#a.-원리",
    "title": "11wk-043: 아이스크림 판매량 / 배깅",
    "section": "A. 원리",
    "text": "A. 원리\n- 알고리즘\n\n80개의 샘플에서 80개를 중복을 허용하여 뽑는다.\n1에서 뽑힌 샘플들을 이용하여 tree를 적합시킨다.\n1-2를 10번 반복하고 10개의 tree의 평균값을 yhat으로 선택한다."
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-043.html#b.-plot_tree-체크",
    "href": "posts/5_STBDA2023/11wk-043.html#b.-plot_tree-체크",
    "title": "11wk-043: 아이스크림 판매량 / 배깅",
    "section": "B. plot_tree 체크",
    "text": "B. plot_tree 체크\n- 10개의 트리들의 리스트\n\ntrees = predictr.estimators_\ntrees\n\n[DecisionTreeRegressor(random_state=783597955),\n DecisionTreeRegressor(random_state=1946035466),\n DecisionTreeRegressor(random_state=1595971771),\n DecisionTreeRegressor(random_state=361190118),\n DecisionTreeRegressor(random_state=895070837),\n DecisionTreeRegressor(random_state=2145562271),\n DecisionTreeRegressor(random_state=1404453795),\n DecisionTreeRegressor(random_state=1587841865),\n DecisionTreeRegressor(random_state=1501120315),\n DecisionTreeRegressor(random_state=1158787413)]\n\n\n- 재표본데이터셋\n\npredictr.estimators_samples_[0] # (X,y)의 쌍을 80개 중복을 허용하여 뽑기 위한 인덱스\n\narray([ 3, 30, 45, 63, 18, 46, 62, 10, 24, 32, 30, 22, 28,  8,  1, 62, 50,\n       46, 25, 17,  7, 34, 51, 77,  8,  7, 38, 15, 12, 49,  3, 24, 52, 19,\n       38, 38, 62, 27,  7, 40, 21, 73, 10, 43, 50,  4, 45, 73, 42, 18, 39,\n       40, 65, 17, 54, 70,  5, 40, 20, 14, 31, 10,  6, 29,  8, 76,  1, 17,\n       24, 29, 70, 56,  5,  3,  3, 25, 24, 21, 49,  6])\n\n\n\npredictr.estimators_samples_[1]\n\narray([10, 70, 38,  4, 47, 25,  0, 67, 28, 25, 66, 60, 79,  4, 51, 44, 36,\n       32, 57, 65, 40, 28,  2, 14, 15, 25, 21, 31, 14,  1, 15, 26,  7, 50,\n       55,  2, 35, 36, 66, 76, 58, 32, 42, 21, 49, 53, 69, 42,  5, 12, 22,\n       45, 69, 39, 52,  3, 64, 75, 52, 15, 11, 45, 10, 52, 57, 33, 37, 18,\n       44, 77, 33, 15, 67, 37, 69, 39, 32, 17, 28, 48])\n\n\n\nlen(predictr.estimators_samples_[0]), len(predictr.estimators_samples_[1])\n\n(80, 80)\n\n\n\nsamples = predictr.estimators_samples_\n\n- 첫번째 트리 재현\n\nsklearn.tree.plot_tree(\n    predictr.estimators_[0],\n    feature_names=X.columns.tolist(),\n    max_depth=1\n);\n\n\n\n\n\nX_array = np.array(X)\ny_array = np.array(y)\n\n\ntree = sklearn.tree.DecisionTreeRegressor()\ntree.fit(X_array[samples[0]],y_array[samples[0]])\n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nsklearn.tree.plot_tree(\n    tree,\n    feature_names=X.columns.tolist(),\n    max_depth=1\n);\n\n\n\n\n- tree 비교 (고정된 \\(i\\))\n\ni=1\nfig,ax = plt.subplots(2,1)\n#---#\nsklearn.tree.plot_tree(\n    predictr.estimators_[i],\n    feature_names = X.columns.tolist(),\n    max_depth=1,\n    ax=ax[0])\nax[0].set_title('predictr.estimator')\n#------#\n# 직접 구현\nmy_tree= sklearn.tree.DecisionTreeRegressor()\nmy_tree.fit(X_array[samples[i]], y_array[samples[i]])\nsklearn.tree.plot_tree(\n    my_tree,\n    feature_names=X.columns.tolist(),\n    max_depth=1,\n    ax = ax[1]\n)\nax[1].set_title('my_tree');\n\n\n\n\n\n직접구현한 것은 sample이 80개로 항상 동일한데, predictr.estimator_를 쓴 것은 sample 수가 달라진다. squared error나 다른 것들은 동일.\n\n\ni=4\nfig, ax = plt.subplots(2,1)\n#---#\nsklearn.tree.plot_tree(\n    predictr.estimators_[i],\n    feature_names=X.columns,\n    max_depth=1,\n    ax=ax[0]\n)\nax[0].set_title('predictr.estimator')\n#---#\nmy_tree = sklearn.tree.DecisionTreeRegressor()\nmy_tree.fit(X_array[samples[i]],y_array[samples[i]])\nsklearn.tree.plot_tree(\n    my_tree,\n    feature_names=X.columns,\n    max_depth=1,\n    ax=ax[1]\n);\nax[1].set_title('my_tree')\n\nText(0.5, 1.0, 'my_tree')\n\n\n\n\n\n- tree 비교 (애니메이션)\n\nfig, ax = plt.subplots(2,1)\nplt.close()\n#---#\ndef func(i):\n    ax[0].clear()\n    sklearn.tree.plot_tree(\n        predictr.estimators_[i],\n        feature_names=X.columns.tolist(),\n        max_depth=1,\n        ax=ax[0]\n    )\n    ax[0].set_title('predictr.estimator')\n    #---#\n    ax[1].clear()\n    my_tree = sklearn.tree.DecisionTreeRegressor()\n    my_tree.fit(X_array[samples[i]],y_array[samples[i]])\n    sklearn.tree.plot_tree(\n        my_tree,\n        feature_names=X.columns.tolist(),\n        max_depth=1,\n        ax=ax[1]\n    );\n    ax[1].set_title('my_tree')\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=10)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-043.html#c.-resampling-fit",
    "href": "posts/5_STBDA2023/11wk-043.html#c.-resampling-fit",
    "title": "11wk-043: 아이스크림 판매량 / 배깅",
    "section": "C. ReSampling + Fit",
    "text": "C. ReSampling + Fit\n- 고정된 \\(i\\)\n\ni=4\nplt.plot(X,y,'o',alpha=0.2,color='gray') # 전체 데이터\nplt.plot(X_array[samples[i]],y_array[samples[i]],'o',alpha=1/3) # sampling된 데이터 (중복 샘플링되면 진하게됨.)\nplt.plot(X,trees[i].predict(X),'--')\n\n\n\n\n- 애니매이션\n\n#---#\nfig = plt.figure()\nax = fig.gca() \nplt.close()\n#---#\ndef func(i):\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.2,color='gray')\n    ax.plot(X_array[samples[i]],y_array[samples[i]],'o',alpha=1/3)\n    ax.plot(X,trees[i].predict(X),'--')\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=10)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n어떠한 점들이 선택되느냐에 따라 적합된 선들의 모양도 달라짐을 확인.\n그 다음 과정은 10개의 주황색 선들의 평균을 내면 된다."
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-043.html#d.-앙상블결과-재현",
    "href": "posts/5_STBDA2023/11wk-043.html#d.-앙상블결과-재현",
    "title": "11wk-043: 아이스크림 판매량 / 배깅",
    "section": "D. 앙상블결과 재현",
    "text": "D. 앙상블결과 재현\n- 최종결과물 (손으로..)\n\npredictr.predict(X)\n\narray([13.15786153, 14.70899254, 16.17470549, 18.10474381, 19.17855259,\n       19.69124728, 15.86986442, 16.03557298, 16.03557298, 20.994342  ,\n       20.994342  , 21.62828924, 21.62828924, 23.44904538, 20.26265835,\n       24.71409475, 24.71409475, 20.46725702, 20.46725702, 26.30357653,\n       26.30357653, 26.30357653, 30.51454393, 27.17822586, 27.17822586,\n       27.17822586, 27.77317135, 27.77317135, 30.0634251 , 30.41422328,\n       29.76338913, 29.76338913, 31.52530765, 30.77355469, 31.60243782,\n       30.86867662, 29.76497122, 31.84198803, 31.84198803, 31.16793067,\n       29.91311756, 29.91311756, 29.91311756, 30.11931519, 30.11931519,\n       35.15885723, 36.11027916, 34.80670456, 34.80670456, 37.19617303,\n       37.19617303, 33.62897832, 33.62897832, 33.42700287, 32.09068107,\n       32.09068107, 36.07894214, 36.52173853, 35.32420594, 35.32420594,\n       39.89219733, 39.89219733, 37.26500621, 36.12326642, 40.3355798 ,\n       40.117366  , 40.89540532, 42.498049  , 42.28532559, 40.28217829,\n       39.3641812 , 41.2223313 , 40.07251328, 43.35675877, 46.02512541,\n       48.41943342, 42.97619508, 44.31910683, 46.69551329, 45.87614746])\n\n\n\nnp.stack([tree.predict(X) for tree in predictr.estimators_]).shape # 평균을 취해서 첫번째 축을 날리자.\n\n(10, 80)\n\n\n\nnp.stack([tree.predict(X) for tree in predictr.estimators_]).mean(axis=0)\n\narray([13.15786153, 14.70899254, 16.17470549, 18.10474381, 19.17855259,\n       19.69124728, 15.86986442, 16.03557298, 16.03557298, 20.994342  ,\n       20.994342  , 21.62828924, 21.62828924, 23.44904538, 20.26265835,\n       24.71409475, 24.71409475, 20.46725702, 20.46725702, 26.30357653,\n       26.30357653, 26.30357653, 30.51454393, 27.17822586, 27.17822586,\n       27.17822586, 27.77317135, 27.77317135, 30.0634251 , 30.41422328,\n       29.76338913, 29.76338913, 31.52530765, 30.77355469, 31.60243782,\n       30.86867662, 29.76497122, 31.84198803, 31.84198803, 31.16793067,\n       29.91311756, 29.91311756, 29.91311756, 30.11931519, 30.11931519,\n       35.15885723, 36.11027916, 34.80670456, 34.80670456, 37.19617303,\n       37.19617303, 33.62897832, 33.62897832, 33.42700287, 32.09068107,\n       32.09068107, 36.07894214, 36.52173853, 35.32420594, 35.32420594,\n       39.89219733, 39.89219733, 37.26500621, 36.12326642, 40.3355798 ,\n       40.117366  , 40.89540532, 42.498049  , 42.28532559, 40.28217829,\n       39.3641812 , 41.2223313 , 40.07251328, 43.35675877, 46.02512541,\n       48.41943342, 42.97619508, 44.31910683, 46.69551329, 45.87614746])\n\n\n\npredictr.predict(X)와 값이 동일함을 확인.\n\n- 최종결과물 (코드로 정리)\n\ndef ensemble(trees,i=None):\n    if i is None:\n        i = len(trees) # 전체 트리를 앙상블 (10개)\n    else: \n        i = i+1\n    yhat = np.stack([tree.predict(X) for tree in trees[:i]]).mean(axis=0) # i번째 트리까지만 앙상블.\n    return yhat\n\n\nlen(trees) # 지금까는 10개의 트리를 앙상블해서 결과를 냈었음.\n\n10\n\n\n\nensemble(trees)\n\narray([13.15786153, 14.70899254, 16.17470549, 18.10474381, 19.17855259,\n       19.69124728, 15.86986442, 16.03557298, 16.03557298, 20.994342  ,\n       20.994342  , 21.62828924, 21.62828924, 23.44904538, 20.26265835,\n       24.71409475, 24.71409475, 20.46725702, 20.46725702, 26.30357653,\n       26.30357653, 26.30357653, 30.51454393, 27.17822586, 27.17822586,\n       27.17822586, 27.77317135, 27.77317135, 30.0634251 , 30.41422328,\n       29.76338913, 29.76338913, 31.52530765, 30.77355469, 31.60243782,\n       30.86867662, 29.76497122, 31.84198803, 31.84198803, 31.16793067,\n       29.91311756, 29.91311756, 29.91311756, 30.11931519, 30.11931519,\n       35.15885723, 36.11027916, 34.80670456, 34.80670456, 37.19617303,\n       37.19617303, 33.62897832, 33.62897832, 33.42700287, 32.09068107,\n       32.09068107, 36.07894214, 36.52173853, 35.32420594, 35.32420594,\n       39.89219733, 39.89219733, 37.26500621, 36.12326642, 40.3355798 ,\n       40.117366  , 40.89540532, 42.498049  , 42.28532559, 40.28217829,\n       39.3641812 , 41.2223313 , 40.07251328, 43.35675877, 46.02512541,\n       48.41943342, 42.97619508, 44.31910683, 46.69551329, 45.87614746])\n\n\n\nensemble(trees,0) # 0번트리만 적용\n\narray([14.0025235 , 14.0025235 , 14.0025235 , 17.67368103, 19.46336233,\n       20.31785349, 15.76077374, 16.03557298, 16.03557298, 21.77077557,\n       21.77077557, 21.21928031, 21.21928031, 21.21928031, 18.34698175,\n       27.5369675 , 27.5369675 , 20.49894593, 20.49894593, 25.61549372,\n       25.61549372, 25.61549372, 32.42440294, 29.79960742, 29.79960742,\n       29.79960742, 25.20513841, 25.20513841, 29.55903213, 30.75418385,\n       28.16433125, 28.16433125, 31.45007539, 32.89828946, 32.89828946,\n       33.12203011, 33.12203011, 33.12203011, 33.12203011, 30.60313283,\n       30.71633217, 30.71633217, 30.71633217, 30.60789344, 30.60789344,\n       36.55080009, 36.5245913 , 37.4829917 , 37.4829917 , 37.4829917 ,\n       37.4829917 , 33.85762996, 33.85762996, 36.21138267, 36.21138267,\n       36.21138267, 40.4283271 , 40.4283271 , 36.20740045, 36.20740045,\n       36.20740045, 36.20740045, 36.20740045, 34.68877582, 34.68877582,\n       39.1744058 , 39.1744058 , 39.1744058 , 39.1744058 , 39.0491277 ,\n       39.0491277 , 39.0491277 , 41.43992372, 41.43992372, 41.43992372,\n       41.43992372, 42.30473921, 45.66201858, 45.66201858, 45.66201858])\n\n\n\nensemble(trees,1) # 0번트리,1번트리의 예측값 평균\n\narray([12.45139248, 14.0025235 , 14.96542912, 17.67368103, 19.46336233,\n       20.31785349, 16.3062271 , 16.44362672, 16.44362672, 21.77077557,\n       21.77077557, 21.4507948 , 21.4507948 , 21.4507948 , 18.34698175,\n       27.5369675 , 27.5369675 , 20.40387921, 20.40387921, 26.44598288,\n       26.44598288, 26.44598288, 32.42440294, 28.58170816, 28.58170816,\n       28.58170816, 27.01131669, 27.01131669, 29.55903213, 30.15660799,\n       28.93512858, 28.93512858, 31.45007539, 30.82608162, 30.82608162,\n       32.12353136, 29.5386332 , 32.19685772, 32.19685772, 30.60313283,\n       30.06490223, 30.06490223, 30.06490223, 30.50546415, 30.50546415,\n       36.55080009, 34.41250129, 34.89170149, 34.89170149, 37.4829917 ,\n       37.4829917 , 33.58414329, 33.58414329, 35.00048583, 32.40765812,\n       32.40765812, 34.51613033, 38.50616836, 35.3418485 , 35.3418485 ,\n       38.85141486, 38.85141486, 38.85141486, 39.58329188, 39.58329188,\n       39.1744058 , 39.68533784, 41.02087424, 41.02087424, 39.92694722,\n       39.0491277 , 39.0491277 , 40.24452571, 46.12683257, 46.12683257,\n       46.12683257, 42.30473921, 45.66201858, 45.83497788, 45.83497788])"
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-043.html#e.-학습과정-시각화",
    "href": "posts/5_STBDA2023/11wk-043.html#e.-학습과정-시각화",
    "title": "11wk-043: 아이스크림 판매량 / 배깅",
    "section": "E. 학습과정 시각화",
    "text": "E. 학습과정 시각화\n- 고정된 \\(i\\)\n\ni=0\nfig,ax = plt.subplots(1,4, figsize=(8,2))\n\nax[0].set_title(\"Step0\")\nax[0].plot(X,y,'o',color='gray',alpha=0.2)\n\nax[1].set_title(\"Step1:Resampling\")\nax[1].plot(X,y,'o',color='gray',alpha=0.2)\nax[1].plot(X_array[samples[i]], y_array[samples[i]],'o',alpha=1/3)\n\nax[2].set_title(\"Step2:Fit\")\nax[2].plot(X,y,'o',color='gray',alpha=0.2)\nax[2].plot(X_array[samples[i]], y_array[samples[i]],'o',alpha=1/3)\nax[2].plot(X, trees[i].predict(X),'--')\n\nax[3].set_title(\"Step3:Update(??)\")\nax[3].plot(X,y,'o',color='gray', alpha=0.2)\nax[3].plot(X,ensemble(trees,i),'--',color='C1') # tree들을 앙상블한 결과.\nplt.tight_layout()\n\n\n\n\n\ni=9\nfig,ax = plt.subplots(1,4,figsize=(8,2))\n#--#\nax[0].set_title(\"Step0\")\nax[0].plot(X,y,'o',color='gray',alpha=0.2)\n#--#\nax[1].set_title(\"Step1:ReSampling\")\nax[1].plot(X,y,'o',color='gray',alpha=0.2)\nax[1].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\n#--#\nax[2].set_title(\"Step2:Fit\")\nax[2].plot(X,y,'o',color='gray',alpha=0.2)\nax[2].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\nax[2].plot(X,trees[i].predict(X),'--')\n#--#\nax[3].set_title(\"Step3:Update(?)\")\nax[3].plot(X,y,'o',color='gray',alpha=0.2)\nax[3].plot(X,ensemble(trees,i),'--',color='C1')\n\n\n\n\n- 애니메이션\n\nfig,ax = plt.subplots(1,4,figsize=(12,4))\nplt.close()\n#--#\ndef func(i):\n    for a in ax:\n        a.clear()\n    #--#\n    ax[0].set_title(\"Step0\")\n    ax[0].plot(X,y,'o',color='gray',alpha=0.2)\n    #--#\n    ax[1].set_title(\"Step1:ReSampling\")\n    ax[1].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[1].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\n    #--#\n    ax[2].set_title(\"Step2:Fit\")\n    ax[2].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[2].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\n    ax[2].plot(X,trees[i].predict(X),'--')\n    #--#\n    ax[3].set_title(\"Step3:Update(?)\")\n    ax[3].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[3].plot(X,ensemble(trees,i),'--',color='C1')\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=10)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n진행할수록 오버피팅이 줄어드는 것을 확인할 수 있음.\n\n\nfig,ax = plt.subplots(1,4,figsize=(8,2))\nplt.close()\n#---#\ndef func(i):\n    for a in ax:\n        a.clear()\n    #--#\n    ax[0].set_title(\"Step0\")\n    ax[0].plot(X,y,'o',color='gray',alpha=0.2)\n    #--#\n    ax[1].set_title(\"Step1:ReSampling\")\n    ax[1].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[1].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\n    #--#\n    ax[2].set_title(\"Step2:Fit\")\n    ax[2].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[2].plot(X_array[samples[i]],y[samples[i]],'o',alpha=1/3)\n    ax[2].plot(X,trees[i].predict(X),'--')\n    #--#\n    ax[3].set_title(\"Step3:Update(?)\")\n    ax[3].plot(X,y,'o',color='gray',alpha=0.2)\n    ax[3].plot(X,ensemble(trees,i),'--',color='C1')\n#---#\nani = matplotlib.animation.FuncAnimation(fig,func,frames=10)\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- tree의 개수 조정\ntree의 개수를 꼭 10개로 안해도 된다. n_estimators=?? 옵션을 사용하면 된다.\n\nX = df_train[['temp']]\ny = df_train['sales']\npredictr = sklearn.ensemble.BaggingRegressor(n_estimators=15)\npredictr.fit(X,y)\n\nBaggingRegressor(n_estimators=15)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BaggingRegressorBaggingRegressor(n_estimators=15)\n\n\n\npredictr.n_estimators\n\n15"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-009.out.html",
    "href": "posts/5_STBDA2023/03wk-009.out.html",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-zXz97SY6e8rSowprIvDS2a&si=Y8pI1Dyw2VaYeayc"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-009.out.html#a.-질문",
    "href": "posts/5_STBDA2023/03wk-009.out.html#a.-질문",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "A. 질문",
    "text": "A. 질문\n- 질문: 기온이 \\(x=-2.0\\) 일 때 아이스크림을 얼마정도 판다고 보는게 타당할까?"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-009.out.html#b.-답1",
    "href": "posts/5_STBDA2023/03wk-009.out.html#b.-답1",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "B. 답1",
    "text": "B. 답1\n- \\(x=-2.0\\) 근처의 데이터를 살펴보자.\n\ndf[(-3.0 &lt; df.temp) & (df.temp &lt; -1.0)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n3\n-1.3\n17.673681\n\n\n\n\n\n\n\n대충 17.67 근처이지 않을까?.."
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-009.out.html#c.-답2",
    "href": "posts/5_STBDA2023/03wk-009.out.html#c.-답2",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "C. 답2",
    "text": "C. 답2\n- 자료를 바탕으로 그림을 그려보자.\n\nplt.plot(df.temp,df.sales,'o')\nplt.plot([-2.0],[17.67],'x')\n\n\n\n\n\n저거 보다 못팔 것 같은데?"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-009.out.html#d.-아이디어",
    "href": "posts/5_STBDA2023/03wk-009.out.html#d.-아이디어",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "D. 아이디어",
    "text": "D. 아이디어\n- 선을 기가 막히게 그어서 추세선을 만들고, 그 추세선 위의 점으로 예측하자.\n- 속마음: 사실 추세선을 알고 있긴함\n\nplt.plot(df.temp,df.sales,'o')\nplt.plot(df.temp,20+df.temp*2.5,'--')\n\n\n\n\n- 사실 \\(y=20+2.5x\\) 라는 추세선을 그으면 된다는 것을 알고 있다.\n- 그래서 \\(x=-2\\) 이라면 \\(y=20-2.5\\times 2=15\\) 라고 보는게 합리적임. (물론 오차가 있을 수 있지만 그건 운이므로 어쩔수 없는것임, 랜덤으로 뭐가 나올지까지 맞출 수는 없음[1])\n- 그렇지만 우리는 사실 \\(20, 2.5\\) 라는 숫자를 모른다. (이 숫자만 안다면 임의의 \\(x\\)에 대한 \\(y\\)값을 알 수 있을 텐데…)\n- 게임셋팅\n\n원래게임: 임의의 \\(x\\)에 대하여 합리적인 \\(y\\)를 잘 찾는 게임\n변형된게임: \\(20,2.5\\) 라는 숫자를 잘 찾는 게임, 즉 데이터를 보고 최대한 \\(y_i \\approx ax_i+b\\) 이 되도록 \\(a,b\\)를 잘 선택하는 게임"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-009.out.html#a.-데이터",
    "href": "posts/5_STBDA2023/03wk-009.out.html#a.-데이터",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 변수 설정\n[1] 만약 그렇다면 랜덤이 아니겠지?\n\nX = df[['temp']] # 독립변수, 설명변수, 피쳐\ny = df[['sales']] # 종속변수, 반응변수, 타겟 \n\n\nplt.plot(X,y,'o')\n\n\n\n\n- 질문: 기온이 \\(x=-2.0\\) 일 때 아이스크림을 얼마정도 판다고 보는게 타당할까?\n\ndf[(-3.0 &lt; df.temp) & (df.temp &lt; -1.0)]\n\n\n\n\n\n\n\n\ntemp\nsales\n\n\n\n\n3\n-1.3\n17.673681\n\n\n\n\n\n\n\n- 답1: 대충 17.67 근처이지 않을까?..\n- 답2: 17.67 보다 작지 않을까?\n- 아이디어: 추세선을 그리고 거기서 예측해보면 어떨까?\n- 데이터를 학습하여 추세선을 적절히 그릴 수 있고, 그려진 추세선으로 예측까지 해줄수 있는 아이(predictor)를 만들자."
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-009.out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/03wk-009.out.html#b.-predictor-생성",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LinearRegression() \npredictr \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlinear regression 이라는 방법으로 추세선을 만들고 예측하는 아이(predictor)를 만드는 코드"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-009.out.html#c.-학습-fit-learn",
    "href": "posts/5_STBDA2023/03wk-009.out.html#c.-학습-fit-learn",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-009.out.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/03wk-009.out.html#d.-예측-predict",
    "title": "03wk-009: 아이스크림, 회귀분석",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- Predictor: 데이터를 살펴보니 원래 true는 이럴것 같아요\n\nyhat = predictr.predict(X)\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat, 'o--',alpha=0.5)\n\n\n\n\n- 최규빈: 저런 추세선을 그었다면, \\(y=ax+b\\) 꼴의 식에서 \\(a\\), \\(b\\)를 적당한 값으로 찾았다는 의미인데, 그 값은 어디있지?\n- Predictor: 아래에 있어요\n\na = predictr.coef_,\nb = predictr.intercept_\n\n\na,b\n\n((array([[2.51561216]]),), array([19.66713127]))\n\n\n- 최규빈: 확인해보자..\n\n(df.temp * 2.51561216 + 19.66713127)[:5], yhat[:5]\n\n(0     9.353121\n 1    10.359366\n 2    12.120295\n 3    16.396835\n 4    18.409325\n Name: temp, dtype: float64,\n array([[ 9.35312141],\n        [10.35936628],\n        [12.12029479],\n        [16.39683546],\n        [18.40932519]]))\n\n\n- 새로운 데이터 \\(x=-2\\) 에 대한 예측 (1) – 수식위주로\n\n2.51561216*(-2) + 19.66713127\n\n14.635906949999999\n\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat,'--',alpha=0.5)\nplt.plot([-2],[14.635906949999999],'xr') # -2에 대한 예측\n\n\n\n\n- 새로운 데이터 \\(x=-2\\) 에 대한 예측 (2) – 코드위주로 (\\(\\star\\))\n\nXnew = pd.DataFrame({'temp':[-2.0]})\nXnew\n\n\n\n\n\n\n\n\ntemp\n\n\n\n\n0\n-2.0\n\n\n\n\n\n\n\n\npredictr.predict(Xnew)\n\narray([[14.63590695]])\n\n\n\nplt.plot(X,y,'o',alpha=0.5)\nplt.plot(X,yhat,'--',alpha=0.5)\nplt.plot(Xnew, predictr.predict(Xnew),'xr')"
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-018.html#a.-학습-이후에-예측평가-가능",
    "href": "posts/5_STBDA2023/04wk-018.html#a.-학습-이후에-예측평가-가능",
    "title": "04wk-018: Predictor 깊은 이해 + 기호정리",
    "section": "A. 학습 이후에 예측/평가 가능",
    "text": "A. 학습 이후에 예측/평가 가능\n- Predictor의 list생성\n\n[sklearn.linear_model.LinearRegression(), sklearn.linear_model.LinearRegression()]\n\n[LinearRegression(), LinearRegression()]\n\n\n\npredictors = [sklearn.linear_model.LinearRegression() for i in range(2)]\npredictors\n\n[LinearRegression(), LinearRegression()]\n\n\n\n두개의 predictor를 만들어서 리스트로 정리함\n\n- 첫번째 predictor에 접근\n\npredictors[0]\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 두번째 predictor에 접근\n\npredictors[1]\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 첫번째 predictor를 학습\n\npredictors[0].fit(df_train_X,df_train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n- 학습이후에는 .coef_, .intercept_ 값이 생성됨\n\npredictors[0].coef_, predictors[0].intercept_\n\n(array([[-1.01643449]]), array([21.40701511]))\n\n\n\npredictors[1].coef_, predictors[1].intercept_\n\nAttributeError: 'LinearRegression' object has no attribute 'coef_'\n\n\n- .coef_와 .intercept_값이 생겨야 .predict(X)를 통하여 예측을 할 수 있음\n\npredictors[0].predict(df_train_X)\n\narray([[ 1.07832538],\n       [ 0.0618909 ],\n       [-0.95454359],\n       [-1.97097808],\n       [-2.98741256],\n       [-4.00384705],\n       [-5.02028154],\n       [-6.03671602]])\n\n\n\npredictors[1].predict(df_train_X) # 학습이 안되어있으므로 당연히 에러!\n\nNotFittedError: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n\n\n- 예측을 해야 평가를 할 수 있음\n\npredictors[0].score(df_train_X,df_train_y)\n\n0.9980028392074787\n\n\n\npredictors[1].score(df_train_X,df_train_y)\n\nNotFittedError: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
  },
  {
    "objectID": "posts/5_STBDA2023/04wk-018.html#b.-.fitxy에서-xy의-형식",
    "href": "posts/5_STBDA2023/04wk-018.html#b.-.fitxy에서-xy의-형식",
    "title": "04wk-018: Predictor 깊은 이해 + 기호정리",
    "section": "B. .fit(X,y)에서 X,y의 형식",
    "text": "B. .fit(X,y)에서 X,y의 형식\n\npredictr = predictors[0]\npredictr\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\npredictr.fit(df_train_X, np.array(df_train_y).reshape(-1)) # 이건 잘 됨.\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ndf_train_X.shape, np.array(df_train_X).reshape(-1).shape\n\n((8, 1), (8,))\n\n\n\nnp.array(df_train_X).tolist(), np.array(df_train_X).reshape(-1).tolist()\n\n([[20], [21], [22], [23], [24], [25], [26], [27]],\n [20, 21, 22, 23, 24, 25, 26, 27])\n\n\n\npredictr.fit(np.array(df_train_X).reshape(-1), np.array(df_train_y).reshape(-1)) # 이건 안됨!\n\nValueError: Expected 2D array, got 1D array instead:\narray=[20 21 22 23 24 25 26 27].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\n\n- 어떤 경우에 에러가 나고 안나는지 관찰해보자.\n\nXs = {'DataFrame(2d)': df_train_X, \n      'Seires(1d)': df_train_X.X,\n      'ndarray(2d)': np.array(df_train_X),\n      'ndarray(1d)': np.array(df_train_X).reshape(-1),\n      'list(2d)': np.array(df_train_X).tolist(),\n      'list(1d)': np.array(df_train_X).reshape(-1).tolist()}\n\n\nys = {'DataFrame(2d)': df_train_y, \n      'Seires(1d)': df_train_y.y,\n      'ndarray(2d)': np.array(df_train_y),\n      'ndarray(1d)': np.array(df_train_y).reshape(-1),\n      'list(2d)': np.array(df_train_y).tolist(),\n      'list(1d)': np.array(df_train_y).reshape(-1).tolist()}\n\n\ndef test(X,y):\n    try: \n        predictr.fit(X,y)\n        return 'no error'\n    except: # 예외상황 발생.\n        return 'error'\n\n\nX = np.array(df_train_X).reshape(-1)\ny = np.array(df_train_y).reshape(-1)\n\n\ntest(X,y)\n\n'error'\n\n\n\n{('X='+i,'y='+j): test(Xs[i],ys[j]) for i,j in itertools.product(Xs.keys(),ys.keys())}\n\n{('X=DataFrame(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=Seires(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(2d)'): 'no error',\n ('X=DataFrame(2d)', 'y=list(1d)'): 'no error',\n ('X=Seires(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=Seires(1d)', 'y=Seires(1d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(2d)'): 'error',\n ('X=Seires(1d)', 'y=ndarray(1d)'): 'error',\n ('X=Seires(1d)', 'y=list(2d)'): 'error',\n ('X=Seires(1d)', 'y=list(1d)'): 'error',\n ('X=ndarray(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=Seires(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(2d)'): 'no error',\n ('X=ndarray(2d)', 'y=list(1d)'): 'no error',\n ('X=ndarray(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=ndarray(1d)', 'y=Seires(1d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(2d)'): 'error',\n ('X=ndarray(1d)', 'y=ndarray(1d)'): 'error',\n ('X=ndarray(1d)', 'y=list(2d)'): 'error',\n ('X=ndarray(1d)', 'y=list(1d)'): 'error',\n ('X=list(2d)', 'y=DataFrame(2d)'): 'no error',\n ('X=list(2d)', 'y=Seires(1d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(2d)'): 'no error',\n ('X=list(2d)', 'y=ndarray(1d)'): 'no error',\n ('X=list(2d)', 'y=list(2d)'): 'no error',\n ('X=list(2d)', 'y=list(1d)'): 'no error',\n ('X=list(1d)', 'y=DataFrame(2d)'): 'error',\n ('X=list(1d)', 'y=Seires(1d)'): 'error',\n ('X=list(1d)', 'y=ndarray(2d)'): 'error',\n ('X=list(1d)', 'y=ndarray(1d)'): 'error',\n ('X=list(1d)', 'y=list(2d)'): 'error',\n ('X=list(1d)', 'y=list(1d)'): 'error'}\n\n\n- 결론: X는 2d만 가능, y는 2d,1d 모두 가능\n- itertools 사용법 (참고)\n(예제1)\n\nfor i,j in itertools.product(['A','B'], [1,2,3]): # iterable object\n    print(i,j)\n\nA 1\nA 2\nA 3\nB 1\nB 2\nB 3\n\n\n(예제2)\n\nX = np.array(df_train_X).reshape(-1)\ny = np.array(df_train_y).reshape(-1)\n\nXs = {'DataFrame(2D)':[1,2,3], 'Series(1d)':[2,3,4]}\nys = {'A':1, 'B':2, 'C':3}\n\n\nfor i,j in itertools.product(Xs.keys(), ys.keys()):\n    print(i,j)\n\nDataFrame(2D) A\nDataFrame(2D) B\nDataFrame(2D) C\nSeries(1d) A\nSeries(1d) B\nSeries(1d) C\n\n\n\nfor i,j in itertools.product(Xs.keys(), ys.keys()):\n    print(Xs[i],ys[j])\n\n[1, 2, 3] 1\n[1, 2, 3] 2\n[1, 2, 3] 3\n[2, 3, 4] 1\n[2, 3, 4] 2\n[2, 3, 4] 3\n\n\n(예제3)\n\nXs = {'DataFrame(2d)':df_train_X,\n      'Series(1d)': df_train_X.X}\n\nys = {'DataFrame(2d)': df_train_y,\n      'Series(1d)': df_train_y.y}\n\n\nfor i,j in itertools.product(Xs, ys):\n    print(Xs[i], ys[j])\n\n    X\n0  20\n1  21\n2  22\n3  23\n4  24\n5  25\n6  26\n7  27           y\n0  1.159315\n1  0.011628\n2 -0.913198\n3 -1.953295\n4 -3.138916\n5 -3.976391\n6 -5.163496\n7 -5.859210\n    X\n0  20\n1  21\n2  22\n3  23\n4  24\n5  25\n6  26\n7  27 0    1.159315\n1    0.011628\n2   -0.913198\n3   -1.953295\n4   -3.138916\n5   -3.976391\n6   -5.163496\n7   -5.859210\nName: y, dtype: float64\n0    20\n1    21\n2    22\n3    23\n4    24\n5    25\n6    26\n7    27\nName: X, dtype: int64           y\n0  1.159315\n1  0.011628\n2 -0.913198\n3 -1.953295\n4 -3.138916\n5 -3.976391\n6 -5.163496\n7 -5.859210\n0    20\n1    21\n2    22\n3    23\n4    24\n5    25\n6    26\n7    27\nName: X, dtype: int64 0    1.159315\n1    0.011628\n2   -0.913198\n3   -1.953295\n4   -3.138916\n5   -3.976391\n6   -5.163496\n7   -5.859210\nName: y, dtype: float64\n\n\n\nfor i,j in itertools.product(Xs, ys):\n    print(test(Xs[i], ys[j]))\n\nno error\nno error\nerror\nerror"
  },
  {
    "objectID": "posts/5_STBDA2023/11wk-041.html",
    "href": "posts/5_STBDA2023/11wk-041.html",
    "title": "11wk-041: Medical Cost / 의사결정나무 max_feature,random_state",
    "section": "",
    "text": "1. 강의영상\n\nhttps://youtu.be/playlist?list=PLQqh36zP38-yQ9Pe1jX-NSI2FqYZn30co&si=ura_7Fm-18S4vxx1\n\n\n\n2. Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn.tree\nimport graphviz\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n3. 데이터준비\n\ndf_train = pd.read_csv('https://raw.githubusercontent.com/guebin/MP2023/main/posts/insurance.csv')\ndf_train\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n\n\n\n\n1338 rows × 7 columns\n\n\n\n\n\n4. max_features\n- max_features에 대한 제한을 주지 않음 \\(\\to\\) 항상 같은 결과가 나옴\n\n# step1\nX = pd.get_dummies(df_train.loc[:,'age':'region'],drop_first=True)\ny = df_train['charges']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor()\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor()\n\n\n\nsklearn.tree.plot_tree(predictr,max_depth=0,feature_names=X.columns.tolist());\n\n\n\n\n- max_features=4로 제한\n\nlen(X.columns) \n\n8\n\n\n\nmax_features=4로 제한한다는 의미는 8개의 설명변수중에서 4개만 임의로 뽑아서 그중에서 “최적의 변수”와 “최적의 \\(c\\)”를 찾겠다는 의미\n\n\n# step1\nX = pd.get_dummies(df_train.loc[:,'age':'region'],drop_first=True)\ny = df_train['charges']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor(max_features=4)\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \n\nDecisionTreeRegressor(max_features=4)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_features=4)\n\n\n\nsklearn.tree.plot_tree(predictr,max_depth=0,feature_names=X.columns.tolist());\n\n\n\n\n\n절반정도는 smoking 유무가 가장 위에 위치한다. (흡연, 흡연, 나이…)\n\n\n\n5. random_state\n- max_features=4로 제한\n\n# step1\nX = pd.get_dummies(df_train.loc[:,'age':'region'],drop_first=True)\ny = df_train['charges']\n# step2 \npredictr = sklearn.tree.DecisionTreeRegressor(max_features=4,random_state=43)\n# step3 \npredictr.fit(X,y)\n# step4 -- pass \n\nDecisionTreeRegressor(max_features=4, random_state=43)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_features=4, random_state=43)\n\n\n\nsklearn.tree.plot_tree(predictr,max_depth=0,feature_names=X.columns);"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-yR3MqhN9-OgAtewojoYoKD&si=U1GTMdGiWFvlppYH"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-1-get-started",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-1-get-started",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 1: Get started",
    "text": "Part 1: Get started\nIn this section, you’ll learn more about the competition and make your first submission.\n\nJoin the competition!\nThe first thing to do is to join the competition! Open a new window with the competition page, and click on the “Join Competition” button, if you haven’t already. (If you see a “Submit Predictions” button instead of a “Join Competition” button, you have already joined the competition, and don’t need to do so again.)\n\nThis takes you to the rules acceptance page. You must accept the competition rules in order to participate. These rules govern how many submissions you can make per day, the maximum team size, and other competition-specific details. Then, click on “I Understand and Accept” to indicate that you will abide by the competition rules.\n\n\nThe challenge\nThe competition is simple: we want you to use the Titanic passenger data (name, age, price of ticket, etc) to try to predict who will survive and who will die.\n\n\nThe data\nTo take a look at the competition data, click on the Data tab at the top of the competition page. Then, scroll down to find the list of files.\nThere are three files in the data: (1) train.csv, (2) test.csv, and (3) gender_submission.csv.\n\n(1) train.csv\ntrain.csv contains the details of a subset of the passengers on board (891 passengers, to be exact – where each passenger gets a different row in the table). To investigate this data, click on the name of the file on the left of the screen. Once you’ve done this, you can view all of the data in the window.\n\nThe values in the second column (“Survived”) can be used to determine whether each passenger survived or not: - if it’s a “1”, the passenger survived. - if it’s a “0”, the passenger died.\nFor instance, the first passenger listed in train.csv is Mr. Owen Harris Braund. He was 22 years old when he died on the Titanic.\n\n\n(2) test.csv\nUsing the patterns you find in train.csv, you have to predict whether the other 418 passengers on board (in test.csv) survived.\nClick on test.csv (on the left of the screen) to examine its contents. Note that test.csv does not have a “Survived” column - this information is hidden from you, and how well you do at predicting these hidden values will determine how highly you score in the competition!\n\n\n(3) gender_submission.csv\nThe gender_submission.csv file is provided as an example that shows how you should structure your predictions. It predicts that all female passengers survived, and all male passengers died. Your hypotheses regarding survival will probably be different, which will lead to a different submission file. But, just like this file, your submission should have: - a “PassengerId” column containing the IDs of each passenger from test.csv. - a “Survived” column (that you will create!) with a “1” for the rows where you think the passenger survived, and a “0” where you predict that the passenger died."
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-2-your-coding-environment",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-2-your-coding-environment",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 2: Your coding environment",
    "text": "Part 2: Your coding environment\nIn this section, you’ll train your own machine learning model to improve your predictions. If you’ve never written code before or don’t have any experience with machine learning, don’t worry! We don’t assume any prior experience in this tutorial.\n\nThe Notebook\nThe first thing to do is to create a Kaggle Notebook where you’ll store all of your code. You can use Kaggle Notebooks to getting up and running with writing code quickly, and without having to install anything on your computer. (If you are interested in deep learning, we also offer free GPU access!)\nBegin by clicking on the Code tab on the competition page. Then, click on “New Notebook”.\n\nYour notebook will take a few seconds to load. In the top left corner, you can see the name of your notebook – something like “kernel2daed3cd79”.\n\nYou can edit the name by clicking on it. Change it to something more descriptive, like “Getting Started with Titanic”.\n\n\n\nYour first lines of code\nWhen you start a new notebook, it has two gray boxes for storing code. We refer to these gray boxes as “code cells”.\n\nThe first code cell already has some code in it. To run this code, put your cursor in the code cell. (If your cursor is in the right place, you’ll notice a blue vertical line to the left of the gray box.) Then, either hit the play button (which appears to the left of the blue line), or hit [Shift] + [Enter] on your keyboard.\nIf the code runs successfully, three lines of output are returned. Below, you can see the same code that you just ran, along with the output that you should see in your notebook.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nThis shows us where the competition data is stored, so that we can load the files into the notebook. We’ll do that next.\n\n\nLoad the data\nThe second code cell in your notebook now appears below the three lines of output with the file locations.\n\nType the two lines of code below into your second code cell. Then, once you’re done, either click on the blue play button, or hit [Shift] + [Enter].\n\n# train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data = pd.read_csv('./titanic/train.csv')\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nYour code should return the output above, which corresponds to the first five rows of the table in train.csv. It’s very important that you see this output in your notebook before proceeding with the tutorial! &gt; If your code does not produce this output, double-check that your code is identical to the two lines above. And, make sure your cursor is in the code cell before hitting [Shift] + [Enter].\nThe code that you’ve just written is in the Python programming language. It uses a Python “module” called pandas (abbreviated as pd) to load the table from the train.csv file into the notebook. To do this, we needed to plug in the location of the file (which we saw was /kaggle/input/titanic/train.csv).\n&gt; If you’re not already familiar with Python (and pandas), the code shouldn’t make sense to you – but don’t worry! The point of this tutorial is to (quickly!) make your first submission to the competition. At the end of the tutorial, we suggest resources to continue your learning.\nAt this point, you should have at least three code cells in your notebook.\n\nCopy the code below into the third code cell of your notebook to load the contents of the test.csv file. Don’t forget to click on the play button (or hit [Shift] + [Enter])!\n\nimport pandas as pd\n\n\n# test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data = pd.read_csv('./titanic/test.csv')\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nAs before, make sure that you see the output above in your notebook before continuing.\nOnce all of the code runs successfully, all of the data (in train.csv and test.csv) is loaded in the notebook. (The code above shows only the first 5 rows of each table, but all of the data is there – all 891 rows of train.csv and all 418 rows of test.csv!)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-3-your-first-submission",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-3-your-first-submission",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 3: Your first submission",
    "text": "Part 3: Your first submission\nRemember our goal: we want to find patterns in train.csv that help us predict whether the passengers in test.csv survived.\nIt might initially feel overwhelming to look for patterns, when there’s so much data to sort through. So, we’ll start simple.\n\nExplore a pattern\nRemember that the sample submission file in gender_submission.csv assumes that all female passengers survived (and all male passengers died).\nIs this a reasonable first guess? We’ll check if this pattern holds true in the data (in train.csv).\nCopy the code below into a new code cell. Then, run the cell.\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n\n% of women who survived: 0.7420382165605095\n\n\n\n여성의 생존률을 구하는 코드입니다, 이전에 accuracy를 구하던 테크닉을 활용하면 아래의 코드도 가능합니다\n\ntrain_data[train_data.Sex == 'female'].Survived.mean()\n\n0.7420382165605095\n\n\n\nBefore moving on, make sure that your code returns the output above. The code above calculates the percentage of female passengers (in train.csv) who survived.\nThen, run the code below in another code cell:\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n\n% of men who survived: 0.18890814558058924\n\n\nThe code above calculates the percentage of male passengers (in train.csv) who survived.\nFrom this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in gender_submission.csv is not a bad first guess!\nBut at the end of the day, this gender-based submission bases its predictions on only a single column. As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions. Since it is quite difficult to consider several columns at once (or, it would take a long time to consider all possible patterns in many different columns simultaneously), we’ll use machine learning to automate this for us."
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#your-first-machine-learning-model",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#your-first-machine-learning-model",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Your first machine learning model",
    "text": "Your first machine learning model\nWe’ll build what’s known as a random forest model. This model is constructed of several “trees” (there are three trees in the picture below, but we’ll construct 100!) that will individually consider each passenger’s data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\n\nThe code cell below looks for patterns in four different columns (“Pclass”, “Sex”, “SibSp”, and “Parch”) of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file submission.csv.\nCopy this code into your notebook, and run it in a new code cell.\n\nimport pandas as pd\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('titanic_sub/submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\nMake sure that your notebook outputs the same message above (Your submission was successfully saved!) before moving on. &gt; Again, don’t worry if this code doesn’t make sense to you! For now, we’ll focus on how to generate and submit predictions.\nOnce you’re ready, click on the “Save Version” button in the top right corner of your notebook. This will generate a pop-up window.\n- Ensure that the “Save and Run All” option is selected, and then click on the “Save” button. - This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the “Save Version” button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (…) to the right of the most recent version, and select Open in Viewer.\n- Click on the Data tab on the top of the screen. Then, click on the “Submit” button to submit your results.\n\nCongratulations for making your first submission to a Kaggle competition! Within ten minutes, you should receive a message providing your spot on the leaderboard. Great work!"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-4-learn-more",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-4-learn-more",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 4: Learn more!",
    "text": "Part 4: Learn more!\nIf you’re interested in learning more, we strongly suggest our (3-hour) Intro to Machine Learning course, which will help you fully understand all of the code that we’ve presented here. You’ll also know enough to generate even better predictions!"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?",
    "text": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?\n- 원래코드\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\n\nlen(predictions), len(X_test)\n\n- 이렇게 수정하면 될 듯\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\nlen(predictions),len(y)\n\n\n(predictions == y).mean()"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#b.-alexis-cook의-코드를-수정해보자",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#b.-alexis-cook의-코드를-수정해보자",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "B. Alexis Cook의 코드를 수정해보자!",
    "text": "B. Alexis Cook의 코드를 수정해보자!\n- 코드를 수정해보자.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=5000, max_depth=1000, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\n(predictions == y).mean()\n\n0.8170594837261503\n\n\n\n내가 만든게 더 좋은데??\n\n- 이것도 제출결과로 만들어보자.\n\npredictions = model.predict(X_test)\n\n- 아래와 같이 제출하면 에러가 발생\n\npd.read_csv(\"./titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정_submission.csv\")\n\n- 아래와 같이 제출파일을 저장해야 한다.\n\npd.read_csv(\"titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정2_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-zZrOGpLc8spPa9L39RiNhR&si=TFl5m9-VohYT_47L"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#a.-데이터",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#a.-데이터",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\n# import os\n# os.getcwd()\n\n\n# tr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\n# tst = TabularDataset(\"/kaggle/input/titanic/test.csv\")\n\ntr = TabularDataset(\"./titanic/train.csv\")\ntst = TabularDataset(\"./titanic/test.csv\")\n\n\ntype(tr), type(tst)\n\n(autogluon.core.dataset.TabularDataset, autogluon.core.dataset.TabularDataset)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#b.-predictor-생성",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230913_055133/\""
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#c.-적합fit",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#c.-적합fit",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\npredictr은 인스턴스니까 숨겨진 메소드가 있다.\n\npredictr.fit(tr) \n# 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230913_055133/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.16\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   675.53 GB / 982.82 GB (68.7%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    37551.07 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.21s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.23s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.14s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.29s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8324   = Validation score   (accuracy)\n    1.05s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8101   = Validation score   (accuracy)\n    0.11s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8212   = Validation score   (accuracy)\n    1.18s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.35s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8324   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 5.26s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230913_055133/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fb971df4220&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0         LightGBMLarge   0.832402       0.002752  0.347409                0.002752           0.347409            1       True         13\n1       NeuralNetFastAI   0.832402       0.007613  1.051858                0.007613           1.051858            1       True         10\n2   WeightedEnsemble_L2   0.832402       0.008036  1.318412                0.000423           0.266554            2       True         14\n3              CatBoost   0.826816       0.003309  0.381638                0.003309           0.381638            1       True          7\n4              LightGBM   0.821229       0.002449  0.144130                0.002449           0.144130            1       True          4\n5        NeuralNetTorch   0.821229       0.007525  1.182258                0.007525           1.182258            1       True         12\n6            LightGBMXT   0.815642       0.002653  0.232583                0.002653           0.232583            1       True          3\n7      RandomForestGini   0.815642       0.021465  0.288951                0.021465           0.288951            1       True          5\n8      RandomForestEntr   0.815642       0.021691  0.250421                0.021691           0.250421            1       True          6\n9        ExtraTreesGini   0.815642       0.022456  0.255295                0.022456           0.255295            1       True          8\n10              XGBoost   0.810056       0.003953  0.111502                0.003953           0.111502            1       True         11\n11       ExtraTreesEntr   0.810056       0.022516  0.263453                0.022516           0.263453            1       True          9\n12       KNeighborsDist   0.653631       0.004180  0.007864                0.004180           0.007864            1       True          2\n13       KNeighborsUnif   0.653631       0.031115  0.007931                0.031115           0.007931            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMLarge\n0.832402\n0.002752\n0.347409\n0.002752\n0.347409\n1\nTrue\n13\n\n\n1\nNeuralNetFastAI\n0.832402\n0.007613\n1.051858\n0.007613\n1.051858\n1\nTrue\n10\n\n\n2\nWeightedEnsemble_L2\n0.832402\n0.008036\n1.318412\n0.000423\n0.266554\n2\nTrue\n14\n\n\n3\nCatBoost\n0.826816\n0.003309\n0.381638\n0.003309\n0.381638\n1\nTrue\n7\n\n\n4\nLightGBM\n0.821229\n0.002449\n0.144130\n0.002449\n0.144130\n1\nTrue\n4\n\n\n5\nNeuralNetTorch\n0.821229\n0.007525\n1.182258\n0.007525\n1.182258\n1\nTrue\n12\n\n\n6\nLightGBMXT\n0.815642\n0.002653\n0.232583\n0.002653\n0.232583\n1\nTrue\n3\n\n\n7\nRandomForestGini\n0.815642\n0.021465\n0.288951\n0.021465\n0.288951\n1\nTrue\n5\n\n\n8\nRandomForestEntr\n0.815642\n0.021691\n0.250421\n0.021691\n0.250421\n1\nTrue\n6\n\n\n9\nExtraTreesGini\n0.815642\n0.022456\n0.255295\n0.022456\n0.255295\n1\nTrue\n8\n\n\n10\nXGBoost\n0.810056\n0.003953\n0.111502\n0.003953\n0.111502\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.022516\n0.263453\n0.022516\n0.263453\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.004180\n0.007864\n0.004180\n0.007864\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.031115\n0.007931\n0.031115\n0.007931\n1\nTrue\n1\n\n\n\n\n\n\n\n\n모형에 대한 accuracy가 나타난다.\nvalidation: 실제 test에서 잘하기 위한 자체적 test라고 보면된다."
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#d.-예측-predict",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.8810325476992144\n\n\n\n(tr.Survived == (tr.Sex == \"female\")).mean() # 예전점수와 비교\n\n0.7867564534231201\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"./titanic_sub/autogluon_submission.csv\",index=False)\n\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n0\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n0\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n- TabularDataset?\n\n# pd.DataFrame &lt;-- 클래스\nTabularDataset?? # 클래스를 상속 --&gt; 상속받은 클래스의 기능을 물려받음.\n\n\nInit signature: TabularDataset(data, **kwargs)\nSource:        \nclass TabularDataset(pd.DataFrame):\n    \"\"\"\n    A dataset in tabular format (with rows = samples, columns = features/variables).\n    This object is essentially a pandas DataFrame (with some extra attributes) and all existing pandas methods can be applied to it.\n    For full list of methods/attributes, see pandas Dataframe documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n    Parameters\n    ----------\n    data : :class:`pd.DataFrame` or str\n        If str, path to data file (CSV or Parquet format).\n        If you already have your data in a :class:`pd.DataFrame`, you can specify it here.\n    Attributes\n    ----------\n    file_path: (str)\n        Path to data file from which this `TabularDataset` was created.\n        None if `data` was a :class:`pd.DataFrame`.\n    Note: In addition to these attributes, `TabularDataset` also shares all the same attributes and methods of a pandas Dataframe.\n    For a detailed list, see:  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n    Examples\n    --------\n    &gt;&gt;&gt; from autogluon.core.dataset import TabularDataset\n    &gt;&gt;&gt; train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n    &gt;&gt;&gt; train_data.head(30)\n    &gt;&gt;&gt; train_data.columns\n    \"\"\"\n    _metadata = [\"file_path\"]  # preserved properties that will be copied to a new instance of TabularDataset\n    @property\n    def _constructor(self):\n        return TabularDataset\n    @property\n    def _constructor_sliced(self):\n        return pd.Series\n    def __init__(self, data, **kwargs):\n        if isinstance(data, str):\n            file_path = data\n            data = load_pd.load(file_path)\n        else:\n            file_path = None\n        super().__init__(data, **kwargs)\n        self.file_path = file_path\nFile:           ~/anaconda3/envs/torch/lib/python3.8/site-packages/autogluon/core/dataset.py\nType:           type\nSubclasses:"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#result",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#result",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "Result",
    "text": "Result\n\npublic Score&gt; 0.75358"
  },
  {
    "objectID": "posts/5_STBDA2023/10wk-039.html",
    "href": "posts/5_STBDA2023/10wk-039.html",
    "title": "10wk-039: 의사결정나무 Discussion",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. 의사결정나무 Discussions\n- 의사결정나무 vs 선형모형\n\n아이스크림+축제: 이상치에 강했음.\n운동+보조제: 교호작용을 고려하지 않아도 괜찮았음.\n토익유사점수: 다중공선성문제가 발생하는 경우에도 모형이 덜 망함.\n밸런스게임: 필요없는 변수가 있을 경우에도 모형이 덜 망함.\n\n- 의사결정나무의 장점들\n\n시각화가 유리하다. 설명력이 좋다.\n특성(feature)의 중요도를 파악하기 용이하다.\n\\({\\bf y} \\sim {\\bf X}\\) 사이에 존재하는 비선형성을 쉽게 모델링 할 수 있다. \\(\\to\\) 쉽게 말해서 잘 맞춘다는 소리에요\n모형에 대한 가정들이 필요 없다. (넌파라메트릭 모형 특징)\n\n- 의사결정나무의 단점: 오버피팅이 일어나기 너무 쉽다. (모형이 너무 흔들려..)\n- 의사결정나무에 대한 자잘한 개념들 (자격증에서 잘 물어봄)\n최소 샘플 분할(Min Samples Split):\n\n노드를 분할하기 위한 최소 샘플 수.\n적절한 설정으로 과소적합 및 과적합 조절 가능.\n\n가지치기(Pruning):\n\n트리의 불필요한 부분을 제거.\n과적합 방지 및 모델 성능 향상에 도움.\n\n정보 이득(Information Gain):\n\n분할 전후의 엔트로피 차이를 측정.\n높은 정보 이득은 더 좋은 분할을 의미.\n\n지니 불순도(Gini Impurity):\n\n노드의 순도 측정 지표.\n낮은 지니 불순도는 높은 클래스 순도를 의미.\n\n\n결국 “트리를 어디까지 성장시킬래?”라는 물음에 대답하기 위해 고안된 개념들이다. 근본적으로 “트리를 어디까지 성장시킬래?”에 대한 이론적인 명확한 기술은 없다. 이는 넌파라메트릭 모형이 가지는 공통적인 특징임.\n\n- 의사결정나무는 오버피팅을 잡기위해서 지루한 싸움을 시작함.\n\n발전과정: 의사결정나무 \\(\\to\\) 배깅, 랜덤포레스트, 부스팅\n의사결정나무를 응용한 다양한 방법들이 개발되었다. (너무 많아요 진짜) \\(\\to\\) 모든 방법들의 원리를 세세하게 파헤치는건 비효율적이다.\n그러한 다양한 방법들을 적덩히 분류해보면 대체로 배깅, 랜덤포레스트, 부스팅 계열로 나뉜다.1 \\(\\to\\) 배깅, 랜덤포레스트, 부스팅에 대한 공통적 아이디어를 파악하는건 효율적이다.\n현재 최고로 (state of the art, SOTA) 로 평가받는 알고리즘은 부스팅계열의 XGBoost, LightGBM, CatBoost 이다.\n\n\n\n\n\n\nFootnotes\n\n\n모든 방법들이 세개의 카테고리중 하나에만 들어가는건 아니다↩︎"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-013.out.html",
    "href": "posts/5_STBDA2023/03wk-013.out.html",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-wi9Mkfc849jTCMENvydI8B&si=hotGT-ErLB8dukhs"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-013.out.html#a.-데이터-정리",
    "href": "posts/5_STBDA2023/03wk-013.out.html#a.-데이터-정리",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "A. 데이터 정리",
    "text": "A. 데이터 정리\n\nX = pd.get_dummies(df_train.drop(['PassengerId','Survived'],axis=1))\ny = df_train[['Survived']]"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-013.out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/03wk-013.out.html#b.-predictor-생성",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()\npredictr \n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-013.out.html#c.-학습-fit-learn",
    "href": "posts/5_STBDA2023/03wk-013.out.html#c.-학습-fit-learn",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "C. 학습 (fit, learn)",
    "text": "C. 학습 (fit, learn)\n\npredictr.fit(X,y) # missing value --&gt; error!\n\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-013.out.html#a.-데이터정리",
    "href": "posts/5_STBDA2023/03wk-013.out.html#a.-데이터정리",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "A. 데이터정리",
    "text": "A. 데이터정리\n\nX = pd.get_dummies(df_train[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]])\ny = df_train[[\"Survived\"]]"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-013.out.html#b.-predictor-생성-1",
    "href": "posts/5_STBDA2023/03wk-013.out.html#b.-predictor-생성-1",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = sklearn.linear_model.LogisticRegression()\npredictr\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-013.out.html#c.-학습",
    "href": "posts/5_STBDA2023/03wk-013.out.html#c.-학습",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "C. 학습",
    "text": "C. 학습\n\npredictr.fit(X, y)\n\n/home/jy/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n## D. 예측\npredictr.predict(X)\n\narray([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n       1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n       0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n       0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n       0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n       0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n       1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0])\n\n\n\ndf_train.assign(Survived_hat=predictr.predict(X)).loc[:,['Survived','Survived_hat']]\n\n\n\n\n\n\n\n\nSurvived\nSurvived_hat\n\n\n\n\n0\n0\n0\n\n\n1\n1\n1\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n0\n0\n\n\n...\n...\n...\n\n\n886\n0\n0\n\n\n887\n1\n1\n\n\n888\n0\n1\n\n\n889\n1\n0\n\n\n890\n0\n0\n\n\n\n\n891 rows × 2 columns"
  },
  {
    "objectID": "posts/5_STBDA2023/03wk-013.out.html#e.-평가",
    "href": "posts/5_STBDA2023/03wk-013.out.html#e.-평가",
    "title": "03wk-013: 타이타닉, 로지스틱",
    "section": "E. 평가",
    "text": "E. 평가\n\npredictr.score(X,y)\n\n0.8002244668911336"
  },
  {
    "objectID": "posts/6_note/2023-09-09_smart_farm_eda.html",
    "href": "posts/6_note/2023-09-09_smart_farm_eda.html",
    "title": "연습장1",
    "section": "",
    "text": "- 하우스 내부\nCO2와 일사량 사이의 관계 곡선을 통해 온실 내부 환경을 제어한다. 즉, 맞추어져 있는 환경이 아니라 현재의 환경을 가지고 최적화 시키는 방식으로 되어져야 한다.\n센서로부터 취득한 데이터는 그 숫자의 의미가 매우 가변적이므로 온도, 습도, 일사 더 나아가 CO2 등 온실에서 계측되는 데이터들은 단일계로서의 의미는 매우 적게 활용되어 진다.\n즉, 온도와 습도가 짝을 이루고, pH와 EC가 짝을 이룬다.\n또한 양액을 공급하기 위한 토양의 습도와 온도 일사도 짝을 이루게 된다.\n평창과 같이 겨울에 기온이 상당히 내려가는 지역에 있어서는 풍향과 풍속도 짝을 이루어서 관리를 해야한다.\n만약 단편적으로 온도를 낮추고 온도를 높이기 위해서 천창, 팬, 난방기 이런 것들을 가동하게 된다면 즉, 온도라는 조건만 가지고 가동하게 된다면 습도에 문제가 생기고, 대지가 마르는 현상이 나타날 수 있다. (복합적으로 판단하여 복합적인 요소로 관리하는 것이 매우 중요하다.)\n천창을 열건지, 팬을 돌릴것인지, 온풍기를 가동할 것인지, 양액을 어떻게 조성할 것인지 등은 지극히 사람들의 경험에 의해서 이루어지고 있다. (AI가 대체할 수 있을까?)\n- 노지\n온도 습도 반비례, 온도 일사량 비례하는 관계가 나타난다.\n겨울철에 나타나는 결로현상1\n결로가 발생할 수 있는 어떤 지점, 장치, 시간대들을 계산하여서 최대한 결로현상을 줄여나가는 것도 관건.\n- 언제 작물에 물을 줄까?\n스파트팜을 이용해서 토경재배를 할 때 언제 물을 주었는지도 중요한 포인트이지만 얼마만큼의 속도로 수분량이 줄어들고 있는지가 더 중요한 요소 중 하나이다. (진흙에 가까운 토양일수록 물빠짐이 나쁘기 때문에 곡선이 완만할 것.)\n가지고 있는 토양의 조건이 어떠냐에 따라서 여기에는 어떤 작물을 선택할 것이며, 또 어떤 패턴의 관리 기법이 들어가야 될 것인지도 동일하게 검토되어야 할 부분이다.\n즉, 물을 주는 것도 중요하지만 물빠짐 현상을 해석하는 것도 중요한 요소 중 하나이다.\n- 적산온도\n예시) 수박의 적산온도로 수박을 언제쯤 수확하면 되겠다~ // 수확을 하기 위해서 최적의 적산온도는 1000도씨. (수박의 경우)\n적산온도로 정확한 출하시기/수확시기를 예측할 수 있는 모델도 고려할 수 있음.\n- 1일 온도와 습도의 변화폭의 관계\n천천히 열던지, 조금열고 많이 기다리고\n바깥쪽의 찬 공기가 작물에 영향을 주었다는 것을 온도변화 폭을 보고 알 수 있다.\n- 온도와 습도 그리고 CO2 변화의 관계\nCO2 공급기로 약 1800에서 2000ppm정도의 co2를 공급했음에도(낮에) 불구하고 작물이 정상적으로 광학성을 하고 있다고 게시할 만한 차트의 변화가 없다. 즉, co2는 공급이 되고 있는데 식물들의 반응이 없었다는 말.\n광합성을 위한 co2가 적정이상의 좋은 조건으로 공급되고 있음에도 불구하고 식물들에게 반응이 없다는 것은 어떤 다른 조건이 만족하고 있지 않다는 것을 말한다.\n예를들어, 식물이 생존하기 위해 습도가 너무 낮거나 공권부에서 흡수되는 수분의 양이 적게 되면 자신이 갖고 있는 수분량을 빼앗기지 않기 위해 기공을 닫고있게 된다. 즉, 호흡을 하지 않는다. 따라서 이 상태에서 아무리 CO2 조건을 좋게 하더라도 식물에게는 좋은 영향을 미칠 수 없다.\n즉, 습도와 CO2가 잘 맞아야지 유의미한 결과가 나온다.\n- 제안 (예시)\n시간에 있어서는 일몰과 일출을 기준으로 접근하는 것이 필요.\n표준화된 구간 안에서 온실 내 환경을 잘 맞출 수 있는 솔루션 개발 필요.\n추가적으로, 우리나라는 여름철/겨울철의 일몰과 일출시간이 다르다. 즉, 몇시에서부터 몇시까지의 개념이 반드시 일몰과 일출의 개념으로 시작 및 종료 시간이 정해져야 한다. (오늘의 6시와 6개월 후의 6시는 일조량에 있어서 전혀 다른 의미에서의 6시)"
  },
  {
    "objectID": "posts/6_note/2023-09-09_smart_farm_eda.html#import",
    "href": "posts/6_note/2023-09-09_smart_farm_eda.html#import",
    "title": "연습장1",
    "section": "Import",
    "text": "Import\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n# 데이터 불러오기\ninputs = pd.read_csv('./farm/train_input.csv')\noutputs = pd.read_csv('./farm/train_output.csv')\n\n\ninputs.head()\n\n\n\n\n\n\n\n\nSample_no\n시설ID\n일\n주차\n내부CO2\n내부습도\n내부온도\n지온\n강우감지\n일사량\n외부온도\n외부풍향\n외부풍속\n지습\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n품종\n재배형태\n\n\n\n\n0\n0\nfarm25\n20220323\n30주차\n517.041667\n84.985417\n20.610833\n0.0\nNaN\n1879\n11.166667\n195.0\n0.083333\n0.0\n14\n2.68\n4.42\n88\ntomato09\nNaN\n\n\n1\n0\nfarm25\n20220324\n30주차\n514.416667\n88.291250\n20.695000\n0.0\nNaN\n1411\n12.708333\n142.5\n0.000000\n0.0\n14\n2.78\n5.63\n97\ntomato09\nNaN\n\n\n2\n0\nfarm25\n20220326\n30주차\n471.875000\n83.514583\n20.402500\n0.0\nNaN\n1955\n8.791667\n202.5\n0.000000\n0.0\n14\n2.69\n4.25\n101\ntomato09\nNaN\n\n\n3\n0\nfarm25\n20220327\n30주차\n469.250000\n80.916250\n20.139167\n0.0\nNaN\n2231\n8.041667\n180.0\n0.000000\n0.0\n14\n2.70\n4.25\n99\ntomato09\nNaN\n\n\n4\n0\nfarm25\n20220328\n30주차\n465.750000\n82.026250\n17.653333\n0.0\nNaN\n2284\n9.000000\n97.5\n0.041667\n0.0\n13\n2.66\n4.21\n94\ntomato09\nNaN\n\n\n\n\n\n\n\n\noutputs.head()\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n0\n20220330\n30주차\n208.0\n6.9\n16.67\n\n\n1\n1\n20220330\n30주차\n172.0\n6.8\n17.33\n\n\n2\n2\n20220330\n30주차\n150.0\n9.3\n16.00\n\n\n3\n3\n20220330\n30주차\n121.0\n5.9\n16.20\n\n\n4\n4\n20220406\n31주차\n175.0\n5.8\n17.40\n\n\n\n\n\n\n\n\n생장길이 = 지난주 생장점에서 금주 생장점 길이\n개화군 = 꽃이 핀 개수를 점수로 측정\n\n\n# pd.DataFrame(inputs['시설ID'].value_counts()).sort_index() # farm01~farm36\n\n\ninputs.isna().sum()\n\nSample_no        0\n시설ID             0\n일                0\n주차               0\n내부CO2            0\n내부습도             0\n내부온도             0\n지온            1749\n강우감지          1505\n일사량              0\n외부온도           201\n외부풍향          6993\n외부풍속           670\n지습            5873\n급액횟수             0\n급액EC(dS/m)       0\n급액pH             0\n급액량(회당)          0\n품종            7114\n재배형태          2408\ndtype: int64\n\n\n\n# nan 제거  -- 베이스라인이므로 간단한 처리를 위해 nan 항목 보간 없이 학습\ninputs = inputs.dropna(axis=1)\n\n\n# 주차 정보 수치 변환\ninputs['주차'] = [int(i.replace('주차', \"\")) for i in inputs['주차']]\n\n\n# scaler\ninput_scaler = MinMaxScaler()\noutput_scaler = MinMaxScaler()\n\n\ninputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n주차\n내부CO2\n내부습도\n내부온도\n일사량\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n\n\n\n\n0\n30\n517.041667\n84.985417\n20.610833\n1879\n14\n2.68\n4.42\n88\n\n\n1\n30\n514.416667\n88.291250\n20.695000\n1411\n14\n2.78\n5.63\n97\n\n\n2\n30\n471.875000\n83.514583\n20.402500\n1955\n14\n2.69\n4.25\n101\n\n\n3\n30\n469.250000\n80.916250\n20.139167\n2231\n14\n2.70\n4.25\n99\n\n\n4\n30\n465.750000\n82.026250\n17.653333\n2284\n13\n2.66\n4.21\n94\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10107\n7\n334.684002\n65.565417\n21.985833\n979\n26\n2.06\n5.80\n81\n\n\n10108\n7\n333.726601\n61.144167\n22.530833\n2515\n28\n2.43\n4.42\n32\n\n\n10109\n7\n344.862883\n72.867917\n20.397917\n1972\n21\n2.71\n5.88\n27\n\n\n10110\n7\n372.708516\n66.672917\n24.401667\n1314\n18\n2.50\n5.39\n82\n\n\n10111\n7\n372.612192\n59.257083\n28.352500\n1310\n16\n2.50\n5.39\n82\n\n\n\n\n10112 rows × 9 columns\n\n\n\n\noutputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n208.0\n6.90\n16.67\n\n\n1\n172.0\n6.80\n17.33\n\n\n2\n150.0\n9.30\n16.00\n\n\n3\n121.0\n5.90\n16.20\n\n\n4\n175.0\n5.80\n17.40\n\n\n...\n...\n...\n...\n\n\n1513\n150.0\n6.95\n2.20\n\n\n1514\n140.0\n10.13\n1.40\n\n\n1515\n200.0\n9.61\n1.40\n\n\n1516\n210.0\n8.47\n2.20\n\n\n1517\n150.0\n9.16\n3.20\n\n\n\n\n1518 rows × 3 columns\n\n\n\n\n# scaling\ninput_sc = input_scaler.fit_transform(inputs.iloc[:,3:].to_numpy())\noutput_sc = output_scaler.fit_transform(outputs.iloc[:,3:].to_numpy())\n\n\nlen(inputs['Sample_no'].unique()) \n\n1518\n\n\n\n# 입력 시계열화\ninput_ts = []\nfor i in outputs['Sample_no']:\n    sample = input_sc[inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    input_ts.append(sample)\ninput_ts = np.concatenate(input_ts, axis=0)\n\n\ninput_ts.shape\n\n(1518, 7, 9)\n\n\n\n# 셋 분리\ntrain_x, val_x, train_y, val_y = train_test_split(input_ts, output_sc, test_size=0.2,\n                                                  shuffle=True, random_state=0)\n\n\ntrain_x.shape, val_x.shape, train_y.shape, val_y.shape\n\n((1214, 7, 9), (304, 7, 9), (1214, 3), (304, 3))\n\n\n\n# 모델 정의\ndef create_model():\n    x = Input(shape=[7, 9])\n    l1 = LSTM(64)(x)\n    out = Dense(3, activation='tanh')(l1)\n    return Model(inputs=x, outputs=out)\n\nmodel = create_model()\nmodel.summary()\ncheckpointer = ModelCheckpoint(monitor='val_loss', filepath='baseline.h5',\n                               verbose=1, save_best_only=True, save_weights_only=True)\n\nmodel.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['mse'])\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 7, 9)]            0         \n                                                                 \n lstm (LSTM)                 (None, 64)                18944     \n                                                                 \n dense (Dense)               (None, 3)                 195       \n                                                                 \n=================================================================\nTotal params: 19,139\nTrainable params: 19,139\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n\n\n\n# 학습\nhist = model.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[checkpointer])\n\nEpoch 1/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0170 - mse: 0.0170 \nEpoch 1: val_loss improved from inf to 0.00995, saving model to baseline.h5\n38/38 [==============================] - 1s 9ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0099 - val_mse: 0.0099\nEpoch 2/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0117 - mse: 0.0117\nEpoch 2: val_loss improved from 0.00995 to 0.00839, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0084 - val_mse: 0.0084\nEpoch 3/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0109 - mse: 0.0109\nEpoch 3: val_loss improved from 0.00839 to 0.00777, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 4/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0106 - mse: 0.0106\nEpoch 4: val_loss did not improve from 0.00777\n38/38 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 5/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0101 - mse: 0.0101\nEpoch 5: val_loss improved from 0.00777 to 0.00757, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0076 - val_mse: 0.0076\nEpoch 6/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0105 - mse: 0.0105\nEpoch 6: val_loss improved from 0.00757 to 0.00751, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0075 - val_mse: 0.0075\nEpoch 7/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0098 - mse: 0.0098\nEpoch 7: val_loss improved from 0.00751 to 0.00738, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0074 - val_mse: 0.0074\nEpoch 8/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 8: val_loss improved from 0.00738 to 0.00709, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0071 - val_mse: 0.0071\nEpoch 9/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 9: val_loss improved from 0.00709 to 0.00702, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 10/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 10: val_loss did not improve from 0.00702\n38/38 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0072 - val_mse: 0.0072\nEpoch 11/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 11: val_loss improved from 0.00702 to 0.00696, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 12/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 12: val_loss improved from 0.00696 to 0.00682, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 13/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 13: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 14/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 14: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 15/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0094 - mse: 0.0094\nEpoch 15: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 16/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 16: val_loss improved from 0.00682 to 0.00674, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 17/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 17: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 18/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 18: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 19/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 19: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 20/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 20: val_loss improved from 0.00674 to 0.00670, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 21/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 21: val_loss improved from 0.00670 to 0.00659, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 22/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 22: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 23/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 23: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 24/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0090 - mse: 0.0090\nEpoch 24: val_loss improved from 0.00659 to 0.00650, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 25/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 25: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 26/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 26: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 27/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 27: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 28/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 28: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 29/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 29: val_loss improved from 0.00650 to 0.00637, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0064 - val_mse: 0.0064\nEpoch 30/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0084 - mse: 0.0084\nEpoch 30: val_loss improved from 0.00637 to 0.00632, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0063 - val_mse: 0.0063\nEpoch 31/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 31: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 32/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0077 - mse: 0.0077\nEpoch 32: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 33/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 33: val_loss improved from 0.00632 to 0.00623, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 34/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0082 - mse: 0.0082\nEpoch 34: val_loss improved from 0.00623 to 0.00614, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0061 - val_mse: 0.0061\nEpoch 35/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0080 - mse: 0.0080\nEpoch 35: val_loss improved from 0.00614 to 0.00604, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 36/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 36: val_loss did not improve from 0.00604\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 37/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 37: val_loss improved from 0.00604 to 0.00601, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 38/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 38: val_loss improved from 0.00601 to 0.00586, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0059 - val_mse: 0.0059\nEpoch 39/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0078 - mse: 0.0078\nEpoch 39: val_loss did not improve from 0.00586\n38/38 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 40/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 40: val_loss improved from 0.00586 to 0.00551, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 41/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 41: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 42/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0068 - mse: 0.0068\nEpoch 42: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 43/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 43: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 44/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0071 - mse: 0.0071\nEpoch 44: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 45/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 45: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 46/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 46: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 47/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 47: val_loss improved from 0.00551 to 0.00509, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 48/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 48: val_loss did not improve from 0.00509\n38/38 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0054 - val_mse: 0.0054\nEpoch 49/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0062 - mse: 0.0062\nEpoch 49: val_loss improved from 0.00509 to 0.00507, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 50/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 50: val_loss did not improve from 0.00507\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0056 - val_mse: 0.0056\n\n\n\n# loss 히스토리 확인\nfig, loss_ax = plt.subplots()\nloss_ax.plot(hist.history['loss'], 'r', label='loss')\nloss_ax.plot(hist.history['val_loss'], 'g', label='val_loss')\nloss_ax.set_xlabel('epoch')\nloss_ax.set_ylabel('loss')\nloss_ax.legend()\nplt.title('Training loss - Validation loss plot')\nplt.show()\n\n\n\n\n\n# 저장된 가중치 불러오기\nmodel.load_weights('baseline.h5')\n\n\n# 테스트셋 전처리 및 추론\ntest_inputs = pd.read_csv('./farm/test_input.csv')\noutput_sample = pd.read_csv('./farm/answer_sample.csv')\n\ntest_inputs = test_inputs[inputs.columns]\ntest_inputs['주차'] = [int(i.replace('주차', \"\")) for i in test_inputs['주차']]\ntest_input_sc = input_scaler.transform(test_inputs.iloc[:,3:].to_numpy())\n\ntest_input_ts = []\nfor i in output_sample['Sample_no']:\n    sample = test_input_sc[test_inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    test_input_ts.append(sample)\ntest_input_ts = np.concatenate(test_input_ts, axis=0)\n\n\ntest_input_ts.shape\n\n(506, 7, 9)\n\n\n\nprediction = model.predict(test_input_ts)\n\nprediction = output_scaler.inverse_transform(prediction)\noutput_sample[['생장길이', '줄기직경', '개화군']] = prediction\n\n16/16 [==============================] - 0s 793us/step\n\n\n\noutput_sample\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n9\n20220413\n32주차\n47.151882\n7.198693\n13.229403\n\n\n1\n12\n20170312\n30주차\n420.956116\n3.321363\n10.489825\n\n\n2\n19\n20170319\n31주차\n589.641235\n4.079537\n7.553223\n\n\n3\n23\n20170326\n32주차\n281.593994\n4.563877\n7.977988\n\n\n4\n27\n20170430\n37주차\n89.870880\n7.789731\n4.768530\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n501\n2015\n20160508\n14주차\n188.253265\n12.274371\n5.467629\n\n\n502\n2016\n20160529\n17주차\n1998.461670\n4.776089\n1.905745\n\n\n503\n2024\n20160828\n7주차\n49.577644\n13.211569\n2.025992\n\n\n504\n2025\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n505\n2026\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n\n\n506 rows × 6 columns\n\n\n\n\n# 제출할 추론 결과 저장\noutput_sample.to_csv('prediction.csv', index=False)\n\n\n# 학습\ncb1 = tf.keras.callbacks.TensorBoard()\nmodel.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[cb1])\n\nEpoch 1/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 2/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 3/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 4/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 5/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 6/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 7/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 8/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 9/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 10/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 11/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 12/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 13/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 14/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 15/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 16/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 17/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0044 - val_mse: 0.0044\nEpoch 18/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 19/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 20/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0050 - val_mse: 0.0050\nEpoch 21/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 22/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 23/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 24/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 25/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 26/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 27/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 28/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 29/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 30/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 31/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 32/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 33/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 34/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 35/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 36/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 37/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 38/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 39/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 40/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 41/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 42/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 43/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 44/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 45/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 46/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0037 - val_mse: 0.0037\nEpoch 47/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 48/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 49/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 50/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n\n\n&lt;keras.callbacks.History at 0x7f2df8e88b80&gt;\n\n\n\n%load_ext tensorboard\n%tensorboard --logdir logs --host \n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\nReusing TensorBoard on port 6006 (pid 109833), started 0:04:21 ago. (Use '!kill 109833' to kill it.)"
  },
  {
    "objectID": "posts/6_note/2023-09-15-autogluon_smartfarm.html",
    "href": "posts/6_note/2023-09-15-autogluon_smartfarm.html",
    "title": "AutoTS",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# also: _hourly, _daily, _weekly, or _yearly\nfrom autots.datasets import load_monthly\n\ndf_long = load_monthly(long=True)\n\nfrom autots import AutoTS\n\nmodel = AutoTS(\n    forecast_length=3,\n    frequency='infer',\n    ensemble='simple',\n    max_generations=5,\n    num_validations=2,\n)\nmodel = model.fit(df_long, date_col='datetime', value_col='value', id_col='series_id')\n\n# Print the description of the best model\nprint(model)\n\nData frequency is: MS, used frequency is: MS\nModel Number: 1 with model ARIMA in generation 0 of 5\nModel Number: 2 with model AverageValueNaive in generation 0 of 5\nModel Number: 3 with model AverageValueNaive in generation 0 of 5\nModel Number: 4 with model AverageValueNaive in generation 0 of 5\nModel Number: 5 with model DatepartRegression in generation 0 of 5\nModel Number: 6 with model DatepartRegression in generation 0 of 5\nModel Number: 7 with model DatepartRegression in generation 0 of 5\nTemplate Eval Error: AttributeError(\"'str' object has no attribute 'decode'\") in model 7 in generation 0: DatepartRegression\nModel Number: 8 with model DatepartRegression in generation 0 of 5\nTemplate Eval Error: ImportError('Tensorflow not available, install with pip install tensorflow.') in model 8 in generation 0: DatepartRegression\nModel Number: 9 with model ETS in generation 0 of 5\nModel Number: 10 with model ETS in generation 0 of 5\nModel Number: 11 with model GLM in generation 0 of 5\nModel Number: 12 with model GLM in generation 0 of 5\nModel Number: 13 with model GLS in generation 0 of 5\nModel Number: 14 with model GLS in generation 0 of 5\nModel Number: 15 with model GluonTS in generation 0 of 5\nTemplate Eval Error: ImportError('GluonTS installation is incompatible with AutoTS. The numpy version is sometimes the issue, try 1.23.1 {as of 06-2023}') in model 15 in generation 0: GluonTS\nModel Number: 16 with model GluonTS in generation 0 of 5\nTemplate Eval Error: ImportError('GluonTS installation is incompatible with AutoTS. The numpy version is sometimes the issue, try 1.23.1 {as of 06-2023}') in model 16 in generation 0: GluonTS\nModel Number: 17 with model GluonTS in generation 0 of 5\nTemplate Eval Error: ImportError('GluonTS installation is incompatible with AutoTS. The numpy version is sometimes the issue, try 1.23.1 {as of 06-2023}') in model 17 in generation 0: GluonTS\nModel Number: 18 with model GluonTS in generation 0 of 5\nTemplate Eval Error: ImportError('GluonTS installation is incompatible with AutoTS. The numpy version is sometimes the issue, try 1.23.1 {as of 06-2023}') in model 18 in generation 0: GluonTS\nModel Number: 19 with model GluonTS in generation 0 of 5\nTemplate Eval Error: ImportError('GluonTS installation is incompatible with AutoTS. The numpy version is sometimes the issue, try 1.23.1 {as of 06-2023}') in model 19 in generation 0: GluonTS\nModel Number: 20 with model LastValueNaive in generation 0 of 5\nModel Number: 21 with model LastValueNaive in generation 0 of 5\nModel Number: 22 with model LastValueNaive in generation 0 of 5\nModel Number: 23 with model LastValueNaive in generation 0 of 5\nModel Number: 24 with model SeasonalNaive in generation 0 of 5\nTemplate Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'rolling_mean_24', 'transformations': {'0': 'SinTrend', '1': 'Round', '2': 'PowerTransformer'}, 'transformation_params': {'0': {}, '1': {'model': 'middle', 'decimals': 2, 'on_transform': False, 'on_inverse': True}, '2': {}}}. fail_on_forecast_nan=True\") in model 24 in generation 0: SeasonalNaive\nModel Number: 25 with model SeasonalNaive in generation 0 of 5\nModel Number: 26 with model SeasonalNaive in generation 0 of 5\nModel Number: 27 with model UnobservedComponents in generation 0 of 5\nModel Number: 28 with model UnobservedComponents in generation 0 of 5\nModel Number: 29 with model UnobservedComponents in generation 0 of 5\nModel Number: 30 with model VAR in generation 0 of 5\nModel Number: 31 with model VAR in generation 0 of 5\nModel Number: 32 with model VECM in generation 0 of 5\nModel Number: 33 with model VECM in generation 0 of 5\nModel Number: 34 with model WindowRegression in generation 0 of 5\nTemplate Eval Error: AttributeError(\"'str' object has no attribute 'decode'\") in model 34 in generation 0: WindowRegression\nModel Number: 35 with model ConstantNaive in generation 0 of 5\nModel Number: 36 with model FBProphet in generation 0 of 5\nTemplate Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 36 in generation 0: FBProphet\nModel Number: 37 with model GluonTS in generation 0 of 5\nTemplate Eval Error: ImportError('GluonTS installation is incompatible with AutoTS. The numpy version is sometimes the issue, try 1.23.1 {as of 06-2023}') in model 37 in generation 0: GluonTS\nModel Number: 38 with model MultivariateRegression in generation 0 of 5\nModel Number: 39 with model MultivariateRegression in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 39 in generation 0: MultivariateRegression\nModel Number: 40 with model DatepartRegression in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 40 in generation 0: DatepartRegression\nModel Number: 41 with model SeasonalNaive in generation 0 of 5\nModel Number: 42 with model DatepartRegression in generation 0 of 5\nModel Number: 43 with model UnobservedComponents in generation 0 of 5\nModel Number: 44 with model UnobservedComponents in generation 0 of 5\nModel Number: 45 with model ETS in generation 0 of 5\nModel Number: 46 with model VECM in generation 0 of 5\nModel Number: 47 with model ARDL in generation 0 of 5\nModel Number: 48 with model MultivariateMotif in generation 0 of 5\nModel Number: 49 with model MultivariateMotif in generation 0 of 5\nModel Number: 50 with model UnivariateMotif in generation 0 of 5\nModel Number: 51 with model UnivariateMotif in generation 0 of 5\nModel Number: 52 with model SectionalMotif in generation 0 of 5\nModel Number: 53 with model SectionalMotif in generation 0 of 5\nModel Number: 54 with model MultivariateRegression in generation 0 of 5\nModel Number: 55 with model FBProphet in generation 0 of 5\nTemplate Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 55 in generation 0: FBProphet\nModel Number: 56 with model SeasonalNaive in generation 0 of 5\nModel Number: 57 with model DatepartRegression in generation 0 of 5\nModel Number: 58 with model NVAR in generation 0 of 5\nModel Number: 59 with model Theta in generation 0 of 5\nModel Number: 60 with model UnivariateRegression in generation 0 of 5\nModel Number: 61 with model ARCH in generation 0 of 5\nTemplate Eval Error: ImportError('`arch` package must be installed from pip') in model 61 in generation 0: ARCH\nModel Number: 62 with model ConstantNaive in generation 0 of 5\nModel Number: 63 with model LastValueNaive in generation 0 of 5\nModel Number: 64 with model AverageValueNaive in generation 0 of 5\nModel Number: 65 with model GLS in generation 0 of 5\nModel Number: 66 with model SeasonalNaive in generation 0 of 5\nModel Number: 67 with model GLM in generation 0 of 5\nTemplate Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 67 in generation 0: GLM\nModel Number: 68 with model ETS in generation 0 of 5\nModel Number: 69 with model FBProphet in generation 0 of 5\nTemplate Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 69 in generation 0: FBProphet\nModel Number: 70 with model GluonTS in generation 0 of 5\nTemplate Eval Error: ImportError('GluonTS installation is incompatible with AutoTS. The numpy version is sometimes the issue, try 1.23.1 {as of 06-2023}') in model 70 in generation 0: GluonTS\nModel Number: 71 with model UnobservedComponents in generation 0 of 5\nModel Number: 72 with model VAR in generation 0 of 5\nTemplate Eval Error: IndexError('tuple index out of range') in model 72 in generation 0: VAR\nModel Number: 73 with model VECM in generation 0 of 5\nModel Number: 74 with model ARIMA in generation 0 of 5\nModel Number: 75 with model WindowRegression in generation 0 of 5\nModel Number: 76 with model DatepartRegression in generation 0 of 5\nModel Number: 77 with model UnivariateRegression in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 77 in generation 0: UnivariateRegression\nModel Number: 78 with model MultivariateRegression in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 78 in generation 0: MultivariateRegression\nModel Number: 79 with model UnivariateMotif in generation 0 of 5\nModel Number: 80 with model MultivariateMotif in generation 0 of 5\nModel Number: 81 with model SectionalMotif in generation 0 of 5\nTemplate Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'rolling_mean', 'transformations': {'0': 'SeasonalDifference', '1': 'AlignLastValue', '2': 'AlignLastValue', '3': 'EWMAFilter'}, 'transformation_params': {'0': {'lag_1': 7, 'method': 'LastValue'}, '1': {'rows': 1, 'lag': 1, 'method': 'multiplicative', 'strength': 1.0, 'first_value_only': False}, '2': {'rows': 1, 'lag': 1, 'method': 'additive', 'strength': 1.0, 'first_value_only': False}, '3': {'span': 12}}}. fail_on_forecast_nan=True\") in model 81 in generation 0: SectionalMotif\nModel Number: 82 with model NVAR in generation 0 of 5\nModel Number: 83 with model Theta in generation 0 of 5\nModel Number: 84 with model ARDL in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 84 in generation 0: ARDL\nModel Number: 85 with model ARCH in generation 0 of 5\nTemplate Eval Error: ImportError('`arch` package must be installed from pip') in model 85 in generation 0: ARCH\nModel Number: 86 with model MetricMotif in generation 0 of 5\nModel Number: 87 with model AverageValueNaive in generation 0 of 5\nModel Number: 88 with model UnobservedComponents in generation 0 of 5\nModel Number: 89 with model AverageValueNaive in generation 0 of 5\nModel Number: 90 with model LastValueNaive in generation 0 of 5\nModel Number: 91 with model ARDL in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 91 in generation 0: ARDL\nModel Number: 92 with model WindowRegression in generation 0 of 5\nModel Number: 93 with model ConstantNaive in generation 0 of 5\nModel Number: 94 with model SeasonalNaive in generation 0 of 5\nModel Number: 95 with model ARDL in generation 0 of 5\nModel Number: 96 with model FBProphet in generation 0 of 5\nTemplate Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 96 in generation 0: FBProphet\nModel Number: 97 with model ConstantNaive in generation 0 of 5\nModel Number: 98 with model ConstantNaive in generation 0 of 5\nModel Number: 99 with model UnobservedComponents in generation 0 of 5\nModel Number: 100 with model ConstantNaive in generation 0 of 5\nModel Number: 101 with model VAR in generation 0 of 5\nModel Number: 102 with model ETS in generation 0 of 5\nModel Number: 103 with model Theta in generation 0 of 5\nModel Number: 104 with model DatepartRegression in generation 0 of 5\nTemplate Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 104 in generation 0: DatepartRegression\nModel Number: 105 with model GLM in generation 0 of 5\nTemplate Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 105 in generation 0: GLM\nModel Number: 106 with model ARCH in generation 0 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 106 in generation 0: ARCH\nModel Number: 107 with model AverageValueNaive in generation 0 of 5\nModel Number: 108 with model MetricMotif in generation 0 of 5\nModel Number: 109 with model ETS in generation 0 of 5\nTemplate Eval Error: Exception('Transformer DifferencedTransformer failed on inverse') in model 109 in generation 0: ETS\nModel Number: 110 with model ETS in generation 0 of 5\nTemplate Eval Error: Exception('Transformer AlignLastValue failed on inverse') in model 110 in generation 0: ETS\nModel Number: 111 with model SeasonalNaive in generation 0 of 5\nModel Number: 112 with model ARCH in generation 0 of 5\nTemplate Eval Error: ImportError('`arch` package must be installed from pip') in model 112 in generation 0: ARCH\nModel Number: 113 with model SectionalMotif in generation 0 of 5\nModel Number: 114 with model SeasonalNaive in generation 0 of 5\nModel Number: 115 with model UnivariateRegression in generation 0 of 5\nModel Number: 116 with model VAR in generation 0 of 5\nTemplate Eval Error: IndexError('tuple index out of range') in model 116 in generation 0: VAR\nModel Number: 117 with model GluonTS in generation 0 of 5\nTemplate Eval Error: ImportError('GluonTS installation is incompatible with AutoTS. The numpy version is sometimes the issue, try 1.23.1 {as of 06-2023}') in model 117 in generation 0: GluonTS\nModel Number: 118 with model GLM in generation 0 of 5\nTemplate Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.') in model 118 in generation 0: GLM\nModel Number: 119 with model MultivariateMotif in generation 0 of 5\nModel Number: 120 with model AverageValueNaive in generation 0 of 5\nModel Number: 121 with model ConstantNaive in generation 0 of 5\nModel Number: 122 with model ARCH in generation 0 of 5\nTemplate Eval Error: ImportError('`arch` package must be installed from pip') in model 122 in generation 0: ARCH\nModel Number: 123 with model GLS in generation 0 of 5\nModel Number: 124 with model LastValueNaive in generation 0 of 5\nTemplate Eval Error: Exception('Transformer RegressionFilter failed on fit') in model 124 in generation 0: LastValueNaive\nModel Number: 125 with model UnobservedComponents in generation 0 of 5\nModel Number: 126 with model LastValueNaive in generation 0 of 5\nModel Number: 127 with model SectionalMotif in generation 0 of 5\nModel Number: 128 with model GLS in generation 0 of 5\nModel Number: 129 with model Theta in generation 0 of 5\nModel Number: 130 with model UnobservedComponents in generation 0 of 5\nModel Number: 131 with model WindowRegression in generation 0 of 5\nModel Number: 132 with model NVAR in generation 0 of 5\nModel Number: 133 with model SeasonalNaive in generation 0 of 5\nModel Number: 134 with model UnobservedComponents in generation 0 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 134 in generation 0: UnobservedComponents\nModel Number: 135 with model GLS in generation 0 of 5\nModel Number: 136 with model MetricMotif in generation 0 of 5\nModel Number: 137 with model VAR in generation 0 of 5\nModel Number: 138 with model WindowRegression in generation 0 of 5\nTemplate Eval Error: ImportError('Tensorflow not available, install with pip install tensorflow.') in model 138 in generation 0: WindowRegression\nModel Number: 139 with model GLM in generation 0 of 5\nModel Number: 140 with model NVAR in generation 0 of 5\nModel Number: 141 with model VAR in generation 0 of 5\nModel Number: 142 with model DatepartRegression in generation 0 of 5\nModel Number: 143 with model GLS in generation 0 of 5\nModel Number: 144 with model DatepartRegression in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 144 in generation 0: DatepartRegression\nModel Number: 145 with model VAR in generation 0 of 5\nModel Number: 146 with model Theta in generation 0 of 5\nModel Number: 147 with model ETS in generation 0 of 5\nModel Number: 148 with model ConstantNaive in generation 0 of 5\nModel Number: 149 with model ARDL in generation 0 of 5\nModel Number: 150 with model SectionalMotif in generation 0 of 5\nModel Number: 151 with model VAR in generation 0 of 5\nModel Number: 152 with model DatepartRegression in generation 0 of 5\nModel Number: 153 with model Theta in generation 0 of 5\nModel Number: 154 with model MetricMotif in generation 0 of 5\nModel Number: 155 with model DatepartRegression in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 155 in generation 0: DatepartRegression\nModel Number: 156 with model GLM in generation 0 of 5\nTemplate Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 156 in generation 0: GLM\nModel Number: 157 with model GluonTS in generation 0 of 5\nTemplate Eval Error: ImportError('GluonTS installation is incompatible with AutoTS. The numpy version is sometimes the issue, try 1.23.1 {as of 06-2023}') in model 157 in generation 0: GluonTS\nModel Number: 158 with model DatepartRegression in generation 0 of 5\nTemplate Eval Error: Exception('Transformer AlignLastValue failed on inverse') in model 158 in generation 0: DatepartRegression\nModel Number: 159 with model NVAR in generation 0 of 5\nModel Number: 160 with model GLS in generation 0 of 5\nModel Number: 161 with model SectionalMotif in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type=='User' but no future_regressor supplied\") in model 161 in generation 0: SectionalMotif\nModel Number: 162 with model FBProphet in generation 0 of 5\nTemplate Eval Error: ModuleNotFoundError(\"No module named 'fbprophet'\") in model 162 in generation 0: FBProphet\nModel Number: 163 with model ETS in generation 0 of 5\nModel Number: 164 with model VAR in generation 0 of 5\nModel Number: 165 with model MultivariateMotif in generation 0 of 5\nModel Number: 166 with model VAR in generation 0 of 5\nModel Number: 167 with model ARIMA in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 167 in generation 0: ARIMA\nModel Number: 168 with model UnobservedComponents in generation 0 of 5\nModel Number: 169 with model UnivariateMotif in generation 0 of 5\nModel Number: 170 with model VAR in generation 0 of 5\nTemplate Eval Error: Exception('Transformer AlignLastValue failed on inverse') in model 170 in generation 0: VAR\nModel Number: 171 with model Theta in generation 0 of 5\nModel Number: 172 with model MultivariateMotif in generation 0 of 5\nModel Number: 173 with model UnobservedComponents in generation 0 of 5\nModel Number: 174 with model WindowRegression in generation 0 of 5\nModel Number: 175 with model SeasonalNaive in generation 0 of 5\nModel Number: 176 with model ARDL in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 176 in generation 0: ARDL\nModel Number: 177 with model GLS in generation 0 of 5\nModel Number: 178 with model SectionalMotif in generation 0 of 5\nModel Number: 179 with model ARDL in generation 0 of 5\nModel Number: 180 with model MultivariateMotif in generation 0 of 5\nModel Number: 181 with model ARCH in generation 0 of 5\nTemplate Eval Error: ImportError('`arch` package must be installed from pip') in model 181 in generation 0: ARCH\nModel Number: 182 with model SectionalMotif in generation 0 of 5\nModel Number: 183 with model ARIMA in generation 0 of 5\nModel Number: 184 with model DatepartRegression in generation 0 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 184 in generation 0: DatepartRegression\nModel Number: 185 with model ETS in generation 0 of 5\nModel Number: 186 with model VECM in generation 0 of 5\nNew Generation: 1 of 5\nModel Number: 187 with model ETS in generation 1 of 5\nModel Number: 188 with model NVAR in generation 1 of 5\nModel Number: 189 with model ConstantNaive in generation 1 of 5\nModel Number: 190 with model Theta in generation 1 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 190 in generation 1: Theta\nModel Number: 191 with model SectionalMotif in generation 1 of 5\nModel Number: 192 with model AverageValueNaive in generation 1 of 5\nModel Number: 193 with model Theta in generation 1 of 5\nModel Number: 194 with model LastValueNaive in generation 1 of 5\nModel Number: 195 with model VAR in generation 1 of 5\nModel Number: 196 with model MetricMotif in generation 1 of 5\nTemplate Eval Error: ZeroDivisionError(\"Weights sum to zero, can't be normalized\") in model 196 in generation 1: MetricMotif\nModel Number: 197 with model DatepartRegression in generation 1 of 5\nModel Number: 198 with model MetricMotif in generation 1 of 5\nModel Number: 199 with model DatepartRegression in generation 1 of 5\nModel Number: 200 with model ConstantNaive in generation 1 of 5\nModel Number: 201 with model VAR in generation 1 of 5\nModel Number: 202 with model VAR in generation 1 of 5\nModel Number: 203 with model ARDL in generation 1 of 5\nModel Number: 204 with model SectionalMotif in generation 1 of 5\nModel Number: 205 with model LastValueNaive in generation 1 of 5\nModel Number: 206 with model GLS in generation 1 of 5\nModel Number: 207 with model ARIMA in generation 1 of 5\nModel Number: 208 with model GLS in generation 1 of 5\nModel Number: 209 with model MultivariateRegression in generation 1 of 5\nModel Number: 210 with model VECM in generation 1 of 5\nModel Number: 211 with model WindowRegression in generation 1 of 5\nModel Number: 212 with model NVAR in generation 1 of 5\nModel Number: 213 with model ARDL in generation 1 of 5\nTemplate Eval Error: ValueError(\"ARDL series CSUSHPISA failed with error IndexError('tuple index out of range') exog train None and predict None\") in model 213 in generation 1: ARDL\nModel Number: 214 with model AverageValueNaive in generation 1 of 5\nTemplate Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'zero', 'transformations': {'0': 'AlignLastValue', '1': 'ClipOutliers'}, 'transformation_params': {'0': {'rows': 1, 'lag': 1, 'method': 'multiplicative', 'strength': 1.0, 'first_value_only': False}, '1': {'method': 'clip', 'std_threshold': 1, 'fillna': None}}}. fail_on_forecast_nan=True\") in model 214 in generation 1: AverageValueNaive\nModel Number: 215 with model GLS in generation 1 of 5\nModel Number: 216 with model MultivariateRegression in generation 1 of 5\nModel Number: 217 with model MultivariateMotif in generation 1 of 5\nTemplate Eval Error: Exception('Transformer RegressionFilter failed on fit') in model 217 in generation 1: MultivariateMotif\nModel Number: 218 with model GLS in generation 1 of 5\nModel Number: 219 with model GLS in generation 1 of 5\nModel Number: 220 with model UnobservedComponents in generation 1 of 5\nModel Number: 221 with model VAR in generation 1 of 5\nModel Number: 222 with model ETS in generation 1 of 5\nModel Number: 223 with model AverageValueNaive in generation 1 of 5\nModel Number: 224 with model MultivariateMotif in generation 1 of 5\nModel Number: 225 with model ARDL in generation 1 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 225 in generation 1: ARDL\nModel Number: 226 with model ConstantNaive in generation 1 of 5\nModel Number: 227 with model MultivariateMotif in generation 1 of 5\nModel Number: 228 with model MultivariateMotif in generation 1 of 5\nModel Number: 229 with model NVAR in generation 1 of 5\nTemplate Eval Error: Exception('Transformer AlignLastValue failed on inverse') in model 229 in generation 1: NVAR\nModel Number: 230 with model VECM in generation 1 of 5\nModel Number: 231 with model GLS in generation 1 of 5\nModel Number: 232 with model AverageValueNaive in generation 1 of 5\nModel Number: 233 with model UnivariateMotif in generation 1 of 5\nModel Number: 234 with model LastValueNaive in generation 1 of 5\nModel Number: 235 with model VAR in generation 1 of 5\nModel Number: 236 with model ETS in generation 1 of 5\nModel Number: 237 with model NVAR in generation 1 of 5\nModel Number: 238 with model UnivariateMotif in generation 1 of 5\nModel Number: 239 with model UnobservedComponents in generation 1 of 5\nModel Number: 240 with model NVAR in generation 1 of 5\nModel Number: 241 with model DatepartRegression in generation 1 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 241 in generation 1: DatepartRegression\nModel Number: 242 with model UnobservedComponents in generation 1 of 5\nModel Number: 243 with model ConstantNaive in generation 1 of 5\nModel Number: 244 with model VAR in generation 1 of 5\nModel Number: 245 with model VAR in generation 1 of 5\nModel Number: 246 with model AverageValueNaive in generation 1 of 5\nModel Number: 247 with model AverageValueNaive in generation 1 of 5\nModel Number: 248 with model AverageValueNaive in generation 1 of 5\nModel Number: 249 with model LastValueNaive in generation 1 of 5\nModel Number: 250 with model ARDL in generation 1 of 5\nTemplate Eval Error: Exception('Transformer AlignLastValue failed on inverse') in model 250 in generation 1: ARDL\nModel Number: 251 with model ARDL in generation 1 of 5\nTemplate Eval Error: ValueError(\"ARDL series CSUSHPISA failed with error IndexError('tuple index out of range') exog train None and predict None\") in model 251 in generation 1: ARDL\nModel Number: 252 with model SectionalMotif in generation 1 of 5\nTemplate Eval Error: ValueError(\"regression_type=='User' but no future_regressor supplied\") in model 252 in generation 1: SectionalMotif\nModel Number: 253 with model VAR in generation 1 of 5\nModel Number: 254 with model ARIMA in generation 1 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 254 in generation 1: ARIMA\nModel Number: 255 with model Theta in generation 1 of 5\nModel Number: 256 with model VAR in generation 1 of 5\nModel Number: 257 with model VAR in generation 1 of 5\nModel Number: 258 with model NVAR in generation 1 of 5\nModel Number: 259 with model MetricMotif in generation 1 of 5\nModel Number: 260 with model LastValueNaive in generation 1 of 5\nModel Number: 261 with model ARDL in generation 1 of 5\nModel Number: 262 with model VECM in generation 1 of 5\nModel Number: 263 with model UnivariateRegression in generation 1 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 263 in generation 1: UnivariateRegression\nModel Number: 264 with model VAR in generation 1 of 5\nModel Number: 265 with model Theta in generation 1 of 5\nModel Number: 266 with model LastValueNaive in generation 1 of 5\nModel Number: 267 with model LastValueNaive in generation 1 of 5\nModel Number: 268 with model ETS in generation 1 of 5\nModel Number: 269 with model MultivariateMotif in generation 1 of 5\nModel Number: 270 with model ARIMA in generation 1 of 5\nModel Number: 271 with model GLM in generation 1 of 5\nModel Number: 272 with model MultivariateRegression in generation 1 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 272 in generation 1: MultivariateRegression\nModel Number: 273 with model SeasonalNaive in generation 1 of 5\nModel Number: 274 with model LastValueNaive in generation 1 of 5\nModel Number: 275 with model Theta in generation 1 of 5\nModel Number: 276 with model MetricMotif in generation 1 of 5\nModel Number: 277 with model VECM in generation 1 of 5\nModel Number: 278 with model AverageValueNaive in generation 1 of 5\nModel Number: 279 with model SeasonalNaive in generation 1 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 279 in generation 1: SeasonalNaive\nModel Number: 280 with model AverageValueNaive in generation 1 of 5\nTemplate Eval Error: Exception('Transformer SinTrend failed on fit') in model 280 in generation 1: AverageValueNaive\nModel Number: 281 with model GLS in generation 1 of 5\nModel Number: 282 with model MetricMotif in generation 1 of 5\nModel Number: 283 with model MultivariateRegression in generation 1 of 5\nTemplate Eval Error: ModuleNotFoundError(\"No module named 'xgboost'\") in model 283 in generation 1: MultivariateRegression\nModel Number: 284 with model MultivariateMotif in generation 1 of 5\nModel Number: 285 with model Theta in generation 1 of 5\nModel Number: 286 with model UnivariateMotif in generation 1 of 5\nModel Number: 287 with model NVAR in generation 1 of 5\nModel Number: 288 with model VECM in generation 1 of 5\nModel Number: 289 with model UnivariateMotif in generation 1 of 5\nModel Number: 290 with model SectionalMotif in generation 1 of 5\nModel Number: 291 with model UnivariateMotif in generation 1 of 5\nModel Number: 292 with model LastValueNaive in generation 1 of 5\nModel Number: 293 with model SeasonalNaive in generation 1 of 5\nModel Number: 294 with model UnobservedComponents in generation 1 of 5\nModel Number: 295 with model MultivariateMotif in generation 1 of 5\nModel Number: 296 with model UnobservedComponents in generation 1 of 5\nTemplate Eval Error: ValueError('Model UnobservedComponents returned NaN for one or more series. fail_on_forecast_nan=True') in model 296 in generation 1: UnobservedComponents\nModel Number: 297 with model MultivariateRegression in generation 1 of 5\nTemplate Eval Error: Exception('Transformer AlignLastValue failed on inverse') in model 297 in generation 1: MultivariateRegression\nModel Number: 298 with model AverageValueNaive in generation 1 of 5\nModel Number: 299 with model DatepartRegression in generation 1 of 5\nTemplate Eval Error: TypeError(\"__init__() got an unexpected keyword argument 'estimator'\") in model 299 in generation 1: DatepartRegression\nModel Number: 300 with model NVAR in generation 1 of 5\nModel Number: 301 with model WindowRegression in generation 1 of 5\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001343 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nModel Number: 302 with model Theta in generation 1 of 5\nModel Number: 303 with model VAR in generation 1 of 5\nModel Number: 304 with model ARDL in generation 1 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 304 in generation 1: ARDL\nModel Number: 305 with model VECM in generation 1 of 5\nModel Number: 306 with model Theta in generation 1 of 5\nModel Number: 307 with model ARDL in generation 1 of 5\nTemplate Eval Error: ValueError(\"ARDL series CSUSHPISA failed with error IndexError('tuple index out of range') exog train None and predict None\") in model 307 in generation 1: ARDL\nModel Number: 308 with model WindowRegression in generation 1 of 5\nModel Number: 309 with model ETS in generation 1 of 5\nModel Number: 310 with model GLS in generation 1 of 5\nModel Number: 311 with model GLS in generation 1 of 5\nNew Generation: 2 of 5\nModel Number: 312 with model MultivariateRegression in generation 2 of 5\nModel Number: 313 with model SectionalMotif in generation 2 of 5\nModel Number: 314 with model VAR in generation 2 of 5\nModel Number: 315 with model UnobservedComponents in generation 2 of 5\nModel Number: 316 with model ARDL in generation 2 of 5\nModel Number: 317 with model MultivariateMotif in generation 2 of 5\nModel Number: 318 with model MultivariateRegression in generation 2 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 318 in generation 2: MultivariateRegression\nModel Number: 319 with model MultivariateMotif in generation 2 of 5\nModel Number: 320 with model ConstantNaive in generation 2 of 5\nModel Number: 321 with model MetricMotif in generation 2 of 5\nModel Number: 322 with model AverageValueNaive in generation 2 of 5\nModel Number: 323 with model GLM in generation 2 of 5\nTemplate Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 323 in generation 2: GLM\nModel Number: 324 with model MultivariateMotif in generation 2 of 5\nModel Number: 325 with model UnivariateRegression in generation 2 of 5\nModel Number: 326 with model DatepartRegression in generation 2 of 5\nModel Number: 327 with model MultivariateMotif in generation 2 of 5\nModel Number: 328 with model LastValueNaive in generation 2 of 5\nModel Number: 329 with model UnivariateMotif in generation 2 of 5\nModel Number: 330 with model Theta in generation 2 of 5\nModel Number: 331 with model ConstantNaive in generation 2 of 5\nModel Number: 332 with model UnivariateMotif in generation 2 of 5\nModel Number: 333 with model MultivariateRegression in generation 2 of 5\nModel Number: 334 with model VECM in generation 2 of 5\nModel Number: 335 with model NVAR in generation 2 of 5\nModel Number: 336 with model UnobservedComponents in generation 2 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 336 in generation 2: UnobservedComponents\nModel Number: 337 with model VAR in generation 2 of 5\nModel Number: 338 with model VAR in generation 2 of 5\nModel Number: 339 with model GLS in generation 2 of 5\nModel Number: 340 with model AverageValueNaive in generation 2 of 5\nModel Number: 341 with model DatepartRegression in generation 2 of 5\nModel Number: 342 with model DatepartRegression in generation 2 of 5\nModel Number: 343 with model AverageValueNaive in generation 2 of 5\nModel Number: 344 with model ARIMA in generation 2 of 5\nModel Number: 345 with model AverageValueNaive in generation 2 of 5\nTemplate Eval Error: Exception('Transformer QuantileTransformer failed on inverse') in model 345 in generation 2: AverageValueNaive\nModel Number: 346 with model AverageValueNaive in generation 2 of 5\nModel Number: 347 with model VAR in generation 2 of 5\nModel Number: 348 with model AverageValueNaive in generation 2 of 5\nModel Number: 349 with model ARDL in generation 2 of 5\nModel Number: 350 with model ARDL in generation 2 of 5\nModel Number: 351 with model WindowRegression in generation 2 of 5\nModel Number: 352 with model VAR in generation 2 of 5\nModel Number: 353 with model AverageValueNaive in generation 2 of 5\nModel Number: 354 with model NVAR in generation 2 of 5\nModel Number: 355 with model ARIMA in generation 2 of 5\nModel Number: 356 with model SectionalMotif in generation 2 of 5\nModel Number: 357 with model Theta in generation 2 of 5\nModel Number: 358 with model ETS in generation 2 of 5\nModel Number: 359 with model GLS in generation 2 of 5\nModel Number: 360 with model ARIMA in generation 2 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 360 in generation 2: ARIMA\nModel Number: 361 with model ConstantNaive in generation 2 of 5\nModel Number: 362 with model VECM in generation 2 of 5\nModel Number: 363 with model ARDL in generation 2 of 5\nModel Number: 364 with model ETS in generation 2 of 5\nModel Number: 365 with model VECM in generation 2 of 5\nModel Number: 366 with model ConstantNaive in generation 2 of 5\nModel Number: 367 with model MultivariateRegression in generation 2 of 5\nModel Number: 368 with model MultivariateRegression in generation 2 of 5\nTemplate Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 368 in generation 2: MultivariateRegression\nModel Number: 369 with model UnivariateMotif in generation 2 of 5\nModel Number: 370 with model GLS in generation 2 of 5\nModel Number: 371 with model MetricMotif in generation 2 of 5\nModel Number: 372 with model SectionalMotif in generation 2 of 5\nModel Number: 373 with model VAR in generation 2 of 5\nModel Number: 374 with model UnobservedComponents in generation 2 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 374 in generation 2: UnobservedComponents\nModel Number: 375 with model SectionalMotif in generation 2 of 5\nModel Number: 376 with model MetricMotif in generation 2 of 5\nTemplate Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'ffill', 'transformations': {'0': 'AlignLastValue', '1': 'LevelShiftTransformer', '2': 'PowerTransformer', '3': 'SeasonalDifference'}, 'transformation_params': {'0': {'rows': 1, 'lag': 1, 'method': 'multiplicative', 'strength': 1.0, 'first_value_only': False}, '1': {'window_size': 30, 'alpha': 2.5, 'grouping_forward_limit': 3, 'max_level_shifts': 30, 'alignment': 'rolling_diff_3nn'}, '2': {}, '3': {'lag_1': 364, 'method': 'LastValue'}}}. fail_on_forecast_nan=True\") in model 376 in generation 2: MetricMotif\nModel Number: 377 with model VAR in generation 2 of 5\nModel Number: 378 with model Theta in generation 2 of 5\nModel Number: 379 with model ARIMA in generation 2 of 5\nModel Number: 380 with model AverageValueNaive in generation 2 of 5\nModel Number: 381 with model Theta in generation 2 of 5\nModel Number: 382 with model GLS in generation 2 of 5\nModel Number: 383 with model UnobservedComponents in generation 2 of 5\nModel Number: 384 with model AverageValueNaive in generation 2 of 5\nModel Number: 385 with model WindowRegression in generation 2 of 5\nModel Number: 386 with model LastValueNaive in generation 2 of 5\nModel Number: 387 with model UnivariateMotif in generation 2 of 5\nModel Number: 388 with model SeasonalNaive in generation 2 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 388 in generation 2: SeasonalNaive\nModel Number: 389 with model MetricMotif in generation 2 of 5\nModel Number: 390 with model Theta in generation 2 of 5\nModel Number: 391 with model SeasonalNaive in generation 2 of 5\nModel Number: 392 with model Theta in generation 2 of 5\nModel Number: 393 with model Theta in generation 2 of 5\nModel Number: 394 with model ETS in generation 2 of 5\nModel Number: 395 with model GLS in generation 2 of 5\nModel Number: 396 with model VECM in generation 2 of 5\nModel Number: 397 with model ConstantNaive in generation 2 of 5\nModel Number: 398 with model UnobservedComponents in generation 2 of 5\nModel Number: 399 with model AverageValueNaive in generation 2 of 5\nModel Number: 400 with model MultivariateMotif in generation 2 of 5\nModel Number: 401 with model LastValueNaive in generation 2 of 5\nModel Number: 402 with model Theta in generation 2 of 5\nModel Number: 403 with model VAR in generation 2 of 5\nModel Number: 404 with model MultivariateRegression in generation 2 of 5\nModel Number: 405 with model DatepartRegression in generation 2 of 5\nModel Number: 406 with model UnivariateMotif in generation 2 of 5\nModel Number: 407 with model UnobservedComponents in generation 2 of 5\nModel Number: 408 with model Theta in generation 2 of 5\nModel Number: 409 with model LastValueNaive in generation 2 of 5\nModel Number: 410 with model ETS in generation 2 of 5\nModel Number: 411 with model VAR in generation 2 of 5\nModel Number: 412 with model UnivariateMotif in generation 2 of 5\nModel Number: 413 with model UnobservedComponents in generation 2 of 5\nModel Number: 414 with model GLM in generation 2 of 5\nModel Number: 415 with model UnivariateMotif in generation 2 of 5\nModel Number: 416 with model MetricMotif in generation 2 of 5\nModel Number: 417 with model AverageValueNaive in generation 2 of 5\nTemplate Eval Error: Exception('Transformer DatepartRegression failed on fit') in model 417 in generation 2: AverageValueNaive\nModel Number: 418 with model GLS in generation 2 of 5\nModel Number: 419 with model NVAR in generation 2 of 5\nModel Number: 420 with model UnobservedComponents in generation 2 of 5\nModel Number: 421 with model SectionalMotif in generation 2 of 5\nModel Number: 422 with model Theta in generation 2 of 5\nModel Number: 423 with model MultivariateMotif in generation 2 of 5\nModel Number: 424 with model UnivariateRegression in generation 2 of 5\nModel Number: 425 with model VAR in generation 2 of 5\nModel Number: 426 with model VECM in generation 2 of 5\nModel Number: 427 with model ARDL in generation 2 of 5\nModel Number: 428 with model ARIMA in generation 2 of 5\nModel Number: 429 with model ETS in generation 2 of 5\nModel Number: 430 with model UnobservedComponents in generation 2 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 430 in generation 2: UnobservedComponents\nModel Number: 431 with model WindowRegression in generation 2 of 5\nModel Number: 432 with model UnivariateMotif in generation 2 of 5\nModel Number: 433 with model SectionalMotif in generation 2 of 5\nModel Number: 434 with model MultivariateMotif in generation 2 of 5\nModel Number: 435 with model Theta in generation 2 of 5\nModel Number: 436 with model ARDL in generation 2 of 5\nNew Generation: 3 of 5\nModel Number: 437 with model UnobservedComponents in generation 3 of 5\nModel Number: 438 with model VAR in generation 3 of 5\nModel Number: 439 with model WindowRegression in generation 3 of 5\nModel Number: 440 with model GLS in generation 3 of 5\nModel Number: 441 with model NVAR in generation 3 of 5\nModel Number: 442 with model DatepartRegression in generation 3 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 442 in generation 3: DatepartRegression\nModel Number: 443 with model MultivariateMotif in generation 3 of 5\nModel Number: 444 with model UnivariateMotif in generation 3 of 5\nModel Number: 445 with model Theta in generation 3 of 5\nModel Number: 446 with model Theta in generation 3 of 5\nModel Number: 447 with model SectionalMotif in generation 3 of 5\nTemplate Eval Error: Exception('Transformer AnomalyRemoval failed on fit') in model 447 in generation 3: SectionalMotif\nModel Number: 448 with model MetricMotif in generation 3 of 5\nModel Number: 449 with model MultivariateMotif in generation 3 of 5\nModel Number: 450 with model MetricMotif in generation 3 of 5\nModel Number: 451 with model SectionalMotif in generation 3 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 451 in generation 3: SectionalMotif\nModel Number: 452 with model MultivariateMotif in generation 3 of 5\nTemplate Eval Error: ValueError('Model MultivariateMotif returned NaN for one or more series. fail_on_forecast_nan=True') in model 452 in generation 3: MultivariateMotif\nModel Number: 453 with model LastValueNaive in generation 3 of 5\nModel Number: 454 with model ETS in generation 3 of 5\nModel Number: 455 with model UnivariateMotif in generation 3 of 5\nModel Number: 456 with model MetricMotif in generation 3 of 5\nModel Number: 457 with model NVAR in generation 3 of 5\nModel Number: 458 with model VAR in generation 3 of 5\nModel Number: 459 with model SectionalMotif in generation 3 of 5\nModel Number: 460 with model AverageValueNaive in generation 3 of 5\nModel Number: 461 with model ARDL in generation 3 of 5\nModel Number: 462 with model LastValueNaive in generation 3 of 5\nModel Number: 463 with model SectionalMotif in generation 3 of 5\nModel Number: 464 with model NVAR in generation 3 of 5\nModel Number: 465 with model WindowRegression in generation 3 of 5\nModel Number: 466 with model UnobservedComponents in generation 3 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 466 in generation 3: UnobservedComponents\nModel Number: 467 with model Theta in generation 3 of 5\nModel Number: 468 with model NVAR in generation 3 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 468 in generation 3: NVAR\nModel Number: 469 with model Theta in generation 3 of 5\nModel Number: 470 with model MultivariateMotif in generation 3 of 5\nModel Number: 471 with model NVAR in generation 3 of 5\nModel Number: 472 with model SectionalMotif in generation 3 of 5\nModel Number: 473 with model ARIMA in generation 3 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 473 in generation 3: ARIMA\nModel Number: 474 with model DatepartRegression in generation 3 of 5\nModel Number: 475 with model GLM in generation 3 of 5\nModel Number: 476 with model UnivariateRegression in generation 3 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 476 in generation 3: UnivariateRegression\nModel Number: 477 with model UnobservedComponents in generation 3 of 5\nModel Number: 478 with model GLS in generation 3 of 5\nModel Number: 479 with model UnobservedComponents in generation 3 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 479 in generation 3: UnobservedComponents\nModel Number: 480 with model DatepartRegression in generation 3 of 5\nTemplate Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 480 in generation 3: DatepartRegression\nModel Number: 481 with model AverageValueNaive in generation 3 of 5\nModel Number: 482 with model MultivariateRegression in generation 3 of 5\nTemplate Eval Error: Exception('Transformer RegressionFilter failed on fit') in model 482 in generation 3: MultivariateRegression\nModel Number: 483 with model WindowRegression in generation 3 of 5\nTemplate Eval Error: TypeError(\"__init__() got an unexpected keyword argument 'estimator'\") in model 483 in generation 3: WindowRegression\nModel Number: 484 with model MetricMotif in generation 3 of 5\nModel Number: 485 with model NVAR in generation 3 of 5\nModel Number: 486 with model SeasonalNaive in generation 3 of 5\nModel Number: 487 with model ARDL in generation 3 of 5\nModel Number: 488 with model MultivariateMotif in generation 3 of 5\nModel Number: 489 with model UnivariateMotif in generation 3 of 5\nModel Number: 490 with model LastValueNaive in generation 3 of 5\nModel Number: 491 with model SectionalMotif in generation 3 of 5\nTemplate Eval Error: ValueError(\"regression_type=='User' but no future_regressor supplied\") in model 491 in generation 3: SectionalMotif\nModel Number: 492 with model VAR in generation 3 of 5\nModel Number: 493 with model SectionalMotif in generation 3 of 5\nModel Number: 494 with model MultivariateMotif in generation 3 of 5\nModel Number: 495 with model UnobservedComponents in generation 3 of 5\nModel Number: 496 with model ETS in generation 3 of 5\nModel Number: 497 with model ARIMA in generation 3 of 5\nModel Number: 498 with model VAR in generation 3 of 5\nModel Number: 499 with model AverageValueNaive in generation 3 of 5\nModel Number: 500 with model ARDL in generation 3 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 500 in generation 3: ARDL\nModel Number: 501 with model ConstantNaive in generation 3 of 5\nModel Number: 502 with model ConstantNaive in generation 3 of 5\nModel Number: 503 with model MultivariateRegression in generation 3 of 5\nTemplate Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 503 in generation 3: MultivariateRegression\nModel Number: 504 with model VECM in generation 3 of 5\nModel Number: 505 with model AverageValueNaive in generation 3 of 5\nModel Number: 506 with model LastValueNaive in generation 3 of 5\nModel Number: 507 with model VAR in generation 3 of 5\nModel Number: 508 with model UnivariateRegression in generation 3 of 5\nModel Number: 509 with model UnivariateMotif in generation 3 of 5\nModel Number: 510 with model Theta in generation 3 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 510 in generation 3: Theta\nModel Number: 511 with model ARDL in generation 3 of 5\nTemplate Eval Error: Exception('Transformer MinMaxScaler failed on fit') in model 511 in generation 3: ARDL\nModel Number: 512 with model GLS in generation 3 of 5\nModel Number: 513 with model AverageValueNaive in generation 3 of 5\nModel Number: 514 with model UnivariateMotif in generation 3 of 5\nModel Number: 515 with model AverageValueNaive in generation 3 of 5\nModel Number: 516 with model GLM in generation 3 of 5\nTemplate Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 516 in generation 3: GLM\nModel Number: 517 with model MultivariateRegression in generation 3 of 5\nModel Number: 518 with model LastValueNaive in generation 3 of 5\nModel Number: 519 with model NVAR in generation 3 of 5\nModel Number: 520 with model VAR in generation 3 of 5\nModel Number: 521 with model MetricMotif in generation 3 of 5\nModel Number: 522 with model MultivariateRegression in generation 3 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 522 in generation 3: MultivariateRegression\nModel Number: 523 with model NVAR in generation 3 of 5\nModel Number: 524 with model GLS in generation 3 of 5\nModel Number: 525 with model Theta in generation 3 of 5\nModel Number: 526 with model UnobservedComponents in generation 3 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 526 in generation 3: UnobservedComponents\nModel Number: 527 with model MultivariateMotif in generation 3 of 5\nModel Number: 528 with model AverageValueNaive in generation 3 of 5\nModel Number: 529 with model AverageValueNaive in generation 3 of 5\nModel Number: 530 with model ConstantNaive in generation 3 of 5\nModel Number: 531 with model Theta in generation 3 of 5\nModel Number: 532 with model MultivariateMotif in generation 3 of 5\nModel Number: 533 with model NVAR in generation 3 of 5\nModel Number: 534 with model SeasonalNaive in generation 3 of 5\nModel Number: 535 with model UnivariateMotif in generation 3 of 5\nModel Number: 536 with model AverageValueNaive in generation 3 of 5\nModel Number: 537 with model ARDL in generation 3 of 5\nModel Number: 538 with model UnivariateMotif in generation 3 of 5\nModel Number: 539 with model ARDL in generation 3 of 5\nTemplate Eval Error: ValueError(\"ARDL series CSUSHPISA failed with error IndexError('tuple index out of range') exog train None and predict None\") in model 539 in generation 3: ARDL\nModel Number: 540 with model ARIMA in generation 3 of 5\nTemplate Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'rolling_mean', 'transformations': {'0': 'AnomalyRemoval', '1': 'QuantileTransformer', '2': 'MinMaxScaler'}, 'transformation_params': {'0': {'method': 'IQR', 'method_params': {'iqr_threshold': 2.0, 'iqr_quantiles': [0.25, 0.75]}, 'fillna': None, 'transform_dict': None}, '1': {'output_distribution': 'uniform', 'n_quantiles': 79}, '2': {}}}. fail_on_forecast_nan=True\") in model 540 in generation 3: ARIMA\nModel Number: 541 with model ARIMA in generation 3 of 5\nModel Number: 542 with model MultivariateMotif in generation 3 of 5\nModel Number: 543 with model ConstantNaive in generation 3 of 5\nModel Number: 544 with model ConstantNaive in generation 3 of 5\nModel Number: 545 with model VECM in generation 3 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 545 in generation 3: VECM\nModel Number: 546 with model VAR in generation 3 of 5\nModel Number: 547 with model AverageValueNaive in generation 3 of 5\nModel Number: 548 with model Theta in generation 3 of 5\nModel Number: 549 with model ARDL in generation 3 of 5\nModel Number: 550 with model NVAR in generation 3 of 5\nModel Number: 551 with model MultivariateRegression in generation 3 of 5\nModel Number: 552 with model MultivariateMotif in generation 3 of 5\nModel Number: 553 with model Theta in generation 3 of 5\nModel Number: 554 with model ARIMA in generation 3 of 5\nModel Number: 555 with model VECM in generation 3 of 5\nModel Number: 556 with model Theta in generation 3 of 5\nModel Number: 557 with model AverageValueNaive in generation 3 of 5\nModel Number: 558 with model GLS in generation 3 of 5\nModel Number: 559 with model MultivariateMotif in generation 3 of 5\nModel Number: 560 with model ARDL in generation 3 of 5\nTemplate Eval Error: ValueError(\"ARDL series CSUSHPISA failed with error IndexError('tuple index out of range') exog train None and predict None\") in model 560 in generation 3: ARDL\nModel Number: 561 with model NVAR in generation 3 of 5\nNew Generation: 4 of 5\nModel Number: 562 with model MultivariateRegression in generation 4 of 5\nModel Number: 563 with model WindowRegression in generation 4 of 5\nModel Number: 564 with model AverageValueNaive in generation 4 of 5\nModel Number: 565 with model NVAR in generation 4 of 5\nModel Number: 566 with model VAR in generation 4 of 5\nModel Number: 567 with model AverageValueNaive in generation 4 of 5\nModel Number: 568 with model VAR in generation 4 of 5\nModel Number: 569 with model VAR in generation 4 of 5\nTemplate Eval Error: Exception('Transformer PCA failed on fit') in model 569 in generation 4: VAR\nModel Number: 570 with model LastValueNaive in generation 4 of 5\nModel Number: 571 with model AverageValueNaive in generation 4 of 5\nModel Number: 572 with model MultivariateMotif in generation 4 of 5\nModel Number: 573 with model MultivariateMotif in generation 4 of 5\nModel Number: 574 with model VAR in generation 4 of 5\nModel Number: 575 with model ARDL in generation 4 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 575 in generation 4: ARDL\nModel Number: 576 with model ConstantNaive in generation 4 of 5\nModel Number: 577 with model UnobservedComponents in generation 4 of 5\nModel Number: 578 with model LastValueNaive in generation 4 of 5\nModel Number: 579 with model ARDL in generation 4 of 5\nModel Number: 580 with model MultivariateMotif in generation 4 of 5\nModel Number: 581 with model ARIMA in generation 4 of 5\nModel Number: 582 with model VECM in generation 4 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 582 in generation 4: VECM\nModel Number: 583 with model SectionalMotif in generation 4 of 5\nModel Number: 584 with model LastValueNaive in generation 4 of 5\nModel Number: 585 with model GLS in generation 4 of 5\nModel Number: 586 with model LastValueNaive in generation 4 of 5\nModel Number: 587 with model Theta in generation 4 of 5\nModel Number: 588 with model ConstantNaive in generation 4 of 5\nModel Number: 589 with model LastValueNaive in generation 4 of 5\nModel Number: 590 with model UnivariateMotif in generation 4 of 5\nModel Number: 591 with model UnivariateMotif in generation 4 of 5\nModel Number: 592 with model ARDL in generation 4 of 5\nModel Number: 593 with model AverageValueNaive in generation 4 of 5\nModel Number: 594 with model NVAR in generation 4 of 5\nModel Number: 595 with model VECM in generation 4 of 5\nModel Number: 596 with model ARIMA in generation 4 of 5\nModel Number: 597 with model ARDL in generation 4 of 5\nModel Number: 598 with model UnivariateMotif in generation 4 of 5\nModel Number: 599 with model AverageValueNaive in generation 4 of 5\nModel Number: 600 with model UnobservedComponents in generation 4 of 5\nModel Number: 601 with model MultivariateMotif in generation 4 of 5\nModel Number: 602 with model UnivariateMotif in generation 4 of 5\nModel Number: 603 with model NVAR in generation 4 of 5\nModel Number: 604 with model WindowRegression in generation 4 of 5\nModel Number: 605 with model UnivariateMotif in generation 4 of 5\nTemplate Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'ffill', 'transformations': {'0': 'AlignLastValue', '1': 'AnomalyRemoval', '2': 'PowerTransformer'}, 'transformation_params': {'0': {'rows': 1, 'lag': 1, 'method': 'multiplicative', 'strength': 1.0, 'first_value_only': False}, '1': {'method': 'IQR', 'method_params': {'iqr_threshold': 3.0, 'iqr_quantiles': [0.25, 0.75]}, 'fillna': 'ffill', 'transform_dict': None}, '2': {}}}. fail_on_forecast_nan=True\") in model 605 in generation 4: UnivariateMotif\nModel Number: 606 with model AverageValueNaive in generation 4 of 5\nModel Number: 607 with model ARDL in generation 4 of 5\nTemplate Eval Error: ValueError(\"ARDL series CSUSHPISA failed with error ValueError('integer orders must be at least 1 when causal is True.') exog train             holiday_flag_US\\ndatetime                   \\n1953-04-01              0.0\\n1953-05-01              0.0\\n1953-06-01              0.0\\n1953-07-01              0.0\\n1953-08-01              0.0\\n...                     ...\\n2019-05-01              0.0\\n2019-06-01              0.0\\n2019-07-01              0.0\\n2019-08-01              0.0\\n2019-09-01              0.0\\n\\n[798 rows x 1 columns] and predict             holiday_flag_US\\n2019-10-01              0.0\\n2019-11-01              0.0\\n2019-12-01              0.0\") in model 607 in generation 4: ARDL\nModel Number: 608 with model WindowRegression in generation 4 of 5\nModel Number: 609 with model MultivariateRegression in generation 4 of 5\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000181 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\nModel Number: 610 with model ARDL in generation 4 of 5\nTemplate Eval Error: Exception('Transformer AlignLastValue failed on inverse') in model 610 in generation 4: ARDL\nModel Number: 611 with model ETS in generation 4 of 5\nModel Number: 612 with model VAR in generation 4 of 5\nModel Number: 613 with model ConstantNaive in generation 4 of 5\nModel Number: 614 with model WindowRegression in generation 4 of 5\nModel Number: 615 with model NVAR in generation 4 of 5\nModel Number: 616 with model Theta in generation 4 of 5\nModel Number: 617 with model ARDL in generation 4 of 5\nModel Number: 618 with model UnobservedComponents in generation 4 of 5\nModel Number: 619 with model MultivariateMotif in generation 4 of 5\nModel Number: 620 with model VAR in generation 4 of 5\nModel Number: 621 with model VECM in generation 4 of 5\nModel Number: 622 with model Theta in generation 4 of 5\nModel Number: 623 with model MetricMotif in generation 4 of 5\nModel Number: 624 with model ARIMA in generation 4 of 5\nModel Number: 625 with model VAR in generation 4 of 5\nModel Number: 626 with model ARDL in generation 4 of 5\nModel Number: 627 with model ARIMA in generation 4 of 5\nModel Number: 628 with model ARIMA in generation 4 of 5\nModel Number: 629 with model ARIMA in generation 4 of 5\nModel Number: 630 with model MultivariateRegression in generation 4 of 5\nModel Number: 631 with model SectionalMotif in generation 4 of 5\nModel Number: 632 with model GLS in generation 4 of 5\nModel Number: 633 with model NVAR in generation 4 of 5\nModel Number: 634 with model VAR in generation 4 of 5\nModel Number: 635 with model AverageValueNaive in generation 4 of 5\nModel Number: 636 with model VAR in generation 4 of 5\nModel Number: 637 with model LastValueNaive in generation 4 of 5\nModel Number: 638 with model MultivariateRegression in generation 4 of 5\nTemplate Eval Error: AttributeError(\"'str' object has no attribute 'decode'\") in model 638 in generation 4: MultivariateRegression\nModel Number: 639 with model UnivariateMotif in generation 4 of 5\nModel Number: 640 with model WindowRegression in generation 4 of 5\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\nTemplate Eval Error: LightGBMError('[gamma]: at least one target label is negative') in model 640 in generation 4: WindowRegression\nModel Number: 641 with model Theta in generation 4 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 641 in generation 4: Theta\nModel Number: 642 with model SectionalMotif in generation 4 of 5\nModel Number: 643 with model DatepartRegression in generation 4 of 5\nModel Number: 644 with model VAR in generation 4 of 5\nModel Number: 645 with model VAR in generation 4 of 5\nModel Number: 646 with model MultivariateRegression in generation 4 of 5\nModel Number: 647 with model ETS in generation 4 of 5\nModel Number: 648 with model MultivariateMotif in generation 4 of 5\nModel Number: 649 with model NVAR in generation 4 of 5\nModel Number: 650 with model MetricMotif in generation 4 of 5\nModel Number: 651 with model AverageValueNaive in generation 4 of 5\nModel Number: 652 with model AverageValueNaive in generation 4 of 5\nModel Number: 653 with model NVAR in generation 4 of 5\nModel Number: 654 with model DatepartRegression in generation 4 of 5\nModel Number: 655 with model Theta in generation 4 of 5\nModel Number: 656 with model MultivariateMotif in generation 4 of 5\nModel Number: 657 with model ConstantNaive in generation 4 of 5\nModel Number: 658 with model AverageValueNaive in generation 4 of 5\nModel Number: 659 with model LastValueNaive in generation 4 of 5\nModel Number: 660 with model ARIMA in generation 4 of 5\nModel Number: 661 with model DatepartRegression in generation 4 of 5\nModel Number: 662 with model ARIMA in generation 4 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 662 in generation 4: ARIMA\nModel Number: 663 with model MultivariateMotif in generation 4 of 5\nModel Number: 664 with model MultivariateMotif in generation 4 of 5\nModel Number: 665 with model MultivariateMotif in generation 4 of 5\nTemplate Eval Error: ValueError(\"Model returned NaN due to a preprocessing transformer {'fillna': 'ffill', 'transformations': {'0': 'PositiveShift', '1': 'SinTrend', '2': 'PowerTransformer', '3': 'Detrend'}, 'transformation_params': {'0': {}, '1': {}, '2': {}, '3': {'model': 'GLS', 'phi': 1, 'window': None, 'transform_dict': None}}}. fail_on_forecast_nan=True\") in model 665 in generation 4: MultivariateMotif\nModel Number: 666 with model Theta in generation 4 of 5\nModel Number: 667 with model UnivariateMotif in generation 4 of 5\nModel Number: 668 with model VECM in generation 4 of 5\nModel Number: 669 with model LastValueNaive in generation 4 of 5\nModel Number: 670 with model VECM in generation 4 of 5\nModel Number: 671 with model GLM in generation 4 of 5\nModel Number: 672 with model ETS in generation 4 of 5\nModel Number: 673 with model ConstantNaive in generation 4 of 5\nModel Number: 674 with model ARDL in generation 4 of 5\nModel Number: 675 with model Theta in generation 4 of 5\nModel Number: 676 with model MultivariateRegression in generation 4 of 5\nModel Number: 677 with model ConstantNaive in generation 4 of 5\nModel Number: 678 with model ConstantNaive in generation 4 of 5\nModel Number: 679 with model UnobservedComponents in generation 4 of 5\nModel Number: 680 with model ConstantNaive in generation 4 of 5\nModel Number: 681 with model SectionalMotif in generation 4 of 5\nModel Number: 682 with model VECM in generation 4 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 682 in generation 4: VECM\nModel Number: 683 with model UnobservedComponents in generation 4 of 5\nModel Number: 684 with model UnobservedComponents in generation 4 of 5\nTemplate Eval Error: LinAlgError('Singular matrix') in model 684 in generation 4: UnobservedComponents\nModel Number: 685 with model UnobservedComponents in generation 4 of 5\nModel Number: 686 with model UnivariateMotif in generation 4 of 5\nNew Generation: 5 of 5\nModel Number: 687 with model Theta in generation 5 of 5\nModel Number: 688 with model GLS in generation 5 of 5\nModel Number: 689 with model LastValueNaive in generation 5 of 5\nModel Number: 690 with model LastValueNaive in generation 5 of 5\nModel Number: 691 with model AverageValueNaive in generation 5 of 5\nModel Number: 692 with model ARIMA in generation 5 of 5\nModel Number: 693 with model ARIMA in generation 5 of 5\nModel Number: 694 with model MultivariateMotif in generation 5 of 5\nModel Number: 695 with model WindowRegression in generation 5 of 5\nTemplate Eval Error: Exception('Transformer AlignLastValue failed on inverse') in model 695 in generation 5: WindowRegression\nModel Number: 696 with model MultivariateRegression in generation 5 of 5\nModel Number: 697 with model UnobservedComponents in generation 5 of 5\nModel Number: 698 with model AverageValueNaive in generation 5 of 5\nModel Number: 699 with model SeasonalNaive in generation 5 of 5\nModel Number: 700 with model NVAR in generation 5 of 5\nModel Number: 701 with model VAR in generation 5 of 5\nModel Number: 702 with model VAR in generation 5 of 5\nModel Number: 703 with model MultivariateMotif in generation 5 of 5\nModel Number: 704 with model WindowRegression in generation 5 of 5\nModel Number: 705 with model LastValueNaive in generation 5 of 5\nModel Number: 706 with model ARDL in generation 5 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 706 in generation 5: ARDL\nModel Number: 707 with model MultivariateRegression in generation 5 of 5\nModel Number: 708 with model GLS in generation 5 of 5\nModel Number: 709 with model ARDL in generation 5 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 709 in generation 5: ARDL\nModel Number: 710 with model ARDL in generation 5 of 5\nModel Number: 711 with model UnivariateMotif in generation 5 of 5\nModel Number: 712 with model Theta in generation 5 of 5\nModel Number: 713 with model AverageValueNaive in generation 5 of 5\nModel Number: 714 with model VAR in generation 5 of 5\nModel Number: 715 with model UnivariateMotif in generation 5 of 5\nModel Number: 716 with model SectionalMotif in generation 5 of 5\nModel Number: 717 with model AverageValueNaive in generation 5 of 5\nModel Number: 718 with model UnivariateMotif in generation 5 of 5\nModel Number: 719 with model MultivariateMotif in generation 5 of 5\nModel Number: 720 with model UnivariateMotif in generation 5 of 5\nTemplate Eval Error: Exception('Transformer AlignLastValue failed on inverse') in model 720 in generation 5: UnivariateMotif\nModel Number: 721 with model ETS in generation 5 of 5\nModel Number: 722 with model GLS in generation 5 of 5\nModel Number: 723 with model ETS in generation 5 of 5\nModel Number: 724 with model AverageValueNaive in generation 5 of 5\nModel Number: 725 with model Theta in generation 5 of 5\nModel Number: 726 with model ARIMA in generation 5 of 5\nModel Number: 727 with model VAR in generation 5 of 5\nModel Number: 728 with model NVAR in generation 5 of 5\nModel Number: 729 with model LastValueNaive in generation 5 of 5\nModel Number: 730 with model ARDL in generation 5 of 5\nModel Number: 731 with model UnobservedComponents in generation 5 of 5\nModel Number: 732 with model SectionalMotif in generation 5 of 5\nModel Number: 733 with model AverageValueNaive in generation 5 of 5\nModel Number: 734 with model VECM in generation 5 of 5\nTemplate Eval Error: Exception('Transformer AlignLastValue failed on inverse') in model 734 in generation 5: VECM\nModel Number: 735 with model UnobservedComponents in generation 5 of 5\nModel Number: 736 with model MultivariateRegression in generation 5 of 5\nModel Number: 737 with model NVAR in generation 5 of 5\nModel Number: 738 with model ARIMA in generation 5 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 738 in generation 5: ARIMA\nModel Number: 739 with model ARIMA in generation 5 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 739 in generation 5: ARIMA\nModel Number: 740 with model AverageValueNaive in generation 5 of 5\nModel Number: 741 with model MultivariateMotif in generation 5 of 5\nModel Number: 742 with model UnobservedComponents in generation 5 of 5\nModel Number: 743 with model Theta in generation 5 of 5\nModel Number: 744 with model LastValueNaive in generation 5 of 5\nModel Number: 745 with model AverageValueNaive in generation 5 of 5\nModel Number: 746 with model GLS in generation 5 of 5\nModel Number: 747 with model ConstantNaive in generation 5 of 5\nModel Number: 748 with model NVAR in generation 5 of 5\nModel Number: 749 with model SeasonalNaive in generation 5 of 5\nModel Number: 750 with model NVAR in generation 5 of 5\nModel Number: 751 with model LastValueNaive in generation 5 of 5\nModel Number: 752 with model ConstantNaive in generation 5 of 5\nModel Number: 753 with model ARIMA in generation 5 of 5\nModel Number: 754 with model Theta in generation 5 of 5\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 754 in generation 5: Theta\nModel Number: 755 with model UnivariateMotif in generation 5 of 5\nModel Number: 756 with model VAR in generation 5 of 5\nTemplate Eval Error: Exception('Transformer AlignLastValue failed on inverse') in model 756 in generation 5: VAR\nModel Number: 757 with model MetricMotif in generation 5 of 5\nModel Number: 758 with model DatepartRegression in generation 5 of 5\nModel Number: 759 with model ARIMA in generation 5 of 5\nModel Number: 760 with model Theta in generation 5 of 5\nModel Number: 761 with model VECM in generation 5 of 5\nModel Number: 762 with model UnobservedComponents in generation 5 of 5\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 762 in generation 5: UnobservedComponents\nModel Number: 763 with model Theta in generation 5 of 5\nModel Number: 764 with model ARDL in generation 5 of 5\nModel Number: 765 with model Theta in generation 5 of 5\nModel Number: 766 with model VECM in generation 5 of 5\nModel Number: 767 with model MultivariateRegression in generation 5 of 5\nModel Number: 768 with model ARIMA in generation 5 of 5\nModel Number: 769 with model UnivariateMotif in generation 5 of 5\nModel Number: 770 with model NVAR in generation 5 of 5\nModel Number: 771 with model MultivariateMotif in generation 5 of 5\nModel Number: 772 with model Theta in generation 5 of 5\nModel Number: 773 with model VAR in generation 5 of 5\nModel Number: 774 with model ARDL in generation 5 of 5\nModel Number: 775 with model SectionalMotif in generation 5 of 5\nModel Number: 776 with model ConstantNaive in generation 5 of 5\nModel Number: 777 with model SectionalMotif in generation 5 of 5\nModel Number: 778 with model VAR in generation 5 of 5\nModel Number: 779 with model WindowRegression in generation 5 of 5\nModel Number: 780 with model MultivariateMotif in generation 5 of 5\nModel Number: 781 with model UnivariateMotif in generation 5 of 5\nModel Number: 782 with model AverageValueNaive in generation 5 of 5\nModel Number: 783 with model ARDL in generation 5 of 5\nModel Number: 784 with model VAR in generation 5 of 5\nModel Number: 785 with model ARIMA in generation 5 of 5\nModel Number: 786 with model MultivariateMotif in generation 5 of 5\nModel Number: 787 with model Ensemble in generation 6 of Ensembles\nModel Number: 788 with model Ensemble in generation 6 of Ensembles\nModel Number: 789 with model Ensemble in generation 6 of Ensembles\nModel Number: 790 with model Ensemble in generation 6 of Ensembles\nModel Number: 791 with model Ensemble in generation 6 of Ensembles\nModel Number: 792 with model Ensemble in generation 6 of Ensembles\nModel Number: 793 with model Ensemble in generation 6 of Ensembles\nModel Number: 794 with model Ensemble in generation 6 of Ensembles\nModel Number: 795 with model Ensemble in generation 6 of Ensembles\nModel Number: 796 with model Ensemble in generation 6 of Ensembles\nModel Number: 797 with model Ensemble in generation 6 of Ensembles\nValidation Round: 1\nModel Number: 1 of 119 with model Ensemble for Validation 1\n📈 1 - Ensemble with avg smape 7.16: \nModel Number: 2 of 119 with model Ensemble for Validation 1\n📈 2 - Ensemble with avg smape 7.0: \nModel Number: 3 of 119 with model Ensemble for Validation 1\n3 - Ensemble with avg smape 7.17: \nModel Number: 4 of 119 with model VAR for Validation 1\n4 - VAR with avg smape 7.31: \nModel Number: 5 of 119 with model VAR for Validation 1\n📈 5 - VAR with avg smape 6.85: \nModel Number: 6 of 119 with model Ensemble for Validation 1\n6 - Ensemble with avg smape 6.9: \nModel Number: 7 of 119 with model ARDL for Validation 1\n7 - ARDL with avg smape 7.85: \nModel Number: 8 of 119 with model VAR for Validation 1\n8 - VAR with avg smape 6.87: \nModel Number: 9 of 119 with model VAR for Validation 1\n9 - VAR with avg smape 7.33: \nModel Number: 10 of 119 with model MultivariateMotif for Validation 1\n10 - MultivariateMotif with avg smape 7.93: \nModel Number: 11 of 119 with model ARDL for Validation 1\n11 - ARDL with avg smape 7.78: \nModel Number: 12 of 119 with model Ensemble for Validation 1\n12 - Ensemble with avg smape 10.17: \nModel Number: 13 of 119 with model VAR for Validation 1\n13 - VAR with avg smape 7.2: \nModel Number: 14 of 119 with model Ensemble for Validation 1\n14 - Ensemble with avg smape 7.61: \nModel Number: 15 of 119 with model VAR for Validation 1\n15 - VAR with avg smape 7.03: \nModel Number: 16 of 119 with model NVAR for Validation 1\n16 - NVAR with avg smape 8.22: \nModel Number: 17 of 119 with model MultivariateMotif for Validation 1\n17 - MultivariateMotif with avg smape 7.59: \nModel Number: 18 of 119 with model NVAR for Validation 1\n18 - NVAR with avg smape 8.22: \nModel Number: 19 of 119 with model NVAR for Validation 1\n19 - NVAR with avg smape 23.01: \nModel Number: 20 of 119 with model ARDL for Validation 1\n20 - ARDL with avg smape 7.76: \nModel Number: 21 of 119 with model MultivariateMotif for Validation 1\n21 - MultivariateMotif with avg smape 7.68: \nModel Number: 22 of 119 with model ARDL for Validation 1\n22 - ARDL with avg smape 7.51: \nModel Number: 23 of 119 with model ARDL for Validation 1\n23 - ARDL with avg smape 7.51: \nModel Number: 24 of 119 with model Theta for Validation 1\n24 - Theta with avg smape 7.5: \nModel Number: 25 of 119 with model ARIMA for Validation 1\n25 - ARIMA with avg smape 8.0: \nModel Number: 26 of 119 with model MultivariateMotif for Validation 1\n26 - MultivariateMotif with avg smape 7.76: \nModel Number: 27 of 119 with model Theta for Validation 1\n27 - Theta with avg smape 7.6: \nModel Number: 28 of 119 with model Theta for Validation 1\n28 - Theta with avg smape 7.5: \nModel Number: 29 of 119 with model ARIMA for Validation 1\n29 - ARIMA with avg smape 8.0: \nModel Number: 30 of 119 with model Theta for Validation 1\n30 - Theta with avg smape 7.59: \nModel Number: 31 of 119 with model ARDL for Validation 1\n31 - ARDL with avg smape 8.0: \nModel Number: 32 of 119 with model ARIMA for Validation 1\n32 - ARIMA with avg smape 7.93: \nModel Number: 33 of 119 with model Theta for Validation 1\n33 - Theta with avg smape 7.51: \nModel Number: 34 of 119 with model NVAR for Validation 1\n34 - NVAR with avg smape 7.66: \nModel Number: 35 of 119 with model MultivariateMotif for Validation 1\n35 - MultivariateMotif with avg smape 7.38: \nModel Number: 36 of 119 with model Theta for Validation 1\n36 - Theta with avg smape 7.59: \nModel Number: 37 of 119 with model AverageValueNaive for Validation 1\n37 - AverageValueNaive with avg smape 7.62: \nModel Number: 38 of 119 with model AverageValueNaive for Validation 1\n38 - AverageValueNaive with avg smape 7.5: \nModel Number: 39 of 119 with model MultivariateMotif for Validation 1\n39 - MultivariateMotif with avg smape 8.14: \nModel Number: 40 of 119 with model NVAR for Validation 1\n40 - NVAR with avg smape 92.61: \nModel Number: 41 of 119 with model ARIMA for Validation 1\n41 - ARIMA with avg smape 27.12: \nModel Number: 42 of 119 with model AverageValueNaive for Validation 1\n42 - AverageValueNaive with avg smape 7.49: \nModel Number: 43 of 119 with model ARIMA for Validation 1\n43 - ARIMA with avg smape 7.52: \nModel Number: 44 of 119 with model AverageValueNaive for Validation 1\n44 - AverageValueNaive with avg smape 7.54: \nModel Number: 45 of 119 with model NVAR for Validation 1\n45 - NVAR with avg smape 9.17: \nModel Number: 46 of 119 with model AverageValueNaive for Validation 1\n46 - AverageValueNaive with avg smape 14.42: \nModel Number: 47 of 119 with model AverageValueNaive for Validation 1\n47 - AverageValueNaive with avg smape 14.6: \nModel Number: 48 of 119 with model UnivariateMotif for Validation 1\n48 - UnivariateMotif with avg smape 8.69: \nModel Number: 49 of 119 with model UnobservedComponents for Validation 1\n49 - UnobservedComponents with avg smape 7.51: \nModel Number: 50 of 119 with model ARIMA for Validation 1\n50 - ARIMA with avg smape 7.63: \nModel Number: 51 of 119 with model UnivariateMotif for Validation 1\n51 - UnivariateMotif with avg smape 7.46: \nModel Number: 52 of 119 with model UnobservedComponents for Validation 1\n52 - UnobservedComponents with avg smape 7.65: \nModel Number: 53 of 119 with model UnivariateMotif for Validation 1\n53 - UnivariateMotif with avg smape 7.5: \nModel Number: 54 of 119 with model UnivariateMotif for Validation 1\n54 - UnivariateMotif with avg smape 8.85: \nModel Number: 55 of 119 with model UnivariateMotif for Validation 1\n55 - UnivariateMotif with avg smape 7.5: \nModel Number: 56 of 119 with model UnobservedComponents for Validation 1\n56 - UnobservedComponents with avg smape 7.62: \nModel Number: 57 of 119 with model UnobservedComponents for Validation 1\n57 - UnobservedComponents with avg smape 7.57: \nModel Number: 58 of 119 with model UnobservedComponents for Validation 1\n58 - UnobservedComponents with avg smape 8.33: \nModel Number: 59 of 119 with model UnobservedComponents for Validation 1\n59 - UnobservedComponents with avg smape 7.6: \nModel Number: 60 of 119 with model ConstantNaive for Validation 1\n60 - ConstantNaive with avg smape 8.4: \nModel Number: 61 of 119 with model ConstantNaive for Validation 1\n61 - ConstantNaive with avg smape 8.4: \nModel Number: 62 of 119 with model ConstantNaive for Validation 1\n62 - ConstantNaive with avg smape 7.53: \nModel Number: 63 of 119 with model ConstantNaive for Validation 1\n63 - ConstantNaive with avg smape 7.63: \nModel Number: 64 of 119 with model GLS for Validation 1\n64 - GLS with avg smape 7.3: \nModel Number: 65 of 119 with model GLS for Validation 1\n65 - GLS with avg smape 26.54: \nModel Number: 66 of 119 with model UnivariateMotif for Validation 1\n66 - UnivariateMotif with avg smape 15.71: \nModel Number: 67 of 119 with model VECM for Validation 1\n67 - VECM with avg smape 10.38: \nModel Number: 68 of 119 with model ConstantNaive for Validation 1\n68 - ConstantNaive with avg smape 7.55: \nModel Number: 69 of 119 with model SectionalMotif for Validation 1\n69 - SectionalMotif with avg smape 8.04: \nModel Number: 70 of 119 with model ConstantNaive for Validation 1\n70 - ConstantNaive with avg smape 7.6: \nModel Number: 71 of 119 with model MultivariateRegression for Validation 1\n71 - MultivariateRegression with avg smape 7.22: \nModel Number: 72 of 119 with model LastValueNaive for Validation 1\n72 - LastValueNaive with avg smape 19.57: \nModel Number: 73 of 119 with model GLS for Validation 1\n73 - GLS with avg smape 7.68: \nModel Number: 74 of 119 with model GLS for Validation 1\n74 - GLS with avg smape 7.52: \nModel Number: 75 of 119 with model LastValueNaive for Validation 1\n75 - LastValueNaive with avg smape 7.6: \nModel Number: 76 of 119 with model LastValueNaive for Validation 1\n76 - LastValueNaive with avg smape 22.04: \nModel Number: 77 of 119 with model GLS for Validation 1\n77 - GLS with avg smape 26.55: \nModel Number: 78 of 119 with model GLS for Validation 1\n78 - GLS with avg smape 7.46: \nModel Number: 79 of 119 with model DatepartRegression for Validation 1\n79 - DatepartRegression with avg smape 7.27: \nModel Number: 80 of 119 with model DatepartRegression for Validation 1\n80 - DatepartRegression with avg smape 7.49: \nModel Number: 81 of 119 with model SectionalMotif for Validation 1\n81 - SectionalMotif with avg smape 7.74: \nModel Number: 82 of 119 with model SectionalMotif for Validation 1\n82 - SectionalMotif with avg smape 7.3: \nModel Number: 83 of 119 with model SectionalMotif for Validation 1\n83 - SectionalMotif with avg smape 7.79: \nModel Number: 84 of 119 with model WindowRegression for Validation 1\n84 - WindowRegression with avg smape 7.18: \nModel Number: 85 of 119 with model SectionalMotif for Validation 1\n85 - SectionalMotif with avg smape 8.33: \nModel Number: 86 of 119 with model LastValueNaive for Validation 1\n86 - LastValueNaive with avg smape 13.94: \nModel Number: 87 of 119 with model MetricMotif for Validation 1\n87 - MetricMotif with avg smape 8.67: \nModel Number: 88 of 119 with model LastValueNaive for Validation 1\n88 - LastValueNaive with avg smape 7.63: \nModel Number: 89 of 119 with model LastValueNaive for Validation 1\n89 - LastValueNaive with avg smape 7.65: \nModel Number: 90 of 119 with model SectionalMotif for Validation 1\n90 - SectionalMotif with avg smape 8.15: \nModel Number: 91 of 119 with model MultivariateRegression for Validation 1\n91 - MultivariateRegression with avg smape 9.44: \nModel Number: 92 of 119 with model MetricMotif for Validation 1\n92 - MetricMotif with avg smape 8.75: \nModel Number: 93 of 119 with model MultivariateRegression for Validation 1\n93 - MultivariateRegression with avg smape 9.42: \nModel Number: 94 of 119 with model WindowRegression for Validation 1\n94 - WindowRegression with avg smape 26.85: \nModel Number: 95 of 119 with model MultivariateRegression for Validation 1\n95 - MultivariateRegression with avg smape 7.56: \nModel Number: 96 of 119 with model MultivariateRegression for Validation 1\n96 - MultivariateRegression with avg smape 10.43: \nModel Number: 97 of 119 with model MultivariateRegression for Validation 1\n97 - MultivariateRegression with avg smape 7.6: \nModel Number: 98 of 119 with model ETS for Validation 1\n98 - ETS with avg smape 22.82: \nModel Number: 99 of 119 with model DatepartRegression for Validation 1\n99 - DatepartRegression with avg smape 14.61: \nModel Number: 100 of 119 with model VECM for Validation 1\n100 - VECM with avg smape 20.81: \nModel Number: 101 of 119 with model VECM for Validation 1\n101 - VECM with avg smape 20.96: \nModel Number: 102 of 119 with model VECM for Validation 1\n102 - VECM with avg smape 7.6: \nModel Number: 103 of 119 with model WindowRegression for Validation 1\n103 - WindowRegression with avg smape 7.91: \nModel Number: 104 of 119 with model MetricMotif for Validation 1\n104 - MetricMotif with avg smape 7.02: \nModel Number: 105 of 119 with model ETS for Validation 1\n105 - ETS with avg smape 90.75: \nModel Number: 106 of 119 with model MetricMotif for Validation 1\n106 - MetricMotif with avg smape 7.87: \nModel Number: 107 of 119 with model VECM for Validation 1\n107 - VECM with avg smape 17.4: \nModel Number: 108 of 119 with model VECM for Validation 1\n108 - VECM with avg smape 21.44: \nModel Number: 109 of 119 with model ETS for Validation 1\n109 - ETS with avg smape 16.53: \nModel Number: 110 of 119 with model MetricMotif for Validation 1\n110 - MetricMotif with avg smape 7.89: \nModel Number: 111 of 119 with model ETS for Validation 1\n111 - ETS with avg smape 87.99: \nModel Number: 112 of 119 with model MetricMotif for Validation 1\n112 - MetricMotif with avg smape 14.11: \nModel Number: 113 of 119 with model WindowRegression for Validation 1\n113 - WindowRegression with avg smape 7.63: \nModel Number: 114 of 119 with model WindowRegression for Validation 1\n114 - WindowRegression with avg smape 10.08: \nModel Number: 115 of 119 with model ETS for Validation 1\n115 - ETS with avg smape 16.58: \nModel Number: 116 of 119 with model WindowRegression for Validation 1\n116 - WindowRegression with avg smape 8.7: \nModel Number: 117 of 119 with model ETS for Validation 1\n117 - ETS with avg smape 17.71: \nModel Number: 118 of 119 with model DatepartRegression for Validation 1\n118 - DatepartRegression with avg smape 10.73: \nModel Number: 119 of 119 with model UnivariateRegression for Validation 1\n119 - UnivariateRegression with avg smape 12.15: \nValidation Round: 2\nModel Number: 1 of 119 with model Ensemble for Validation 2\n📈 1 - Ensemble with avg smape 8.75: \nModel Number: 2 of 119 with model Ensemble for Validation 2\n2 - Ensemble with avg smape 8.76: \nModel Number: 3 of 119 with model Ensemble for Validation 2\n📈 3 - Ensemble with avg smape 7.88: \nModel Number: 4 of 119 with model VAR for Validation 2\n4 - VAR with avg smape 10.8: \nModel Number: 5 of 119 with model VAR for Validation 2\n5 - VAR with avg smape 9.23: \nModel Number: 6 of 119 with model Ensemble for Validation 2\n📈 6 - Ensemble with avg smape 7.83: \nModel Number: 7 of 119 with model ARDL for Validation 2\n7 - ARDL with avg smape 7.84: \nModel Number: 8 of 119 with model VAR for Validation 2\n8 - VAR with avg smape 9.21: \nModel Number: 9 of 119 with model VAR for Validation 2\n9 - VAR with avg smape 10.83: \nModel Number: 10 of 119 with model MultivariateMotif for Validation 2\n10 - MultivariateMotif with avg smape 11.14: \nModel Number: 11 of 119 with model ARDL for Validation 2\n11 - ARDL with avg smape 11.04: \nModel Number: 12 of 119 with model Ensemble for Validation 2\n12 - Ensemble with avg smape 22.83: \nModel Number: 13 of 119 with model VAR for Validation 2\n13 - VAR with avg smape 8.96: \nModel Number: 14 of 119 with model Ensemble for Validation 2\n📈 14 - Ensemble with avg smape 7.36: \nModel Number: 15 of 119 with model VAR for Validation 2\n15 - VAR with avg smape 10.62: \nModel Number: 16 of 119 with model NVAR for Validation 2\n16 - NVAR with avg smape 8.42: \nModel Number: 17 of 119 with model MultivariateMotif for Validation 2\n17 - MultivariateMotif with avg smape 10.14: \nModel Number: 18 of 119 with model NVAR for Validation 2\n18 - NVAR with avg smape 8.42: \nModel Number: 19 of 119 with model NVAR for Validation 2\n19 - NVAR with avg smape 27.34: \nModel Number: 20 of 119 with model ARDL for Validation 2\n20 - ARDL with avg smape 10.56: \nModel Number: 21 of 119 with model MultivariateMotif for Validation 2\n21 - MultivariateMotif with avg smape 10.1: \nModel Number: 22 of 119 with model ARDL for Validation 2\n22 - ARDL with avg smape 8.28: \nModel Number: 23 of 119 with model ARDL for Validation 2\n23 - ARDL with avg smape 8.28: \nModel Number: 24 of 119 with model Theta for Validation 2\n24 - Theta with avg smape 7.63: \nModel Number: 25 of 119 with model ARIMA for Validation 2\n25 - ARIMA with avg smape 9.91: \nModel Number: 26 of 119 with model MultivariateMotif for Validation 2\n26 - MultivariateMotif with avg smape 8.05: \nModel Number: 27 of 119 with model Theta for Validation 2\n27 - Theta with avg smape 10.55: \nModel Number: 28 of 119 with model Theta for Validation 2\n28 - Theta with avg smape 7.63: \nModel Number: 29 of 119 with model ARIMA for Validation 2\n29 - ARIMA with avg smape 9.92: \nModel Number: 30 of 119 with model Theta for Validation 2\n30 - Theta with avg smape 10.54: \nModel Number: 31 of 119 with model ARDL for Validation 2\n31 - ARDL with avg smape 10.19: \nModel Number: 32 of 119 with model ARIMA for Validation 2\n32 - ARIMA with avg smape 7.43: \nModel Number: 33 of 119 with model Theta for Validation 2\n33 - Theta with avg smape 7.65: \nModel Number: 34 of 119 with model NVAR for Validation 2\n34 - NVAR with avg smape 10.51: \nModel Number: 35 of 119 with model MultivariateMotif for Validation 2\n35 - MultivariateMotif with avg smape 10.43: \nModel Number: 36 of 119 with model Theta for Validation 2\n36 - Theta with avg smape 10.54: \nModel Number: 37 of 119 with model AverageValueNaive for Validation 2\n37 - AverageValueNaive with avg smape 10.39: \nModel Number: 38 of 119 with model AverageValueNaive for Validation 2\n38 - AverageValueNaive with avg smape 7.64: \nModel Number: 39 of 119 with model MultivariateMotif for Validation 2\n39 - MultivariateMotif with avg smape 10.51: \nModel Number: 40 of 119 with model NVAR for Validation 2\n40 - NVAR with avg smape 112.38: \nModel Number: 41 of 119 with model ARIMA for Validation 2\n41 - ARIMA with avg smape 18.83: \nModel Number: 42 of 119 with model AverageValueNaive for Validation 2\n42 - AverageValueNaive with avg smape 10.38: \nModel Number: 43 of 119 with model ARIMA for Validation 2\n43 - ARIMA with avg smape 10.62: \nModel Number: 44 of 119 with model AverageValueNaive for Validation 2\n44 - AverageValueNaive with avg smape 10.45: \nModel Number: 45 of 119 with model NVAR for Validation 2\n45 - NVAR with avg smape 7.91: \nModel Number: 46 of 119 with model AverageValueNaive for Validation 2\n46 - AverageValueNaive with avg smape 18.09: \nModel Number: 47 of 119 with model AverageValueNaive for Validation 2\n47 - AverageValueNaive with avg smape 18.24: \nModel Number: 48 of 119 with model UnivariateMotif for Validation 2\n📈 48 - UnivariateMotif with avg smape 6.45: \nModel Number: 49 of 119 with model UnobservedComponents for Validation 2\n49 - UnobservedComponents with avg smape 10.48: \nModel Number: 50 of 119 with model ARIMA for Validation 2\n50 - ARIMA with avg smape 10.46: \nModel Number: 51 of 119 with model UnivariateMotif for Validation 2\n51 - UnivariateMotif with avg smape 10.68: \nModel Number: 52 of 119 with model UnobservedComponents for Validation 2\n52 - UnobservedComponents with avg smape 7.42: \nModel Number: 53 of 119 with model UnivariateMotif for Validation 2\n53 - UnivariateMotif with avg smape 7.64: \nModel Number: 54 of 119 with model UnivariateMotif for Validation 2\n54 - UnivariateMotif with avg smape 11.94: \nModel Number: 55 of 119 with model UnivariateMotif for Validation 2\n55 - UnivariateMotif with avg smape 7.61: \nModel Number: 56 of 119 with model UnobservedComponents for Validation 2\n56 - UnobservedComponents with avg smape 8.08: \nModel Number: 57 of 119 with model UnobservedComponents for Validation 2\n57 - UnobservedComponents with avg smape 10.11: \nModel Number: 58 of 119 with model UnobservedComponents for Validation 2\n58 - UnobservedComponents with avg smape 11.41: \nModel Number: 59 of 119 with model UnobservedComponents for Validation 2\n59 - UnobservedComponents with avg smape 10.56: \nModel Number: 60 of 119 with model ConstantNaive for Validation 2\n60 - ConstantNaive with avg smape 10.55: \nModel Number: 61 of 119 with model ConstantNaive for Validation 2\n61 - ConstantNaive with avg smape 10.55: \nModel Number: 62 of 119 with model ConstantNaive for Validation 2\n62 - ConstantNaive with avg smape 7.67: \nModel Number: 63 of 119 with model ConstantNaive for Validation 2\n63 - ConstantNaive with avg smape 10.57: \nModel Number: 64 of 119 with model GLS for Validation 2\n64 - GLS with avg smape 7.49: \nModel Number: 65 of 119 with model GLS for Validation 2\n65 - GLS with avg smape 18.81: \nModel Number: 66 of 119 with model UnivariateMotif for Validation 2\n66 - UnivariateMotif with avg smape 23.5: \nModel Number: 67 of 119 with model VECM for Validation 2\n67 - VECM with avg smape 13.5: \nModel Number: 68 of 119 with model ConstantNaive for Validation 2\n68 - ConstantNaive with avg smape 10.41: \nModel Number: 69 of 119 with model SectionalMotif for Validation 2\n69 - SectionalMotif with avg smape 10.47: \nModel Number: 70 of 119 with model ConstantNaive for Validation 2\n70 - ConstantNaive with avg smape 10.55: \nModel Number: 71 of 119 with model MultivariateRegression for Validation 2\n71 - MultivariateRegression with avg smape 11.51: \nModel Number: 72 of 119 with model LastValueNaive for Validation 2\n72 - LastValueNaive with avg smape 50.07: \nModel Number: 73 of 119 with model GLS for Validation 2\n73 - GLS with avg smape 10.71: \nModel Number: 74 of 119 with model GLS for Validation 2\n74 - GLS with avg smape 10.62: \nModel Number: 75 of 119 with model LastValueNaive for Validation 2\n75 - LastValueNaive with avg smape 10.55: \nModel Number: 76 of 119 with model LastValueNaive for Validation 2\n76 - LastValueNaive with avg smape 53.56: \nModel Number: 77 of 119 with model GLS for Validation 2\n77 - GLS with avg smape 18.97: \nModel Number: 78 of 119 with model GLS for Validation 2\n78 - GLS with avg smape 7.33: \nModel Number: 79 of 119 with model DatepartRegression for Validation 2\n79 - DatepartRegression with avg smape 9.73: \nModel Number: 80 of 119 with model DatepartRegression for Validation 2\n80 - DatepartRegression with avg smape 9.99: \nModel Number: 81 of 119 with model SectionalMotif for Validation 2\n81 - SectionalMotif with avg smape 7.18: \nModel Number: 82 of 119 with model SectionalMotif for Validation 2\n82 - SectionalMotif with avg smape 11.92: \nModel Number: 83 of 119 with model SectionalMotif for Validation 2\n83 - SectionalMotif with avg smape 6.6: \nModel Number: 84 of 119 with model WindowRegression for Validation 2\n84 - WindowRegression with avg smape 10.84: \nModel Number: 85 of 119 with model SectionalMotif for Validation 2\n85 - SectionalMotif with avg smape 6.53: \nModel Number: 86 of 119 with model LastValueNaive for Validation 2\n86 - LastValueNaive with avg smape 22.08: \nModel Number: 87 of 119 with model MetricMotif for Validation 2\n87 - MetricMotif with avg smape 8.88: \nModel Number: 88 of 119 with model LastValueNaive for Validation 2\n88 - LastValueNaive with avg smape 10.57: \nModel Number: 89 of 119 with model LastValueNaive for Validation 2\n89 - LastValueNaive with avg smape 10.44: \nModel Number: 90 of 119 with model SectionalMotif for Validation 2\n90 - SectionalMotif with avg smape 12.54: \nModel Number: 91 of 119 with model MultivariateRegression for Validation 2\n91 - MultivariateRegression with avg smape 10.22: \nModel Number: 92 of 119 with model MetricMotif for Validation 2\n92 - MetricMotif with avg smape 13.43: \nModel Number: 93 of 119 with model MultivariateRegression for Validation 2\n93 - MultivariateRegression with avg smape 8.01: \nModel Number: 94 of 119 with model WindowRegression for Validation 2\n94 - WindowRegression with avg smape 19.63: \nModel Number: 95 of 119 with model MultivariateRegression for Validation 2\n95 - MultivariateRegression with avg smape 9.65: \nModel Number: 96 of 119 with model MultivariateRegression for Validation 2\n96 - MultivariateRegression with avg smape 10.82: \nModel Number: 97 of 119 with model MultivariateRegression for Validation 2\n97 - MultivariateRegression with avg smape 10.55: \nModel Number: 98 of 119 with model ETS for Validation 2\n98 - ETS with avg smape 47.37: \nModel Number: 99 of 119 with model DatepartRegression for Validation 2\n99 - DatepartRegression with avg smape 17.9: \nModel Number: 100 of 119 with model VECM for Validation 2\n100 - VECM with avg smape 17.56: \nModel Number: 101 of 119 with model VECM for Validation 2\n101 - VECM with avg smape 17.63: \nModel Number: 102 of 119 with model VECM for Validation 2\n102 - VECM with avg smape 10.67: \nModel Number: 103 of 119 with model WindowRegression for Validation 2\n103 - WindowRegression with avg smape 12.43: \nModel Number: 104 of 119 with model MetricMotif for Validation 2\n104 - MetricMotif with avg smape 10.96: \nModel Number: 105 of 119 with model ETS for Validation 2\n105 - ETS with avg smape 95.46: \nModel Number: 106 of 119 with model MetricMotif for Validation 2\n106 - MetricMotif with avg smape 10.29: \nModel Number: 107 of 119 with model VECM for Validation 2\n107 - VECM with avg smape 17.59: \nModel Number: 108 of 119 with model VECM for Validation 2\n108 - VECM with avg smape 17.49: \nModel Number: 109 of 119 with model ETS for Validation 2\n109 - ETS with avg smape 17.65: \nModel Number: 110 of 119 with model MetricMotif for Validation 2\n110 - MetricMotif with avg smape 7.82: \nModel Number: 111 of 119 with model ETS for Validation 2\n111 - ETS with avg smape 97.04: \nModel Number: 112 of 119 with model MetricMotif for Validation 2\n112 - MetricMotif with avg smape 8.88: \nModel Number: 113 of 119 with model WindowRegression for Validation 2\n113 - WindowRegression with avg smape 10.57: \nModel Number: 114 of 119 with model WindowRegression for Validation 2\n114 - WindowRegression with avg smape 12.49: \nModel Number: 115 of 119 with model ETS for Validation 2\n115 - ETS with avg smape 17.91: \nModel Number: 116 of 119 with model WindowRegression for Validation 2\n116 - WindowRegression with avg smape 10.38: \nModel Number: 117 of 119 with model ETS for Validation 2\n117 - ETS with avg smape 18.08: \nModel Number: 118 of 119 with model DatepartRegression for Validation 2\n118 - DatepartRegression with avg smape 8.47: \nModel Number: 119 of 119 with model UnivariateRegression for Validation 2\n119 - UnivariateRegression with avg smape 8.93: \nModel Number: 1036 with model Ensemble in generation 7 of Ensembles\nModel Number: 1037 with model Ensemble in generation 7 of Ensembles\nModel Number: 1038 with model Ensemble in generation 7 of Ensembles\nModel Number: 1039 with model Ensemble in generation 7 of Ensembles\nModel Number: 1040 with model Ensemble in generation 7 of Ensembles\nModel Number: 1041 with model Ensemble in generation 7 of Ensembles\nModel Number: 1042 with model Ensemble in generation 7 of Ensembles\nModel Number: 1043 with model Ensemble in generation 7 of Ensembles\nModel Number: 1044 with model Ensemble in generation 7 of Ensembles\nModel Number: 1045 with model Ensemble in generation 7 of Ensembles\nModel Number: 1046 with model Ensemble in generation 7 of Ensembles\nValidation Round: 1\nModel Number: 1 of 11 with model Ensemble for Validation 1\n📈 1 - Ensemble with avg smape 7.33: \nModel Number: 2 of 11 with model Ensemble for Validation 1\n2 - Ensemble with avg smape 7.45: \nModel Number: 3 of 11 with model Ensemble for Validation 1\n📈 3 - Ensemble with avg smape 7.32: \nModel Number: 4 of 11 with model Ensemble for Validation 1\n4 - Ensemble with avg smape 7.48: \nModel Number: 5 of 11 with model Ensemble for Validation 1\n5 - Ensemble with avg smape 7.42: \nModel Number: 6 of 11 with model Ensemble for Validation 1\n6 - Ensemble with avg smape 7.39: \nModel Number: 7 of 11 with model Ensemble for Validation 1\n7 - Ensemble with avg smape 7.42: \nModel Number: 8 of 11 with model Ensemble for Validation 1\n📈 8 - Ensemble with avg smape 7.09: \nModel Number: 9 of 11 with model Ensemble for Validation 1\n📈 9 - Ensemble with avg smape 6.68: \nModel Number: 10 of 11 with model Ensemble for Validation 1\n10 - Ensemble with avg smape 7.44: \nModel Number: 11 of 11 with model Ensemble for Validation 1\n11 - Ensemble with avg smape 7.44: \nValidation Round: 2\nModel Number: 1 of 11 with model Ensemble for Validation 2\n📈 1 - Ensemble with avg smape 7.57: \nModel Number: 2 of 11 with model Ensemble for Validation 2\n2 - Ensemble with avg smape 7.79: \nModel Number: 3 of 11 with model Ensemble for Validation 2\n3 - Ensemble with avg smape 8.1: \nModel Number: 4 of 11 with model Ensemble for Validation 2\n4 - Ensemble with avg smape 7.79: \nModel Number: 5 of 11 with model Ensemble for Validation 2\n5 - Ensemble with avg smape 7.73: \nModel Number: 6 of 11 with model Ensemble for Validation 2\n6 - Ensemble with avg smape 7.7: \nModel Number: 7 of 11 with model Ensemble for Validation 2\n7 - Ensemble with avg smape 7.73: \nModel Number: 8 of 11 with model Ensemble for Validation 2\n📈 8 - Ensemble with avg smape 7.5: \nModel Number: 9 of 11 with model Ensemble for Validation 2\n9 - Ensemble with avg smape 7.75: \nModel Number: 10 of 11 with model Ensemble for Validation 2\n10 - Ensemble with avg smape 7.66: \nModel Number: 11 of 11 with model Ensemble for Validation 2\n11 - Ensemble with avg smape 7.66: \nInitiated AutoTS object with best model: \nEnsemble\n{}\n{'model_name': 'BestN', 'model_count': 5, 'model_metric': 'bestn_horizontal', 'models': {'f428a80ccccad8c57a230adb80e04850': {'Model': 'ARDL', 'ModelParameters': '{\"lags\": 4, \"trend\": \"c\", \"order\": 0, \"causal\": false, \"regression_type\": \"holiday\"}', 'TransformationParameters': '{\"fillna\": \"rolling_mean\", \"transformations\": {\"0\": \"AlignLastValue\", \"1\": \"Detrend\"}, \"transformation_params\": {\"0\": {\"rows\": 1, \"lag\": 1, \"method\": \"multiplicative\", \"strength\": 1.0, \"first_value_only\": false}, \"1\": {\"model\": \"GLS\", \"phi\": 1, \"window\": 90, \"transform_dict\": null}}}'}, '070450f038a98462d2a6d8ca0e31fe47': {'Model': 'VAR', 'ModelParameters': '{\"regression_type\": null, \"maxlags\": 5, \"ic\": \"fpe\"}', 'TransformationParameters': '{\"fillna\": \"ffill\", \"transformations\": {\"0\": \"AlignLastValue\", \"1\": \"Detrend\", \"2\": \"PowerTransformer\"}, \"transformation_params\": {\"0\": {\"rows\": 1, \"lag\": 1, \"method\": \"multiplicative\", \"strength\": 1.0, \"first_value_only\": false}, \"1\": {\"model\": \"GLS\", \"phi\": 1, \"window\": 90, \"transform_dict\": null}, \"2\": {}}}'}, '35bafb1742c64b667a793c280a7a5e60': {'Model': 'VAR', 'ModelParameters': '{\"regression_type\": null, \"maxlags\": 5, \"ic\": \"aic\"}', 'TransformationParameters': '{\"fillna\": \"rolling_mean\", \"transformations\": {\"0\": \"AlignLastValue\", \"1\": \"Detrend\", \"2\": \"PowerTransformer\"}, \"transformation_params\": {\"0\": {\"rows\": 1, \"lag\": 1, \"method\": \"multiplicative\", \"strength\": 1.0, \"first_value_only\": false}, \"1\": {\"model\": \"GLS\", \"phi\": 1, \"window\": 90, \"transform_dict\": null}, \"2\": {}}}'}, 'b3803f0cdca0b37cf3546dea5e7e381c': {'Model': 'VAR', 'ModelParameters': '{\"regression_type\": null, \"maxlags\": 5, \"ic\": \"fpe\"}', 'TransformationParameters': '{\"fillna\": \"ffill\", \"transformations\": {\"0\": \"AlignLastValue\", \"1\": \"Detrend\", \"2\": \"PowerTransformer\", \"3\": \"AlignLastValue\"}, \"transformation_params\": {\"0\": {\"rows\": 1, \"lag\": 1, \"method\": \"multiplicative\", \"strength\": 1.0, \"first_value_only\": false}, \"1\": {\"model\": \"GLS\", \"phi\": 1, \"window\": 90, \"transform_dict\": null}, \"2\": {}, \"3\": {\"rows\": 1, \"lag\": 1, \"method\": \"additive\", \"strength\": 1.0, \"first_value_only\": false}}}'}, '17c6b2f36b83e72f2c6db2cea6de3af1': {'Model': 'VAR', 'ModelParameters': '{\"regression_type\": null, \"maxlags\": 5, \"ic\": \"fpe\"}', 'TransformationParameters': '{\"fillna\": \"rolling_mean\", \"transformations\": {\"0\": \"AlignLastValue\", \"1\": \"Detrend\", \"2\": \"PowerTransformer\", \"3\": \"AlignLastValue\"}, \"transformation_params\": {\"0\": {\"rows\": 1, \"lag\": 1, \"method\": \"multiplicative\", \"strength\": 1.0, \"first_value_only\": false}, \"1\": {\"model\": \"GLS\", \"phi\": 1, \"window\": 90, \"transform_dict\": null}, \"2\": {}, \"3\": {\"rows\": 1, \"lag\": 1, \"method\": \"additive\", \"strength\": 1.0, \"first_value_only\": false}}}'}}, 'point_method': 'median', 'model_weights': {}}\nValidation: 0, 1, 2\nSMAPE: 3.334263867021404, 6.897190449095346, 7.83271043570819\nMAE: 1.5650624127614106, 3.2035416216361603, 3.0230056916095402\nSPL: 0.4164914451086903, 1.0519609055486687, 0.6211997569313331\n\n\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2995: RuntimeWarning: divide by zero encountered in log\n  loglike = -n_samples / 2 * np.log(x_trans.var())\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/autots/tools/probabilistic.py:67: RuntimeWarning: invalid value encountered in divide\n  (prior_mu / prior_sigma**2) + ((n * data_mu) / prior_sigma**2)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/autots/tools/probabilistic.py:68: RuntimeWarning: divide by zero encountered in divide\n  ) / ((1 / prior_sigma**2) + (n / prior_sigma**2))\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2937: RuntimeWarning: divide by zero encountered in power\n  x_inv[pos] = np.power(x[pos] * lmbda + 1, 1 / lmbda) - 1\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2937: RuntimeWarning: divide by zero encountered in power\n  x_inv[pos] = np.power(x[pos] * lmbda + 1, 1 / lmbda) - 1\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2937: RuntimeWarning: divide by zero encountered in power\n  x_inv[pos] = np.power(x[pos] * lmbda + 1, 1 / lmbda) - 1\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:1912: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19628.945218516492, tolerance: 4.884749429436865\n  cd_fast.enet_coordinate_descent_multi_task(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neighbors/_regression.py:366: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n  warnings.warn(empty_warning_msg)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/decomposition/_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n  warnings.warn('FastICA did not converge. Consider increasing '\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35132e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35132e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/decomposition/_fastica.py:118: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n  warnings.warn('FastICA did not converge. Consider increasing '\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/autots/tools/probabilistic.py:67: RuntimeWarning: invalid value encountered in divide\n  (prior_mu / prior_sigma**2) + ((n * data_mu) / prior_sigma**2)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/autots/tools/probabilistic.py:68: RuntimeWarning: divide by zero encountered in divide\n  ) / ((1 / prior_sigma**2) + (n / prior_sigma**2))\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neighbors/_regression.py:366: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n  warnings.warn(empty_warning_msg)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2995: RuntimeWarning: divide by zero encountered in log\n  loglike = -n_samples / 2 * np.log(x_trans.var())\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2937: RuntimeWarning: divide by zero encountered in power\n  x_inv[pos] = np.power(x[pos] * lmbda + 1, 1 / lmbda) - 1\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2937: RuntimeWarning: divide by zero encountered in power\n  x_inv[pos] = np.power(x[pos] * lmbda + 1, 1 / lmbda) - 1\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2937: RuntimeWarning: divide by zero encountered in power\n  x_inv[pos] = np.power(x[pos] * lmbda + 1, 1 / lmbda) - 1\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.06017e-40): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.06017e-40): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.06017e-40): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.06017e-40): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2995: RuntimeWarning: divide by zero encountered in log\n  loglike = -n_samples / 2 * np.log(x_trans.var())\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/autots/tools/impute.py:54: RuntimeWarning: Mean of empty slice\n  arr = np.nan_to_num(arr) + np.isnan(arr) * np.nan_to_num(np.nanmean(arr, axis=0))\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2995: RuntimeWarning: divide by zero encountered in log\n  loglike = -n_samples / 2 * np.log(x_trans.var())\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/neighbors/_regression.py:366: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n  warnings.warn(empty_warning_msg)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35132e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom &lt;= 0 for slice\n  c = cov(x, y, rowvar, dtype=dtype)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1559: RuntimeWarning: All-NaN slice encountered\n  r, k = function_base._ureduce(a,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:373: RuntimeWarning: All-NaN slice encountered\n  data_min = np.nanmin(X, axis=0)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:374: RuntimeWarning: All-NaN slice encountered\n  data_max = np.nanmax(X, axis=0)\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35132e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2995: RuntimeWarning: divide by zero encountered in log\n  loglike = -n_samples / 2 * np.log(x_trans.var())\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35132e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35132e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35132e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2995: RuntimeWarning: divide by zero encountered in log\n  loglike = -n_samples / 2 * np.log(x_trans.var())\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2937: RuntimeWarning: divide by zero encountered in power\n  x_inv[pos] = np.power(x[pos] * lmbda + 1, 1 / lmbda) - 1\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2937: RuntimeWarning: divide by zero encountered in power\n  x_inv[pos] = np.power(x[pos] * lmbda + 1, 1 / lmbda) - 1\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2937: RuntimeWarning: divide by zero encountered in power\n  x_inv[pos] = np.power(x[pos] * lmbda + 1, 1 / lmbda) - 1\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35132e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35132e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35647e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35647e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35647e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35647e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.36167e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.36167e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.36167e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.36167e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35132e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.35647e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n/home/jy/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.36167e-25): result may not be accurate.\n  return linalg.solve(A, Xy, sym_pos=True,\n\n\n\nprediction = model.predict()\nforecast_df = prediction.forecast\n\n\nforecast_df\n\n\n\n\n\n\n\n\nCSUSHPISA\nEMVOVERALLEMV\nEXCAUS\nEXCHUS\nEXUSEU\nGS10\nMCOILWTICO\nT10YIEM\nUSEPUINDXM\n\n\n\n\n2020-01-01\n209.534000\n21.543140\n1.316900\n7.013700\n1.111400\n1.860000\n59.880000\n1.720000\n145.033390\n\n\n2020-02-01\n210.265264\n20.407826\n1.294738\n7.013400\n1.113149\n1.834176\n60.827180\n1.748423\n133.108656\n\n\n2020-03-01\n210.428308\n20.749977\n1.294934\n7.017098\n1.120527\n1.835217\n61.367419\n1.754400\n133.613286\n\n\n\n\n\n\n\n\nupper_forecasts_df = prediction.upper_forecast\nlower_forecasts_df = prediction.lower_forecast"
  },
  {
    "objectID": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html",
    "href": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html",
    "title": "트랜스포머 (전력사용량 데이터)",
    "section": "",
    "text": "Click to run on colab (if you’re not already there): \nThe goal of this notebook is to illustrate the use of a transformer for timeseries prediction. This notebook was built by Alice Martin and adapted to pytorch by Charles Ollion\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport time\nimport matplotlib.pyplot as plt\n\n\n\nEnergy consumption dataset from https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction * gathers 10-min measurements of household appliances energy consumption (20 first features), coupled with local meteorological data (8 last features). * The time-series forecasting task is to predict the first 20 features, given as input data the 28 features. A window of observations of 12 time steps is considered to predict the next series of observations (this corresponds to a 2-hours window of observations.\nyou may get the dataset (a single csv file) by running the following cell:\n\n!wget https://raw.githubusercontent.com/LuisM78/Appliances-energy-prediction-data/master/energydata_complete.csv\n\n\n# load and preprocess the energy dataset:\ndef convert_col_into_float(df, list_cols):\n    for col in list_cols:\n        df[col] = df[col].astype(str)\n        df[col] = df[col].str.replace(',', '.')\n        df[col] = df[col].astype(np.float32)\n    return df\ndf = pd.read_csv(\"energydata_complete.csv\", index_col='date', parse_dates=['date'])\nprint(df.head())\nlist_cols = list(df.columns)\n# gathers 10-min measurements of household appliances energy consumption (20 first features), coupled with local meteorological data. (8 last features)\nprint(\"dataset variables\", list_cols)\ndf = convert_col_into_float(df, list_cols)\ndata = df.values\n\n\ndef split_dataset_into_seq(dataset, start_index=0, end_index=None, history_size=13, step=1):\n    '''split the dataset to have sequence of observations of length history size'''\n    data = []\n    start_index = start_index + history_size\n    if end_index is None:\n        end_index = len(dataset)\n    for i in range(start_index, end_index):\n        indices = range(i - history_size, i, step)\n        data.append(dataset[indices])\n    return np.array(data)\n\n\ndef split_dataset(data, TRAIN_SPLIT=0.7, VAL_SPLIT=0.5, save_path=None):\n    '''split the dataset into train, val and test splits'''\n    # normalization\n    data_mean = data.mean(axis=0)\n    data_std = data.std(axis=0)\n    data = (data - data_mean) / data_std\n    stats = (data_mean, data_std)\n\n    data_in_seq = split_dataset_into_seq(data, start_index=0, end_index=None, history_size=13, step=1)\n\n    # split between validation dataset and test set:\n    train_data, val_data = train_test_split(data_in_seq, train_size=TRAIN_SPLIT, shuffle=True, random_state=123)\n    val_data, test_data = train_test_split(val_data, train_size=VAL_SPLIT, shuffle=True, random_state=123)\n\n    return train_data, val_data, test_data\n\n\ndef split_fn(chunk):\n    \"\"\"to split the dataset sequences into input and targets sequences\"\"\"\n    inputs = torch.tensor(chunk[:, :-1, :], device=device)\n    targets = torch.tensor(chunk[:, 1:, :], device=device)\n    return inputs, targets\n\n\ndef data_to_dataset(train_data, val_data, test_data, batch_size=32, target_features=list(range(20))):\n    '''\n    split each train split into inputs and targets\n    convert each train split into a tf.dataset\n    '''\n    x_train, y_train = split_fn(train_data)\n    x_val, y_val = split_fn(val_data)\n    x_test, y_test = split_fn(test_data)\n    # selecting only the first 20 features for prediction:\n    y_train = y_train[:, :, target_features]\n    y_val = y_val[:, :, target_features]\n    y_test = y_test[:, :, target_features]\n    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n    test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n    return train_loader, val_loader, test_loader\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\ntrain_data, val_data, test_data = split_dataset(data)\ntrain_dataset, val_dataset, test_dataset = data_to_dataset(train_data, val_data, test_data)\n\n\n\n\n\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    '''Multi-head self-attention module'''\n    def __init__(self, D, H):\n        super(MultiHeadAttention, self).__init__()\n        self.H = H # number of heads\n        self.D = D # dimension\n\n        self.wq = nn.Linear(D, D*H)\n        self.wk = nn.Linear(D, D*H)\n        self.wv = nn.Linear(D, D*H)\n\n        self.dense = nn.Linear(D*H, D)\n\n    def concat_heads(self, x):\n        '''(B, H, S, D) =&gt; (B, S, D*H)'''\n        B, H, S, D = x.shape\n        x = x.permute((0, 2, 1, 3)).contiguous()  # (B, S, H, D)\n        x = x.reshape((B, S, H*D))   # (B, S, D*H)\n        return x\n\n    def split_heads(self, x):\n        '''(B, S, D*H) =&gt; (B, H, S, D)'''\n        B, S, D_H = x.shape\n        x = x.reshape(B, S, self.H, self.D)    # (B, S, H, D)\n        x = x.permute((0, 2, 1, 3))  # (B, H, S, D)\n        return x\n\n    def forward(self, x, mask):\n\n        q = self.wq(x)  # (B, S, D*H)\n        k = self.wk(x)  # (B, S, D*H)\n        v = self.wv(x)  # (B, S, D*H)\n\n        q = self.split_heads(q)  # (B, H, S, D)\n        k = self.split_heads(k)  # (B, H, S, D)\n        v = self.split_heads(v)  # (B, H, S, D)\n\n        attention_scores = torch.matmul(q, k.transpose(-1, -2)) #(B,H,S,S)\n        attention_scores = attention_scores / math.sqrt(self.D)\n\n        # add the mask to the scaled tensor.\n        if mask is not None:\n            attention_scores += (mask * -1e9)\n\n        attention_weights = nn.Softmax(dim=-1)(attention_scores)\n        scaled_attention = torch.matmul(attention_weights, v)  # (B, H, S, D)\n        concat_attention = self.concat_heads(scaled_attention) # (B, S, D*H)\n        output = self.dense(concat_attention)  # (B, S, D)\n\n        return output, attention_weights\n\n\nB, S, H, D = 9, 11, 5, 8\nmha = MultiHeadAttention(D, H)\nout, att = mha.forward(torch.zeros(B, S, D), mask=None)\nout.shape, att.shape\n\n\n# Positional encodings\ndef get_angles(pos, i, D):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(D))\n    return pos * angle_rates\n\n\ndef positional_encoding(D, position=20, dim=3, device=device):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                            np.arange(D)[np.newaxis, :],\n                            D)\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    if dim == 3:\n        pos_encoding = angle_rads[np.newaxis, ...]\n    elif dim == 4:\n        pos_encoding = angle_rads[np.newaxis,np.newaxis,  ...]\n    return torch.tensor(pos_encoding, device=device)\n\n\n# function that implement the look_ahead mask for masking future time steps.\ndef create_look_ahead_mask(size, device=device):\n    mask = torch.ones((size, size), device=device)\n    mask = torch.triu(mask, diagonal=1)\n    return mask  # (size, size)\n\n\ncreate_look_ahead_mask(6)\n\n\nclass TransformerLayer(nn.Module):\n    def __init__(self, D, H, hidden_mlp_dim, dropout_rate):\n        super(TransformerLayer, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.mlp_hidden = nn.Linear(D, hidden_mlp_dim)\n        self.mlp_out = nn.Linear(hidden_mlp_dim, D)\n        self.layernorm1 = nn.LayerNorm(D, eps=1e-9)\n        self.layernorm2 = nn.LayerNorm(D, eps=1e-9)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.mha = MultiHeadAttention(D, H)\n\n\n    def forward(self, x, look_ahead_mask):\n\n        attn, attn_weights = self.mha(x, look_ahead_mask)  # (B, S, D)\n        attn = self.dropout1(attn) # (B,S,D)\n        attn = self.layernorm1(attn + x) # (B,S,D)\n\n        mlp_act = torch.relu(self.mlp_hidden(attn))\n        mlp_act = self.mlp_out(mlp_act)\n        mlp_act = self.dropout2(mlp_act)\n\n        output = self.layernorm2(mlp_act + attn)  # (B, S, D)\n\n        return output, attn_weights\n\n\ndl = TransformerLayer(16, 3, 32, 0.1)\nout, attn = dl(x=torch.zeros(5, 7, 16), look_ahead_mask=None)\nout.shape, attn.shape\n\n\nclass Transformer(nn.Module):\n    '''Transformer Decoder Implementating several Decoder Layers.\n    '''\n    def __init__(self, num_layers, D, H, hidden_mlp_dim, inp_features, out_features, dropout_rate):\n        super(Transformer, self).__init__()\n        self.sqrt_D = torch.tensor(math.sqrt(D))\n        self.num_layers = num_layers\n        self.input_projection = nn.Linear(inp_features, D) # multivariate input\n        self.output_projection = nn.Linear(D, out_features) # multivariate output\n        self.pos_encoding = positional_encoding(D)\n        self.dec_layers = nn.ModuleList([TransformerLayer(D, H, hidden_mlp_dim,\n                                        dropout_rate=dropout_rate\n                                       ) for _ in range(num_layers)])\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, mask):\n        B, S, D = x.shape\n        attention_weights = {}\n        x = self.input_projection(x)\n        x *= self.sqrt_D\n\n        x += self.pos_encoding[:, :S, :]\n\n        x = self.dropout(x)\n\n        for i in range(self.num_layers):\n            x, block = self.dec_layers[i](x=x,\n                                          look_ahead_mask=mask)\n            attention_weights['decoder_layer{}'.format(i + 1)] = block\n\n        x = self.output_projection(x)\n\n        return x, attention_weights # (B,S,S)\n\n\n# Test Forward pass on the Transformer:\ntransformer = Transformer(num_layers=1, D=32, H=1, hidden_mlp_dim=32,\n                                       inp_features=28, out_features=20, dropout_rate=0.1)\ntransformer.to(device)\n(inputs, targets) = next(iter(train_dataset))\n\nS = inputs.shape[1]\nmask = create_look_ahead_mask(S)\nout, attn = transformer (x=inputs, mask=mask)\nout.shape, attn[\"decoder_layer1\"].shape"
  },
  {
    "objectID": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html#transformers-for-timeseries",
    "href": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html#transformers-for-timeseries",
    "title": "트랜스포머 (전력사용량 데이터)",
    "section": "",
    "text": "Click to run on colab (if you’re not already there): \nThe goal of this notebook is to illustrate the use of a transformer for timeseries prediction. This notebook was built by Alice Martin and adapted to pytorch by Charles Ollion\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport time\nimport matplotlib.pyplot as plt\n\n\n\nEnergy consumption dataset from https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction * gathers 10-min measurements of household appliances energy consumption (20 first features), coupled with local meteorological data (8 last features). * The time-series forecasting task is to predict the first 20 features, given as input data the 28 features. A window of observations of 12 time steps is considered to predict the next series of observations (this corresponds to a 2-hours window of observations.\nyou may get the dataset (a single csv file) by running the following cell:\n\n!wget https://raw.githubusercontent.com/LuisM78/Appliances-energy-prediction-data/master/energydata_complete.csv\n\n\n# load and preprocess the energy dataset:\ndef convert_col_into_float(df, list_cols):\n    for col in list_cols:\n        df[col] = df[col].astype(str)\n        df[col] = df[col].str.replace(',', '.')\n        df[col] = df[col].astype(np.float32)\n    return df\ndf = pd.read_csv(\"energydata_complete.csv\", index_col='date', parse_dates=['date'])\nprint(df.head())\nlist_cols = list(df.columns)\n# gathers 10-min measurements of household appliances energy consumption (20 first features), coupled with local meteorological data. (8 last features)\nprint(\"dataset variables\", list_cols)\ndf = convert_col_into_float(df, list_cols)\ndata = df.values\n\n\ndef split_dataset_into_seq(dataset, start_index=0, end_index=None, history_size=13, step=1):\n    '''split the dataset to have sequence of observations of length history size'''\n    data = []\n    start_index = start_index + history_size\n    if end_index is None:\n        end_index = len(dataset)\n    for i in range(start_index, end_index):\n        indices = range(i - history_size, i, step)\n        data.append(dataset[indices])\n    return np.array(data)\n\n\ndef split_dataset(data, TRAIN_SPLIT=0.7, VAL_SPLIT=0.5, save_path=None):\n    '''split the dataset into train, val and test splits'''\n    # normalization\n    data_mean = data.mean(axis=0)\n    data_std = data.std(axis=0)\n    data = (data - data_mean) / data_std\n    stats = (data_mean, data_std)\n\n    data_in_seq = split_dataset_into_seq(data, start_index=0, end_index=None, history_size=13, step=1)\n\n    # split between validation dataset and test set:\n    train_data, val_data = train_test_split(data_in_seq, train_size=TRAIN_SPLIT, shuffle=True, random_state=123)\n    val_data, test_data = train_test_split(val_data, train_size=VAL_SPLIT, shuffle=True, random_state=123)\n\n    return train_data, val_data, test_data\n\n\ndef split_fn(chunk):\n    \"\"\"to split the dataset sequences into input and targets sequences\"\"\"\n    inputs = torch.tensor(chunk[:, :-1, :], device=device)\n    targets = torch.tensor(chunk[:, 1:, :], device=device)\n    return inputs, targets\n\n\ndef data_to_dataset(train_data, val_data, test_data, batch_size=32, target_features=list(range(20))):\n    '''\n    split each train split into inputs and targets\n    convert each train split into a tf.dataset\n    '''\n    x_train, y_train = split_fn(train_data)\n    x_val, y_val = split_fn(val_data)\n    x_test, y_test = split_fn(test_data)\n    # selecting only the first 20 features for prediction:\n    y_train = y_train[:, :, target_features]\n    y_val = y_val[:, :, target_features]\n    y_test = y_test[:, :, target_features]\n    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n    test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n    return train_loader, val_loader, test_loader\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\ntrain_data, val_data, test_data = split_dataset(data)\ntrain_dataset, val_dataset, test_dataset = data_to_dataset(train_data, val_data, test_data)\n\n\n\n\n\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    '''Multi-head self-attention module'''\n    def __init__(self, D, H):\n        super(MultiHeadAttention, self).__init__()\n        self.H = H # number of heads\n        self.D = D # dimension\n\n        self.wq = nn.Linear(D, D*H)\n        self.wk = nn.Linear(D, D*H)\n        self.wv = nn.Linear(D, D*H)\n\n        self.dense = nn.Linear(D*H, D)\n\n    def concat_heads(self, x):\n        '''(B, H, S, D) =&gt; (B, S, D*H)'''\n        B, H, S, D = x.shape\n        x = x.permute((0, 2, 1, 3)).contiguous()  # (B, S, H, D)\n        x = x.reshape((B, S, H*D))   # (B, S, D*H)\n        return x\n\n    def split_heads(self, x):\n        '''(B, S, D*H) =&gt; (B, H, S, D)'''\n        B, S, D_H = x.shape\n        x = x.reshape(B, S, self.H, self.D)    # (B, S, H, D)\n        x = x.permute((0, 2, 1, 3))  # (B, H, S, D)\n        return x\n\n    def forward(self, x, mask):\n\n        q = self.wq(x)  # (B, S, D*H)\n        k = self.wk(x)  # (B, S, D*H)\n        v = self.wv(x)  # (B, S, D*H)\n\n        q = self.split_heads(q)  # (B, H, S, D)\n        k = self.split_heads(k)  # (B, H, S, D)\n        v = self.split_heads(v)  # (B, H, S, D)\n\n        attention_scores = torch.matmul(q, k.transpose(-1, -2)) #(B,H,S,S)\n        attention_scores = attention_scores / math.sqrt(self.D)\n\n        # add the mask to the scaled tensor.\n        if mask is not None:\n            attention_scores += (mask * -1e9)\n\n        attention_weights = nn.Softmax(dim=-1)(attention_scores)\n        scaled_attention = torch.matmul(attention_weights, v)  # (B, H, S, D)\n        concat_attention = self.concat_heads(scaled_attention) # (B, S, D*H)\n        output = self.dense(concat_attention)  # (B, S, D)\n\n        return output, attention_weights\n\n\nB, S, H, D = 9, 11, 5, 8\nmha = MultiHeadAttention(D, H)\nout, att = mha.forward(torch.zeros(B, S, D), mask=None)\nout.shape, att.shape\n\n\n# Positional encodings\ndef get_angles(pos, i, D):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(D))\n    return pos * angle_rates\n\n\ndef positional_encoding(D, position=20, dim=3, device=device):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                            np.arange(D)[np.newaxis, :],\n                            D)\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    if dim == 3:\n        pos_encoding = angle_rads[np.newaxis, ...]\n    elif dim == 4:\n        pos_encoding = angle_rads[np.newaxis,np.newaxis,  ...]\n    return torch.tensor(pos_encoding, device=device)\n\n\n# function that implement the look_ahead mask for masking future time steps.\ndef create_look_ahead_mask(size, device=device):\n    mask = torch.ones((size, size), device=device)\n    mask = torch.triu(mask, diagonal=1)\n    return mask  # (size, size)\n\n\ncreate_look_ahead_mask(6)\n\n\nclass TransformerLayer(nn.Module):\n    def __init__(self, D, H, hidden_mlp_dim, dropout_rate):\n        super(TransformerLayer, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.mlp_hidden = nn.Linear(D, hidden_mlp_dim)\n        self.mlp_out = nn.Linear(hidden_mlp_dim, D)\n        self.layernorm1 = nn.LayerNorm(D, eps=1e-9)\n        self.layernorm2 = nn.LayerNorm(D, eps=1e-9)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.mha = MultiHeadAttention(D, H)\n\n\n    def forward(self, x, look_ahead_mask):\n\n        attn, attn_weights = self.mha(x, look_ahead_mask)  # (B, S, D)\n        attn = self.dropout1(attn) # (B,S,D)\n        attn = self.layernorm1(attn + x) # (B,S,D)\n\n        mlp_act = torch.relu(self.mlp_hidden(attn))\n        mlp_act = self.mlp_out(mlp_act)\n        mlp_act = self.dropout2(mlp_act)\n\n        output = self.layernorm2(mlp_act + attn)  # (B, S, D)\n\n        return output, attn_weights\n\n\ndl = TransformerLayer(16, 3, 32, 0.1)\nout, attn = dl(x=torch.zeros(5, 7, 16), look_ahead_mask=None)\nout.shape, attn.shape\n\n\nclass Transformer(nn.Module):\n    '''Transformer Decoder Implementating several Decoder Layers.\n    '''\n    def __init__(self, num_layers, D, H, hidden_mlp_dim, inp_features, out_features, dropout_rate):\n        super(Transformer, self).__init__()\n        self.sqrt_D = torch.tensor(math.sqrt(D))\n        self.num_layers = num_layers\n        self.input_projection = nn.Linear(inp_features, D) # multivariate input\n        self.output_projection = nn.Linear(D, out_features) # multivariate output\n        self.pos_encoding = positional_encoding(D)\n        self.dec_layers = nn.ModuleList([TransformerLayer(D, H, hidden_mlp_dim,\n                                        dropout_rate=dropout_rate\n                                       ) for _ in range(num_layers)])\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, mask):\n        B, S, D = x.shape\n        attention_weights = {}\n        x = self.input_projection(x)\n        x *= self.sqrt_D\n\n        x += self.pos_encoding[:, :S, :]\n\n        x = self.dropout(x)\n\n        for i in range(self.num_layers):\n            x, block = self.dec_layers[i](x=x,\n                                          look_ahead_mask=mask)\n            attention_weights['decoder_layer{}'.format(i + 1)] = block\n\n        x = self.output_projection(x)\n\n        return x, attention_weights # (B,S,S)\n\n\n# Test Forward pass on the Transformer:\ntransformer = Transformer(num_layers=1, D=32, H=1, hidden_mlp_dim=32,\n                                       inp_features=28, out_features=20, dropout_rate=0.1)\ntransformer.to(device)\n(inputs, targets) = next(iter(train_dataset))\n\nS = inputs.shape[1]\nmask = create_look_ahead_mask(S)\nout, attn = transformer (x=inputs, mask=mask)\nout.shape, attn[\"decoder_layer1\"].shape"
  },
  {
    "objectID": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html#training-the-transformer",
    "href": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html#training-the-transformer",
    "title": "트랜스포머 (전력사용량 데이터)",
    "section": "Training the Transformer",
    "text": "Training the Transformer\n\nparam_sizes = [p.numel() for p in transformer.parameters()]\nprint(f\"number of weight/biases matrices: {len(param_sizes)} \"\n      f\"for a total of {np.sum(param_sizes)} parameters \")\n\n\ntransformer = Transformer(num_layers=1, D=32, H=4, hidden_mlp_dim=32,\n                          inp_features=28, out_features=20, dropout_rate=0.1).to(device)\noptimizer = torch.optim.RMSprop(transformer.parameters(),\n                                lr=0.00005)\n\n\nfrom tqdm import tqdm\n\nn_epochs = 20\nniter = len(train_dataset)\nlosses, val_losses = [], []\n\nfor e in tqdm(range(n_epochs)):\n\n    # one epoch on train set\n    transformer.train()\n    sum_train_loss = 0.0\n    for x,y in train_dataset:\n        S = x.shape[1]\n        mask = create_look_ahead_mask(S)\n        out, _ = transformer(x, mask)\n        loss = torch.nn.MSELoss()(out, y)\n        sum_train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n    losses.append(sum_train_loss / niter)\n\n    # Evaluate on val set\n    transformer.eval()\n    sum_val_loss = 0.0\n    for i, (x, y) in enumerate(val_dataset):\n        S = x.shape[1]\n        mask = create_look_ahead_mask(S)\n        out, _ = transformer(x, mask)\n        loss = torch.nn.MSELoss()(out, y)\n        sum_val_loss += loss.item()\n    val_losses.append(sum_val_loss / (i + 1))\n\n\nplt.plot(losses)\nplt.plot(val_losses);\n\n\nEvaluation on Test set\n\ntest_losses, test_preds  = [], []\ntransformer.eval()\nfor (x, y) in test_dataset:\n    S = x.shape[-2]\n    y_pred, _ = transformer(x,\n                            mask=create_look_ahead_mask(S))\n    loss_test = torch.nn.MSELoss()(y_pred, y)  # (B,S)\n    test_losses.append(loss_test.item())\n    test_preds.append(y_pred.detach().cpu().numpy())\ntest_preds = np.vstack(test_preds)\nnp.mean(test_losses)\n\n\n# Display predictions vs ground truth:\n# we'll take one random element of the first batch\n# and display the first feature\nseq_len = 12\nindex = np.random.randint(32)\nfeature_num = 0\n\nx_test, _ = test_dataset.dataset.tensors\nx_test = x_test[index, :, feature_num].cpu().numpy()\npred = test_preds[index, :, feature_num]\nx = np.linspace(1, seq_len, seq_len)\nplt.plot(x, pred, 'red', lw=2, label='predictions for sample: {}'.format(index))\nplt.plot(x, x_test, 'cyan', lw=2, label='ground-truth for sample: {}'.format(index))\nplt.legend(fontsize=10)\nplt.show()"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-11-06-pheatmap.html",
    "href": "posts/7_study/5_etc/2023-11-06-pheatmap.html",
    "title": "[Vis] pheatmap",
    "section": "",
    "text": "library(tidyverse)\nlibrary(pheatmap)\nlibrary(RColorBrewer)\n\n── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.1 ──\n\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n\n── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-11-06-pheatmap.html#import",
    "href": "posts/7_study/5_etc/2023-11-06-pheatmap.html#import",
    "title": "[Vis] pheatmap",
    "section": "",
    "text": "library(tidyverse)\nlibrary(pheatmap)\nlibrary(RColorBrewer)\n\n── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.1 ──\n\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n\n── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-11-06-pheatmap.html#example1",
    "href": "posts/7_study/5_etc/2023-11-06-pheatmap.html#example1",
    "title": "[Vis] pheatmap",
    "section": "Example1",
    "text": "Example1\n\n# Create sample data ===================================================\nset.seed(43)\ndata &lt;- matrix(rnorm(500), 50, 10)\ncolnames(data) &lt;- paste0(\"Sample_\", 1:10)\nrownames(data) &lt;- paste0(\"Gene_\", 1:50)\n\nhead(data)\n\n\nA matrix: 6 × 10 of type dbl\n\n\n\nSample_1\nSample_2\nSample_3\nSample_4\nSample_5\nSample_6\nSample_7\nSample_8\nSample_9\nSample_10\n\n\n\n\nGene_1\n-0.03751376\n0.2937911\n-0.85262613\n0.6780038\n-1.2208217\n0.7869834\n-1.40953704\n1.124523697\n0.9924993\n2.0304134\n\n\nGene_2\n-1.57460441\n0.4005673\n0.01941194\n0.5387474\n0.3632384\n0.3111118\n0.25783267\n-0.478325716\n-0.1422318\n-0.2574697\n\n\nGene_3\n-0.48596752\n-0.1745172\n-2.37084068\n-0.7893876\n-1.6752909\n0.9192161\n0.75005975\n0.440113300\n-1.1419259\n0.1863076\n\n\nGene_4\n0.46518623\n0.2400995\n0.85504226\n1.2212672\n2.7282491\n-1.4367307\n-0.08023358\n-0.699330119\n1.4048809\n-1.1354877\n\n\nGene_5\n-0.90409807\n-1.2410525\n1.38602057\n-1.6071631\n-1.3018109\n-0.3908323\n0.30627901\n-0.487474084\n-0.7002450\n-1.2086034\n\n\nGene_6\n-0.27743280\n-2.0873554\n0.85888042\n-0.2446592\n-0.9075520\n1.0514593\n0.27197678\n-0.006470896\n0.6484149\n-0.4279669\n\n\n\n\n\n\npheatmap(data)\n\n\n\n\n\ndim(data)\n\n\n5010\n\n\n\n# Annotations ===================================================\n\n# create a data frame for column annotation\nann_df &lt;- data.frame(Group = rep(c(\"Disease\", \"Control\"), c(5, 5)),\n                          Lymphocyte_count = rnorm(10))\nrow.names(ann_df) &lt;- colnames(data)\nhead(ann_df)\n\n\nA data.frame: 6 × 2\n\n\n\nGroup\nLymphocyte_count\n\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\nSample_1\nDisease\n0.13097892\n\n\nSample_2\nDisease\n0.32708392\n\n\nSample_3\nDisease\n0.50072083\n\n\nSample_4\nDisease\n0.89569721\n\n\nSample_5\nDisease\n-0.08225117\n\n\nSample_6\nControl\n-0.48868421\n\n\n\n\n\n\ndim(ann_df)\n\n\n102\n\n\n\ngene_functions_df &lt;- data.frame(gene_functions = rep(c('Oxidative_phosphorylation', \n                                                       'Cell_cycle',\n                                                       'Immune_regulation',\n                                                       'Signal_transduction',\n                                                       'Transcription'), rep(10, 5)))\nrow.names(gene_functions_df) &lt;- rownames(data)\nhead(gene_functions_df)\n\n\nA data.frame: 6 × 1\n\n\n\ngene_functions\n\n\n\n&lt;chr&gt;\n\n\n\n\nGene_1\nOxidative_phosphorylation\n\n\nGene_2\nOxidative_phosphorylation\n\n\nGene_3\nOxidative_phosphorylation\n\n\nGene_4\nOxidative_phosphorylation\n\n\nGene_5\nOxidative_phosphorylation\n\n\nGene_6\nOxidative_phosphorylation\n\n\n\n\n\n\ndim(gene_functions_df)\n\n\n501\n\n\n\nann_colors &lt;- list(\n  gene_functions = c(\"Oxidative_phosphorylation\" = \"#F46D43\",\n              \"Cell_cycle\" = \"#708238\",\n              \"Immune_regulation\" = \"#9E0142\",\n              \"Signal_transduction\" = \"beige\", \n              \"Transcription\" = \"violet\"), \n  Group = c(\"Disease\" = \"darkgreen\",\n              \"Control\" = \"blueviolet\"),\n  Lymphocyte_count = brewer.pal(5, 'PuBu')\n)\n\n\nann_colors\n\n\n    $gene_functions\n        Oxidative_phosphorylation'#F46D43'Cell_cycle'#708238'Immune_regulation'#9E0142'Signal_transduction'beige'Transcription'violet'\n\n    $Group\n        Disease'darkgreen'Control'blueviolet'\n\n    $Lymphocyte_count\n        \n'#F1EEF6''#BDC9E1''#74A9CF''#2B8CBE''#045A8D'\n\n\n\n\n\n# Base heatmap ===================================================\nheat_plot &lt;- pheatmap(data, \n                      col = brewer.pal(10, 'RdYlGn'), # choose a colour scale for your data\n                      cluster_rows = T, cluster_cols = T, # set to FALSE if you want to remove the dendograms\n                      clustering_distance_cols = 'euclidean',\n                      clustering_distance_rows = 'euclidean',\n                      clustering_method = 'ward.D',\n                      annotation_row = gene_functions_df, # row (gene) annotations\n                      annotation_col = ann_df, # column (sample) annotations\n                      annotation_colors = ann_colors, # colours for your annotations\n                      annotation_names_row = F, \n                      annotation_names_col = F,\n                      fontsize_row = 10,          # row label font size\n                      fontsize_col = 7,          # column label font size \n                      angle_col = 45, # sample names at an angle\n                      legend_breaks = c(-2, 0, 2), # legend customisation\n                      legend_labels = c(\"Low\", \"Medium\", \"High\"), # legend customisation\n                      show_colnames = T, show_rownames = F, # displaying column and row names\n                      main = \"Super heatmap with annotations\") # a title for our heatmap"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-11-06-pheatmap.html#example2",
    "href": "posts/7_study/5_etc/2023-11-06-pheatmap.html#example2",
    "title": "[Vis] pheatmap",
    "section": "Example2",
    "text": "Example2\n\nref: https://jokergoo.github.io/2020/05/06/translate-from-pheatmap-to-complexheatmap/\n\n가상의 유전자 데이터를 예시로 들어보자.\n유전자 발현 데이터는 유전자와 실험 조건 간의 발현 수준을 나타내는 행렬로서, 각 행은 다른 유전자를 나타내고 각 열은 다른 실험 조건 또는 시간 포인트를 나타내며,\n각 셀은 해당 유전자의 특정 실험 조건에서의 발현 수준을 나타내는 숫자이다.\n\n# 필요한 라이브러리 로드\nlibrary(pheatmap)\n\n# 가상의 유전자 발현 데이터 생성 (3개의 유전자, 4개의 실험 조건)\ngene_names &lt;- c(\"Gene1\", \"Gene2\", \"Gene3\")\nexperiment_conditions &lt;- c(\"Condition1\", \"Condition2\", \"Condition3\", \"Condition4\")\nexpression_data &lt;- matrix(\n  data = c(\n    2.3, 1.8, 3.5, 4.2,\n    1.5, 2.7, 2.0, 3.9,\n    4.0, 3.1, 2.8, 2.2\n  ),\n  nrow = 3,\n  ncol = 4,\n  byrow = TRUE\n)\n\n# 행렬을 데이터 프레임으로 변환\nexpression_data_df &lt;- as.data.frame(expression_data, row.names = gene_names, col.names = experiment_conditions)\n\n# pheatmap 함수로 히트맵 생성\npheatmap(expression_data_df, cluster_rows = TRUE, cluster_cols = TRUE)\n\n\n\n\n\nexpression_data_df\n\n\nA data.frame: 3 × 4\n\n\n\nV1\nV2\nV3\nV4\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nGene1\n2.3\n1.8\n3.5\n4.2\n\n\nGene2\n1.5\n2.7\n2.0\n3.9\n\n\nGene3\n4.0\n3.1\n2.8\n2.2"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-11-06-pheatmap.html#율이예제",
    "href": "posts/7_study/5_etc/2023-11-06-pheatmap.html#율이예제",
    "title": "[Vis] pheatmap",
    "section": "율이예제",
    "text": "율이예제\n\n미생물이랑 군 별 대사 지표랑 상관관계를 나타내고 싶음. (논문의 Fig와 유사하게 – 미생물이 가로축으로 갔으면 좋겠음.)\n\n\nicrobd_df &lt;- data.frame(Akkerma = rnorm(10, 1000, 10),\n                        Lactobacil = rnorm(10, 400, 40),\n                        Ligila = rnorm(10, 300, 50),\n                        Rhodospirillum = rnorm(10, 1000, 20),\n                        Parabacte = rnorm(10, 1500, 30),\n                        Vescimonas = rnorm(10, 1200, 10),\n                        Flintibacter = rnorm(10, 300, 5)               \n                        )\n\nmetabolite_df &lt;- data.frame(Endotoxin = rnorm(10, 400, 10),\n                        Insulin = rnorm(10, 1000, 40),\n                        Homa_Ir = rnorm(10, 600, 50),\n                        HDL = rnorm(10, 10, 20),\n                        AST = rnorm(10, 140, 30),\n                        TG = rnorm(10, 10, 10),\n                        T_CHOL = rnorm(10, 40, 5),\n                        Leptin = rnorm(10, 30, 4),\n                        Adiponectin = rnorm(10, 14,2),\n                        Liver_weight = rnorm(10, 2, 1),\n                        FAS = rnorm(10, 2, 4),\n                        CHREBP = rnorm(10, 40, 2),\n                        Ka = rnorm(10, 30, 5),\n                        CO2 = rnorm(10, 20, 4),\n                        Na = rnorm(10, 40, 5),\n                        H2 = rnorm(10, 24, 5),\n                        H20 = rnorm(10, 4, 100)\n                        )\n\ndf_ &lt;- cbind(icrobd_df, metabolite_df)\n\n\n# 데이터프레임 결합\ndf_ &lt;- cbind(icrobd_df, metabolite_df)\n\n# 상관계수 행렬 생성\ncor_matrix &lt;- cor(df_)\n\n# p-value 행렬 초기화\np_value_matrix &lt;- matrix(1, nrow = ncol(df_), ncol = ncol(df_))\n\n# p-value 계산\nfor (i in 1:(ncol(df_) - 1)) {\n  for (j in (i + 1):ncol(df_)) {\n    test_result &lt;- cor.test(df_[, i], df_[, j])\n    p_value_matrix[i, j] &lt;- p_value_matrix[j, i] &lt;- test_result$p.value\n  }\n}\n\ncolnames(p_value_matrix) &lt;- colnames(df_)\nrownames(p_value_matrix) &lt;- colnames(df_)\n# p_value_matrix\n\nour_pmatrix &lt;- p_value_matrix[colnames(metabolite_df),colnames(icrobd_df)]\nour_cormatrix &lt;- cor_matrix[colnames(metabolite_df),colnames(icrobd_df)]\n# 유의수준 설정\nsignificance_level &lt;- 0.10\n\n# 결과 출력\nstar_matrix &lt;- matrix(\"\", nrow = nrow(our_cormatrix), ncol = ncol(our_cormatrix))\nstar_matrix[our_pmatrix &lt; significance_level] &lt;- \"★\"\n\n\nrownames(star_matrix) = rownames(our_cormatrix)\ncolnames(star_matrix) = colnames(our_cormatrix)\n\n\ncor_df &lt;- cor(df_)[colnames(metabolite_df),colnames(icrobd_df)]\npheatmap(cor_df, cluster_cols = TRUE, cluster_rows = TRUE,\n        main = \"Correlation Heatmap between microbd and metabolite\",\n        title=\"FITURE15\",\n        angle_col = 45,\n        cellwidth=25,\n        cellheight=20)\n\n\n\n\n\n# 유의한 상관계수에 대한 별표시를 annotation_col로 추가\npheatmap(our_cormatrix,\n         cluster_cols = TRUE,\n         cluster_rows = TRUE,\n         main = \"Correlation Heatmap between microbd and metabolite\",\n         angle_col = 45,\n         cellwidth = 25,\n         cellheight = 20,\n         fontsize_row = 10,\n         fontsize_col = 10,\n         display_numbers = star_matrix,\n         fontsize_number = 10  # 별표시 크기 조절\n)"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-02-20-simul_eq.html",
    "href": "posts/7_study/5_etc/2023-02-20-simul_eq.html",
    "title": "simultaneous equation",
    "section": "",
    "text": "파이썬에서 Numpy는 행렬 계산을 쉽게하기 위해 사용하는 패키지이다. R로도 행렬과 매트릭스를 구현해보자.\n- 예를 들어 아래와 같은 문제가 있다고 하자.\n\\[\\begin{cases}w+2x+ey+4z = 1 \\\\2w+2x+y=9 \\\\x-y = 4 \\\\3w+x-y+3y=7\\end{cases}\\]\n- 매트릭스 형태로 위의 식을 표현하면 아래와 같다.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nw \\\\ x \\\\ y \\\\z\n\\end{bmatrix}=\\begin{bmatrix}\n1 \\\\ 9 \\\\ 4 \\\\7\n\\end{bmatrix}\n\\]\n- 양변에\n\\[\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}\\]\n의 역행렬을 취하면\n\\[\\begin{bmatrix}\nw \\\\ x \\\\ y \\\\z\n\\end{bmatrix}=\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}^{-1}\\begin{bmatrix}\n1 \\\\ 9 \\\\ 4 \\\\7\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-02-20-simul_eq.html#행렬과-연립일차-방정식",
    "href": "posts/7_study/5_etc/2023-02-20-simul_eq.html#행렬과-연립일차-방정식",
    "title": "simultaneous equation",
    "section": "",
    "text": "파이썬에서 Numpy는 행렬 계산을 쉽게하기 위해 사용하는 패키지이다. R로도 행렬과 매트릭스를 구현해보자.\n- 예를 들어 아래와 같은 문제가 있다고 하자.\n\\[\\begin{cases}w+2x+ey+4z = 1 \\\\2w+2x+y=9 \\\\x-y = 4 \\\\3w+x-y+3y=7\\end{cases}\\]\n- 매트릭스 형태로 위의 식을 표현하면 아래와 같다.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nw \\\\ x \\\\ y \\\\z\n\\end{bmatrix}=\\begin{bmatrix}\n1 \\\\ 9 \\\\ 4 \\\\7\n\\end{bmatrix}\n\\]\n- 양변에\n\\[\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}\\]\n의 역행렬을 취하면\n\\[\\begin{bmatrix}\nw \\\\ x \\\\ y \\\\z\n\\end{bmatrix}=\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}^{-1}\\begin{bmatrix}\n1 \\\\ 9 \\\\ 4 \\\\7\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-02-20-simul_eq.html#r로-구현",
    "href": "posts/7_study/5_etc/2023-02-20-simul_eq.html#r로-구현",
    "title": "simultaneous equation",
    "section": "R로 구현",
    "text": "R로 구현\n\n- 방법1\n\nA=rbind(c(1,2,3,4),c(2,2,1,0),c(0,1,-1,0),c(3,1,-1,3))\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    2    2    1    0\n[3,]    0    1   -1    0\n[4,]    3    1   -1    3\n\n\n\nb=c(1,9,4,7)\ndim(b)=c(4,1)\nb\n\n     [,1]\n[1,]    1\n[2,]    9\n[3,]    4\n[4,]    7\n\n\n\nsolve(A) %*% b \n\n     [,1]\n[1,]    2\n[2,]    3\n[3,]   -1\n[4,]   -1\n\n\n따라서 \\((w,x,y,z) = (2,3,-1,-1)\\) 이다.\n\n\n- 방법2\n\nA = rbind(c(1,2,3,4),c(2,2,1,0),c(0,1,-1,0),c(3,1,-1,3))\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    2    2    1    0\n[3,]    0    1   -1    0\n[4,]    3    1   -1    3\n\n\n\nb = c(1,9,4,7)\nb\n\n[1] 1 9 4 7\n\n\n\nsolve(A) %*% b\n\n     [,1]\n[1,]    2\n[2,]    3\n[3,]   -1\n[4,]   -1"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html",
    "href": "posts/7_study/6_as2023/2023-09-11.html",
    "title": "2wk 파이썬",
    "section": "",
    "text": "나머지 연산, 조건문\n\n\ndef main():\n    # a = int(input(\"숫자를 입력하세요.\"))\n    number = 3\n    if num % 2 == 0:\n        # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\n        if number % 10 in [2, 4, 5, 9]:\n            print(f\"{number}는 짝수입니다.\")\n        else:\n            print(f\"{number}은 짝수입니다.\")\n    else:\n        if number % 10 in [2, 4, 5, 9]: # 는\n            print(f\"{number}는 홀수입니다.\")\n        else:\n            print(f\"{number}은 홀수입니다.\")\n            \n# if __name__ == \"__main__\":\n#     main()\n\n\nnumber = 3\nif number % 2 == 0:\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\n    if number % 10 in [2, 4, 5, 9]:\n        print(f\"{number}는 짝수입니다.\")\n    else:\n        print(f\"{number}은 짝수입니다.\")\nelse:\n    if number % 10 in [2, 4, 5, 9]: # 는\n        print(f\"{number}는 홀수입니다.\")\n    else:\n        print(f\"{number}은 홀수입니다.\")\n\n3은 홀수입니다."
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#홀짝-구분",
    "href": "posts/7_study/6_as2023/2023-09-11.html#홀짝-구분",
    "title": "2wk 파이썬",
    "section": "",
    "text": "나머지 연산, 조건문\n\n\ndef main():\n    # a = int(input(\"숫자를 입력하세요.\"))\n    number = 3\n    if num % 2 == 0:\n        # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\n        if number % 10 in [2, 4, 5, 9]:\n            print(f\"{number}는 짝수입니다.\")\n        else:\n            print(f\"{number}은 짝수입니다.\")\n    else:\n        if number % 10 in [2, 4, 5, 9]: # 는\n            print(f\"{number}는 홀수입니다.\")\n        else:\n            print(f\"{number}은 홀수입니다.\")\n            \n# if __name__ == \"__main__\":\n#     main()\n\n\nnumber = 3\nif number % 2 == 0:\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\n    if number % 10 in [2, 4, 5, 9]:\n        print(f\"{number}는 짝수입니다.\")\n    else:\n        print(f\"{number}은 짝수입니다.\")\nelse:\n    if number % 10 in [2, 4, 5, 9]: # 는\n        print(f\"{number}는 홀수입니다.\")\n    else:\n        print(f\"{number}은 홀수입니다.\")\n\n3은 홀수입니다."
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#단위변환-온도-길이-등",
    "href": "posts/7_study/6_as2023/2023-09-11.html#단위변환-온도-길이-등",
    "title": "2wk 파이썬",
    "section": "2. 단위변환 (온도, 길이 등)",
    "text": "2. 단위변환 (온도, 길이 등)\n\n함수, 포맷팅\n\n\ndef f2c(temp_f):\n    return (temp_f - 32) * 5 / 9\n\ndef main():\n    # 두 줄\n    temp_f = 80\n    temp_c = f2c(temp_f)\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\n    \n    # 한 줄\n    print(f\"{temp_f}F =&gt; {f2c(temp_f):.2f}C\")\n\n\ntemp_f = 80\ntemp_c = f2c(temp_f)\nprint(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\n\n# 한 줄\nprint(f\"{temp_f}F =&gt; {f2c(temp_f):.2f}C\")\n\n80F =&gt; 26.67C\n80F =&gt; 26.67C\n\n\n\nshift F6: num 이라고 써있는 것을 한꺼번에 바꿔준다.\n1헥타르 = 10,000 제곱미터 = 대략 3천평\n보통 밭의 크기 300평 = 1,000 제곱미터 = 0.1헥타르 = 10a(아르)"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#소수-판별하기",
    "href": "posts/7_study/6_as2023/2023-09-11.html#소수-판별하기",
    "title": "2wk 파이썬",
    "section": "3. 소수 판별하기",
    "text": "3. 소수 판별하기\n\n함수, 조건문(break 등)\n\n\ndef is_prime(num):\n    if num &lt; 2:\n        return False\n    for i in range(2, num):\n        if num % i == 0:\n            return False\n    return True\n\ndef main():\n    num = 8\n    \n    if is_prime(num):\n        print(f\"{num}은/는 소수입니다.\")\n    else:\n        print(f\"{num}은/는 소수가 아닙니다.\")\n\n\nnum = 8\n\nif is_prime(num):\n    print(f\"{num} 은/는 소수입니다.\")\nelse:\n    print(f\"{num} 은/는 소수가 아닙니다.\")\n\n8 은/는 소수가 아닙니다.\n\n\n- 에라토스테네스의 체"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#소수-구하기",
    "href": "posts/7_study/6_as2023/2023-09-11.html#소수-구하기",
    "title": "2wk 파이썬",
    "section": "4. 소수 구하기",
    "text": "4. 소수 구하기\n\n함수, 반복문\n\n\ndef main():\n    list_prime = [x for x in range(1, 1000) if is_prime(x)]\n    print(f\"1-100까지 중 소수는 {list_prime}입니다.\")\n    print(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")\n\n\nlist_prime = [x for x in range(1, 1000) if is_prime(x)]\nprint(f\"1-100까지 중 소수는 {list_prime}입니다.\")\nprint(f\"1-100까지 중 소수의 개수는 {len(list_prime)}입니다.\")\n\n1-100까지 중 소수는 [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997]입니다.\n1-100까지 중 소수의 개수는 168입니다."
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#팩토리얼-구하기",
    "href": "posts/7_study/6_as2023/2023-09-11.html#팩토리얼-구하기",
    "title": "2wk 파이썬",
    "section": "5. 팩토리얼 구하기",
    "text": "5. 팩토리얼 구하기\n\n재귀함수\n\n\n# factorial\ndef fact(num):\n    result = 1\n    for i in range(1, num+1):\n        result = result * i\n    return result\n\ndef main():\n    print(f\"{10}!은 {fact(10)}입니다.\")\n    \nif __name__ == \"__main__\":\n    main()\n\n10!은 3628800입니다.\n\n\n\ndef fact(num):\n    if num == 1:\n        return 1\n    return num * fact(num - 1) # 재귀식 이용\n\n\nfact(3)\n\n6\n\n\n- 재귀함수\n구글에 Recursion 검색.. &gt; recursion의 의미를 알 수 있음. (구글의 장난)\nchat : 재귀 함수(Recursive function)는 자기 자신을 호출하는 함수입니다. 다시 말해, 함수 내에서 함수 자신을 호출하는 것을 의미합니다\n- 점화식\n\\(F_n = F_{n-1} + F_{n-2}, \\quad F_0=0, F_1=1\\)"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#까지-짝수-합",
    "href": "posts/7_study/6_as2023/2023-09-11.html#까지-짝수-합",
    "title": "2wk 파이썬",
    "section": "6. 1-100까지 짝수 합",
    "text": "6. 1-100까지 짝수 합\n\n반복문, 조건문\n\n- 지능형 리스트 (=리스트 컴프리헨션)\n [x for x in ~~]\n\ndef is_even(a):\n    return a % 2 == 0\n\ndef main():\n    evens = [x for x in range(1, 101) if is_even(x)]  # 뒤에 if가 붙으면 filter역할\n    sum_even = sum(evens)\n    \n    print(f\"1-100까지 숫자 중 짝수의 합은 {sum_even}입니다.\")\n\n\nmain()\n\n1-100까지 숫자 중 짝수의 합은 2550입니다."
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#예외처리",
    "href": "posts/7_study/6_as2023/2023-09-11.html#예외처리",
    "title": "2wk 파이썬",
    "section": "예외처리",
    "text": "예외처리\n코드 잘 짜고 에러가 안나는데 돌릴 때 에러가 나는 경우.\n\nwhile True:\n    try:\n        x = int(input('Please enter a number: '))\n        break # 에러가 안나면 break!\n    except ValueError:\n        print('Oops! That was no valid number. Try Again.')\n\nPlease enter a number:  d\nPlease enter a number:  1\n\n\nOops! That was no valid number. Try Again."
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#gui",
    "href": "posts/7_study/6_as2023/2023-09-11.html#gui",
    "title": "2wk 파이썬",
    "section": "GUI",
    "text": "GUI\nimport tkinter as tk\nfrom tkinter import simpledialog\n\n# ROOT = tk.Tk()\n\n# ROOT.withdraw()\n\ndef simple_gui_input(text=\"값을 입력하세요.\"): # 값을 입력하지 않으면 디폴트로 표시\n    return simpledialog.askstring(title='GUI 창', # title도 이런식으로 바꿀 수 있다.\n                                   prompt = text)\n\nif __name__ == \"__main__\":\n    user_input = simple_gui_input(\"첫번째 숫자를 입력해주세요\")\n    user_input2 = simple_gui_input(\"두번째 숫자를 입력해주세요\")\n    \n    print(f\"입력된 값은 {user_iuput}와 {user_input2}\")\n\n    \n# USER_INP = simpledialog.askstring(title='GUI 창',\n#                                   prompt = \"숫자를 입력해주세요.\")\n\n# print(f\"입력된 값은 {user_input}\")\n# 소수판별기\ndef main():\n    num = int(input(\"숫자를 입력하세요.\"))\n    \n    if is_prime(num):\n        print(f\"{num}은/는 소수입니다.\")\n        \n    ....\n\nGUI버전으로 바꿔보기.\n\n\nfrom rich import print\n\nprint(\"Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())\n\n# run &gt; edit~~ / edit Emulate? 체크해주면 된다. (pycharm에서) -- 에디터 특성.\n\nHello, [bold magentalWorld/bold magental! 🧛\n{\n    '__name__': '__main__',\n    '__doc__': 'Automatically created module for IPython interactive environment',\n    '__package__': None,\n    '__loader__': None,\n    '__spec__': None,\n    '__builtin__': &lt;module 'builtins' (built-in)&gt;,\n    '__builtins__': &lt;module 'builtins' (built-in)&gt;,\n    '_ih': [\n        '',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}\\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\\n        \\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")\\n        \\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")',\n        'main(5)',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은/는 홀수입니다.\")',\n        'main',\n        'main()',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        if \nnumber % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n            print(f\"{number}는 짝수입니다.\")\\n       \nelse:\\n            print(f\"{number}은 짝수입니다.\\n        print(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        if number % 10 in [2, 4, 5, 9]: # 는\\n            \nprint(f\"{number}는 홀수입니다.)\\n        else:\\n            print(f\"{number}은 홀수입니다.\")\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")',\n        'main',\n        'main(3)',\n        'main',\n        'main.item()',\n        'main',\n        'main.main',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"}',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f C})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}C\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":\\n    main()',\n        'def is_prime(num):\\n    for i in range(2, num):\\n        if num % i == 0:\\n            return False\\n    \nreturn True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n        print(f\"{num}은/는 소수입니다.\")\\n   \nelse:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\n# if __name__ == \"__main__\":\\n#     main()',\n        'number = 3\\nif num % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n    \nif number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은 \n짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 홀수입니다.\")\\n    \nelse:\\n        print(f\"{number}은 홀수입니다.\")',\n        'number = 3\\nif number % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n  \nif number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은 \n짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 홀수입니다.\")\\n    \nelse:\\n        print(f\"{number}은 홀수입니다.\")',\n        'def is_prime(num):\\n    if num &lt; 2:\\n        return False\\n    for i in range(2, num):\\n        if num % i\n== 0:\\n            return False\\n    return True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n        \nprint(f\"{num}은/는 소수입니다.\")\\n    else:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n        'temp_f = 80\\ntemp_c = f2c(temp_f)\\nprint(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n\\n# 한 줄\\nprint(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}C\")',\n        'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num}은/는 소수입니다.\")\\nelse:\\n    print(f\"{num}은/는 소수가 \n아닙니다.\")',\n        'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num} 은/는 소수입니다.\")\\nelse:\\n    print(f\"{num} 은/는 소수가\n아닙니다.\")',\n        'def main():\\n    list_prime = [x for x in range(1, 1000) if is_prime(x)]\\n    print(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\n    print(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n        'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n        'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {len(list_prime)}입니다.\")',\n        'main()',\n        'def fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n    return \nresult',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result',\n        'def fact(num):\\n    if num == 1:\\n        return 1\\n    return num * fact(num - 1)',\n        'fact(3)',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{num}!dms {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{num}!은 {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{10}!은 {fact(10)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        'def is_even(a):\\n    return a % 2 == 0\\n\\ndef main():\\n    evens = [x for x in range(1, 101) if \nis_even(x)]  # 뒤에 if가 붙으면 filter역할\\n    sum_even = sum(evens)\\n    \\n    print(f\"1-100까지 숫자 중 짝수의 \n합은 {sum_even}입니다.\")',\n        'main()',\n        'var1 = 34\\nvar2 = \"p1234n\"\\nvar6 = 35.1\\n\\n# numbers\\nprint(isinstance(var1, int))\\nprint(isinstance(var6,\nfloat))\\nprint(var1 &lt; 35)\\nprint(var1 &lt;= var6)',\n        \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n        \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n        'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()',\n        \"get_ipython().system('pip install tkinter')\",\n        'import tkinter as tk',\n        'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = \ntk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n                                  \nprompt = \"What\\'s your name?:\")\\n\\nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP = \nsimpledialog.askstring(title=\\'Test\\',\\n                                  prompt = \"What\\'s your \nname?:\")\\n\\nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# USER_INP \n= simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your name?:\")\\n\\n# \nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# \nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your \nname?:\")\\n\\n# print(\"Hello\", USER_INP)',\n        'echo $DISPLAY',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n\\ndef \nsimple_gui_input():\\n    return simpledialog.askstring(title=\\'GUI 창\\',\\n                                   prompt\n= \"숫자를 입력해주세요.\")\\n\\nif __name__ == \"__main__\":\\n    user_input = simple_gui_input()\\n    user_input2 = \nsimple_gui_input()\\n    \\n    print(f\"입력된 값은 {user_iuput}와 {user_input2}\")\\n\\n    \\n# USER_INP = \nsimpledialog.askstring(title=\\'GUI 창\\',\\n#                                   prompt = \"숫자를 입력해주세요.\")\\n\\n#\nprint(f\"입력된 값은 {user_input}\")',\n        '# 소수판별기\\ndef main():\\n    num = int(input(',\n        \"get_ipython().system('pip install rich')\",\n        'from rich import print\\n\\nprint(\\'Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())',\n        'from rich import print\\n\\nprint(\"Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())'\n    ],\n    '_oh': {\n        7: &lt;function main at 0x7f808835f4c0&gt;,\n        13: &lt;function main at 0x7f807a7ea430&gt;,\n        15: &lt;function main at 0x7f807a7ea430&gt;,\n        17: &lt;function main at 0x7f807a7ea430&gt;,\n        42: 6\n    },\n    '_dh': [PosixPath('/home/jy/Dropbox/임지윤/Quarto-Blog/posts/5_study')],\n    'In': [\n        '',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}\\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\\n        \\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")\\n        \\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")',\n        'main(5)',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은/는 홀수입니다.\")',\n        'main',\n        'main()',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        if \nnumber % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n            print(f\"{number}는 짝수입니다.\")\\n       \nelse:\\n            print(f\"{number}은 짝수입니다.\\n        print(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        if number % 10 in [2, 4, 5, 9]: # 는\\n            \nprint(f\"{number}는 홀수입니다.)\\n        else:\\n            print(f\"{number}은 홀수입니다.\")\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")',\n        'main',\n        'main(3)',\n        'main',\n        'main.item()',\n        'main',\n        'main.main',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"}',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f C})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}C\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":\\n    main()',\n        'def is_prime(num):\\n    for i in range(2, num):\\n        if num % i == 0:\\n            return False\\n    \nreturn True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n        print(f\"{num}은/는 소수입니다.\")\\n   \nelse:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\n# if __name__ == \"__main__\":\\n#     main()',\n        'number = 3\\nif num % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n    \nif number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은 \n짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 홀수입니다.\")\\n    \nelse:\\n        print(f\"{number}은 홀수입니다.\")',\n        'number = 3\\nif number % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n  \nif number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은 \n짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 홀수입니다.\")\\n    \nelse:\\n        print(f\"{number}은 홀수입니다.\")',\n        'def is_prime(num):\\n    if num &lt; 2:\\n        return False\\n    for i in range(2, num):\\n        if num % i\n== 0:\\n            return False\\n    return True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n        \nprint(f\"{num}은/는 소수입니다.\")\\n    else:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n        'temp_f = 80\\ntemp_c = f2c(temp_f)\\nprint(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n\\n# 한 줄\\nprint(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}C\")',\n        'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num}은/는 소수입니다.\")\\nelse:\\n    print(f\"{num}은/는 소수가 \n아닙니다.\")',\n        'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num} 은/는 소수입니다.\")\\nelse:\\n    print(f\"{num} 은/는 소수가\n아닙니다.\")',\n        'def main():\\n    list_prime = [x for x in range(1, 1000) if is_prime(x)]\\n    print(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\n    print(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n        'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n        'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {len(list_prime)}입니다.\")',\n        'main()',\n        'def fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n    return \nresult',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result',\n        'def fact(num):\\n    if num == 1:\\n        return 1\\n    return num * fact(num - 1)',\n        'fact(3)',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{num}!dms {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{num}!은 {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{10}!은 {fact(10)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        'def is_even(a):\\n    return a % 2 == 0\\n\\ndef main():\\n    evens = [x for x in range(1, 101) if \nis_even(x)]  # 뒤에 if가 붙으면 filter역할\\n    sum_even = sum(evens)\\n    \\n    print(f\"1-100까지 숫자 중 짝수의 \n합은 {sum_even}입니다.\")',\n        'main()',\n        'var1 = 34\\nvar2 = \"p1234n\"\\nvar6 = 35.1\\n\\n# numbers\\nprint(isinstance(var1, int))\\nprint(isinstance(var6,\nfloat))\\nprint(var1 &lt; 35)\\nprint(var1 &lt;= var6)',\n        \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n        \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n        'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()',\n        \"get_ipython().system('pip install tkinter')\",\n        'import tkinter as tk',\n        'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = \ntk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n                                  \nprompt = \"What\\'s your name?:\")\\n\\nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP = \nsimpledialog.askstring(title=\\'Test\\',\\n                                  prompt = \"What\\'s your \nname?:\")\\n\\nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# USER_INP \n= simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your name?:\")\\n\\n# \nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# \nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your \nname?:\")\\n\\n# print(\"Hello\", USER_INP)',\n        'echo $DISPLAY',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n\\ndef \nsimple_gui_input():\\n    return simpledialog.askstring(title=\\'GUI 창\\',\\n                                   prompt\n= \"숫자를 입력해주세요.\")\\n\\nif __name__ == \"__main__\":\\n    user_input = simple_gui_input()\\n    user_input2 = \nsimple_gui_input()\\n    \\n    print(f\"입력된 값은 {user_iuput}와 {user_input2}\")\\n\\n    \\n# USER_INP = \nsimpledialog.askstring(title=\\'GUI 창\\',\\n#                                   prompt = \"숫자를 입력해주세요.\")\\n\\n#\nprint(f\"입력된 값은 {user_input}\")',\n        '# 소수판별기\\ndef main():\\n    num = int(input(',\n        \"get_ipython().system('pip install rich')\",\n        'from rich import print\\n\\nprint(\\'Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())',\n        'from rich import print\\n\\nprint(\"Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())'\n    ],\n    'Out': {\n        7: &lt;function main at 0x7f808835f4c0&gt;,\n        13: &lt;function main at 0x7f807a7ea430&gt;,\n        15: &lt;function main at 0x7f807a7ea430&gt;,\n        17: &lt;function main at 0x7f807a7ea430&gt;,\n        42: 6\n    },\n    'get_ipython': &lt;bound method InteractiveShell.get_ipython of &lt;ipykernel.zmqshell.ZMQInteractiveShell object at \n0x7f8089ca0eb0&gt;&gt;,\n    'exit': &lt;IPython.core.autocall.ZMQExitAutocall object at 0x7f8089ca97f0&gt;,\n    'quit': &lt;IPython.core.autocall.ZMQExitAutocall object at 0x7f8089ca97f0&gt;,\n    'open': &lt;function open at 0x7f808c7f2700&gt;,\n    '_': 6,\n    '__': &lt;function main at 0x7f807a7ea430&gt;,\n    '___': &lt;function main at 0x7f807a7ea430&gt;,\n    '_i': 'from rich import print\\n\\nprint(\\'Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())',\n    '_ii': '!pip install rich',\n    '_iii': '# 소수판별기\\ndef main():\\n    num = int(input(',\n    '_i1': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}\\n    ',\n    '_i2': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\\n        \\n    ',\n    '_i3': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")\\n        \\n    ',\n    'main': &lt;function main at 0x7f807a3e65e0&gt;,\n    '_i4': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")',\n    '_i5': 'main(5)',\n    '_i6': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은/는 홀수입니다.\")',\n    '_i7': 'main',\n    '_7': &lt;function main at 0x7f808835f4c0&gt;,\n    '_i8': 'main()',\n    '_i9': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n            print(f\"{number}는 짝수입니다.\")\\n    \nelse:\\n            print(f\"{number}은 짝수입니다.\\n        print(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n    '_i10': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n       \n# if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n       \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        if number % 10 in [2, 4, 5, 9]: # 는\\n            \nprint(f\"{number}는 홀수입니다.)\\n        else:\\n            print(f\"{number}은 홀수입니다.\")\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n    '_i11': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n       \n# if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n       \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":',\n    '_i12': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n       \n# if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n       \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")',\n    '_i13': 'main',\n    '_13': &lt;function main at 0x7f807a7ea430&gt;,\n    '_i14': 'main(3)',\n    '_i15': 'main',\n    '_15': &lt;function main at 0x7f807a7ea430&gt;,\n    '_i16': 'main.item()',\n    '_i17': 'main',\n    '_17': &lt;function main at 0x7f807a7ea430&gt;,\n    '_i18': 'main.main',\n    '_i19': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9',\n    'f2c': &lt;function f2c at 0x7f807a4b7d30&gt;,\n    '_i20': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"}',\n    '_i21': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"})',\n    '_i22': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}\\'C\"})',\n    '_i23': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f\\'C\"})',\n    '_i24': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f C})',\n    '_i25': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}C\")',\n    '_i26': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n       \n# if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n       \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":\\n    main()',\n    '_i27': 'def is_prime(num):\\n    for i in range(2, num):\\n        if num % i == 0:\\n            return False\\n \nreturn True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n        print(f\"{num}은/는 소수입니다.\")\\n   \nelse:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n    'is_prime': &lt;function is_prime at 0x7f807a3e6280&gt;,\n    '_i28': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n       \n# if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n       \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\n# if __name__ == \"__main__\":\\n#     main()',\n    '_i29': 'number = 3\\nif num % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n \nif number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은 \n짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 홀수입니다.\")\\n    \nelse:\\n        print(f\"{number}은 홀수입니다.\")',\n    'number': 3,\n    '_i30': 'number = 3\\nif number % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == \n5:#는\\n    if number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        \nprint(f\"{number}은 짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 \n홀수입니다.\")\\n    else:\\n        print(f\"{number}은 홀수입니다.\")',\n    '_i31': 'def is_prime(num):\\n    if num &lt; 2:\\n        return False\\n    for i in range(2, num):\\n        if num\n% i == 0:\\n            return False\\n    return True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n    \nprint(f\"{num}은/는 소수입니다.\")\\n    else:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n    '_i32': 'temp_f = 80\\ntemp_c = f2c(temp_f)\\nprint(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n\\n# 한 줄\\nprint(f\"{temp_f}F \n=&gt; {f2c(temp_f):.2f}C\")',\n    'temp_f': 80,\n    'temp_c': 26.666666666666668,\n    '_i33': 'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num}은/는 소수입니다.\")\\nelse:\\n    print(f\"{num}은/는 \n소수가 아닙니다.\")',\n    'num': 8,\n    '_i34': 'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num} 은/는 소수입니다.\")\\nelse:\\n    print(f\"{num} 은/는 \n소수가 아닙니다.\")',\n    '_i35': 'def main():\\n    list_prime = [x for x in range(1, 1000) if is_prime(x)]\\n    print(f\"1-100까지 중 \n소수는 {list_prime}입니다.\")\\n    print(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n    '_i36': 'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n    'list_prime': [\n        2,\n        3,\n        5,\n        7,\n        11,\n        13,\n        17,\n        19,\n        23,\n        29,\n        31,\n        37,\n        41,\n        43,\n        47,\n        53,\n        59,\n        61,\n        67,\n        71,\n        73,\n        79,\n        83,\n        89,\n        97,\n        101,\n        103,\n        107,\n        109,\n        113,\n        127,\n        131,\n        137,\n        139,\n        149,\n        151,\n        157,\n        163,\n        167,\n        173,\n        179,\n        181,\n        191,\n        193,\n        197,\n        199,\n        211,\n        223,\n        227,\n        229,\n        233,\n        239,\n        241,\n        251,\n        257,\n        263,\n        269,\n        271,\n        277,\n        281,\n        283,\n        293,\n        307,\n        311,\n        313,\n        317,\n        331,\n        337,\n        347,\n        349,\n        353,\n        359,\n        367,\n        373,\n        379,\n        383,\n        389,\n        397,\n        401,\n        409,\n        419,\n        421,\n        431,\n        433,\n        439,\n        443,\n        449,\n        457,\n        461,\n        463,\n        467,\n        479,\n        487,\n        491,\n        499,\n        503,\n        509,\n        521,\n        523,\n        541,\n        547,\n        557,\n        563,\n        569,\n        571,\n        577,\n        587,\n        593,\n        599,\n        601,\n        607,\n        613,\n        617,\n        619,\n        631,\n        641,\n        643,\n        647,\n        653,\n        659,\n        661,\n        673,\n        677,\n        683,\n        691,\n        701,\n        709,\n        719,\n        727,\n        733,\n        739,\n        743,\n        751,\n        757,\n        761,\n        769,\n        773,\n        787,\n        797,\n        809,\n        811,\n        821,\n        823,\n        827,\n        829,\n        839,\n        853,\n        857,\n        859,\n        863,\n        877,\n        881,\n        883,\n        887,\n        907,\n        911,\n        919,\n        929,\n        937,\n        941,\n        947,\n        953,\n        967,\n        971,\n        977,\n        983,\n        991,\n        997\n    ],\n    '_i37': 'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {len(list_prime)}입니다.\")',\n    '_i38': 'main()',\n    '_i39': 'def fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n    return\nresult',\n    'fact': &lt;function fact at 0x7f807a3e6670&gt;,\n    '_i40': '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * \ni\\n    return result',\n    '_i41': 'def fact(num):\\n    if num == 1:\\n        return 1\\n    return num * fact(num - 1)',\n    '_i42': 'fact(3)',\n    '_42': 6,\n    '_i43': '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * \ni\\n    return result\\n\\ndef main():\\n    print(f\"{num}!dms {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n\nmain()',\n    '_i44': '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * \ni\\n    return result\\n\\ndef main():\\n    print(f\"{num}!은 {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n \nmain()',\n    '_i45': '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * \ni\\n    return result\\n\\ndef main():\\n    print(f\"{10}!은 {fact(10)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n   \nmain()',\n    '_i46': 'def is_even(a):\\n    return a % 2 == 0\\n\\ndef main():\\n    evens = [x for x in range(1, 101) if \nis_even(x)]  # 뒤에 if가 붙으면 filter역할\\n    sum_even = sum(evens)\\n    \\n    print(f\"1-100까지 숫자 중 짝수의 \n합은 {sum_even}입니다.\")',\n    'is_even': &lt;function is_even at 0x7f807a3e6550&gt;,\n    '_i47': 'main()',\n    '_i48': 'var1 = 34\\nvar2 = \"p1234n\"\\nvar6 = 35.1\\n\\n# numbers\\nprint(isinstance(var1, \nint))\\nprint(isinstance(var6, float))\\nprint(var1 &lt; 35)\\nprint(var1 &lt;= var6)',\n    'var1': 34,\n    'var2': 'p1234n',\n    'var6': 35.1,\n    '_i49': \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n    'x': 1,\n    '_i50': \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n    '_i51': 'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()',\n    'tk': &lt;module 'tkinter' from '/home/jy/anaconda3/envs/torch/lib/python3.8/tkinter/__init__.py'&gt;,\n    '_i52': '!pip install tkinter',\n    '_exit_code': 0,\n    '_i53': 'import tkinter as tk',\n    '_i54': 'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = \ntk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n                                  \nprompt = \"What\\'s your name?:\")\\n\\nprint(\"Hello\", USER_INP)',\n    '_i55': 'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP \n= simpledialog.askstring(title=\\'Test\\',\\n                                  prompt = \"What\\'s your \nname?:\")\\n\\nprint(\"Hello\", USER_INP)',\n    'simpledialog': &lt;module 'tkinter.simpledialog' from \n'/home/jy/anaconda3/envs/torch/lib/python3.8/tkinter/simpledialog.py'&gt;,\n    '_i56': 'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# \nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your \nname?:\")\\n\\n# print(\"Hello\", USER_INP)',\n    '_i57': 'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# \nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your \nname?:\")\\n\\n# print(\"Hello\", USER_INP)',\n    '_i58': 'echo $DISPLAY',\n    '_i59': 'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n\\ndef\nsimple_gui_input():\\n    return simpledialog.askstring(title=\\'GUI 창\\',\\n                                   prompt\n= \"숫자를 입력해주세요.\")\\n\\nif __name__ == \"__main__\":\\n    user_input = simple_gui_input()\\n    user_input2 = \nsimple_gui_input()\\n    \\n    print(f\"입력된 값은 {user_iuput}와 {user_input2}\")\\n\\n    \\n# USER_INP = \nsimpledialog.askstring(title=\\'GUI 창\\',\\n#                                   prompt = \"숫자를 입력해주세요.\")\\n\\n#\nprint(f\"입력된 값은 {user_input}\")',\n    'simple_gui_input': &lt;function simple_gui_input at 0x7f807879a040&gt;,\n    '_i60': '# 소수판별기\\ndef main():\\n    num = int(input(',\n    '_i61': '!pip install rich',\n    '_i62': 'from rich import print\\n\\nprint(\\'Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())',\n    '_i63': 'from rich import print\\n\\nprint(\"Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())',\n    'print': &lt;function print at 0x7f807889b9d0&gt;\n}\n\n\n\n예제 6개, 예제 1개?"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-14-graph.html",
    "href": "posts/7_study/7_ds2023/2023-09-14-graph.html",
    "title": "2wk-2 그래프",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-14-graph.html#import",
    "href": "posts/7_study/7_ds2023/2023-09-14-graph.html#import",
    "title": "2wk-2 그래프",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-14-graph.html#막대그래프",
    "href": "posts/7_study/7_ds2023/2023-09-14-graph.html#막대그래프",
    "title": "2wk-2 그래프",
    "section": "막대그래프",
    "text": "막대그래프\n\n# Make a dataset 지게차 톤수별 1회 평균 주유량\noil = [30, 35, 40, 42, 45]\nton = ['5TON', '7TON', '10TON', '11TON', '15TON']\ny_pos = np.arange(len(ton))\n\n# 막대그래프 만들기\nplt.bar(y_pos, oil)\n# 색상 변경 옵션\n# plt.bar(y_pos, oil, color='g')\n# plt.bar(y_pos, oil, color=['black', 'red', 'green', 'blue', 'cyan'])\n\n# 축이름 바꾸기\nplt.xticks(y_pos, ton)\n\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Look at index 4 and 6, which demonstrate overlapping cases.\nx1 = [1, 3, 4, 5, 6, 7, 9]\ny1 = [4, 7, 2, 4, 7, 8, 3]\n\nx2 = [2, 4, 6, 8, 10]\ny2 = [5, 6, 2, 6, 2]\n\n# Colors: https://matplotlib.org/api/colors_api.html\n\nplt.bar(x1, y1, label=\"Blue Bar\", color='b')\nplt.bar(x2, y2, label=\"Green Bar\", color='g')\nplt.plot()\n\nplt.xlabel(\"bar number\")\nplt.ylabel(\"bar height\")\nplt.title(\"Bar Chart Example\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-14-graph.html#선그래프",
    "href": "posts/7_study/7_ds2023/2023-09-14-graph.html#선그래프",
    "title": "2wk-2 그래프",
    "section": "선그래프",
    "text": "선그래프\n\nimport matplotlib.pyplot as plt\n\nx  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\ny1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\ny2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\nplt.plot(x, y1, label=\"line L\")\nplt.plot(x, y2, label=\"line H\")\nplt.plot()\n\nplt.xlabel(\"x axis\")\nplt.ylabel(\"y axis\")\nplt.title(\"Line Graph Example\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-14-graph.html#중첩-그래프",
    "href": "posts/7_study/7_ds2023/2023-09-14-graph.html#중첩-그래프",
    "title": "2wk-2 그래프",
    "section": "중첩 그래프",
    "text": "중첩 그래프\n\nimport matplotlib.pyplot as plt\n\nidxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\narr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\narr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\narr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n\n# Adding legend for stack plots is tricky.\nplt.plot([], [], color='r', label = 'D 1')\nplt.plot([], [], color='g', label = 'D 2')\nplt.plot([], [], color='b', label = 'D 3')\n\nplt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\nplt.title('Stack Plot Example')\nplt.legend()\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nys = 200 + np.random.randn(100)\nx = [x for x in range(len(ys))]\n\nplt.plot(x, ys, '-')\nplt.fill_between(x, ys, 195, where=(ys &gt; 195), facecolor='g', alpha=0.6)\n\nplt.title(\"Fills and Alpha Example\")\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-26-LinearAlgebra.html",
    "href": "posts/7_study/7_ds2023/2023-09-26-LinearAlgebra.html",
    "title": "4wk-1 선형대수",
    "section": "",
    "text": "선형대수에서 다루는 데이터는 개수나 형태에 따라 크게 스칼라(scalar), 벡터(vector), 행렬(matrix), 텐서(tensor) 유형으로 나뉜다. 스칼라는 숫자 하나로 이루어진 데이터이고, 벡터는 여러 숫자로 이루어진 데이터 레코드(data record)이며, 행렬은 이러한 벡터, 즉 데이터 레코드가 여럿인 데이터 집합이라고 볼 수 있다. 텐서는 같은 크기의 행렬이 여러 개 있는 것이라고 생각하면 된다.\n\nimport numpy as np\n\n\n# 열 벡터\nx1 = np.array([[5.1], [3.5], [1.4], [0.2]])\nprint(x1)\nprint(x1.shape)\n\n[[5.1]\n [3.5]\n [1.4]\n [0.2]]\n(4, 1)\n\n\n\n# 행 벡터\nx2 = np.array([5.1, 3.5, 1.4, 0.2])\nprint(x2)\nprint(x2.shape)\n\n[5.1 3.5 1.4 0.2]\n(4,)\n\n\n\n# 행렬\nA = np.array([[11,12,13],[14,15,16]])\nA\n\narray([[11, 12, 13],\n       [14, 15, 16]])\n\n\n\nA.shape\n\n(2, 3)\n\n\n\n# 전치행렬\nprint(A.T)\n\n[[11 14]\n [12 15]\n [13 16]]\n\n\n\n# 벡터의 전치는?\nprint(\"x1=\")\nprint(x1.T)\nprint(x1.T.shape)\n\nx1=\n[[5.1 3.5 1.4 0.2]]\n(1, 4)"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-26-LinearAlgebra.html#데이터의-유형",
    "href": "posts/7_study/7_ds2023/2023-09-26-LinearAlgebra.html#데이터의-유형",
    "title": "4wk-1 선형대수",
    "section": "",
    "text": "선형대수에서 다루는 데이터는 개수나 형태에 따라 크게 스칼라(scalar), 벡터(vector), 행렬(matrix), 텐서(tensor) 유형으로 나뉜다. 스칼라는 숫자 하나로 이루어진 데이터이고, 벡터는 여러 숫자로 이루어진 데이터 레코드(data record)이며, 행렬은 이러한 벡터, 즉 데이터 레코드가 여럿인 데이터 집합이라고 볼 수 있다. 텐서는 같은 크기의 행렬이 여러 개 있는 것이라고 생각하면 된다.\n\nimport numpy as np\n\n\n# 열 벡터\nx1 = np.array([[5.1], [3.5], [1.4], [0.2]])\nprint(x1)\nprint(x1.shape)\n\n[[5.1]\n [3.5]\n [1.4]\n [0.2]]\n(4, 1)\n\n\n\n# 행 벡터\nx2 = np.array([5.1, 3.5, 1.4, 0.2])\nprint(x2)\nprint(x2.shape)\n\n[5.1 3.5 1.4 0.2]\n(4,)\n\n\n\n# 행렬\nA = np.array([[11,12,13],[14,15,16]])\nA\n\narray([[11, 12, 13],\n       [14, 15, 16]])\n\n\n\nA.shape\n\n(2, 3)\n\n\n\n# 전치행렬\nprint(A.T)\n\n[[11 14]\n [12 15]\n [13 16]]\n\n\n\n# 벡터의 전치는?\nprint(\"x1=\")\nprint(x1.T)\nprint(x1.T.shape)\n\nx1=\n[[5.1 3.5 1.4 0.2]]\n(1, 4)"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-26-LinearAlgebra.html#행렬의-역행렬",
    "href": "posts/7_study/7_ds2023/2023-09-26-LinearAlgebra.html#행렬의-역행렬",
    "title": "4wk-1 선형대수",
    "section": "행렬의 역행렬",
    "text": "행렬의 역행렬\n\nA = np.array([[1, 1, 0], [0, 1, 1], [1, 1, 1]])\n\n\nAinv = np.linalg.inv(A)\nAinv\n\narray([[ 0., -1.,  1.],\n       [ 1.,  1., -1.],\n       [-1.,  0.,  1.]])"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-26-LinearAlgebra.html#행렬의-활용-연립방정식-풀이",
    "href": "posts/7_study/7_ds2023/2023-09-26-LinearAlgebra.html#행렬의-활용-연립방정식-풀이",
    "title": "4wk-1 선형대수",
    "section": "행렬의 활용 (연립방정식 풀이)",
    "text": "행렬의 활용 (연립방정식 풀이)\n\\(\\begin{cases} 2x + 3y = 1 \\\\ x-2y = 4 \\end{cases}\\)\n\nA = np.array([[2,3], [1,-2]])\nb = np.array([[1], [4]])\n\n\nA.shape, b.shape\n\n((2, 2), (2, 1))\n\n\n\nprint(A)\n\n[[ 2  3]\n [ 1 -2]]\n\n\n\nprint(b)\n\n[[1]\n [4]]\n\n\n\nx = np.linalg.inv(A) @ b\nx\n\narray([[ 2.],\n       [-1.]])\n\n\nFHD: 3 x 1080 x 1920"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-10-12-DealingData.html",
    "href": "posts/7_study/7_ds2023/2023-10-12-DealingData.html",
    "title": "6wk-2 데이터 다루기",
    "section": "",
    "text": "데이터과학은 일반적으로 파이썬과 R를 이용합니다. 우리 수업에서는 파이썬을 이용하기 때문에, 이번 수업시간에는 파이썬을 이용하여 데이터를 다루는 법에 대하여 살펴보겠습니다. 결론부터 말씀드리면, 파이썬에서 데이터를 다루는 내용은 주로 pandas를 이용합니다. 정제되지 않은 자료를 처리하기 위해서 numpy 혹은 기본 자료형인 컨테이너형 자료를 쓰기도 합니다. &gt; ref: https://jakevdp.github.io/PythonDataScienceHandbook/\nimport numpy as np\nimport pandas as pd\n\nprint(pd.__version__)\n\n2.0.3"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-10-12-DealingData.html#직접-입력하기",
    "href": "posts/7_study/7_ds2023/2023-10-12-DealingData.html#직접-입력하기",
    "title": "6wk-2 데이터 다루기",
    "section": "직접 입력하기",
    "text": "직접 입력하기\n판다스 데이터프레임은 표형식으로 되어있고, 판다스 시리즈는 프레임의 한줄에 인덱스가 붙어서 나옴.\n\ndata = pd.Series([0.25, 0.5, 0.75, 1.0])\nprint(data)\n\n0    0.25\n1    0.50\n2    0.75\n3    1.00\ndtype: float64\n\n\n\ndata.values\n\narray([0.25, 0.5 , 0.75, 1.  ])\n\n\n\ndata.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\n\ndata[1]\n\n0.5\n\n\n\ndata[1:3]\n\n5    0.50\n3    0.75\ndtype: float64\n\n\n\n# 인덱스 추가하기\ndata = pd.Series([0.25, 0.5, 0.75, 1.0],\n                 index=['a', 'b', 'c', 'd'])\ndata\n\na    0.25\nb    0.50\nc    0.75\nd    1.00\ndtype: float64\n\n\n\ndata['b']  # 앞에서는 data[1]로 접근하였음\n\n0.5\n\n\n\n# 인덱스를 바꿔줄 수도 있음.\n\ndata.index = [2,5,3,7]\ndata\n\n# data = pd.Series([0.25, 0.5, 0.75, 1.0],\n#                  index=[2, 5, 3, 7])\n# data\n\n2    0.25\n5    0.50\n3    0.75\n7    1.00\ndtype: float64\n\n\n\ndata[5] # 6번째 자료가 아님\n\n0.5\n\n\n\n# 사전형을 이용하여 생성하기\npopulation_dict = {'California': 38332521,\n                   'Texas': 26448193,\n                   'New York': 19651127,\n                   'Florida': 19552860,\n                   'Illinois': 12882135}\npopulation = pd.Series(population_dict)\npopulation\n\nCalifornia    38332521\nTexas         26448193\nNew York      19651127\nFlorida       19552860\nIllinois      12882135\ndtype: int64\n\n\n인덱스에 주 이름이 들어감. \\(\\to\\) 뽑아올 때도 주이름으로 뽑아오면 됨.\n\npopulation['California']\n\n38332521\n\n\n\npopulation['California':'Illinois'] # 원래의 규칙이 깨지는데?\n\nCalifornia    38332521\nTexas         26448193\nNew York      19651127\nFlorida       19552860\nIllinois      12882135\ndtype: int64\n\n\n우리가 알고있는 규칙에 따르면 Illinois 전까지 나와야 하는데, 인덱스에 이름을 주니까 인덱스의 처음부터 끝까지 다 나온다.\n\n# 사전형으로 인구수 외에 면적도 생성하여, 데이터프레임(df) 만들기\narea_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n             'Florida': 170312, 'Illinois': 149995}\narea = pd.Series(area_dict)\narea\n\nCalifornia    423967\nTexas         695662\nNew York      141297\nFlorida       170312\nIllinois      149995\ndtype: int64\n\n\n\nstates = pd.DataFrame({'population':population,\n                       'area': area})\nstates\n\n\n\n\n\n\n\n\npopulation\narea\n\n\n\n\nCalifornia\n38332521\n423967\n\n\nTexas\n26448193\n695662\n\n\nNew York\n19651127\n141297\n\n\nFlorida\n19552860\n170312\n\n\nIllinois\n12882135\n149995\n\n\n\n\n\n\n\n\n# 데이터프레임의 행(인덱스)와 열(컬럼명) 살펴보기\nprint(states.index)\nprint(states.columns)\n\nIndex(['California', 'Texas', 'New York', 'Florida', 'Illinois'], dtype='object')\nIndex(['population', 'area'], dtype='object')\n\n\n\n# Seies 객체를 데이터프레임으로 바꾸려면,\npd.DataFrame(population, columns=['population'])\n\n\n\n\n\n\n\n\npopulation\n\n\n\n\nCalifornia\n38332521\n\n\nTexas\n26448193\n\n\nNew York\n19651127\n\n\nFlorida\n19552860\n\n\nIllinois\n12882135\n\n\n\n\n\n\n\n\n# 지능형 리스트를 이용하여 사전형 자료를 만들고, 데이터프레임 생성\ndata = [{'a': i, 'b': 2 * i}\n        for i in range(3)]\npd.DataFrame(data)\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n0\n0\n\n\n1\n1\n2\n\n\n2\n2\n4\n\n\n\n\n\n\n\n\n[{'a':i, 'b':2 *i} for i in range(3)]\n\n[{'a': 0, 'b': 0}, {'a': 1, 'b': 2}, {'a': 2, 'b': 4}]\n\n\n\n# 빈값이 있는 경우\npd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1.0\n2\nNaN\n\n\n1\nNaN\n3\n4.0\n\n\n\n\n\n\n\n\n센서가 갑자기 멈추거나 네트워크가 끊겨 빈 값이 생기는 경우가 있다.\n가장 기본적인 방법은 데이터를 지우는 것이고, 다른값으로 추정하여 대치하는 방법이 있다.\n\n\n# 2차원 배열을 데이터프레임으로 바꾸는 경우\npd.DataFrame(np.random.rand(3,2),\n             columns = ['foo', 'bar'],\n             index = ['a', 'b', 'c'])\n\n\n\n\n\n\n\n\nfoo\nbar\n\n\n\n\na\n0.277854\n0.973513\n\n\nb\n0.614319\n0.116820\n\n\nc\n0.934636\n0.069115\n\n\n\n\n\n\n\n\n# 데이터프레임의 인덱스는 순서가 있는 집합(ordered set)\nindA = pd.Index([1, 3, 5, 7, 9])\nindB = pd.Index([2, 3, 5, 7, 11])\n\nprint(indA & indB)  # intersection\nprint(indA | indB)  # union\nprint(indA ^ indB)  # symmetric difference\n\nIndex([0, 3, 5, 7, 9], dtype='int64')\nIndex([3, 3, 5, 7, 11], dtype='int64')\nIndex([3, 0, 0, 0, 2], dtype='int64')\n\n\n\nindA\n\nIndex([1, 3, 5, 7, 9], dtype='int64')\n\n\n\nindB\n\nIndex([2, 3, 5, 7, 11], dtype='int64')\n\n\n\nprint(indA.intersection(indB))  # intersection\nprint(indA.union(indB))  # union\nprint(indA.symmetric_difference(indB))  # symmetric difference\n\nIndex([3, 5, 7], dtype='int64')\nIndex([1, 2, 3, 5, 7, 9, 11], dtype='int64')\nIndex([1, 2, 9, 11], dtype='int64')"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-10-12-DealingData.html#텍스트-파일-읽어서-처리하기-txt",
    "href": "posts/7_study/7_ds2023/2023-10-12-DealingData.html#텍스트-파일-읽어서-처리하기-txt",
    "title": "6wk-2 데이터 다루기",
    "section": "텍스트 파일 읽어서 처리하기 (txt)",
    "text": "텍스트 파일 읽어서 처리하기 (txt)\n\nwith open(\"data/housing_sample.csv\", \"r\") as f: # 집 값에 대한 데이터\n  for line in f:\n    print(line.strip())\n\nlongitude,latitude,housing_median_age,total_rooms,total_bedrooms,population,households,median_income,median_house_value,ocean_proximity\n-122.23,37.88,41.0,880.0,129.0,322.0,126.0,8.3252,452600.0,NEAR BAY\n-122.22,37.86,21.0,7099.0,1106.0,2401.0,1138.0,8.3014,358500.0,NEAR BAY\n-122.24,37.85,52.0,1467.0,190.0,496.0,177.0,7.2574,352100.0,NEAR BAY\n-122.25,37.85,52.0,1274.0,235.0,558.0,219.0,5.6431,341300.0,NEAR BAY\n-122.25,37.85,52.0,1627.0,280.0,565.0,259.0,3.8462,342200.0,NEAR BAY\n-122.25,37.85,52.0,919.0,213.0,413.0,193.0,4.0368,269700.0,NEAR BAY\n-122.25,37.84,52.0,2535.0,489.0,1094.0,514.0,3.6591,299200.0,NEAR BAY\n-122.25,37.84,52.0,3104.0,687.0,1157.0,647.0,3.12,241400.0,NEAR BAY\n-122.26,37.84,42.0,2555.0,665.0,1206.0,595.0,2.0804,226700.0,NEAR BAY\n-122.25,37.84,52.0,3549.0,707.0,1551.0,714.0,3.6912,261100.0,NEAR BAY\n-122.26,37.85,52.0,2202.0,434.0,910.0,402.0,3.2031,281500.0,NEAR BAY\n-122.26,37.85,52.0,3503.0,752.0,1504.0,734.0,3.2705,241800.0,NEAR BAY\n-122.26,37.85,52.0,2491.0,474.0,1098.0,468.0,3.075,213500.0,NEAR BAY\n-122.26,37.84,52.0,696.0,191.0,345.0,174.0,2.6736,191300.0,NEAR BAY\n-122.26,37.85,52.0,2643.0,626.0,1212.0,620.0,1.9167,159200.0,NEAR BAY\n-122.26,37.85,50.0,1120.0,283.0,697.0,264.0,2.125,140000.0,NEAR BAY\n-122.27,37.85,52.0,1966.0,347.0,793.0,331.0,2.775,152500.0,NEAR BAY\n-122.27,37.85,52.0,1228.0,293.0,648.0,303.0,2.1202,155500.0,NEAR BAY\n-122.26,37.84,50.0,2239.0,455.0,990.0,419.0,1.9911,158700.0,NEAR BAY\n-122.27,37.84,52.0,1503.0,298.0,690.0,275.0,2.6033,162900.0,NEAR BAY\n-122.27,37.85,40.0,751.0,184.0,409.0,166.0,1.3578,147500.0,NEAR BAY\n-122.27,37.85,42.0,1639.0,367.0,929.0,366.0,1.7135,159800.0,NEAR BAY\n-122.27,37.84,52.0,2436.0,541.0,1015.0,478.0,1.725,113900.0,NEAR BAY\n-122.27,37.84,52.0,1688.0,337.0,853.0,325.0,2.1806,99700.0,NEAR BAY\n-122.27,37.84,52.0,2224.0,437.0,1006.0,422.0,2.6,132600.0,NEAR BAY\n-122.28,37.85,41.0,535.0,123.0,317.0,119.0,2.4038,107500.0,NEAR BAY\n-122.28,37.85,49.0,1130.0,244.0,607.0,239.0,2.4597,93800.0,NEAR BAY\n-122.28,37.85,52.0,1898.0,421.0,1102.0,397.0,1.808,105500.0,NEAR BAY\n-122.28,37.84,50.0,2082.0,492.0,1131.0,473.0,1.6424,108900.0,NEAR BAY\n-122.28,37.84,52.0,729.0,160.0,395.0,155.0,1.6875,132000.0,NEAR BAY\n-122.28,37.84,49.0,1916.0,447.0,863.0,378.0,1.9274,122300.0,NEAR BAY\n-122.28,37.84,52.0,2153.0,481.0,1168.0,441.0,1.9615,115200.0,NEAR BAY\n-122.27,37.84,48.0,1922.0,409.0,1026.0,335.0,1.7969,110400.0,NEAR BAY\n-122.27,37.83,49.0,1655.0,366.0,754.0,329.0,1.375,104900.0,NEAR BAY\n-122.27,37.83,51.0,2665.0,574.0,1258.0,536.0,2.7303,109700.0,NEAR BAY\n-122.27,37.83,49.0,1215.0,282.0,570.0,264.0,1.4861,97200.0,NEAR BAY\n-122.27,37.83,48.0,1798.0,432.0,987.0,374.0,1.0972,104500.0,NEAR BAY\n-122.28,37.83,52.0,1511.0,390.0,901.0,403.0,1.4103,103900.0,NEAR BAY\n-122.26,37.83,52.0,1470.0,330.0,689.0,309.0,3.48,191400.0,NEAR BAY\n-122.26,37.83,52.0,2432.0,715.0,1377.0,696.0,2.5898,176000.0,NEAR BAY\n-122.26,37.83,52.0,1665.0,419.0,946.0,395.0,2.0978,155400.0,NEAR BAY\n-122.26,37.83,51.0,936.0,311.0,517.0,249.0,1.2852,150000.0,NEAR BAY\n-122.26,37.84,49.0,713.0,202.0,462.0,189.0,1.025,118800.0,NEAR BAY\n-122.26,37.84,52.0,950.0,202.0,467.0,198.0,3.9643,188800.0,NEAR BAY\n-122.26,37.83,52.0,1443.0,311.0,660.0,292.0,3.0125,184400.0,NEAR BAY\n-122.26,37.83,52.0,1656.0,420.0,718.0,382.0,2.6768,182300.0,NEAR BAY\n-122.26,37.83,50.0,1125.0,322.0,616.0,304.0,2.026,142500.0,NEAR BAY\n-122.27,37.82,43.0,1007.0,312.0,558.0,253.0,1.7348,137500.0,NEAR BAY\n-122.26,37.82,40.0,624.0,195.0,423.0,160.0,0.9506,187500.0,NEAR BAY\n-122.27,37.82,40.0,946.0,375.0,700.0,352.0,1.775,112500.0,NEAR BAY\n-122.27,37.82,21.0,896.0,453.0,735.0,438.0,0.9218,171900.0,NEAR BAY\n-122.27,37.82,43.0,1868.0,456.0,1061.0,407.0,1.5045,93800.0,NEAR BAY\n-122.27,37.82,41.0,3221.0,853.0,1959.0,720.0,1.1108,97500.0,NEAR BAY\n-122.27,37.82,52.0,1630.0,456.0,1162.0,400.0,1.2475,104200.0,NEAR BAY\n-122.28,37.82,52.0,1170.0,235.0,701.0,233.0,1.6098,87500.0,NEAR BAY\n-122.28,37.82,52.0,945.0,243.0,576.0,220.0,1.4113,83100.0,NEAR BAY\n-122.28,37.82,52.0,1238.0,288.0,622.0,259.0,1.5057,87500.0,NEAR BAY\n-122.28,37.82,52.0,1489.0,335.0,728.0,244.0,0.8172,85300.0,NEAR BAY\n-122.28,37.82,52.0,1387.0,341.0,1074.0,304.0,1.2171,80300.0,NEAR BAY\n-122.29,37.82,2.0,158.0,43.0,94.0,57.0,2.5625,60000.0,NEAR BAY\n-122.29,37.83,52.0,1121.0,211.0,554.0,187.0,3.3929,75700.0,NEAR BAY\n-122.29,37.82,49.0,135.0,29.0,86.0,23.0,6.1183,75000.0,NEAR BAY\n-122.29,37.81,50.0,760.0,190.0,377.0,122.0,0.9011,86100.0,NEAR BAY\n-122.3,37.81,52.0,1224.0,237.0,521.0,159.0,1.191,76100.0,NEAR BAY\n-122.3,37.81,48.0,828.0,182.0,392.0,133.0,2.5938,73500.0,NEAR BAY\n-122.3,37.81,52.0,1010.0,209.0,604.0,187.0,1.1667,78400.0,NEAR BAY\n-122.3,37.81,48.0,1455.0,354.0,788.0,332.0,0.8056,84400.0,NEAR BAY\n-122.29,37.8,52.0,1027.0,244.0,492.0,147.0,2.6094,81300.0,NEAR BAY\n-122.3,37.81,52.0,572.0,109.0,274.0,82.0,1.8516,85000.0,NEAR BAY\n-122.29,37.81,46.0,2801.0,644.0,1823.0,611.0,0.9802,129200.0,NEAR BAY\n-122.29,37.81,26.0,768.0,152.0,392.0,127.0,1.7719,82500.0,NEAR BAY\n-122.29,37.81,46.0,935.0,297.0,582.0,277.0,0.7286,95200.0,NEAR BAY\n-122.29,37.81,49.0,844.0,204.0,560.0,152.0,1.75,75000.0,NEAR BAY\n-122.29,37.81,46.0,12.0,4.0,18.0,7.0,0.4999,67500.0,NEAR BAY\n-122.29,37.81,20.0,835.0,161.0,290.0,133.0,2.483,137500.0,NEAR BAY\n-122.28,37.81,17.0,1237.0,462.0,762.0,439.0,0.9241,177500.0,NEAR BAY\n-122.28,37.81,36.0,2914.0,562.0,1236.0,509.0,2.4464,102100.0,NEAR BAY\n-122.28,37.81,19.0,1207.0,243.0,721.0,207.0,1.1111,108300.0,NEAR BAY\n-122.29,37.81,23.0,1745.0,374.0,1054.0,325.0,0.8026,112500.0,NEAR BAY\n-122.28,37.8,38.0,684.0,176.0,344.0,155.0,2.0114,131300.0,NEAR BAY\n-122.28,37.81,17.0,924.0,289.0,609.0,289.0,1.5,162500.0,NEAR BAY\n-122.27,37.81,52.0,210.0,56.0,183.0,56.0,1.1667,112500.0,NEAR BAY\n-122.28,37.81,52.0,340.0,97.0,200.0,87.0,1.5208,112500.0,NEAR BAY\n-122.28,37.81,52.0,386.0,164.0,346.0,155.0,0.8075,137500.0,NEAR BAY\n-122.28,37.81,35.0,948.0,184.0,467.0,169.0,1.8088,118800.0,NEAR BAY\n-122.28,37.81,52.0,773.0,143.0,377.0,115.0,2.4083,98200.0,NEAR BAY\n-122.27,37.81,40.0,880.0,451.0,582.0,380.0,0.977,118800.0,NEAR BAY\n-122.27,37.81,10.0,875.0,348.0,546.0,330.0,0.76,162500.0,NEAR BAY\n-122.27,37.8,10.0,105.0,42.0,125.0,39.0,0.9722,137500.0,NEAR BAY\n-122.27,37.8,52.0,249.0,78.0,396.0,85.0,1.2434,500001.0,NEAR BAY\n-122.27,37.8,16.0,994.0,392.0,800.0,362.0,2.0938,162500.0,NEAR BAY\n-122.28,37.8,52.0,215.0,87.0,904.0,88.0,0.8668,137500.0,NEAR BAY\n-122.28,37.8,52.0,96.0,31.0,191.0,34.0,0.75,162500.0,NEAR BAY\n-122.27,37.79,27.0,1055.0,347.0,718.0,302.0,2.6354,187500.0,NEAR BAY\n-122.27,37.8,39.0,1715.0,623.0,1327.0,467.0,1.8477,179200.0,NEAR BAY\n-122.26,37.8,36.0,5329.0,2477.0,3469.0,2323.0,2.0096,130000.0,NEAR BAY\n-122.26,37.82,31.0,4596.0,1331.0,2048.0,1180.0,2.8345,183800.0,NEAR BAY\n-122.26,37.81,29.0,335.0,107.0,202.0,91.0,2.0062,125000.0,NEAR BAY\n-122.26,37.82,22.0,3682.0,1270.0,2024.0,1250.0,1.2185,170000.0,NEAR BAY\n-122.26,37.82,37.0,3633.0,1085.0,1838.0,980.0,2.6104,193100.0,NEAR BAY\n-122.25,37.81,29.0,4656.0,1414.0,2304.0,1250.0,2.4912,257800.0,NEAR BAY\n-122.25,37.81,28.0,5806.0,1603.0,2563.0,1497.0,3.2177,273400.0,NEAR BAY\n-122.25,37.81,39.0,854.0,242.0,389.0,228.0,3.125,237500.0,NEAR BAY\n-122.25,37.81,52.0,2155.0,701.0,895.0,613.0,2.5795,350000.0,NEAR BAY\n-122.26,37.81,34.0,5871.0,1914.0,2689.0,1789.0,2.8406,335700.0,NEAR BAY\n-122.24,37.82,52.0,1509.0,225.0,674.0,244.0,4.9306,313400.0,NEAR BAY\n-122.24,37.81,52.0,2026.0,482.0,709.0,456.0,3.2727,268500.0,NEAR BAY\n-122.25,37.81,52.0,1758.0,460.0,686.0,422.0,3.1691,259400.0,NEAR BAY\n-122.24,37.82,52.0,3481.0,751.0,1444.0,718.0,3.9,275700.0,NEAR BAY\n-122.25,37.82,28.0,3337.0,855.0,1520.0,802.0,3.9063,225000.0,NEAR BAY\n-122.25,37.82,52.0,1424.0,289.0,550.0,253.0,5.0917,262500.0,NEAR BAY\n-122.25,37.82,32.0,3809.0,1098.0,1806.0,1022.0,2.6429,218500.0,NEAR BAY\n-122.25,37.82,26.0,3959.0,1196.0,1749.0,1217.0,3.0233,255000.0,NEAR BAY\n-122.25,37.83,52.0,2376.0,559.0,939.0,519.0,3.1484,224100.0,NEAR BAY\n-122.25,37.83,35.0,1613.0,428.0,675.0,422.0,3.4722,243100.0,NEAR BAY\n-122.25,37.83,52.0,1279.0,287.0,534.0,291.0,3.1429,231600.0,NEAR BAY\n-122.25,37.83,28.0,5022.0,1750.0,2558.0,1661.0,2.4234,218500.0,NEAR BAY\n-122.25,37.83,52.0,4190.0,1105.0,1786.0,1037.0,3.0897,234100.0,NEAR BAY\n-122.23,37.84,50.0,2515.0,399.0,970.0,373.0,5.8596,327600.0,NEAR BAY\n-122.23,37.84,47.0,3175.0,454.0,1098.0,485.0,5.2868,347600.0,NEAR BAY\n-122.24,37.83,41.0,2576.0,406.0,794.0,376.0,5.956,366100.0,NEAR BAY\n-122.24,37.85,37.0,334.0,54.0,98.0,47.0,4.9643,335000.0,NEAR BAY\n-122.23,37.85,52.0,2800.0,411.0,1061.0,403.0,6.3434,373600.0,NEAR BAY\n-122.24,37.84,52.0,3529.0,574.0,1177.0,555.0,5.1773,389500.0,NEAR BAY\n-122.24,37.85,52.0,2612.0,365.0,901.0,367.0,7.2354,391100.0,NEAR BAY\n-122.22,37.85,28.0,5287.0,1048.0,2031.0,956.0,5.457,337300.0,NEAR BAY\n-122.22,37.84,50.0,2935.0,473.0,1031.0,479.0,7.5,295200.0,NEAR BAY\n-122.21,37.84,44.0,3424.0,597.0,1358.0,597.0,6.0194,292300.0,NEAR BAY\n-122.21,37.83,40.0,4991.0,674.0,1616.0,654.0,7.5544,411500.0,NEAR BAY\n-122.2,37.84,30.0,2211.0,346.0,844.0,343.0,6.0666,311500.0,NEAR BAY\n-122.21,37.84,34.0,3038.0,490.0,1140.0,496.0,7.0548,325900.0,NEAR BAY\n-122.19,37.84,18.0,1617.0,210.0,533.0,194.0,11.6017,392600.0,NEAR BAY\n-122.2,37.84,35.0,2865.0,460.0,1072.0,443.0,7.4882,319300.0,NEAR BAY\n-122.21,37.83,34.0,5065.0,788.0,1627.0,766.0,6.8976,333300.0,NEAR BAY\n-122.19,37.83,28.0,1326.0,184.0,463.0,190.0,8.2049,335200.0,NEAR BAY\n-122.2,37.83,26.0,1589.0,223.0,542.0,211.0,8.401,351200.0,NEAR BAY\n-122.19,37.83,29.0,1791.0,271.0,661.0,269.0,6.8538,368900.0,NEAR BAY\n-122.19,37.82,32.0,1835.0,264.0,635.0,263.0,8.317,365900.0,NEAR BAY\n-122.2,37.82,37.0,1229.0,181.0,420.0,176.0,7.0175,366700.0,NEAR BAY\n-122.2,37.82,39.0,3770.0,534.0,1265.0,500.0,6.3302,362800.0,NEAR BAY\n-122.18,37.81,30.0,292.0,38.0,126.0,52.0,6.3624,483300.0,NEAR BAY\n-122.21,37.82,52.0,2375.0,333.0,813.0,350.0,7.0549,331400.0,NEAR BAY\n-122.2,37.81,45.0,2964.0,436.0,1067.0,426.0,6.7851,323500.0,NEAR BAY\n-122.21,37.8,50.0,2833.0,605.0,1260.0,552.0,2.8929,216700.0,NEAR BAY\n-122.21,37.8,38.0,2254.0,535.0,951.0,487.0,3.0812,233100.0,NEAR BAY\n-122.21,37.81,52.0,1389.0,212.0,510.0,224.0,5.2402,296400.0,NEAR BAY\n-122.22,37.81,52.0,1971.0,335.0,765.0,308.0,6.5217,273700.0,NEAR BAY\n-122.22,37.8,52.0,2183.0,465.0,1129.0,460.0,3.2632,227700.0,NEAR BAY\n-122.22,37.8,52.0,2286.0,464.0,1073.0,441.0,3.0298,199600.0,NEAR BAY\n-122.22,37.8,52.0,2721.0,541.0,1185.0,515.0,4.5428,239800.0,NEAR BAY\n-122.22,37.81,52.0,2024.0,339.0,756.0,340.0,4.072,270100.0,NEAR BAY\n-122.22,37.81,52.0,2944.0,536.0,1034.0,521.0,5.3509,302100.0,NEAR BAY\n-122.23,37.8,52.0,2033.0,486.0,787.0,459.0,3.1603,269500.0,NEAR BAY\n-122.23,37.81,52.0,1433.0,229.0,612.0,213.0,4.7708,314700.0,NEAR BAY\n-122.22,37.81,52.0,2927.0,402.0,1021.0,380.0,8.1564,390100.0,NEAR BAY\n-122.23,37.81,52.0,2315.0,292.0,861.0,258.0,8.8793,410300.0,NEAR BAY\n-122.24,37.81,52.0,2485.0,313.0,953.0,327.0,6.8591,352400.0,NEAR BAY\n-122.24,37.81,52.0,1490.0,238.0,634.0,256.0,6.0302,287300.0,NEAR BAY\n-122.23,37.81,52.0,2814.0,365.0,878.0,352.0,7.508,348700.0,NEAR BAY\n-122.24,37.81,52.0,2093.0,550.0,918.0,483.0,2.7477,243800.0,NEAR BAY\n-122.24,37.8,52.0,888.0,168.0,360.0,175.0,2.1944,211500.0,NEAR BAY\n-122.25,37.8,52.0,2087.0,510.0,1197.0,488.0,3.0149,218400.0,NEAR BAY\n-122.24,37.81,52.0,2513.0,502.0,1048.0,518.0,3.675,269900.0,NEAR BAY\n-122.25,37.81,46.0,3232.0,835.0,1373.0,747.0,3.225,218800.0,NEAR BAY\n-122.25,37.8,42.0,4120.0,1065.0,1715.0,1015.0,2.9345,225000.0,NEAR BAY\n-122.25,37.8,43.0,2364.0,792.0,1359.0,722.0,2.1429,250000.0,NEAR BAY\n-122.25,37.8,41.0,1471.0,469.0,1062.0,413.0,1.6121,171400.0,NEAR BAY\n-122.25,37.8,29.0,2468.0,864.0,1335.0,773.0,1.3929,193800.0,NEAR BAY\n-122.24,37.79,27.0,1632.0,492.0,1171.0,429.0,2.3173,125000.0,NEAR BAY\n-122.25,37.79,45.0,1786.0,526.0,1475.0,460.0,1.7772,97500.0,NEAR BAY\n-122.25,37.79,50.0,629.0,188.0,742.0,196.0,2.6458,125000.0,NEAR BAY\n-122.25,37.79,52.0,1339.0,391.0,1086.0,363.0,2.181,138800.0,NEAR BAY\n-122.25,37.8,36.0,1678.0,606.0,1645.0,543.0,2.2303,116700.0,NEAR BAY\n-122.25,37.8,43.0,2344.0,647.0,1710.0,644.0,1.6504,151800.0,NEAR BAY\n-122.24,37.8,52.0,996.0,228.0,731.0,228.0,2.2697,127000.0,NEAR BAY\n-122.24,37.8,52.0,1591.0,373.0,1118.0,347.0,2.1563,128600.0,NEAR BAY\n-122.24,37.8,52.0,1586.0,398.0,1006.0,335.0,2.1348,140600.0,NEAR BAY\n-122.24,37.8,47.0,2046.0,588.0,1213.0,554.0,2.6292,182700.0,NEAR BAY\n-122.23,37.8,52.0,1192.0,289.0,772.0,257.0,2.3833,146900.0,NEAR BAY\n-122.24,37.8,52.0,1803.0,420.0,1321.0,401.0,2.957,122800.0,NEAR BAY\n-122.24,37.8,49.0,2838.0,749.0,1487.0,677.0,2.5238,169300.0,NEAR BAY\n-122.23,37.8,52.0,783.0,184.0,488.0,186.0,1.9375,126600.0,NEAR BAY\n-122.23,37.8,51.0,1590.0,414.0,949.0,392.0,1.9028,127900.0,NEAR BAY\n-122.23,37.8,50.0,1746.0,480.0,1149.0,415.0,2.25,123500.0,NEAR BAY\n-122.23,37.8,52.0,1252.0,299.0,844.0,280.0,2.3929,111900.0,NEAR BAY\n-122.23,37.79,43.0,5963.0,1344.0,4367.0,1231.0,2.1917,112800.0,NEAR BAY\n-122.23,37.79,52.0,1783.0,395.0,1659.0,412.0,2.9357,107900.0,NEAR BAY\n-122.23,37.79,30.0,999.0,264.0,1011.0,263.0,1.8854,137500.0,NEAR BAY\n-122.24,37.79,39.0,1469.0,431.0,1464.0,389.0,2.1638,105500.0,NEAR BAY\n-122.24,37.79,47.0,1372.0,395.0,1237.0,303.0,2.125,95500.0,NEAR BAY\n-122.24,37.79,52.0,674.0,180.0,647.0,168.0,3.375,116100.0,NEAR BAY\n-122.24,37.79,43.0,1626.0,376.0,1284.0,357.0,2.2542,112200.0,NEAR BAY\n-122.25,37.79,51.0,175.0,43.0,228.0,55.0,2.1,75000.0,NEAR BAY\n-122.25,37.79,39.0,461.0,129.0,381.0,123.0,1.6,112500.0,NEAR BAY\n-122.25,37.79,52.0,902.0,237.0,846.0,227.0,3.625,125000.0,NEAR BAY\n-122.26,37.8,20.0,2373.0,779.0,1659.0,676.0,1.6929,115000.0,NEAR BAY\n-122.22,37.77,52.0,391.0,128.0,520.0,138.0,1.6471,95000.0,NEAR BAY\n-122.22,37.77,52.0,1137.0,301.0,866.0,259.0,2.59,96400.0,NEAR BAY\n-122.23,37.77,52.0,769.0,206.0,612.0,183.0,2.57,72000.0,NEAR BAY\n-122.23,37.78,52.0,472.0,146.0,415.0,126.0,2.6429,71300.0,NEAR BAY\n-122.23,37.78,52.0,862.0,215.0,994.0,213.0,3.0257,80800.0,NEAR BAY\n-122.22,37.78,50.0,1920.0,530.0,1525.0,477.0,1.4886,128800.0,NEAR BAY\n-122.23,37.78,43.0,1420.0,472.0,1506.0,438.0,1.9338,112500.0,NEAR BAY\n-122.23,37.78,52.0,986.0,258.0,1008.0,255.0,1.4844,119400.0,NEAR BAY\n-122.23,37.78,44.0,2340.0,825.0,2813.0,751.0,1.6009,118100.0,NEAR BAY\n-122.23,37.79,48.0,1696.0,396.0,1481.0,343.0,2.0375,122500.0,NEAR BAY\n-122.23,37.79,49.0,1175.0,217.0,859.0,219.0,2.293,106300.0,NEAR BAY\n-122.22,37.79,37.0,2343.0,574.0,1608.0,523.0,2.1494,132500.0,NEAR BAY\n-122.23,37.79,30.0,610.0,145.0,425.0,140.0,1.6198,122700.0,NEAR BAY\n-122.23,37.79,40.0,930.0,199.0,564.0,184.0,1.3281,113300.0,NEAR BAY\n-122.22,37.79,44.0,1487.0,314.0,961.0,272.0,3.5156,109500.0,NEAR BAY\n-122.22,37.79,52.0,3424.0,690.0,2273.0,685.0,3.9048,164700.0,NEAR BAY\n-122.21,37.79,52.0,762.0,190.0,600.0,195.0,3.0893,125000.0,NEAR BAY\n-122.22,37.79,46.0,2366.0,575.0,1647.0,527.0,2.6042,124700.0,NEAR BAY\n-122.22,37.79,49.0,1826.0,450.0,1201.0,424.0,2.5,136700.0,NEAR BAY\n-122.22,37.79,38.0,3049.0,711.0,2167.0,659.0,2.7969,141700.0,NEAR BAY\n-122.2,37.79,29.0,1640.0,376.0,939.0,340.0,2.8321,150000.0,NEAR BAY\n-122.21,37.79,47.0,1543.0,307.0,859.0,292.0,2.9583,138800.0,NEAR BAY\n-122.21,37.79,34.0,2364.0,557.0,1517.0,516.0,2.8365,139200.0,NEAR BAY\n-122.21,37.79,35.0,1745.0,409.0,1143.0,386.0,2.875,143800.0,NEAR BAY\n-122.21,37.8,39.0,2003.0,500.0,1109.0,464.0,3.0682,156500.0,NEAR BAY\n-122.21,37.8,39.0,2018.0,447.0,1221.0,446.0,3.0757,151000.0,NEAR BAY\n-122.2,37.8,43.0,3045.0,499.0,1115.0,455.0,4.9559,273000.0,NEAR BAY\n-122.2,37.8,52.0,1547.0,293.0,706.0,268.0,4.7721,217100.0,NEAR BAY\n-122.21,37.8,52.0,3519.0,711.0,1883.0,706.0,3.4861,187100.0,NEAR BAY\n-122.2,37.8,41.0,2070.0,354.0,804.0,340.0,5.1184,239600.0,NEAR BAY\n-122.21,37.8,48.0,1321.0,263.0,506.0,252.0,4.0977,229700.0,NEAR BAY\n-122.19,37.8,48.0,1694.0,259.0,610.0,238.0,4.744,257300.0,NEAR BAY\n-122.19,37.8,46.0,1938.0,341.0,768.0,332.0,4.2727,246900.0,NEAR BAY\n-122.19,37.79,50.0,968.0,195.0,462.0,184.0,2.9844,179900.0,NEAR BAY\n-122.2,37.79,40.0,1060.0,256.0,667.0,235.0,4.1739,169600.0,NEAR BAY\n-122.2,37.8,46.0,2041.0,405.0,1059.0,399.0,3.8487,203300.0,NEAR BAY\n-122.19,37.8,52.0,1813.0,271.0,637.0,277.0,4.0114,263400.0,NEAR BAY\n-122.19,37.79,45.0,2718.0,451.0,1106.0,454.0,4.6563,231800.0,NEAR BAY\n-122.19,37.79,28.0,3144.0,761.0,1737.0,669.0,2.9297,140500.0,NEAR BAY\n-122.2,37.79,35.0,1802.0,459.0,1009.0,390.0,2.3036,126000.0,NEAR BAY\n-122.2,37.79,49.0,882.0,195.0,737.0,210.0,2.6667,122000.0,NEAR BAY\n-122.2,37.79,44.0,1621.0,452.0,1354.0,491.0,2.619,134700.0,NEAR BAY\n-122.21,37.79,45.0,2115.0,533.0,1530.0,474.0,2.4167,139400.0,NEAR BAY\n-122.2,37.79,45.0,2021.0,528.0,1410.0,480.0,2.7788,115400.0,NEAR BAY\n-122.21,37.78,46.0,2239.0,508.0,1390.0,569.0,2.7352,137300.0,NEAR BAY\n-122.21,37.78,52.0,1477.0,300.0,1065.0,269.0,1.8472,137000.0,NEAR BAY\n-122.21,37.78,52.0,1056.0,224.0,792.0,245.0,2.6583,142600.0,NEAR BAY\n-122.21,37.78,49.0,898.0,244.0,779.0,245.0,3.0536,137500.0,NEAR BAY\n-122.22,37.78,44.0,2968.0,710.0,2269.0,610.0,2.3906,111700.0,NEAR BAY\n-122.21,37.78,43.0,1702.0,460.0,1227.0,407.0,1.7188,126800.0,NEAR BAY\n-122.21,37.78,47.0,881.0,248.0,753.0,241.0,2.625,111300.0,NEAR BAY\n-122.22,37.77,40.0,494.0,114.0,547.0,135.0,2.8015,114800.0,NEAR BAY\n-122.22,37.78,50.0,1776.0,473.0,1807.0,440.0,1.7276,102300.0,NEAR BAY\n-122.22,37.78,44.0,1678.0,514.0,1700.0,495.0,2.0801,131900.0,NEAR BAY\n-122.22,37.78,51.0,1637.0,463.0,1543.0,393.0,2.489,119100.0,NEAR BAY\n-122.21,37.76,52.0,1420.0,314.0,1085.0,300.0,1.7546,80600.0,NEAR BAY\n-122.21,37.77,52.0,591.0,173.0,353.0,137.0,4.0904,80600.0,NEAR BAY\n-122.21,37.77,52.0,745.0,153.0,473.0,149.0,2.6765,88800.0,NEAR BAY\n-122.2,37.77,49.0,2272.0,498.0,1621.0,483.0,2.4338,102400.0,NEAR BAY\n-122.21,37.77,46.0,1234.0,375.0,1183.0,354.0,2.3309,98700.0,NEAR BAY\n-122.21,37.77,43.0,1017.0,328.0,836.0,277.0,2.2604,100000.0,NEAR BAY\n-122.19,37.77,42.0,932.0,254.0,900.0,263.0,1.8039,92300.0,NEAR BAY\n-122.2,37.77,39.0,2689.0,597.0,1888.0,537.0,2.2562,94800.0,NEAR BAY\n-122.2,37.77,41.0,1547.0,415.0,1024.0,341.0,2.0562,102000.0,NEAR BAY\n-122.2,37.78,52.0,2300.0,443.0,1225.0,423.0,3.5398,158400.0,NEAR BAY\n-122.2,37.78,39.0,1752.0,399.0,1071.0,376.0,3.1167,121600.0,NEAR BAY\n-122.2,37.78,50.0,1867.0,403.0,1128.0,378.0,2.5401,129100.0,NEAR BAY\n-122.2,37.77,43.0,2430.0,502.0,1537.0,484.0,2.898,121400.0,NEAR BAY\n-122.21,37.78,44.0,1729.0,414.0,1240.0,393.0,2.3125,102800.0,NEAR BAY\n-122.19,37.78,52.0,1026.0,180.0,469.0,168.0,2.875,160000.0,NEAR BAY\n-122.19,37.77,52.0,2170.0,428.0,1086.0,425.0,3.3715,143900.0,NEAR BAY\n-122.19,37.77,52.0,2329.0,445.0,1144.0,417.0,3.5114,151200.0,NEAR BAY\n-122.19,37.78,52.0,2492.0,415.0,1109.0,375.0,4.3125,164400.0,NEAR BAY\n-122.19,37.78,52.0,2198.0,397.0,984.0,369.0,3.22,156500.0,NEAR BAY\n-122.18,37.78,33.0,142.0,31.0,575.0,47.0,3.875,225000.0,NEAR BAY\n-122.19,37.78,49.0,1183.0,205.0,496.0,209.0,5.2328,174200.0,NEAR BAY\n-122.19,37.78,52.0,1070.0,193.0,555.0,190.0,3.7262,166900.0,NEAR BAY\n-122.2,37.78,45.0,1766.0,332.0,869.0,327.0,4.5893,163500.0,NEAR BAY\n-122.18,37.79,39.0,617.0,95.0,236.0,106.0,5.2578,253000.0,NEAR BAY\n-122.18,37.79,41.0,1411.0,233.0,626.0,214.0,7.0875,240700.0,NEAR BAY\n-122.18,37.79,46.0,2109.0,387.0,922.0,329.0,3.9712,208100.0,NEAR BAY\n-122.19,37.79,47.0,1229.0,243.0,582.0,256.0,2.9514,198100.0,NEAR BAY\n-122.19,37.79,50.0,954.0,217.0,546.0,201.0,2.6667,172800.0,NEAR BAY\n-122.18,37.81,37.0,1643.0,262.0,620.0,266.0,5.4446,336700.0,NEAR BAY\n-122.18,37.8,34.0,1355.0,195.0,442.0,195.0,6.2838,318200.0,NEAR BAY\n-122.18,37.8,23.0,2317.0,336.0,955.0,328.0,6.7527,285800.0,NEAR BAY\n-122.13,37.77,24.0,2459.0,317.0,916.0,324.0,7.0712,293000.0,NEAR BAY\n-122.16,37.79,22.0,12842.0,2048.0,4985.0,1967.0,5.9849,371000.0,NEAR BAY\n-122.17,37.78,42.0,1524.0,260.0,651.0,267.0,3.6875,157300.0,NEAR BAY\n-122.17,37.77,30.0,3326.0,746.0,1704.0,703.0,2.875,135300.0,NEAR BAY\n-122.18,37.78,43.0,1985.0,440.0,1085.0,407.0,3.4205,136700.0,NEAR BAY\n-122.18,37.78,50.0,1642.0,322.0,713.0,284.0,3.2984,160700.0,NEAR BAY\n-122.17,37.78,49.0,893.0,177.0,468.0,181.0,3.875,140600.0,NEAR BAY\n-122.17,37.78,52.0,653.0,128.0,296.0,121.0,4.175,144000.0,NEAR BAY\n-122.16,37.77,47.0,1256.0,,570.0,218.0,4.375,161900.0,NEAR BAY\n-122.16,37.77,48.0,977.0,194.0,446.0,180.0,4.7708,156300.0,NEAR BAY\n-122.16,37.77,45.0,2324.0,397.0,968.0,384.0,3.5739,176000.0,NEAR BAY\n-122.16,37.77,39.0,1583.0,349.0,857.0,316.0,3.0958,145800.0,NEAR BAY\n-122.17,37.77,39.0,1612.0,342.0,912.0,322.0,3.3958,141900.0,NEAR BAY\n-122.17,37.77,31.0,2424.0,533.0,1360.0,452.0,1.871,90700.0,NEAR BAY\n-122.17,37.76,41.0,1594.0,367.0,1074.0,355.0,1.9356,90600.0,NEAR BAY\n-122.17,37.76,47.0,2118.0,413.0,965.0,382.0,2.1842,107900.0,NEAR BAY\n-122.18,37.76,37.0,1575.0,358.0,933.0,320.0,2.2917,107000.0,NEAR BAY\n-122.17,37.76,38.0,1764.0,397.0,987.0,354.0,2.4333,98200.0,NEAR BAY\n\n\n\nheader = []\ndataset = []\nwith open(\"data/housing_sample.csv\", \"r\") as f:\n  for i, line in enumerate(f):\n    if i == 0:\n      header = line.strip().split(\",\")\n      continue\n    dataset.append(line.strip().split(\",\"))\n    if i &gt; 5:\n      break\n\nprint(\"제목줄은, \", header)\nprint(\"내용은\")\nprint(dataset)\n\n제목줄은,  ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value', 'ocean_proximity']\n내용은\n[['-122.23', '37.88', '41.0', '880.0', '129.0', '322.0', '126.0', '8.3252', '452600.0', 'NEAR BAY'], ['-122.22', '37.86', '21.0', '7099.0', '1106.0', '2401.0', '1138.0', '8.3014', '358500.0', 'NEAR BAY'], ['-122.24', '37.85', '52.0', '1467.0', '190.0', '496.0', '177.0', '7.2574', '352100.0', 'NEAR BAY'], ['-122.25', '37.85', '52.0', '1274.0', '235.0', '558.0', '219.0', '5.6431', '341300.0', 'NEAR BAY'], ['-122.25', '37.85', '52.0', '1627.0', '280.0', '565.0', '259.0', '3.8462', '342200.0', 'NEAR BAY'], ['-122.25', '37.85', '52.0', '919.0', '213.0', '413.0', '193.0', '4.0368', '269700.0', 'NEAR BAY']]"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-10-12-DealingData.html#csv-파일-이용하기-csv",
    "href": "posts/7_study/7_ds2023/2023-10-12-DealingData.html#csv-파일-이용하기-csv",
    "title": "6wk-2 데이터 다루기",
    "section": "CSV 파일 이용하기 (csv)",
    "text": "CSV 파일 이용하기 (csv)\n\nimport csv\nheader = []\ndataset = []\nwith open(\"data/housing_sample.csv\", \"r\") as f:\n  csv_reader = csv.reader(f)\n  for i, line in enumerate(csv_reader):\n    if i == 0:\n      header = line\n      continue\n    dataset.append(line)\n    if i &gt; 5:\n      break\n\nprint(\"제목줄은, \", header)\nprint(\"내용은\")\nprint(dataset)\n\n제목줄은,  ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value', 'ocean_proximity']\n내용은\n[['-122.23', '37.88', '41.0', '880.0', '129.0', '322.0', '126.0', '8.3252', '452600.0', 'NEAR BAY'], ['-122.22', '37.86', '21.0', '7099.0', '1106.0', '2401.0', '1138.0', '8.3014', '358500.0', 'NEAR BAY'], ['-122.24', '37.85', '52.0', '1467.0', '190.0', '496.0', '177.0', '7.2574', '352100.0', 'NEAR BAY'], ['-122.25', '37.85', '52.0', '1274.0', '235.0', '558.0', '219.0', '5.6431', '341300.0', 'NEAR BAY'], ['-122.25', '37.85', '52.0', '1627.0', '280.0', '565.0', '259.0', '3.8462', '342200.0', 'NEAR BAY'], ['-122.25', '37.85', '52.0', '919.0', '213.0', '413.0', '193.0', '4.0368', '269700.0', 'NEAR BAY']]\n\n\n\n# csv 파일 읽어서 엑셀로 저장\nimport csv\nheader = []\ndataset = []\nwith open(\"data/housing_sample.csv\", \"r\") as f:\n  csv_reader = csv.reader(f)\n  for i, line in enumerate(csv_reader):\n    if i == 0:\n      header = line\n      continue\n    dataset.append(line)\ndf = pd.DataFrame(dataset, columns=header)\ndf.to_excel(\"./data/housing_test.xlsx\", index=False)\n\nModuleNotFoundError: No module named 'openpyxl'"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#이론-및-예시",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#이론-및-예시",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "이론 및 예시",
    "text": "이론 및 예시\n- 이론: real-valued signal은 무조건 \\(|X[k]|^2\\)의 그래프가 대칭으로 나옴 (단, \\(X[0]\\)은 제외)\n- 예시1:\n\nx = np.array([1,2,3,4,5])\nX = np.fft.fft(x) \n\n\n# plt.plot(abs(X)**2,'o')\nplt.plot(abs(X)**2,'o')\nplt.plot((abs(X)**2)[0],'x')\nplt.axvline(x=2.5, linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7f5fc5098040&gt;\n\n\n\n\n\n\n첫항을 제외하고 대칭임\n\n\n# 확인\nabs(X)**2\n\narray([225.        ,  18.09016994,   6.90983006,   6.90983006,\n        18.09016994])\n\n\n첫 항인 \\(X[0]=225.\\)을 제외하고 대칭, 그래프 역시 대칭인 그래프가 나옴.\n- 예시2:\n\nx = np.array([1,2,3,-3,-2,-1]) # 이산신호\nX = np.fft.fft(x) # 퓨리에 변환된 이산신호\n\n\nplt.plot(abs(X)**2,'o')\nplt.plot((abs(X)**2)[0], 'x')\nplt.axvline(x=3, linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7f5fc4861dc0&gt;\n\n\n\n\n\n\n첫항을 제외하고 대칭임\n\n\nabs(X)**2\n\narray([ 0., 64., 12., 16., 12., 64.])\n\n\n- 예시3: \\({\\bf x}\\)가 복소수일 경우는 첫항을 제외하고 대칭이 되지 않음\n\nx = np.array([1+1j,2+2j,3+3j,-3-3j,-2-2j,1-1j]) \nX = np.fft.fft(x) \n\n\nplt.plot(abs(X)**2,'o')\n\n\n\n\n직관적으로, 첫항을 제외하고 대칭이 아님을 알 수 있음."
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#왜-xn이-실수일-경우만-xk2이-대칭으로-나올까",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#왜-xn이-실수일-경우만-xk2이-대칭으로-나올까",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "왜 \\(x[n]\\)이 실수일 경우만 \\(|X[k]|^2\\)이 대칭으로 나올까?",
    "text": "왜 \\(x[n]\\)이 실수일 경우만 \\(|X[k]|^2\\)이 대칭으로 나올까?\n- 예비학습1\n임의의 \\(0 \\leq \\alpha \\leq 1\\)에 대하여 \\(\\cos(2\\pi \\alpha) =\\cos(2\\pi (1-\\alpha))\\) 가 성립함\n\nalpha = 0.2\nnp.cos(2*np.pi*alpha),np.cos(2*np.pi*(1-alpha))\n\n(0.30901699437494745, 0.30901699437494723)\n\n\n\n그래프를 잘 그려보세여\n\n\nN = 30\nalpha_ = np.linspace(0,1,N)\nplt.plot(alpha_, np.cos(2*np.pi*alpha_), 'o', alpha=0.6, label='cos(2*pi*alpha)')\nplt.plot(alpha_, np.cos(2*np.pi*(1-alpha_)), 'x', color='red', label='cos(2*pi*(1-alpha))')\nplt.legend(loc=1)\n\n&lt;matplotlib.legend.Legend at 0x7f5fc44edee0&gt;\n\n\n\n\n\n- 예비학습2\n임의의 \\(0 \\leq \\alpha \\leq 1\\)에 대하여 \\(\\sin(2\\pi \\alpha) = -\\sin(2\\pi (1-\\alpha))\\) 가 성립함\n\nalpha = 0.2\nnp.sin(2*np.pi*alpha),np.sin(2*np.pi*(1-alpha))\n\n(0.9510565162951535, -0.9510565162951536)\n\n\n\n그래프를 잘 그려보세여\n\n\nN = 30\nalpha_ = np.linspace(0,1,N)\nplt.plot(alpha_, np.sin(2*np.pi*alpha_), 'o', alpha=0.6, label='sin(2*pi*alpha)')\nplt.plot(alpha_, -np.sin(2*np.pi*(1-alpha_)), 'x', color='red', label='-sin(2*pi*(1-alpha))')\nplt.legend(loc=1)\n\n&lt;matplotlib.legend.Legend at 0x7f5fc6533df0&gt;\n\n\n\n\n\n\n# 잘못된 그림..\nN = 30\nalpha_ = np.linspace(0,1,N)\nplt.plot(alpha_, np.sin(2*np.pi*alpha_), 'o', alpha=0.6, label='sin(2*pi*alpha)')\nplt.plot(alpha_, np.sin(2*np.pi*(1-alpha_)), 'x', color='red', label='sin(2*pi*(1-alpha))')\nplt.legend(loc=1)\n\n&lt;matplotlib.legend.Legend at 0x7f5fc63676d0&gt;\n\n\n\n\n\n- 왜 실수일경우만 대칭인지? (어디 정리된걸 아무리 찾아도 못찾겠어서 그냥 직접 수식을 썼는데요, 이걸 기억할 필요는 없어요.. 아마 제가 쓴것보다 쉽게 설명하는 방법이 있을겁니다)\n(해설) \\(k=0,1,2,\\dots,N-1\\)에 대하여 \\(X[k]\\)는 아래와 같이 표현가능하다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]e^{-\\frac{j2\\pi kn}{N}}\\]\n오일러공식1을 사용하면 아래와 같이 정리할 수 있다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(-\\frac{2\\pi kn}{N}\\right) + i \\sum_{n=0}^{N-1}x[n]\\sin\\left(-\\frac{2\\pi kn}{N}\\right)\\]\n\\(\\cos\\)은 짝함수2, \\(\\sin\\)은 홀함수3임을 이용하여 다시정리하면\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\ncase1 \\(k=1\\) 인 경우와 \\(k=N-1\\)인 경우는 서로 \\(|X[k]|^2\\)이 같음을 보이자.\n\\[X[1] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi n}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi n}{N}\\right)\\]\n\\[X[N-1] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\]\n여기에서 예비학습1,2를 떠올리면 \\(\\cos\\left(\\frac{2\\pi n}{N}\\right)=\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right)\\) 이고 \\(\\sin\\left(\\frac{2\\pi n}{N}\\right)=-\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\) 임을 알 수 있다. 따라서 \\(X[1]\\)와 \\(X[N-1]\\)은 켤레복소수이다. 즉\n\\[X[1] = X[N-1]^\\ast, \\quad X[1]^\\ast = X[N-1]\\]\n이다. 그런데 임의의 복소수 \\(x=a+bi\\)에 대하여 \\(|x|^2 = a^2+b^2 = (a+bi)(a-bi)= x x^\\ast\\) 가 성립하므로\n\\[|X[1]|^2 = |X[N-1]|^2\\]\n이 성립한다.\n\n만약에 \\(x[n]\\)이 실수가 아닌경우는 \\(\\cos\\left(\\frac{2\\pi n}{N}\\right)=\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right)\\) 이고 \\(\\sin\\left(\\frac{2\\pi n}{N}\\right)=-\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\) 이라고 하여도 \\(X[1]\\)와 \\(X[N-1]\\)은 켤레복소수라고 주장할수 없다.\n\ncase2 \\(k=2\\) 인 경우와 \\(k=N-2\\)인 경우는 서로 \\(|X[k]|^2\\)이 같음도 비슷한 논리로 보일 수 있다."
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#허수파트-해석",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#허수파트-해석",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "허수파트 해석",
    "text": "허수파트 해석\n관찰1: 모든 \\(k\\)에 대하여 \\(X[k]\\)의 허수파트는 항상 \\(0\\)이다.\nk=0\n\nk=0\nsin_part_0 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nx*sin_part_0\n\narray([ 0.,  0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,\n        0.,  0.])\n\n\n\nsum(x*sin_part_0)\n\n0.0\n\n\n\n\\(X[k]\\)의 허수파트는 항상 \\(0\\)\n\nk=1\n\nk=1\nsin_part_1 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nx*sin_part_1\n\narray([ 0.        ,  0.37157241,  0.49726095,  0.29389263, -0.10395585,\n       -0.4330127 , -0.47552826, -0.20336832,  0.20336832,  0.47552826,\n        0.4330127 ,  0.10395585, -0.29389263, -0.49726095, -0.37157241])\n\n\n\nsum(x*sin_part_1)\n\n1.0547118733938987e-15\n\n\n\n거의 \\(0\\)\n\n약간을 직관을 위해서 그림을 그려보자.\n\nplt.plot(x,'--o')\nplt.plot(sin_part_1,'--o')\n\n\n\n\n\\(x\\)가 \\(\\mathbb{R}\\)에서 정의된 연속함수라고 상상하면 \\(\\sum_{n=0}^{N-1}\\cos\\left(\\frac{2\\pi n}{N} \\right)\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\)에 대응하는 식은 \\(\\int_0^{2\\pi}\\cos(t)\\sin(t)dt\\)라고 볼 수 있어서 sum(x*sin_part_1)=0임을 더 쉽게 이해할 수 있다.\nk=2\n\nk=2\nsin_part_2 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o', label='cos(t)')\nplt.plot(sin_part_2,'--o', label='sin(2t)')\nplt.legend(loc=3)\n\n&lt;matplotlib.legend.Legend at 0x7f5fc6129190&gt;\n\n\n\n\n\n\\(x\\)가 \\(\\mathbb{R}\\)에서 정의된 연속함수라고 상상하면 파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(\\sin(2t)\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\nsum(x*sin_part_2)\n\n-1.2212453270876722e-15\n\n\n\\(\\cos(t)\\)는 임의의 \\(\\sin(kt)\\)와 항상 직교하므로, 임의의 \\(k\\)에 대하여 허수파트는 항상 0이다.\n따라서 이 경우 \\(X[k]\\)는 아래와 같이 써도 무방하다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right)\\]"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#실수파트-해석",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#실수파트-해석",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "실수파트 해석",
    "text": "실수파트 해석\n관찰2: \\(X[k]\\)의 실수파트는 \\(k=1\\)혹은 \\(k=N-1\\)일때 아래와 같이 정리된다.\n\\[\\sum_{n=0}^{N-1}\\cos\\left(\\frac{2\\pi n}{N} \\right)^2\\]\n그외의 경우에는 아래와 같이 된다.\nk=0\n\nk=0\ncos_part_0 = np.array([np.cos(2*np.pi*k*n/N) for n in range(N)])\n\n\ncos_part_0\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nplt.plot(x,'--o')\nplt.plot(cos_part_0,'--o')\n\n\n\n\n\n파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(1\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\nk=2\n\nk=2\ncos_part_2 = np.array([np.cos(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o', label='cos(t)')\nplt.plot(cos_part_2,'--o', label='cos(2t)')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f5fc5fc2fd0&gt;\n\n\n\n\n\n\n파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(\\cos(2t)\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\n임의의 \\(k\\)에 대하여 \\(\\cos(t)\\)와 \\(\\cos(kt)\\)는 항상 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n- 요약: 만약에 \\(x[n]\\)이 아래와 같은 꼴이라고 하자.\n\\[x[n] = \\cos\\left(\\frac{2\\pi n}{N} \\right)\\]\n이때 퓨리에변환 \\(X[k]\\)는 아래와 같이 정리된다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\n\\(X[k]\\)의 허수파트는 항상 0이 되고, 실수 파트는 \\(k=1,N-1\\)일 경우에만 값을 가지고 나머지는 0의 값을 가진다.\n\nX = np.fft.fft(x) # 퓨리에 변환된 이산신호\nfig, ax = plt.subplots(1,3)\nax[0].plot(x,'o--'); ax[0].set_title('x[n]') \nax[1].plot(np.real(X),'x'); ax[1].set_title('real(X[k])') # 실수파트\nax[2].plot(np.imag(X),'x'); ax[2].set_title('imag(X[k])') # 허수파트\nfig.set_figwidth(15)\n\n\n\n\n- 응용: \\(x[n]\\)이 아래와 같은 꼴이라고 하자.\n\\[x[n] = \\cos\\left(\\frac{6\\pi n}{N} \\right)\\]\n퓨리에변환 \\(X[k]\\)는 아래와 같이 정리된다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\n\\(X[k]\\)의 허수파트는 항상 0이 되고, 실수 파트는 \\(k=3,N-3\\)일 경우에만 값을 가지고 나머지는 0의 값을 가진다.\n\nN = 15 \nx = np.array([np.cos(6*np.pi*n/N) for n in range(N)]) # 이산신호\nX = np.fft.fft(x) # 퓨리에 변환된 이산신호\nfig, ax = plt.subplots(1,3)\nax[0].plot(x,'o--'); ax[0].set_title('x[n]')\nax[1].plot(np.real(X),'x'); ax[1].set_title('real(X[k])') # 실수파트\nax[2].plot(np.imag(X),'x'); ax[2].set_title('imag(X[k])') # 허수파트\nfig.set_figwidth(15)\n\n\n\n\n\n\\(X[k]\\)의 허수파트는 항상 \\(0\\)\n\\(X[k]\\)의 실수파트는 \\(X[3], X[12]\\)일 경우에만 값을 가지고 나머지는 \\(0\\)"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#책갈피",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#책갈피",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "책갈피",
    "text": "책갈피\n\n\n\n\n\n\nImportant\n\n\n\n이것은 \\(x[n]\\)을 아래와 같이 나눈뒤\n\n\\(x_1[n] = \\cos\\left(\\frac{2\\pi n}{N} \\right)\\)\n\\(x_2[n] = 2\\times\\cos\\left(\\frac{6\\pi n}{N} \\right)\\)\n\n각각 퓨리에변환한 결과를 합친것과 같다.\n\n\n\nN = 15 \nx1 = np.array([np.cos(2*np.pi*n/N) for n in range(N)]) # signal1\nx2 = np.array([2*np.cos(6*np.pi*n/N) for n in range(N)]) # signal2\nX1 = np.fft.fft(x1) # fft(signal1)\nX2 = np.fft.fft(x2) # fft(signal2)\nfig, ax = plt.subplots(2,3)\nax[0][0].plot(x1,'o--'); ax[0][0].set_title('x1[n]')\nax[0][1].plot(np.real(X1),'x'); ax[0][1].set_title('real(X1[k])')\nax[0][2].plot(np.imag(X1),'x'); ax[0][2].set_title('imag(X1[k])')\nax[1][0].plot(x2,'o--'); ax[1][0].set_title('x2[n]')\nax[1][1].plot(np.real(X2),'x'); ax[1][1].set_title('real(X2[k])')\nax[1][2].plot(np.imag(X2),'x'); ax[1][2].set_title('imag(X2[k])')\nfig.set_figwidth(15)\nfig.set_figheight(8)"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-23-퓨리에변환4jy.html",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-23-퓨리에변환4jy.html",
    "title": "[Fourier] 퓨리에변환4jy",
    "section": "",
    "text": "회귀분석 느낌으로\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n회귀모형 (1)\n\nx = np.linspace(-10,10,1000)\nx0 = x*0+1\nx1 = x \nbeta0 = 3 \nbeta1 = 2\ny = x0*beta0+x1*beta1+np.random.randn(1000)\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n회귀모형 (2)\n\\[f_i = 2\\times \\sin(2\\pi t_i) + 1 \\times \\sin(4\\pi t_i) + 3\\times\\sin(6\\pi t_i)+\\epsilon_i, \\quad t_i = \\frac{i}{1000}\\]\n회귀분석 느낌의 표현은 아래와 같다.\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3} + \\epsilon_i\\]\n여기서 \\(x_{i1} = \\sin{2\\pi t_i}\\) 이고, \\(x_{i2} = \\sin(4\\pi t_i), \\quad x_{i3} = \\sin(6\\pi t_i)\\).\n- 관측한자료\n\nN=1000\nx=np.linspace(0,1,N)\neps = np.random.randn(N)\nX0 = np.sin(x*0*np.pi)\nX1 = np.sin(x*2*np.pi)\nX2 = np.sin(x*4*np.pi)\nX3 = np.sin(x*6*np.pi)\n\ny_true = 2*X1+1*X2+3*X3\ny= y_true + eps\n\n\nplt.plot(x,y,'o',alpha=0.1)\n# plt.plot(x,y_true)\n\n\n\n\n\nobserved signal\n\n- 위의 자료를 해석하는 방법\n\ndef spec(y):\n    N= len(y)\n    return abs(np.fft.fft(y)/N)*2 \n\n\nreal symmetric frequency\n\n\ny=2*X1+1*X2+3*X3+eps\nyfft =spec(y) \ny1=2*X1\ny2=1*X2\ny3=3*X3\nyfft1=spec(y1)\nyfft2=spec(y2)\nyfft3=spec(y3)\nepsfft=spec(eps)\n\n\nplt.plot(yfft[:20],'o',alpha=0.5)\nplt.plot(yfft1[:20],'x',alpha=1,)\nplt.plot(yfft2[:20],'x',alpha=1)\nplt.plot(yfft3[:20],'x',alpha=1)\nplt.plot(epsfft[:20],'x',alpha=1)\n\n\n\n\n- 퓨리에변환 -&gt; threshold -&gt; 역퓨리에변환을 이용한 스킬\n\nyfft=np.fft.fft(y)\n\n\nplt.plot(abs(yfft[1:50]),'o')\n\n\n\n\n\nyfft[abs(yfft)&lt;100] = 0\n\n\nplt.plot(y,'o',alpha=0.1)\nyhat=np.fft.ifft(yfft)\nplt.plot(yhat,'--')\nplt.plot(y-eps,'-')\n\n/home/jy/anaconda3/envs/torch/lib/python3.8/site-packages/matplotlib/cbook/__init__.py:1335: ComplexWarning: Casting complex values to real discards the imaginary part\n  return np.asarray(x, float)\n\n\n\n\n\n\nplt.plot(spec(y)[:50],'o')\nplt.plot(spec(yhat)[:50],'x')\n\n\n\n\n\n- Fast Fourier Transfors (FFT)\n\\[X(k) = \\frac{1}{N}\\sum_{n=0}^{N-1} x(n) \\cdot e^{-j \\frac{2\\pi}{N}kn}\\]\n- np.fft.fft() 함수 이용\n\nnp.fft.fft(y)[:10]\n\narray([ 28.92715005   +0.j        ,  34.05878195-1028.97157083j,\n        17.69086431 -530.11724055j,   7.37432739-1494.50531202j,\n        29.03159355  -33.71928164j,  20.2513632    -3.83449785j,\n         4.98960501   -9.5019045j , -18.28763105   +8.27271731j,\n       -43.73889873  +19.33055512j,  32.90966965  -34.52773359j])\n\n\n\nplt.plot(abs(np.fft.fft(y))[1:50], 'o')\n\n\n\n\n- 직접 구현\n\nN = len(y)\nfft_y_ = 0\nfor i in range(N):\n    fft_y_ += y[i]*np.exp(-2j*np.pi/N*np.arange(N))\n\n\nplt.plot(abs(np.fft.fft(y))[1:50],'o')\n\n\n\n\n- 비교\n\nfig, axes = plt.subplots(2,1)\naxes[0].plot(abs(np.fft.fft(y))[1:50], 'o')\naxes[0].set_title('Method1')\naxes[1].plot(abs(np.fft.fft(y))[1:50],'o')\naxes[1].set_title('Method2')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n삼성전자 주가자료를 스무딩해보기\n- 삼성전자 자료\n\nimport yfinance as yf\n\n\nstart_date = \"2023-01-01\"\nend_date = \"2023-05-02\"\ny = yf.download(\"005930.KS\", start=start_date, end=end_date)['Adj Close'].to_numpy()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\nplt.plot(y)\n\n\n\n\n- 스펙트럼\n\nyfft = np.fft.fft(y)\n\n\nplt.plot(abs(yfft))\n\n\n\n\n- 처음 50개정도만 관찰\n\nplt.plot(abs(yfft[:50]),'o')\n\n\n\n\n\n첫값이 너무커서 나머지는 잘안보임\n\n- 2번째부터 50번째까지만 관찰\n\nplt.plot(abs(yfft)[2:50],'o')\nplt.axhline(y=22500, color='r', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7f85162b57c0&gt;\n\n\n\n\n\n\n대충 이정도 짜르면 될것같음\n\n- thresholded value\n\ntresh_value = 22500\n\n\nyfft[abs(yfft)&lt;tresh_value] =0 \n\n- 퓨리에역변환\n\nyhat = np.fft.ifft(yfft)\nyhat[:5]\n\narray([59664.72193044+8.87311904e-14j, 58572.98839934+8.87311904e-14j,\n       58066.07369126+3.39894326e-14j, 58169.18671667-6.87747670e-14j,\n       58706.41986821-1.14383435e-13j])\n\n\n실수화\n\nyhat = np.real(yhat)\nyhat[:5]\n\narray([59664.72193044, 58572.98839934, 58066.07369126, 58169.18671667,\n       58706.41986821])\n\n\n- 적합결과 시각화\n\nplt.plot(y)\nplt.plot(yhat,'--')\n\n\n\n\n\n- 숙제: treshold value를 관찰하며 시각화해볼것\n\n# 스펙트럼\nyfft1 = yfft.copy()\nyfft2 = yfft.copy()\nyfft3 = yfft.copy()\n\n\nplt.plot(abs(yfft)[2:50],'o')\nplt.axhline(y=30000, color='r', linestyle='--')\nplt.axhline(y=50000, color='g', linestyle='--')\nplt.axhline(y=100000, color='y', linestyle='--')\nplt.show()\n\n\n\n\n- thresholded value\n\ntresh_value1 = 30000\ntresh_value2 = 50000\ntresh_value3 = 100000\n\n\nyfft1[abs(yfft)&lt;tresh_value1] =0 \nyfft2[abs(yfft)&lt;tresh_value2] =0 \nyfft3[abs(yfft)&lt;tresh_value3] =0 \n\n- 퓨리에역변환\n\nyhat1 = np.real(np.fft.ifft(yfft1))\nyhat2 = np.real(np.fft.ifft(yfft2))\nyhat3 = np.real(np.fft.ifft(yfft3))\nyhat1[:5], yhat2[:5], yhat3[:5] \n\n(array([60302.63175219, 59674.04944237, 59176.52517726, 58830.97854078,\n        58648.54453033]),\n array([60610.76706766, 60334.04540323, 60094.64069051, 59898.16437304,\n        59749.22168918]),\n array([61926.12309451, 61926.12309451, 61926.12309451, 61926.12309451,\n        61926.12309451]))\n\n\n- 적합결과 시각화\n\nplt.plot(y)\nplt.plot(yhat1, color='r', linestyle='--', label='thresh=30000')\nplt.plot(yhat2,color='g', linestyle='--', label='thresh=50000')\nplt.plot(yhat3, color='y', linestyle='--', label='thresh=100000')\nplt.legend()\nplt.show()\n\n\n\n\n\nthreshold 값이 커질수록 스무딩 되는 느낌\n\\(\\text{thresh}=100,000\\) \\(\\to\\) underfitting\n\\(\\text{thresh}=30,000\\) 으로 잡았을 때 원자료와 비슷\n\n\n\n\nminor topics\n- y의 FFT 결과는 항상 y와 같은길이임\n\nlen(y)\n\n82\n\n\n\nlen(np.fft.fft(y))\n\n82\n\n\n- 에일리어싱: number of observation은 얼마나 세밀한 주파수까지 측정가능하냐를 결정함\n예시1: 에일리어싱\n\nx = np.linspace(-3.14,3.14,10)\n\n\nx1 = np.sin(8*x)\nx2 = np.sin(10*x)\n\n\nnp.corrcoef([x1,x2])\n\narray([[ 1.        , -0.99975131],\n       [-0.99975131,  1.        ]])\n\n\n\nplt.plot(x1,label='x1')\nplt.plot(x2,label='x2')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f8514838190&gt;\n\n\n\n\n\n\n실제로는 x2가 더 고주파인데, 같은 주파수처럼 보임\n\n예시2: 에일리어싱이 없는 경우\n\nx = np.linspace(-3.14,3.14,100000)\n\n\nx1 = np.sin(8*x)\nx2 = np.sin(10*x)\n\n\nnp.corrcoef([x1,x2])\n\narray([[ 1.00000000e+00, -6.45767105e-08],\n       [-6.45767105e-08,  1.00000000e+00]])\n\n\n\nplt.plot(x1)\nplt.plot(x2)\n\n\n\n\n\n주파수 왜곡떄문에 실제로는 corr ceof = 0 일지라도 관측되는건 corr coef &gt;0 일 수 있음\n\n\n\n에일리어싱: https://moonnote.tistory.com/133\n나이퀴스트 이론: https://ralasun.github.io/signal%20analysis/2021/07/01/nyq/\nkernel : https://sonsnotation.blogspot.com/2020/11/11-1-kernel.html\nFFT: https://towardsdatascience.com/fourier-transform-the-practical-python-implementation-acdd32f1b96a"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class02.html",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class02.html",
    "title": "[IP2022] class 02단계",
    "section": "",
    "text": "__init__\nself의 의미\n파이썬의 비밀1\n파이썬의 비밀2\n\n\n\n\n\n\n# 이미지 출력을 위한 패키지 불러오기\nfrom PIL import Image\nimport requests\n\n\n\n- STOOOP을 다시 복습\n\nurl1 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\nurl2 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop2.png?raw=true' \n\n\nclass STOOOP:\n    title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~'\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\ns1 = STOOOP() # STOOOP 이라는 클래스에서 s1이라는 인스턴스를 만드는 과정\n\n\ns1.title, s1.url, s1.end\n\n('학교폭력!',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰~~~')\n\n\n\ns1.stop()\n\n학교폭력!\n멈춰~~~\n\n\n\n\n\n- 왜 s1의 default title이 항상 ‘학교폭력’ 이어야 하는가? \\(\\to\\) __init__ 의 개발\n- 성능4: __init__() 함수를 이용하여 ‘클래스 \\(\\to\\) 인스턴스’ 의 시점에서 수행하는 일련의 동작들을 묶어서 수행할 수 있음.\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n- 잘못된 사용\n\ns1 = STOOOP() # 이 시점에서 __init__ 이 수행된다.\n\nTypeError: __init__() missing 1 required positional argument: 'title'\n\n\n- 올바른 사용\n\ns1 = STOOOP('수강신청매크로') # 이 시점에서 __init__ 이 수행된다!\n\n\ns1.title, s1.url, s1.end\n\n('수강신청매크로',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰~~~~')\n\n\n\ns1.stop()\n\n수강신청매크로\n멈춰~~~~\n\n\n\n\n\n- 잘못된 사용에서 에러가 발생한 이유는?\nTypeError: __init__() missing 1 required positional argument: 'title'\n\ns1 = STOOOP() 이 실행되는 순간 __init__() 이 내부적으로 실행된다.\n그런데 __init__() 의 첫번째 입력인 self는 입력안해도 무방했음. 그런데 두번째 입력은 title은 입력을 해야했음.\n그런데 title을 입력하지 않아서 발생하는 에러.\n\n- __init__(self, arg1, arg2,...) 함수에 대하여\n\n엄청나게 특별해 보이지만 사실 몇가지 특별한 점을 제외하고는 어떠한 마법도 없는 함수이다.\n특별한 점1: 첫번째 입력으로 반드시 self를 넣어야함. (이건 사실 클래스 내의 메소드 거의 다 그러함)\n특별한 점2: 클래스에서 인스턴스를 만드는 시점에 자동으로 실행된다.\n특별한 점3: __init(self, arg1, arg2,...)의 입력중 self 이외의 입력들은 ‘클래스 \\(\\to\\) 인스턴스’ 시점에서 ’인스턴스이름 = 클래스이름(arg1, arg2,…)’와 같이 사용한다. (이 예제의 경우 STOOOP(title) 와 같이 사용해야함)\n\n- title이 디폴트로 들어가는 상황도 불편했지만, title을 명시적으로 넣지 않으면 에러가 발생하는 것도 불편하다?\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title=None):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\ns2 = STOOOP()\ns3 = STOOOP('KOSPI 하락')\n\n\ns2.stop() # title 없는 경우\n\nNone\n멈춰~~~~\n\n\n\n\n\n\n제목이 없으면 없는대로 잘 출력이 된다.\n\n\ns3.stop() # title = 'KOSPI 하락'\n\nKOSPI 하락\n멈춰~~~~\n\n\n\n\n\n\n\n\n\n- 이전 예제를 복습\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(\"당신은 이 이미지를 {}번 보았습니다\".format(self.n))\n        self.n = self.n+1 \n\n\nk4 = Klass4()\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 2번 보았습니다\n\n\n- 위의 예제는 아래와 같이 구현할 수도 있다.\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 이미지를 {}번 보았습니다.'.format(self.n))\n        # slef.n = self.n + 1\n\n\nk4 = Klass4()\n\n\nk4.n\n\n1\n\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 1번 보았습니다.\n\n\n\nk4.n = k4.n + 1\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 2번 보았습니다.\n\n\n\nk4.n = k4.n + 1\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 3번 보았습니다.\n\n\n\n결국에는 k4.n = k4.n + 1의 기능을 구현하여 넣은 것이 self.n = self.n + 1 이다.\n따라서 self는 k4에 대응한다. 즉, self는 인스턴스 이름에 대응한다.\n\n우리가 하고 싶은 것은 클래스를 선언하는 시점에서 인스턴스가 생성된 이후 시점에 대한 어떠한 동작들을 정의하고 싶다.\n그런데 클래스가 설계하는 시점에서 인스턴스의 이름이 정해지지 않았으므로 이러한 동작들을 정의하기에 불편하다.\n그래서 클래스를 설계하는 시점에 그 클래스로부터 만들어지는 인스턴스는 그냥 self라는 가칭으로 부른다.\n\n굳이 비유를 하자면 self는 인스턴스의 태명 같은 것이다.\n\n\n요약: self의 의미는 (후에 만들어질 ) 인스턴스의 이름이다. (즉, self는 인스턴스의 태명같은 것!)\n\n\n\n탐구: 인스턴스의 자료형이 무엇인지 탐구해보자.\n- 아래의 두 클래스를 선언해보자.\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title=None):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 이미지를 {}번 보았습니다.'.format(self.n))\n        # self.n = self.n + 1\n\n- 인스턴스를 생성해보자.\n\nk4 = Klass4()\ns1 = STOOOP()\n\n\n\n\nk4?\n\n\nType:        Klass4\nString form: &lt;__main__.Klass4 object at 0x7fb4956082b0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\ns1?\n\n\nType:        STOOOP\nString form: &lt;__main__.STOOOP object at 0x7fb495608310&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n- ??? 타입은 자료형 즉, int, float, list 이런 것 아니었나?\n\na = [1,2,3]\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n- 그런데 지금 k4, s1의 타입은 Klass4, STOOOP이다.\n\n가설1 : 사실 파이썬 내부에 Klass4, STOOOP이라는 자료형이 있었다. 그런데 내가 만든 k4, s1이 우연히 그 자료형을 따르는 것! (이건 너무 억지스럽다.)\n가설2: type이 list인 것은 사실 list라는 클래스에서 생긴 인스턴스이다. \\(\\to\\) 리스트 자료형을 찍어낼 수 있는 어떤 클래스가 파이썬에 내부적으로 존재할 것이다. (이게 맞는 것 같다.)\n\n꺠달음1\n- 가설2가 맞다? 그렇다면 아래는 모두 어딘가에서 찍혀진 인스턴스이다.\n\na = [1,2,3]\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\na = 1,2,3\na\n\n(1, 2, 3)\n\n\n\na = 1\na?\n\n\nType:        int\nString form: 1\nDocstring:  \nint([x]) -&gt; integer\nint(x, base=10) -&gt; integer\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n&gt;&gt;&gt; int('0b100', base=0)\n4\n\n\n\n\na = '1'\na?\n\n\nType:        str\nString form: 1\nLength:      1\nDocstring:  \nstr(object='') -&gt; str\nstr(bytes_or_buffer[, encoding[, errors]]) -&gt; str\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\n\n- 그리고 위의 a=[1,2,3] 과 같은 것들은 모두 ‘클래스\\(\\to\\) 인스턴스’ 에 해당하는 과정이었다.\n깨달음2\n- 생각해보니까 아래와 같이 list를 선언하는 방식도 있었음\n\na = list()\na\n\n[]\n\n\n\n이거 지금 생각해보니까 list라는 이름의 클래스에서 a라는 인스턴스를 찍어내는 문법이다?!\n\n- 아래도 가능함\n\na = list((1,2,3))\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\n이것도 지금 보니까 list라는 이름의 클래스에서 a라는 인스턴스를 찍어내는 문법이다. 여기에서 (1,2,3)은 __init__() 의 입력이다.\n\n깨달음3\n- 그러고보니까 각 자료형마다 특수한 기능들이 있었음.\n- a. + tab을 하면 append, clear 등등이 나온다.\n- 이러한 기능은 지금까지 우리가 ‘list자료형 특수기능들’ 이라고 부르면서 사용했었다. 그런데 a가 list 클래스에서 생성된 인스턴스라는 관점에서 보면 이러한 기능들은 list 클래스에서 정의된 메소드라고 볼 수 있다.\n깨달음4 - a.f() 는 f(a) 로 해석 가능하다고 하였다. 이 해석에 따르면 메소드의 첫번째 입력은 메소드가 소속된 인스턴스라고 해석할 수 있다.\n- 동일한 논리로 아래의 코드는 stop() 의 입력에서 s1을 넣는다는 의미이다.\n\ns1.stop()\n\nNone\n멈춰~~~~\n\n\n\n\n\n\n\n\n\n\n아래의 조건에 맞는 클래스를 생성하라.\n\n['가위', '바위'] 와 같은 리스트를 입력으로 받아 인스턴스를 생성한다.\n위의 리스트에서 하나의 값을 뽑는 메소드 f를 가지고 있다.\n\n# 사용예시\na = Klass(['가위', '바위'])\na.f() # 가위가 1/2 바위가 1/2의 확률로 출력\nb = Klass(['가위', '바위', '보'])\nb.f() # 가위, 바위, 보가 1/3의 확률로 출력"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class02.html#contents",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class02.html#contents",
    "title": "[IP2022] class 02단계",
    "section": "",
    "text": "__init__\nself의 의미\n파이썬의 비밀1\n파이썬의 비밀2"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class02.html#imports",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class02.html#imports",
    "title": "[IP2022] class 02단계",
    "section": "",
    "text": "# 이미지 출력을 위한 패키지 불러오기\nfrom PIL import Image\nimport requests\n\n\n\n- STOOOP을 다시 복습\n\nurl1 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\nurl2 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop2.png?raw=true' \n\n\nclass STOOOP:\n    title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~'\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\ns1 = STOOOP() # STOOOP 이라는 클래스에서 s1이라는 인스턴스를 만드는 과정\n\n\ns1.title, s1.url, s1.end\n\n('학교폭력!',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰~~~')\n\n\n\ns1.stop()\n\n학교폭력!\n멈춰~~~\n\n\n\n\n\n- 왜 s1의 default title이 항상 ‘학교폭력’ 이어야 하는가? \\(\\to\\) __init__ 의 개발\n- 성능4: __init__() 함수를 이용하여 ‘클래스 \\(\\to\\) 인스턴스’ 의 시점에서 수행하는 일련의 동작들을 묶어서 수행할 수 있음.\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n- 잘못된 사용\n\ns1 = STOOOP() # 이 시점에서 __init__ 이 수행된다.\n\nTypeError: __init__() missing 1 required positional argument: 'title'\n\n\n- 올바른 사용\n\ns1 = STOOOP('수강신청매크로') # 이 시점에서 __init__ 이 수행된다!\n\n\ns1.title, s1.url, s1.end\n\n('수강신청매크로',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰~~~~')\n\n\n\ns1.stop()\n\n수강신청매크로\n멈춰~~~~\n\n\n\n\n\n- 잘못된 사용에서 에러가 발생한 이유는?\nTypeError: __init__() missing 1 required positional argument: 'title'\n\ns1 = STOOOP() 이 실행되는 순간 __init__() 이 내부적으로 실행된다.\n그런데 __init__() 의 첫번째 입력인 self는 입력안해도 무방했음. 그런데 두번째 입력은 title은 입력을 해야했음.\n그런데 title을 입력하지 않아서 발생하는 에러.\n\n- __init__(self, arg1, arg2,...) 함수에 대하여\n\n엄청나게 특별해 보이지만 사실 몇가지 특별한 점을 제외하고는 어떠한 마법도 없는 함수이다.\n특별한 점1: 첫번째 입력으로 반드시 self를 넣어야함. (이건 사실 클래스 내의 메소드 거의 다 그러함)\n특별한 점2: 클래스에서 인스턴스를 만드는 시점에 자동으로 실행된다.\n특별한 점3: __init(self, arg1, arg2,...)의 입력중 self 이외의 입력들은 ‘클래스 \\(\\to\\) 인스턴스’ 시점에서 ’인스턴스이름 = 클래스이름(arg1, arg2,…)’와 같이 사용한다. (이 예제의 경우 STOOOP(title) 와 같이 사용해야함)\n\n- title이 디폴트로 들어가는 상황도 불편했지만, title을 명시적으로 넣지 않으면 에러가 발생하는 것도 불편하다?\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title=None):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\ns2 = STOOOP()\ns3 = STOOOP('KOSPI 하락')\n\n\ns2.stop() # title 없는 경우\n\nNone\n멈춰~~~~\n\n\n\n\n\n\n제목이 없으면 없는대로 잘 출력이 된다.\n\n\ns3.stop() # title = 'KOSPI 하락'\n\nKOSPI 하락\n멈춰~~~~"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class02.html#self의-의미",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class02.html#self의-의미",
    "title": "[IP2022] class 02단계",
    "section": "",
    "text": "- 이전 예제를 복습\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(\"당신은 이 이미지를 {}번 보았습니다\".format(self.n))\n        self.n = self.n+1 \n\n\nk4 = Klass4()\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 2번 보았습니다\n\n\n- 위의 예제는 아래와 같이 구현할 수도 있다.\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 이미지를 {}번 보았습니다.'.format(self.n))\n        # slef.n = self.n + 1\n\n\nk4 = Klass4()\n\n\nk4.n\n\n1\n\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 1번 보았습니다.\n\n\n\nk4.n = k4.n + 1\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 2번 보았습니다.\n\n\n\nk4.n = k4.n + 1\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 3번 보았습니다.\n\n\n\n결국에는 k4.n = k4.n + 1의 기능을 구현하여 넣은 것이 self.n = self.n + 1 이다.\n따라서 self는 k4에 대응한다. 즉, self는 인스턴스 이름에 대응한다.\n\n우리가 하고 싶은 것은 클래스를 선언하는 시점에서 인스턴스가 생성된 이후 시점에 대한 어떠한 동작들을 정의하고 싶다.\n그런데 클래스가 설계하는 시점에서 인스턴스의 이름이 정해지지 않았으므로 이러한 동작들을 정의하기에 불편하다.\n그래서 클래스를 설계하는 시점에 그 클래스로부터 만들어지는 인스턴스는 그냥 self라는 가칭으로 부른다.\n\n굳이 비유를 하자면 self는 인스턴스의 태명 같은 것이다.\n\n\n요약: self의 의미는 (후에 만들어질 ) 인스턴스의 이름이다. (즉, self는 인스턴스의 태명같은 것!)\n\n\n\n탐구: 인스턴스의 자료형이 무엇인지 탐구해보자.\n- 아래의 두 클래스를 선언해보자.\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title=None):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 이미지를 {}번 보았습니다.'.format(self.n))\n        # self.n = self.n + 1\n\n- 인스턴스를 생성해보자.\n\nk4 = Klass4()\ns1 = STOOOP()\n\n\n\n\nk4?\n\n\nType:        Klass4\nString form: &lt;__main__.Klass4 object at 0x7fb4956082b0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\ns1?\n\n\nType:        STOOOP\nString form: &lt;__main__.STOOOP object at 0x7fb495608310&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n- ??? 타입은 자료형 즉, int, float, list 이런 것 아니었나?\n\na = [1,2,3]\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n- 그런데 지금 k4, s1의 타입은 Klass4, STOOOP이다.\n\n가설1 : 사실 파이썬 내부에 Klass4, STOOOP이라는 자료형이 있었다. 그런데 내가 만든 k4, s1이 우연히 그 자료형을 따르는 것! (이건 너무 억지스럽다.)\n가설2: type이 list인 것은 사실 list라는 클래스에서 생긴 인스턴스이다. \\(\\to\\) 리스트 자료형을 찍어낼 수 있는 어떤 클래스가 파이썬에 내부적으로 존재할 것이다. (이게 맞는 것 같다.)\n\n꺠달음1\n- 가설2가 맞다? 그렇다면 아래는 모두 어딘가에서 찍혀진 인스턴스이다.\n\na = [1,2,3]\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\na = 1,2,3\na\n\n(1, 2, 3)\n\n\n\na = 1\na?\n\n\nType:        int\nString form: 1\nDocstring:  \nint([x]) -&gt; integer\nint(x, base=10) -&gt; integer\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n&gt;&gt;&gt; int('0b100', base=0)\n4\n\n\n\n\na = '1'\na?\n\n\nType:        str\nString form: 1\nLength:      1\nDocstring:  \nstr(object='') -&gt; str\nstr(bytes_or_buffer[, encoding[, errors]]) -&gt; str\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\n\n- 그리고 위의 a=[1,2,3] 과 같은 것들은 모두 ‘클래스\\(\\to\\) 인스턴스’ 에 해당하는 과정이었다.\n깨달음2\n- 생각해보니까 아래와 같이 list를 선언하는 방식도 있었음\n\na = list()\na\n\n[]\n\n\n\n이거 지금 생각해보니까 list라는 이름의 클래스에서 a라는 인스턴스를 찍어내는 문법이다?!\n\n- 아래도 가능함\n\na = list((1,2,3))\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\n이것도 지금 보니까 list라는 이름의 클래스에서 a라는 인스턴스를 찍어내는 문법이다. 여기에서 (1,2,3)은 __init__() 의 입력이다.\n\n깨달음3\n- 그러고보니까 각 자료형마다 특수한 기능들이 있었음.\n- a. + tab을 하면 append, clear 등등이 나온다.\n- 이러한 기능은 지금까지 우리가 ‘list자료형 특수기능들’ 이라고 부르면서 사용했었다. 그런데 a가 list 클래스에서 생성된 인스턴스라는 관점에서 보면 이러한 기능들은 list 클래스에서 정의된 메소드라고 볼 수 있다.\n깨달음4 - a.f() 는 f(a) 로 해석 가능하다고 하였다. 이 해석에 따르면 메소드의 첫번째 입력은 메소드가 소속된 인스턴스라고 해석할 수 있다.\n- 동일한 논리로 아래의 코드는 stop() 의 입력에서 s1을 넣는다는 의미이다.\n\ns1.stop()\n\nNone\n멈춰~~~~"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class02.html#homework",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class02.html#homework",
    "title": "[IP2022] class 02단계",
    "section": "",
    "text": "아래의 조건에 맞는 클래스를 생성하라.\n\n['가위', '바위'] 와 같은 리스트를 입력으로 받아 인스턴스를 생성한다.\n위의 리스트에서 하나의 값을 뽑는 메소드 f를 가지고 있다.\n\n# 사용예시\na = Klass(['가위', '바위'])\na.f() # 가위가 1/2 바위가 1/2의 확률로 출력\nb = Klass(['가위', '바위', '보'])\nb.f() # 가위, 바위, 보가 1/3의 확률로 출력"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class07.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class07.html",
    "title": "[IP2022] class 07단계",
    "section": "",
    "text": "함수형 프로그래밍, callable object, 파이썬의 비밀"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제1-숫자입력-함수출력",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제1-숫자입력-함수출력",
    "title": "[IP2022] class 07단계",
    "section": "(예제1) 숫자입력, 함수출력",
    "text": "(예제1) 숫자입력, 함수출력\n\ndef f(a):\n    def _f(x):\n        return (x-a)**2\n    return _f\n\n\ng = f(10) # g(x) = (x-10)**2\n\n\ng(2) # (2-10)**2 = 64\n\n64\n\n\n\n해석: \\(f(a)\\)는 \\(a\\)를 입력으로 받고 \\(g(x)=(x-a)^2\\)를 함수를 리턴해주는 함수"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제1의-다른-표현-익명함수-lambda",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제1의-다른-표현-익명함수-lambda",
    "title": "[IP2022] class 07단계",
    "section": "(예제1)의 다른 표현: 익명함수 lambda",
    "text": "(예제1)의 다른 표현: 익명함수 lambda\n\n표현1\n\ndef f(a):\n    _f = lambda x: (x-a)**2 ### lambda x: (x-a)**2 가 실행되는 순간 함수오브젝트가 만들어지고 그것이 _f 로 저장됨 \n    return _f\n\n\ng = f(10) # g(x) = (x-10)**2\n\n\ng(3)\n\n49\n\n\n\n\n표현2\n\ndef f(a):\n    return lambda x: (x-a)**2\n\n\ng = f(10)\n\n\ng(3)\n\n49\n\n\n\nlambda x: (x-a)**2는 \\(\\text{lambda}(x) = (x-a)^2\\)의 느낌으로 기억하면 외우기 쉽다.\nlambda x: (x-a)**2는 “아직 이름이 없는 함수 오브젝트를 (가칭 lambda라고 하자) 만들고 기능은 x를 입력으로 하고 (x-2)**2를 출력하도록 하자” 라는 뜻으로 해석하면 된다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제2-함수입력-숫자출력",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제2-함수입력-숫자출력",
    "title": "[IP2022] class 07단계",
    "section": "(예제2) 함수입력, 숫자출력",
    "text": "(예제2) 함수입력, 숫자출력\n\ndef f(x):\n    return x**2\n\n\ndef d(f,x): # 함수를 입력을 받는 함수를 정의\n    h=0.000000000001\n    return (f(x+h)-f(x))/h \n\n\\[f'(x)\\approx \\frac{f(x+h)-f(x)}{h}\\]\n\n\\(h\\)의 값이 점점 0에 가까울수록 등호에 가까워짐.\n\n\nd(f,4) # f'(4) = 2*4 = 8\n\n8.000711204658728"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제3-함수입력-함수출력",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제3-함수입력-함수출력",
    "title": "[IP2022] class 07단계",
    "section": "(예제3) 함수입력, 함수출력",
    "text": "(예제3) 함수입력, 함수출력\n\ndef f(x): \n    return x**2 \n\n\ndef derivate(f): \n    def df(x): \n        h=0.000000000001\n        return (f(x+h)-f(x))/h \n    return df\n\n\nff = derivate(f)\n\n\nff(7) # f의 도함수\n\n14.004797321831575\n\n\n원래함수 시각화\n\nx = np.linspace(-1,1,100)\nplt.plot(x,f(x))\n\n\n\n\n도함수 시각화\n\nx = np.linspace(-1,1,100)\nplt.plot(x, ff(x))"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제3의-다른-표현",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제3의-다른-표현",
    "title": "[IP2022] class 07단계",
    "section": "(예제3)의 다른 표현",
    "text": "(예제3)의 다른 표현\n\ndef f(x): \n    return x**2\n\n\ndef derivate(f): \n    h=0.000000000001\n    return lambda x: (f(x+h)-f(x))/h \n\n\nff = derivate(f)\n\n\nff(10)\n\n20.00888343900442"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제4-함수들의-리스트",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class07.html#예제4-함수들의-리스트",
    "title": "[IP2022] class 07단계",
    "section": "(예제4) 함수들의 리스트",
    "text": "(예제4) 함수들의 리스트\n[오브젝트, 오브젝트, 오브젝트]\n\nflst = [lambda x: x, lambda x: x**2, lambda x: x**3]  # [함수오브젝트,함수오브젝트,함수오브젝트]\nflst # 이것의 타입은 function\n\n[&lt;function __main__.&lt;lambda&gt;(x)&gt;,\n &lt;function __main__.&lt;lambda&gt;(x)&gt;,\n &lt;function __main__.&lt;lambda&gt;(x)&gt;]\n\n\n\nfor f in flst:\n    print(f(2))\n\n2\n4\n8\n\n\n\n첫번째 함수에 적용될 때는 2출력, 2번째 함수에 적용될 때는 4출력, 3번째 함수에 적용될 때는 8출력\n\n\nfor f in flst:\n    plt.plot(x,f(x),'--')\n\n\n\n\n위의 코드는 아래와 같음.\n\nplt.plot(x, (lambda x: x)(x),'--')\nplt.plot(x, (lambda x: x**2)(x),'--')\nplt.plot(x, (lambda x: x**3)(x),'--')"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class09.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class09.html",
    "title": "[IP2022] class 09단계",
    "section": "",
    "text": "global/local 변수, 인스턴스/클래스 변수, 인스턴스/클래스 메서드\n\n\n\n커널을 재시작하고 아래를 관찰하자.\n\n\n- 관찰1: 함수내의 변수 출력\n\ndef f():\n    x = 10\n    print(x)\n\n\nf()\n\n10\n\n\n- 관찰2: 함수내의 변수가 없을 경우 출력이 되지 않음\n\ndef g():\n    print(x)\n\n\ng()\n\nNameError: name 'x' is not defined\n\n\n- 관찰3: 동일한 이름의 변수가 global에 있다면 함수내에 (local) 그 이름의 변수가 선언되지 않아도 global 변수를 빌려서 사용함.\n\nx = 20\ndef g():\n    print(x)\n\n\ng()\n\n20\n\n\n- 관찰4: f()가 실행되면서 x=10이 함수내에(=local에) 실행되지만 이 결과가 외부의 x=20에(=global에) 영향을 미치지는 못함.\n\nf()\n\n10\n\n\n\nx\n\n20\n\n\n\n\n\n(코드1)\n\nx = 38\ndef nextyear():\n    y = x+1\n    print(x,y)\nnextyear()\n\n38 39\n\n\n(코드2)\n\nx = 38\ndef nextyear():\n    y = x+1\n    print(x,y)\n    x = 0\nnextyear()\n\nUnboundLocalError: local variable 'x' referenced before assignment\n\n\n- 해석: - 잘못된해석: 코드1은 실행되었고, 코드2에서 에러가 났다. 코드1과 2의 차이점은 x=0 이라는 코드가 코드2에 추가로 포함되어있다는 것이다. 따라서 x=0이 잘못된 코드이고 이걸 실행하는 과정에서 에러가 발생했다.\n\n올바른해석: 코드1에서는 x가 global variable이고 코드2에서는 x가 local variable이어서 생기는 문제\n\n- 코드2의 올바른 수정\n\nx = 38\ndef nextyear():\n    x = 0\n    y = x+1\n    print(x,y)\nnextyear()\n\n0 1\n\n\n\n\n\n\n- 예비학습이 주는 교훈\n(원칙1) global에서 정의된 이름은 local에서 정의된 이름이 없을 경우 그를 대신할 수 있다. (local은 경우에 따라서 global에 있는 변수를 빌려 쓸 수 있다.)\n(원칙2) local과 global에서 같은 이름 ’x’가 각각 정의되어 있는 경우? global의 변수와 local의 변수는 각각 따로 행동하여 서로 영향을 주지 않는다. (독립적이다)\n\n만약에 local에 global의 변수를 같이 쓰고 있었다고 할지라도, 추후 새롭게 local에 이름이 새롭게 정의된다면 그 순간 local과 global의 변수를 각자 따로 행동하며 서로 영향을 주지 않는다.\\(\\to\\) 아래예제 확인\n\n\nx = 10\ndef f():\n    print(x)\n\n\nf() # x를 빌려쓰는 신세\n\n10\n\n\n\ndef f():\n    x = 20 # 이제 새롭게 x를 정의했으니까\n    print(x)\n\n\nf() # 다른길을 간다.\n\n20\n\n\n- 이전에 공부하였던 인스턴스변수와 클래스변수 역시 비슷한 행동을 보인다.\n\nclass Moo:\n    x = 0 # 클래스 변수\n\n\nmoo=Moo()\n\n(관찰1)\n\nMoo.x, moo.x\n\n(0, 0)\n\n\n\nmoo.x는 사실 정의한적이 없지만 Moo.x를 빌려쓰고 있다. (원칙1)\n\n(관찰2)\n\nMoo.x = 100\n\n\nMoo.x, moo.x\n\n(100, 100)\n\n\n\nMoo.x를 변화시키면 moo.x도 변화한다. (빌려쓰고 있는 것이니까, 원칙1의 재확인)\n\n(관찰3)\n\nmoo.x = 200\n\n\nMoo.x, moo.x\n\n(100, 200)\n\n\n\nmoo.x=200을 하는 순간 새롭게 인스턴스변수를 선언한 셈이된다. 따라서 원칙2가 적용되어 이제부터 Moo.x와 moo.x는 서로 독립적으로 행동한다.\n\n(관찰4)\n\nMoo.x = -99\n\n\nMoo.x, moo.x\n\n(-99, 200)\n\n\n\nmoo.x = 99\n\n\nMoo.x, moo.x\n\n(-99, 99)\n\n\n\nMoo.x를 바꾼다고 해서 moo.x가 영향받지 않고 moo.x를 바꿔도 Moo.x가 영향받지 않음. (완전히 독립, 원칙2의 재확인)\n\n\n\n\n\n클래스변수와 인스턴스 변수의 구분\n\n\n인스턴스 변수가 정의되지 않으면 클래스변수를 빌려쓸 수 있음(클래스변수가 상위개념)\n\n\n인스턴스변수와 클래스변수가 같은 이름으로 저장되어 있으면 각각 독립적으로 행동\n\n\n\n\n\n\n- self 비밀: 사실 클래스에서 정의된 함수의 첫번째 인자의 이름이 꼭 self일 필요는 없다. (무엇으로 전달하든 클래스 안에서 정의된 메소드의 첫번째 인자는 기본적으로 태명역할을 한다.)\n\nclass Moo:\n    def __init__(self):\n        self.name = 'jordy'\n    def f(self):\n        print(self.name)\n\n\nmoo = Moo()\n\n\nmoo.name\n\n'jordy'\n\n\n\nmoo.f()\n\njordy\n\n\n\n꼭 위와 같이 할 필요는 없다.\n\n\nclass Moo:\n    def __init__(abab):\n        abab.name = 'jordy'\n    def f(cdcd):\n        print(cdcd.name)\n\n\nmoo = Moo()\n\n\nmoo.name\n\n'jordy'\n\n\n\nmoo.f()\n\njordy\n\n\n- 인스턴스 메서드: 위의 __init__와 f와 같이 첫번째 인자를 인스턴스의 태명으로 받는 함수를 인스턴스 메서드 (간단히 메서드) 라고 한다.\n\n인스턴스 메소드는 self.f()와 같이 사용한다. 의미는 f(self) 이다.\n\n\nmoo.name = 'chunsik'\n\n\nmoo.name\n\n'chunsik'\n\n\n\nmoo.__init__()\n\n\nmoo.name # 인스턴스 메서드의 사용예시: self.__init__()의 꼴로 사용\n\n'jordy'\n\n\n\n오 신기하다.\n\n- 아래와 같이 사용할 수 없다.\n\nMoo.__init__() # 인스턴스가 들어와야하는데 클래스가 들어와버려서 이렇게 쓸순 없다.\n\nTypeError: __init__() missing 1 required positional argument: 'abab'\n\n\n\n인스턴스 메소드이기때문에 에러가 난다. 즉, 첫번째 입력 (.__init__()앞에)에 인스턴스가 들어가야 하는데 클래스가 들어와버렸다.\n\n\n\n\n- 클래스 메서드: 함수의 첫 인자로 클래스오브젝트를 받는 메서드를 클래스메서드라고 한다.\n- 목표: Moo.f() 와 같은 형태로 사용할 수 있는 함수를 만들어 보자. \\(\\to\\) 클래스메서드를 만들어보자.\n\nclass Moo:\n    def f(self): # 클래스 안에서 함수를 선언하면 디폴트로 인스턴스 메서드가 만들어진다.\n        print('인스턴스 메서드') \n\n\nmoo = Moo()\n\n\nmoo.f()\n\n인스턴스 메서드\n\n\n\nMoo.f() # 인스턴스 메서드니까 안되는게 당연\n\nTypeError: f() missing 1 required positional argument: 'self'\n\n\n\nclass Moo:\n    @classmethod\n    def f(cls): # 함수의 첫 인자로 클래스오브젝트를 받는다. cls는 클래스 Moo의 별명? 이라고 생각하면 된다.\n        print('클래스 메서드')\n\n\nmoo = Moo()\n\n\nMoo.f()\n\n클래스 메서드\n\n\n\nmoo.f() # 인스턴스 메서드를 따로 정의한적은 없지만 같은 이름의 클래스 메서드가 있으므로 빌려와서 씀!\n\n클래스 메서드\n\n\n- 예제\n\nclass Moo:\n    @classmethod\n    def set_class_x(cls, value): # 클래스 메서드\n        cls.x = value # 클래스변수선언, Moo.x = value와 같은 코드!\n    def set_instance_x(self, value): # 인스턴스 메서드\n        self.x = value # 인스턴스 변수선언\n\n\nmoo = Moo()\n\n\nMoo.set_class_x(10) # 클래스메서드로 클래스변수에 10을 설정\n\n\nMoo.x\n\n10\n\n\n\nMoo.set_instance_x(10) # 클래스에서 인스턴스 메서드 사용 -&gt; 사용불가\n\nTypeError: set_instance_x() missing 1 required positional argument: 'value'\n\n\n\nMoo.x, moo.x # 인스턴스변수는 따로 설정하지 않았지만 클래스 변수값을 빌려쓰고 있음\n\n(10, 10)\n\n\n\nmoo.set_class_x(20) # 인스턴스에서는 원래 set_class_x 라는 메서드는 없지만 클래스에서 빌려씀\n\n\nMoo.x, moo.x # 현재 moo.x(인스턴스)는 클래스 변수를 빌려쓰고 있는 상황이므로 같이 바뀜\n\n(20, 20)\n\n\n\nmoo.set_instance_x(-20) \n# 인스턴스에서 인스턴스 메서드를 사용하여 인스턴스 변수값을 -20으로 설정 \n# -&gt; 이때부터 인스턴스 변수와 클래스 변수는 서로 독립적인 노선을 간다.\n\n\nMoo.set_class_x(30) # 독립적인 노선을 가기로 헀으므로 클래스변수만 30으로 바뀜.\nMoo.x, moo.x\n\n(30, -20)\n\n\n\nmoo.set_class_x(-40) # 여전히 인스턴스에서 set_class_x라는 함수는 없으므로 클래스메소드를 빌려쓰고 있음.\n\n\n\n\n- 스태틱 메서드: 첫 인자로 인스턴스와 클래스 모두 받지 않음. (클래스안에 정의되어 있지만 그냥 함수와 같음)\n\nclass Cals:\n    @staticmethod\n    def add(a,b):\n        return a+b\n    @staticmethod\n    def sub(a,b):\n        return a-b\n\n\nfs = Cals()\n\n\nfs.add(1,2)\n\n3\n\n\n\nfs.sub(1,2)\n\n-1\n\n\n\nfs는 그냥 함수들을 묶어놓은 느낌? 정리하기 편하게?"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class09.html#예비학습-변수의-범위",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class09.html#예비학습-변수의-범위",
    "title": "[IP2022] class 09단계",
    "section": "",
    "text": "커널을 재시작하고 아래를 관찰하자.\n\n\n- 관찰1: 함수내의 변수 출력\n\ndef f():\n    x = 10\n    print(x)\n\n\nf()\n\n10\n\n\n- 관찰2: 함수내의 변수가 없을 경우 출력이 되지 않음\n\ndef g():\n    print(x)\n\n\ng()\n\nNameError: name 'x' is not defined\n\n\n- 관찰3: 동일한 이름의 변수가 global에 있다면 함수내에 (local) 그 이름의 변수가 선언되지 않아도 global 변수를 빌려서 사용함.\n\nx = 20\ndef g():\n    print(x)\n\n\ng()\n\n20\n\n\n- 관찰4: f()가 실행되면서 x=10이 함수내에(=local에) 실행되지만 이 결과가 외부의 x=20에(=global에) 영향을 미치지는 못함.\n\nf()\n\n10\n\n\n\nx\n\n20\n\n\n\n\n\n(코드1)\n\nx = 38\ndef nextyear():\n    y = x+1\n    print(x,y)\nnextyear()\n\n38 39\n\n\n(코드2)\n\nx = 38\ndef nextyear():\n    y = x+1\n    print(x,y)\n    x = 0\nnextyear()\n\nUnboundLocalError: local variable 'x' referenced before assignment\n\n\n- 해석: - 잘못된해석: 코드1은 실행되었고, 코드2에서 에러가 났다. 코드1과 2의 차이점은 x=0 이라는 코드가 코드2에 추가로 포함되어있다는 것이다. 따라서 x=0이 잘못된 코드이고 이걸 실행하는 과정에서 에러가 발생했다.\n\n올바른해석: 코드1에서는 x가 global variable이고 코드2에서는 x가 local variable이어서 생기는 문제\n\n- 코드2의 올바른 수정\n\nx = 38\ndef nextyear():\n    x = 0\n    y = x+1\n    print(x,y)\nnextyear()\n\n0 1"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class09.html#인스턴스-변수-클래스-변수",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class09.html#인스턴스-변수-클래스-변수",
    "title": "[IP2022] class 09단계",
    "section": "",
    "text": "- 예비학습이 주는 교훈\n(원칙1) global에서 정의된 이름은 local에서 정의된 이름이 없을 경우 그를 대신할 수 있다. (local은 경우에 따라서 global에 있는 변수를 빌려 쓸 수 있다.)\n(원칙2) local과 global에서 같은 이름 ’x’가 각각 정의되어 있는 경우? global의 변수와 local의 변수는 각각 따로 행동하여 서로 영향을 주지 않는다. (독립적이다)\n\n만약에 local에 global의 변수를 같이 쓰고 있었다고 할지라도, 추후 새롭게 local에 이름이 새롭게 정의된다면 그 순간 local과 global의 변수를 각자 따로 행동하며 서로 영향을 주지 않는다.\\(\\to\\) 아래예제 확인\n\n\nx = 10\ndef f():\n    print(x)\n\n\nf() # x를 빌려쓰는 신세\n\n10\n\n\n\ndef f():\n    x = 20 # 이제 새롭게 x를 정의했으니까\n    print(x)\n\n\nf() # 다른길을 간다.\n\n20\n\n\n- 이전에 공부하였던 인스턴스변수와 클래스변수 역시 비슷한 행동을 보인다.\n\nclass Moo:\n    x = 0 # 클래스 변수\n\n\nmoo=Moo()\n\n(관찰1)\n\nMoo.x, moo.x\n\n(0, 0)\n\n\n\nmoo.x는 사실 정의한적이 없지만 Moo.x를 빌려쓰고 있다. (원칙1)\n\n(관찰2)\n\nMoo.x = 100\n\n\nMoo.x, moo.x\n\n(100, 100)\n\n\n\nMoo.x를 변화시키면 moo.x도 변화한다. (빌려쓰고 있는 것이니까, 원칙1의 재확인)\n\n(관찰3)\n\nmoo.x = 200\n\n\nMoo.x, moo.x\n\n(100, 200)\n\n\n\nmoo.x=200을 하는 순간 새롭게 인스턴스변수를 선언한 셈이된다. 따라서 원칙2가 적용되어 이제부터 Moo.x와 moo.x는 서로 독립적으로 행동한다.\n\n(관찰4)\n\nMoo.x = -99\n\n\nMoo.x, moo.x\n\n(-99, 200)\n\n\n\nmoo.x = 99\n\n\nMoo.x, moo.x\n\n(-99, 99)\n\n\n\nMoo.x를 바꾼다고 해서 moo.x가 영향받지 않고 moo.x를 바꿔도 Moo.x가 영향받지 않음. (완전히 독립, 원칙2의 재확인)\n\n\n\n\n\n클래스변수와 인스턴스 변수의 구분\n\n\n인스턴스 변수가 정의되지 않으면 클래스변수를 빌려쓸 수 있음(클래스변수가 상위개념)\n\n\n인스턴스변수와 클래스변수가 같은 이름으로 저장되어 있으면 각각 독립적으로 행동"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class09.html#인스턴스-메서드",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class09.html#인스턴스-메서드",
    "title": "[IP2022] class 09단계",
    "section": "",
    "text": "- self 비밀: 사실 클래스에서 정의된 함수의 첫번째 인자의 이름이 꼭 self일 필요는 없다. (무엇으로 전달하든 클래스 안에서 정의된 메소드의 첫번째 인자는 기본적으로 태명역할을 한다.)\n\nclass Moo:\n    def __init__(self):\n        self.name = 'jordy'\n    def f(self):\n        print(self.name)\n\n\nmoo = Moo()\n\n\nmoo.name\n\n'jordy'\n\n\n\nmoo.f()\n\njordy\n\n\n\n꼭 위와 같이 할 필요는 없다.\n\n\nclass Moo:\n    def __init__(abab):\n        abab.name = 'jordy'\n    def f(cdcd):\n        print(cdcd.name)\n\n\nmoo = Moo()\n\n\nmoo.name\n\n'jordy'\n\n\n\nmoo.f()\n\njordy\n\n\n- 인스턴스 메서드: 위의 __init__와 f와 같이 첫번째 인자를 인스턴스의 태명으로 받는 함수를 인스턴스 메서드 (간단히 메서드) 라고 한다.\n\n인스턴스 메소드는 self.f()와 같이 사용한다. 의미는 f(self) 이다.\n\n\nmoo.name = 'chunsik'\n\n\nmoo.name\n\n'chunsik'\n\n\n\nmoo.__init__()\n\n\nmoo.name # 인스턴스 메서드의 사용예시: self.__init__()의 꼴로 사용\n\n'jordy'\n\n\n\n오 신기하다.\n\n- 아래와 같이 사용할 수 없다.\n\nMoo.__init__() # 인스턴스가 들어와야하는데 클래스가 들어와버려서 이렇게 쓸순 없다.\n\nTypeError: __init__() missing 1 required positional argument: 'abab'\n\n\n\n인스턴스 메소드이기때문에 에러가 난다. 즉, 첫번째 입력 (.__init__()앞에)에 인스턴스가 들어가야 하는데 클래스가 들어와버렸다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class09.html#클래스-메서드",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class09.html#클래스-메서드",
    "title": "[IP2022] class 09단계",
    "section": "",
    "text": "- 클래스 메서드: 함수의 첫 인자로 클래스오브젝트를 받는 메서드를 클래스메서드라고 한다.\n- 목표: Moo.f() 와 같은 형태로 사용할 수 있는 함수를 만들어 보자. \\(\\to\\) 클래스메서드를 만들어보자.\n\nclass Moo:\n    def f(self): # 클래스 안에서 함수를 선언하면 디폴트로 인스턴스 메서드가 만들어진다.\n        print('인스턴스 메서드') \n\n\nmoo = Moo()\n\n\nmoo.f()\n\n인스턴스 메서드\n\n\n\nMoo.f() # 인스턴스 메서드니까 안되는게 당연\n\nTypeError: f() missing 1 required positional argument: 'self'\n\n\n\nclass Moo:\n    @classmethod\n    def f(cls): # 함수의 첫 인자로 클래스오브젝트를 받는다. cls는 클래스 Moo의 별명? 이라고 생각하면 된다.\n        print('클래스 메서드')\n\n\nmoo = Moo()\n\n\nMoo.f()\n\n클래스 메서드\n\n\n\nmoo.f() # 인스턴스 메서드를 따로 정의한적은 없지만 같은 이름의 클래스 메서드가 있으므로 빌려와서 씀!\n\n클래스 메서드\n\n\n- 예제\n\nclass Moo:\n    @classmethod\n    def set_class_x(cls, value): # 클래스 메서드\n        cls.x = value # 클래스변수선언, Moo.x = value와 같은 코드!\n    def set_instance_x(self, value): # 인스턴스 메서드\n        self.x = value # 인스턴스 변수선언\n\n\nmoo = Moo()\n\n\nMoo.set_class_x(10) # 클래스메서드로 클래스변수에 10을 설정\n\n\nMoo.x\n\n10\n\n\n\nMoo.set_instance_x(10) # 클래스에서 인스턴스 메서드 사용 -&gt; 사용불가\n\nTypeError: set_instance_x() missing 1 required positional argument: 'value'\n\n\n\nMoo.x, moo.x # 인스턴스변수는 따로 설정하지 않았지만 클래스 변수값을 빌려쓰고 있음\n\n(10, 10)\n\n\n\nmoo.set_class_x(20) # 인스턴스에서는 원래 set_class_x 라는 메서드는 없지만 클래스에서 빌려씀\n\n\nMoo.x, moo.x # 현재 moo.x(인스턴스)는 클래스 변수를 빌려쓰고 있는 상황이므로 같이 바뀜\n\n(20, 20)\n\n\n\nmoo.set_instance_x(-20) \n# 인스턴스에서 인스턴스 메서드를 사용하여 인스턴스 변수값을 -20으로 설정 \n# -&gt; 이때부터 인스턴스 변수와 클래스 변수는 서로 독립적인 노선을 간다.\n\n\nMoo.set_class_x(30) # 독립적인 노선을 가기로 헀으므로 클래스변수만 30으로 바뀜.\nMoo.x, moo.x\n\n(30, -20)\n\n\n\nmoo.set_class_x(-40) # 여전히 인스턴스에서 set_class_x라는 함수는 없으므로 클래스메소드를 빌려쓰고 있음."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class09.html#스태틱-메서드",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class09.html#스태틱-메서드",
    "title": "[IP2022] class 09단계",
    "section": "",
    "text": "- 스태틱 메서드: 첫 인자로 인스턴스와 클래스 모두 받지 않음. (클래스안에 정의되어 있지만 그냥 함수와 같음)\n\nclass Cals:\n    @staticmethod\n    def add(a,b):\n        return a+b\n    @staticmethod\n    def sub(a,b):\n        return a-b\n\n\nfs = Cals()\n\n\nfs.add(1,2)\n\n3\n\n\n\nfs.sub(1,2)\n\n-1\n\n\n\nfs는 그냥 함수들을 묶어놓은 느낌? 정리하기 편하게?"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class03.html",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class03.html",
    "title": "[IP2022] class 03단계",
    "section": "",
    "text": "이 단계에서는 클래스오브젝트에 소속된 변수와 인스턴스오브젝트에 소속된 변수를 설명한다.\n\n\n\n- 파이썬은 모든 것이 오브젝트로 이루어져 있다. \\(\\leftarrow\\) 우선 그냥 외우기!\n- 오브젝트는 메모리 주소에 저장되는 모든 것을 의미한다.\n\na = 1\nid(a) # 메모리주소를 보는 명령어\n\n7618240\n\n\n\na = 'asdf'\nid(a)\n\n140366991918512\n\n\n\na = [1,2,3]\nid(a)\n\n140366923845376\n\n\n- 클래스와 인스턴스도 오브젝트다.\n\nclass A:\n    x = 0\n    def f(self):\n        print(self.x)\n\n\nid(A)\n\n39987760\n\n\n\nA는 오브젝트\n\n\nb = A()\n\n\nid(b)\n\n140366932540960\n\n\n\nb는 오브젝트\n\n- 앞으로는 A를 클래스 오브젝트, a,b를 인스턴스 오브젝트라고 부르자.\n\n\n- 시점0\n\n# 클래스 선언 시점\nclass A:\n    x = 0\n    y = 0\n    def f(self):\n        self.x = self.x + 1\n        A.y = A.y + 1\n        print('현재 인스턴스에서 f가 {}번 실행'.format(self.x))\n        print('A클래스에서 만들어진 모든 인스턴스들에서 f가 {}번 실행'.format(self.y))\n\n\nid(A) # A라는게 메모리 어딘가에 저장되어 있음.\n\n53014736\n\n\n\nA.x, A.y\n\n(0, 0)\n\n\n- 시점1\n\n# a라는 인스턴스\na = A()\n\n\n# b라는 인스턴스\nb = A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 0], [0, 0], [0, 0])\n\n\n- 시점2\n\na.f() # a에서 f라는 메소드 사용\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 1번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 1], [1, 1], [0, 1])\n\n\n\n여기서 현재 인스턴스라 함은 a를 의미한다.\n\n[1,1] 에서 첫번째 1은 현재 인스턴스(a)에서 f가 1번 실행되었다는 것을 의미하고\n[1,1] 에서 두번째 1은 A클래스에서 만들어진 모든 인스턴스들에서 f가 1번 실행되었음을 의미한다.\n\n\n- 시점3\n\nb.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 2번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 2], [1, 2], [1, 2])\n\n\n\n여기서 현재 인스턴스라 함은 b를 의미한다.\n\n[1,2] 에서 첫번째 1은 현재 인스턴스(b)에서 f가 1번 실행되었다는 것을 의미하고\n[1,2] 에서 두번째 2는 A클래스에서 만들어진 모든 인스턴스들에서 f가 2번 실행되었음을 의미한다 (왜냐면, 위에서 이미 한번 실행을 했기 때문)\n\n\n- 시점4\n\nb.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 3번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 3], [1, 3], [2, 3])\n\n\n- 시점5\n\na.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 4번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 4], [2, 4], [2, 4])\n\n\n- 시점6\n\n# c라는 인스턴스를 만들어보자.\nc = A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y], [c.x, c.y]\n\n([0, 4], [2, 4], [2, 4], [0, 4])\n\n\n- 시점7\n\nc.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 5번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y], [c.x, c.y]\n\n([0, 5], [2, 5], [2, 5], [1, 5])\n\n\n- 신기한 점: 각 인스턴스에서 인스턴스이름.f()를 실행한 횟수를 서로 공유하는 듯 하다. (마치 A가 관리하는 것 처럼 느껴진다.)"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class03.html#오브젝트의-개념",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class03.html#오브젝트의-개념",
    "title": "[IP2022] class 03단계",
    "section": "",
    "text": "- 파이썬은 모든 것이 오브젝트로 이루어져 있다. \\(\\leftarrow\\) 우선 그냥 외우기!\n- 오브젝트는 메모리 주소에 저장되는 모든 것을 의미한다.\n\na = 1\nid(a) # 메모리주소를 보는 명령어\n\n7618240\n\n\n\na = 'asdf'\nid(a)\n\n140366991918512\n\n\n\na = [1,2,3]\nid(a)\n\n140366923845376\n\n\n- 클래스와 인스턴스도 오브젝트다.\n\nclass A:\n    x = 0\n    def f(self):\n        print(self.x)\n\n\nid(A)\n\n39987760\n\n\n\nA는 오브젝트\n\n\nb = A()\n\n\nid(b)\n\n140366932540960\n\n\n\nb는 오브젝트\n\n- 앞으로는 A를 클래스 오브젝트, a,b를 인스턴스 오브젝트라고 부르자.\n\n\n- 시점0\n\n# 클래스 선언 시점\nclass A:\n    x = 0\n    y = 0\n    def f(self):\n        self.x = self.x + 1\n        A.y = A.y + 1\n        print('현재 인스턴스에서 f가 {}번 실행'.format(self.x))\n        print('A클래스에서 만들어진 모든 인스턴스들에서 f가 {}번 실행'.format(self.y))\n\n\nid(A) # A라는게 메모리 어딘가에 저장되어 있음.\n\n53014736\n\n\n\nA.x, A.y\n\n(0, 0)\n\n\n- 시점1\n\n# a라는 인스턴스\na = A()\n\n\n# b라는 인스턴스\nb = A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 0], [0, 0], [0, 0])\n\n\n- 시점2\n\na.f() # a에서 f라는 메소드 사용\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 1번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 1], [1, 1], [0, 1])\n\n\n\n여기서 현재 인스턴스라 함은 a를 의미한다.\n\n[1,1] 에서 첫번째 1은 현재 인스턴스(a)에서 f가 1번 실행되었다는 것을 의미하고\n[1,1] 에서 두번째 1은 A클래스에서 만들어진 모든 인스턴스들에서 f가 1번 실행되었음을 의미한다.\n\n\n- 시점3\n\nb.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 2번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 2], [1, 2], [1, 2])\n\n\n\n여기서 현재 인스턴스라 함은 b를 의미한다.\n\n[1,2] 에서 첫번째 1은 현재 인스턴스(b)에서 f가 1번 실행되었다는 것을 의미하고\n[1,2] 에서 두번째 2는 A클래스에서 만들어진 모든 인스턴스들에서 f가 2번 실행되었음을 의미한다 (왜냐면, 위에서 이미 한번 실행을 했기 때문)\n\n\n- 시점4\n\nb.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 3번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 3], [1, 3], [2, 3])\n\n\n- 시점5\n\na.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 4번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 4], [2, 4], [2, 4])\n\n\n- 시점6\n\n# c라는 인스턴스를 만들어보자.\nc = A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y], [c.x, c.y]\n\n([0, 4], [2, 4], [2, 4], [0, 4])\n\n\n- 시점7\n\nc.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 5번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y], [c.x, c.y]\n\n([0, 5], [2, 5], [2, 5], [1, 5])\n\n\n- 신기한 점: 각 인스턴스에서 인스턴스이름.f()를 실행한 횟수를 서로 공유하는 듯 하다. (마치 A가 관리하는 것 처럼 느껴진다.)"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class10.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class10.html",
    "title": "[IP2022] class 10단계",
    "section": "",
    "text": "문자열 join, matplotlib, 참조와 에일리어싱\n\n\n\n- 예제\n\n'abcd'\n\n'abcd'\n\n\n\nlst = list('abcd')\nlst\n\n['a', 'b', 'c', 'd']\n\n\n\n['a','b','c','d']를 붙여서 'abcd'로 하고 싶은데?\n\n\n''.join(lst)\n\n'abcd'\n\n\n\na='' # string object\n\n\na?\n\n\nType:        str\nString form: \nLength:      0\nDocstring:  \nstr(object='') -&gt; str\nstr(bytes_or_buffer[, encoding[, errors]]) -&gt; str\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\n\n\na.join? # iterable이 와야함.\n\n\nSignature: a.join(iterable, /)\nDocstring:\nConcatenate any number of strings.\nThe string whose method is called is inserted in between each given string.\nThe result is returned as a new string.\nExample: '.'.join(['ab', 'pq', 'rs']) -&gt; 'ab.pq.rs'\nType:      builtin_function_or_method\n\n\n\n\na.join(lst) # lst도 일단 리스트니까 iterable object\n\n'abcd'\n\n\n\nset(dir(lst)) & {'__iter__' , '__next__'} # iterable object임을 확인\n\n{'__iter__'}\n\n\n- 해설: ''는 string object이고, .join는 string object에 소속된 메서드이다.\n\na = ''\na.join(lst) # join(a,lst) 와 같은 효과\n\n'abcd'\n\n\n- join의 간단한 사용방법\n\n'-'.join(lst)\n\n'a-b-c-d'\n\n\n\n\n\n- 파이썬의 모든것은 객체이다:matplotlib의 다른 사용 (객체지향적 언어로 그림그리기!)\n- 그림오브젝트 생성\n\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure() # plt라는 모듈안에서 figure()라는 함수 실행\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n그림오브젝트가 실행되고 fig라는 이름이 붙음\n\nid(fig)\n\n140529470770096\n\n\n\nfig\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n- 그림오브젝트의 액시즈를 확인 -&gt; 아무것도 없음\n\nfig.axes\n\n[]\n\n\n- (0,0) 자리에 (가로=1, 세로=1) 크기의 액시즈를 넣어보자.\n\nfig.add_axes([0,0,1,1])\n\n&lt;Axes: &gt;\n\n\n\nfig.axes\n\n[&lt;Axes: &gt;]\n\n\n\n아까는 빈 리스트였는데 뭔가 추가되어 있다.\n\n\nfig\n\n\n\n\n- (0,1.2) 위치에 (가로=1,세로=1) 크기의 엑시즈 추가\n\nfig.add_axes([0,1.2, 1,1])\nfig\n\n\n\n\n- (0.5,0.5) 위치에 (가로=1, 세로=1) 크기의 그림 추가\n\nfig.add_axes([0.5,0.5,1,1])\n\n&lt;Axes: &gt;\n\n\n\nfig\n\n\n\n\n- fig의 세번째 엑시즈에 접근\n\na3 = fig.axes[2] # 이것역시 오브젝트임.\na3\n\n&lt;Axes: &gt;\n\n\n\nid(fig.axes[2]) # 어딘가에 저장이 되어있으니까 오브젝트!\n\n140529466106976\n\n\n- 엑시즈의 메소드 중에 plot이 있음 \\(\\to\\) 이것으로 그림을 그려봄.\n\na3.plot([1,2,3],[4,5,3],'--r')\n\n\nfig\n\n\n\n\n- 다시 세번째 축에 접근하여 다른 그림을 그려보자.\n\nfig.axes[-1].plot([1,2,3],[5,4,3],':o')\nfig\n\n\n\n\n- 이제 첫번째 축에 접근하여 새로운 그림을 그려보자.\n\nfig.axes[0].plot([1,2,3],[4,1,4],'--b')\nfig\n\n\n\n\n- 클래스에 대한 이해가 없다면 위와 같은 그림을 그리기도 힘들고 코드를 해석하기도 힘듬\n\n\n\n\n# !conda install -c conda-forge rise -y\n\n- 아래의 코드를 관찰하자.\n\na = [1,2,3]\nb = a\n\n\na, b\n\n([1, 2, 3], [1, 2, 3])\n\n\n\nid(a), id(b)\n\n(140529440320192, 140529440320192)\n\n\n같은 방문 앞에 a라는 포스트잇과, b라는 포스트잇이 같이 붙어있었음.\n\na = a + [4] ## 추가\na,b\n\n([1, 2, 3, 4], [1, 2, 3])\n\n\n\nid(a), id(b) # id 추적 -&gt; a의 id 달라짐.\n\n(140529450518400, 140529440320192)\n\n\n새로운 공간(다른방)에 a라는 포스트잇을 붙인것 (방이 바뀐것)\n- 이제 다시 아래의 코드를 관찰하자.\n\na = [1,2,3]\nb = a\na.append(4)\n\n현재 a,b의 출력결과는?\n\na, b\n\n([1, 2, 3, 4], [1, 2, 3, 4])\n\n\n- 아래의 코드를 다시 살펴보자.\n\na = [1,2,3]\nb = a\na.append(4)\n\na,b라는 변수들은 메모리에 어떻게 저장이 되어있을까?\n상상력을 조금 발휘하면 아래와 같이 여길 수 있다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\n\na = [1,2,3]\n\n\n메모리주소1에 존재하는 방을 a라고 하고, 그 방에 [1,2,3]을 넣는다.\n\n\n아래를 실행하였을 경우\n\n\nb = a\n\n\n메모리주소 38에 존재하는 방을 b라고 하고, 그 방에 a를 넣어야하는데 a는 [1,2,3]이니까 [1,2,3]을 넣는다.\n\n\n아래를 실행하면\n\n\na.append(4)\n\n\n방 a로 가서 [1,2,3]을 [1,2,3,4]로 바꾼다.\n그리고 방 b에는 아무것도 하지 않는다.\n\n- R에서는 맞는 비유인데, 파이썬은 적절하지 않은 비유이다.\n\nid(a)\n\n140529439072192\n\n\n\nid(b)\n\n140529439072192\n\n\n실제로는 a,b가 저장된 메모리 주소가 동일함\n- 파이썬에서는 아래가 더 적절한 비유이다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\n\na = [1,2,3]\n\n\n메모리주소 140529439072192에서 [1,2,3]을 생성한다.\n방 140529439072192의 방문에 a라는 포스트잇을 붙인다.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방중에서 a라는 포스트잇이 붙은 방을 찾아가면 된다.\n\n\n아래를 실행하였을 경우\n\n\nb=a\n\n\na 라는 포스트잇이 있는데, a라는 포스트잇이랑 b라는 포스트잇과 같은 효과를 주도록 한다.\n쉽게말하면, b라는 포스트잇을 방 140529439072192의 방문에 붙인다는 이야기.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방 중에서 a라는 포스트잇이 붙어있거나, b라는 포스트잇이 붙어있는 방을 찾아가면 된다.\n\n\n아래를 실행하면\n\n\na.append(4)\n\n\na라는 포스트잇이 붙어있는 방으로 가서, 그 내용물 append함수를 써서 4를 추가하라. 즉 내용물 [1,2,3]을 [1,2,3,4]로 바꾸라.\n같은방에 a,b라는 포스트잇이 모두 붙어있음. 따라서 b라는 포스트잇이 붙은 방을 찾아가서 내용물을 열어보면 [1,2,3,4]가 나온다.\n\n- 결론: 파이썬의 모든것은 오브젝트이다. 그리고 모든 오브젝트는 메모리주소 위에 올라간다. 하지만 그 메모리 주소에 붙어있는 포스트잇이 하나라는 보장은 없다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class10.html#문자열-join",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class10.html#문자열-join",
    "title": "[IP2022] class 10단계",
    "section": "",
    "text": "- 예제\n\n'abcd'\n\n'abcd'\n\n\n\nlst = list('abcd')\nlst\n\n['a', 'b', 'c', 'd']\n\n\n\n['a','b','c','d']를 붙여서 'abcd'로 하고 싶은데?\n\n\n''.join(lst)\n\n'abcd'\n\n\n\na='' # string object\n\n\na?\n\n\nType:        str\nString form: \nLength:      0\nDocstring:  \nstr(object='') -&gt; str\nstr(bytes_or_buffer[, encoding[, errors]]) -&gt; str\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\n\n\na.join? # iterable이 와야함.\n\n\nSignature: a.join(iterable, /)\nDocstring:\nConcatenate any number of strings.\nThe string whose method is called is inserted in between each given string.\nThe result is returned as a new string.\nExample: '.'.join(['ab', 'pq', 'rs']) -&gt; 'ab.pq.rs'\nType:      builtin_function_or_method\n\n\n\n\na.join(lst) # lst도 일단 리스트니까 iterable object\n\n'abcd'\n\n\n\nset(dir(lst)) & {'__iter__' , '__next__'} # iterable object임을 확인\n\n{'__iter__'}\n\n\n- 해설: ''는 string object이고, .join는 string object에 소속된 메서드이다.\n\na = ''\na.join(lst) # join(a,lst) 와 같은 효과\n\n'abcd'\n\n\n- join의 간단한 사용방법\n\n'-'.join(lst)\n\n'a-b-c-d'"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class10.html#matplotlib",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class10.html#matplotlib",
    "title": "[IP2022] class 10단계",
    "section": "",
    "text": "- 파이썬의 모든것은 객체이다:matplotlib의 다른 사용 (객체지향적 언어로 그림그리기!)\n- 그림오브젝트 생성\n\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure() # plt라는 모듈안에서 figure()라는 함수 실행\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n그림오브젝트가 실행되고 fig라는 이름이 붙음\n\nid(fig)\n\n140529470770096\n\n\n\nfig\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n- 그림오브젝트의 액시즈를 확인 -&gt; 아무것도 없음\n\nfig.axes\n\n[]\n\n\n- (0,0) 자리에 (가로=1, 세로=1) 크기의 액시즈를 넣어보자.\n\nfig.add_axes([0,0,1,1])\n\n&lt;Axes: &gt;\n\n\n\nfig.axes\n\n[&lt;Axes: &gt;]\n\n\n\n아까는 빈 리스트였는데 뭔가 추가되어 있다.\n\n\nfig\n\n\n\n\n- (0,1.2) 위치에 (가로=1,세로=1) 크기의 엑시즈 추가\n\nfig.add_axes([0,1.2, 1,1])\nfig\n\n\n\n\n- (0.5,0.5) 위치에 (가로=1, 세로=1) 크기의 그림 추가\n\nfig.add_axes([0.5,0.5,1,1])\n\n&lt;Axes: &gt;\n\n\n\nfig\n\n\n\n\n- fig의 세번째 엑시즈에 접근\n\na3 = fig.axes[2] # 이것역시 오브젝트임.\na3\n\n&lt;Axes: &gt;\n\n\n\nid(fig.axes[2]) # 어딘가에 저장이 되어있으니까 오브젝트!\n\n140529466106976\n\n\n- 엑시즈의 메소드 중에 plot이 있음 \\(\\to\\) 이것으로 그림을 그려봄.\n\na3.plot([1,2,3],[4,5,3],'--r')\n\n\nfig\n\n\n\n\n- 다시 세번째 축에 접근하여 다른 그림을 그려보자.\n\nfig.axes[-1].plot([1,2,3],[5,4,3],':o')\nfig\n\n\n\n\n- 이제 첫번째 축에 접근하여 새로운 그림을 그려보자.\n\nfig.axes[0].plot([1,2,3],[4,1,4],'--b')\nfig\n\n\n\n\n- 클래스에 대한 이해가 없다면 위와 같은 그림을 그리기도 힘들고 코드를 해석하기도 힘듬"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class10.html#참조와-에일리어싱",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class10.html#참조와-에일리어싱",
    "title": "[IP2022] class 10단계",
    "section": "",
    "text": "# !conda install -c conda-forge rise -y\n\n- 아래의 코드를 관찰하자.\n\na = [1,2,3]\nb = a\n\n\na, b\n\n([1, 2, 3], [1, 2, 3])\n\n\n\nid(a), id(b)\n\n(140529440320192, 140529440320192)\n\n\n같은 방문 앞에 a라는 포스트잇과, b라는 포스트잇이 같이 붙어있었음.\n\na = a + [4] ## 추가\na,b\n\n([1, 2, 3, 4], [1, 2, 3])\n\n\n\nid(a), id(b) # id 추적 -&gt; a의 id 달라짐.\n\n(140529450518400, 140529440320192)\n\n\n새로운 공간(다른방)에 a라는 포스트잇을 붙인것 (방이 바뀐것)\n- 이제 다시 아래의 코드를 관찰하자.\n\na = [1,2,3]\nb = a\na.append(4)\n\n현재 a,b의 출력결과는?\n\na, b\n\n([1, 2, 3, 4], [1, 2, 3, 4])\n\n\n- 아래의 코드를 다시 살펴보자.\n\na = [1,2,3]\nb = a\na.append(4)\n\na,b라는 변수들은 메모리에 어떻게 저장이 되어있을까?\n상상력을 조금 발휘하면 아래와 같이 여길 수 있다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\n\na = [1,2,3]\n\n\n메모리주소1에 존재하는 방을 a라고 하고, 그 방에 [1,2,3]을 넣는다.\n\n\n아래를 실행하였을 경우\n\n\nb = a\n\n\n메모리주소 38에 존재하는 방을 b라고 하고, 그 방에 a를 넣어야하는데 a는 [1,2,3]이니까 [1,2,3]을 넣는다.\n\n\n아래를 실행하면\n\n\na.append(4)\n\n\n방 a로 가서 [1,2,3]을 [1,2,3,4]로 바꾼다.\n그리고 방 b에는 아무것도 하지 않는다.\n\n- R에서는 맞는 비유인데, 파이썬은 적절하지 않은 비유이다.\n\nid(a)\n\n140529439072192\n\n\n\nid(b)\n\n140529439072192\n\n\n실제로는 a,b가 저장된 메모리 주소가 동일함\n- 파이썬에서는 아래가 더 적절한 비유이다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\n\na = [1,2,3]\n\n\n메모리주소 140529439072192에서 [1,2,3]을 생성한다.\n방 140529439072192의 방문에 a라는 포스트잇을 붙인다.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방중에서 a라는 포스트잇이 붙은 방을 찾아가면 된다.\n\n\n아래를 실행하였을 경우\n\n\nb=a\n\n\na 라는 포스트잇이 있는데, a라는 포스트잇이랑 b라는 포스트잇과 같은 효과를 주도록 한다.\n쉽게말하면, b라는 포스트잇을 방 140529439072192의 방문에 붙인다는 이야기.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방 중에서 a라는 포스트잇이 붙어있거나, b라는 포스트잇이 붙어있는 방을 찾아가면 된다.\n\n\n아래를 실행하면\n\n\na.append(4)\n\n\na라는 포스트잇이 붙어있는 방으로 가서, 그 내용물 append함수를 써서 4를 추가하라. 즉 내용물 [1,2,3]을 [1,2,3,4]로 바꾸라.\n같은방에 a,b라는 포스트잇이 모두 붙어있음. 따라서 b라는 포스트잇이 붙은 방을 찾아가서 내용물을 열어보면 [1,2,3,4]가 나온다.\n\n- 결론: 파이썬의 모든것은 오브젝트이다. 그리고 모든 오브젝트는 메모리주소 위에 올라간다. 하지만 그 메모리 주소에 붙어있는 포스트잇이 하나라는 보장은 없다."
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022_06_09_2021년_파이썬입문_기말고사_(풀이포함).html",
    "href": "posts/1_IP2022/04_시험/2022_06_09_2021년_파이썬입문_기말고사_(풀이포함).html",
    "title": "[2021 EXAM] 2021 final exam solution",
    "section": "",
    "text": "2021년 1학기 파이썬입문 기말고사 (풀이포함)\nLINK HERE!\n\n# 1. (20점)\nN사에서 게임유저들에게 여름방학 기념이벤트로 진명왕의 집판검이라는 이름의 아이템을 선물했다고 하자. 진명왕의 집판검은 총 5회에 걸쳐서 강화(upgrade)될 수 있데 강화의 성공확률은 10%라고 하자. 강화가 5번성공하면 더 이상 강화가 진행되지 않는다고 하자. (따라서 더 이상 강화시도를 하지 않아도 무방하다) 아래는 이 아이템에 강화를 진행하였을때 각 강화상태를 설명한 예시이다.\n\n\n\n시도횟수\n강화성공여부\n강화상태\n비고\n\n\n\n\n1\n강화실패\n+0 \\(\\to\\) +0\n강화실패로 인하여 강화상태 변화없음\n\n\n2\n강화성공\n+0 \\(\\to\\) +1\n강화성공으로 인한 강화상태 변화\n\n\n3\n강화실패\n+1 \\(\\to\\) +1\n강화실패로 인하여 강화상태 변화없음\n\n\n4\n강화성공\n+1 \\(\\to\\) +2\n강화성공으로 인한 강화상태 변화\n\n\n5\n강화성공\n+2 \\(\\to\\) +3\n강화성공으로 인한 강화상태 변화\n\n\n6\n강화성공\n+3 \\(\\to\\) +4\n강화성공으로 인한 강화상태 변화\n\n\n7\n강화실패\n+4 \\(\\to\\) +4\n강화실패로 인하여 강화상태 변화없음\n\n\n8\n강화성공\n+4 \\(\\to\\) +5\n모든 강화 성공\n\n\n9\n-\n+5 \\(\\to\\) +5\n더 이상 강화시도 하지 않음\n\n\n10\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\n강화는 하루에 한 번씩만 시도할 수 있으며 시도가능한 기간은 7월1일부터 8월31일까지로 한정되어 있다고 하자. 따라서 방학동안 유저들은 총 62번 시도를 할 수 있다. 방학이 끝난이후 100명 유저중 대략 몇명정도 +5 강화상태에 있겠는가? 파이썬을 통한 시뮬레이션을 활용하여 추론하라. (단, +5강화에 성공하지 못한 모든 유저는 반드시 하루에 한번 강화를 시도해야 한다고 가정하자.)\n(풀이1)\n\nimport numpy as np\nnp.random.seed(1)\nsum(np.random.binomial(n=62, p=0.1, size=10000)&gt;=5)/10000\n\n0.7514\n\n\n(풀이2)\n\nclass ExecutionSword():\n    def __init__(self,prob):\n        self.nuser=100000\n        self.prob=prob\n        self.attemptresult=None\n        self.upgradestate=pd.DataFrame({'day0':[0]*self.nuser})\n        self.failstate=pd.DataFrame({'day0':[0]*self.nuser})\n        self.ratio=0\n        self.day=0\n    def addday(self):\n        self.day=self.day+1            \n    def attempt(self):\n        self.attemptresult = np.random.binomial(n=1, p=self.prob, size=self.nuser)\n    def update(self):\n        # 강화상태 업데이트\n        self.upgradestate['day%s' % self.day] = np.minimum(5,self.upgradestate['day%s' % (self.day-1)]+self.attemptresult)\n        # 강화실패누적횟수 업데이트 \n        self.failstate['day%s' % self.day]=self.failstate['day%s' % (self.day-1)]+(self.attemptresult==0)*1\n        # 강화상태==5 or 강화상태==0 일 경우 강화실패누적횟수 초기화 \n        self.failstate['day%s' % self.day][self.upgradestate['day%s' % self.day]== 0]=0\n        self.failstate['day%s' % self.day][self.upgradestate['day%s' % self.day]== 5]=0\n    def reset(self):\n        # 실패횟수 = 2 인것을 찾아 index_ 에 저장 -&gt; index_ 에 해당하는 유저의 강화횟수와 실패횟수를 모두 0으로 초기화 \n        index_= self.failstate['day%s' % self.day]==2\n        self.failstate['day%s' % self.day][index_] = 0\n        self.upgradestate['day%s' % self.day][index_] = 0\n    def arrangeprob(self):\n        self.ratio=sum(self.upgradestate['day%s' % self.day]==5) / self.nuser\n        if self.ratio &gt; 0.5:\n            self.prob = 0.9\n\n\n# 1 \nimport pandas as pd\ns1=ExecutionSword(0.1)\nfor i in range(62):\n    s1.addday()\n    s1.attempt()\n    s1.update()\n\n\nsum(s1.upgradestate.day62==5)/s1.nuser\n\n0.75551\n\n\n\n\n# 2. (70점)\n강화성공확률을 40%로 수정한다. 강화에 누적2회 실패하면 강화상태가 초기화 된다고 하자. (따라서 강화실패 누적횟수를 카운트하는 변수가 필요하다) 단, 강화실패 누적횟수는 누적2회 달성시 0으로 초기화 된다. 또한 강화상태가 +0인 경우는 실패하여도 강화실패 누적횟수가 추가되지 않는다.\n\n\n\n시도횟수\n강화성공여부\n강화상태\n강화실패누적\n비고\n\n\n\n\n1\n강화성공\n+0 \\(\\to\\) +1\n0 \\(\\to\\) 0\n-\n\n\n2\n강화성공\n+1 \\(\\to\\) +2\n0 \\(\\to\\) 0\n-\n\n\n3\n강화실패\n+2 \\(\\to\\) +2\n0 \\(\\to\\) 1\n-\n\n\n4\n강화성공\n+2 \\(\\to\\) +3\n1 \\(\\to\\) 1\n-\n\n\n5\n강화실패\n+3 \\(\\to\\) +0\n1 \\(\\to\\) 0\n강화실패로 누적2회로 인한 초기화\n\n\n6\n강화실패\n+0 \\(\\to\\) +0\n0 \\(\\to\\) 0\n강화실패 누적횟수 증가하지 않음\n\n\n7\n강화성공\n+0 \\(\\to\\) +1\n0 \\(\\to\\) 0\n-\n\n\n8\n강화성공\n+1 \\(\\to\\) +2\n0 \\(\\to\\) 0\n-\n\n\n9\n강화성공\n+2 \\(\\to\\) +3\n0 \\(\\to\\) 0\n-\n\n\n10\n강화성공\n+3 \\(\\to\\) +4\n0 \\(\\to\\) 0\n-\n\n\n11\n강화성공\n+4 \\(\\to\\) +5\n0 \\(\\to\\) 0\n모든 강화 성공\n\n\n12\n-\n+5 \\(\\to\\) +5\n0 \\(\\to\\) 0\n더 이상 강화시도 하지 않음\n\n\n13\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\n(1) 이 경우 62일의 방학뒤에 100명의 유저중 대략 몇명정도 +5 강화상태에 있겠는가? 시뮬레이션을 활용하여 추론하라. (단, +5강화에 성공하지 못한 모든 유저는 반드시 하루에 한번 강화를 시도해야 한다고 가정하자.)\n(2) 31번째 시도 이후 대략 몇명의 유저가 +5 강화상태에 있겠는가?\n\n# 2-1,2 \ns2=ExecutionSword(0.4)\n\n\nfor i in range(62):\n    s2.addday()\n    s2.attempt()\n    s2.update()\n    s2.reset() ## 초기화가 되는 조건이 있으므로 문제1에서 reset함수만 추가하면 된다. \n\n\n# 2-1\nsum(s2.upgradestate.day31==5)/s2.nuser\n\n0.36392\n\n\n\n# 2-2\nsum(s2.upgradestate.day62==5)/s2.nuser\n\n0.61803\n\n\n(3) 100명의 유저중 50명이상의 유저가 +5 강화상태에 도달하는 순간 모든 유저의 강화성공확률을 90%로 증가시킨다고 하자. 62일의 방학뒤에 100명의 유저 중 몇명 정도가 +5 강화상태에 있겠는가?\n\n# 2-3 \ns3=ExecutionSword(0.4)\n\n\nfor i in range(62):\n    s3.addday()\n    s3.attempt()\n    s3.update()\n    s3.reset() ## 초기화가 되는 조건이 있으므로 reset함수 추가\n    s3.arrangeprob() ## 전체유저의 50%가 강화성공하면 강화확률이 조정되는 조건이 있으므로 arragneprob 추가 \n\n\nsum(s3.upgradestate.day62==5)/s3.nuser\n\n0.9993"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html",
    "href": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html",
    "title": "[IP2022] Numpy 4단계(concat, stack)",
    "section": "",
    "text": "Numpy array를 결합하는 기능들에 대해 알아보자. (np.concatenate, np.concat)\n\n\n\n- 기본예제\n\nimport numpy as np\n\n\na = np.array([1,2])\nb = -a\n\n\nnp.concatenate([a,b])\n\narray([ 1,  2, -1, -2])\n\n\n- 응용\n\na = np.array([1,2])\nb = -a\nc = np.array([3,4,5])\n\n\nnp.concatenate([a,b,c])\n\narray([ 1,  2, -1, -2,  3,  4,  5])\n\n\n\n여기까진 딱히 concatenate의 메리트가 없어보임\n리스트였다면 a+b+c하면 되는 기능이니까?\n\n- 2d array에 적용해보자.\n\na = np.arange(4).reshape(2,2)\nb = -a\n\n\na\n\narray([[0, 1],\n       [2, 3]])\n\n\n\nb\n\narray([[ 0, -1],\n       [-2, -3]])\n\n\n\nnp.concatenate([a,b])\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n\n위아래로 붙었네! 그럼 옆으로 붙이려면 어떻게 하지?\n\n- 옆으로 붙이려면?\n\nnp.concatenate([a,b], axis=1)\n\narray([[ 0,  1,  0, -1],\n       [ 2,  3, -2, -3]])\n\n\n- 위의 코드에서 axis=1 이 뭐지? axis=0,2 등을 치면 결과가 어떻게 될까?\n\nnp.concatenate([a,b],axis=0)\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n\n이건 그냥 np.concatenate([a,b])와 같다.\nnp.concatenate([a,b])는 np.concatenate([a,b],axis=0)의 생략버전이군?\n\n\nnp.concatenate([a,b],axis=2)\n\nAxisError: axis 2 is out of bounds for array of dimension 2\n\n\n\n이런건 없다.\n\n- axis의 의미가 뭔지 궁금함. 좀 더 예제를 살펴보자.\n\na = np.array(range(2*3*4)).reshape(2,3,4) # 3d array\na\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nb = -a\nb\n\narray([[[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=0)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=1)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b], axis=2)\n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\n이번에는 axis=2까지 된다?\n\n\nnp.concatenate([a,b], axis=3)\n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\naxis=3까지는 안된다?\n\n- 뭔가 나름의 방식으로 합쳐지는데 원리가 뭘까?\n(분석1) np.concatenate([a,b], axis=0)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=0).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\n첫번째 차원이 바뀌었다. \\(\\Rightarrow\\) 첫번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=0 (파이썬은 0부터 시작하니까!)\n\n(분석2) np.concatenate([a,b], axis=1)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n두번째 차원이 바뀌었다. \\(\\Rightarrow\\) 두번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=1\n\n(분석3) np.concatenate([a,b], axis=2)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n세번째 차원이 바뀌었다. \\(\\Rightarrow\\) 세번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=2\n\n(분석4) np.concatenate([a,b], axis=3)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=3).shape\n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\n네번째 차원이 없다. \\(\\Rightarrow\\) 세번째 축이 없다. \\(\\Rightarrow\\) axis=3으로 하면 에러가 난다.\n\n(보너스)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-1)\n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n마지막 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막 축이 바뀌었다. \\(\\Rightarrow\\) axis=-1\n\n(보너스2)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-2)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n마지막에서 2번째 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막에서 2번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=-2\n\n(보너스3)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-3)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-3).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\n마지막에서 3번째 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막에서 3번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=-3\n\n(보너스4)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-4)\n\nAxisError: axis -4 is out of bounds for array of dimension 3\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-4).shape\n\nAxisError: axis -4 is out of bounds for array of dimension 3\n\n\n\n마지막에서 4번째 차원은 없다. \\(\\Rightarrow\\) 마지막에서 4번째 축은 없다. \\(\\Rightarrow\\) axis=-4는 에러가 난다.\n\n- 0차원은 축이 없으므로 concatenate를 쓸 수 없다.\n\na = np.array(1)\nb = np.array(-1)\n\n\na.shape, b.shape\n\n((), ())\n\n\n\nnp.concatenate([a,b])\n\nValueError: zero-dimensional arrays cannot be concatenated\n\n\n이게 만약에 이렇게 바뀌면 1차원이니까 쓸 수 있다.\n\na = np.array([1])\nb = np.array([-1])\na.shape, b.shape\n\n((1,), (1,))\n\n\n\nnp.concatenate([a,b])\n\narray([ 1, -1])\n\n\n- 꼭 a,b가 같은 차원일 필요는 없다.\n\na = np.array(range(4)).reshape(2,2)\nb = np.array(range(2)).reshape(2,1)\n\n\nnp.concatenate([a,b], axis=1)\n\narray([[0, 1, 0],\n       [2, 3, 1]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=1).shape\n\n((2, 2), (2, 1), (2, 3))\n\n\n\n\n\n- 혹시 아래가 가능할까?\n\n\\((3,)\\) 결합 : \\((3,) \\Rightarrow (3,2)\\)\n\n\na = np.array([1,2,3])\nb = -a\n\n\na,b\n\n(array([1, 2, 3]), array([-1, -2, -3]))\n\n\n\na.shape, b.shape\n\n((3,), (3,))\n\n\n\nnp.concatenate([a,b], axis=1)\n\nAxisError: axis 1 is out of bounds for array of dimension 1\n\n\n\n불가능\n\n- 아래와 같이 하면 해결 가능\n\na = np.array([1,2,3]).reshape(3,1)\nb = -a\n\n\na.shape, b.shape\n\n((3, 1), (3, 1))\n\n\n\na,b\n\n(array([[1],\n        [2],\n        [3]]),\n array([[-1],\n        [-2],\n        [-3]]))\n\n\n\nnp.concatenate([a,b], axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n\n분석: \\((3) (3) \\Rightarrow (3,1),(3,1)\\Rightarrow (3,1) \\space \\tt{concat} \\space (3,1)\\)\n\n- 위의 과정을 줄여서 아래와 같이 할 수 있다.\n\na = np.array([1,2,3])\nb = -a\n\n\nnp.stack([a,b], axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n- 아래도 가능\n\nnp.stack([a,b],axis=0)\n\narray([[ 1,  2,  3],\n       [-1, -2, -3]])\n\n\n- 분석해보고 외우자\n(분석1)\n\na = np.array([1,2,3])\nb = -a\n\n\na.shape, b.shape, np.stack([a,b],axis=0).shape\n\n((3,), (3,), (2, 3))\n\n\n\n\\((3)(3) \\Rightarrow \\text{첫 위치에 축을 추가 (axis=0)} \\Rightarrow (1,3)(1,3) \\Rightarrow (2,3)\\)\n\n(분석2)\n\na = np.array([1,2,3])\nb = -a\n\n\na.shape, b.shape, np.stack([a,b],axis=1).shape\n\n((3,), (3,), (3, 2))\n\n\n\\((3)(3)\\Rightarrow \\text{두번째 위치에 축을 추가 (axis=1)} \\Rightarrow (3,1)(3,1) \\Rightarrow (3,2)\\)\n- 고차원예제\n\na = np.arange(3*4*5).reshape(3,4,5)\nb = -a\n\n\na.shape, b.shape\n\n((3, 4, 5), (3, 4, 5))\n\n\n\nnp.stack([a,b], axis=0).shape # (3,4,5) =&gt; (1,3,4,5) // 첫 위치에 축이 추가되고 스택\n\n(2, 3, 4, 5)\n\n\n\nnp.stack([a,b], axis=1).shape # (3,4,5) =&gt; (3,1,4,5) // 두번째 위치에 축이 추가되고 스택\n\n(3, 2, 4, 5)\n\n\n\nnp.stack([a,b], axis=2).shape # (3,4,5) =&gt; (3,4,1,5) // 세번째 위치에 축이 추가되고 스택\n\n(3, 4, 2, 5)\n\n\n\nnp.stack([a,b], axis=3).shape # (3,4,5) =&gt; (3,4,5,1) // 네번째 위치에 축이 추가되고 스택\n\n(3, 4, 5, 2)\n\n\n\nnp.stack([a,b], axis=-1).shape # axis=-1 &lt;=&gt; axis=3\n\n(3, 4, 5, 2)\n\n\n\nnp.stack([a,b], axis=-2).shape # axis=-2 &lt;=&gt; axis=2\n\n(3, 4, 2, 5)\n\n\nnp.concatenate 는 축의 총 개수를 유지하면서 결합, np.stack은 축의 개수를 하나 증가시키면서 결합"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html#np.concatenate",
    "href": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html#np.concatenate",
    "title": "[IP2022] Numpy 4단계(concat, stack)",
    "section": "",
    "text": "- 기본예제\n\nimport numpy as np\n\n\na = np.array([1,2])\nb = -a\n\n\nnp.concatenate([a,b])\n\narray([ 1,  2, -1, -2])\n\n\n- 응용\n\na = np.array([1,2])\nb = -a\nc = np.array([3,4,5])\n\n\nnp.concatenate([a,b,c])\n\narray([ 1,  2, -1, -2,  3,  4,  5])\n\n\n\n여기까진 딱히 concatenate의 메리트가 없어보임\n리스트였다면 a+b+c하면 되는 기능이니까?\n\n- 2d array에 적용해보자.\n\na = np.arange(4).reshape(2,2)\nb = -a\n\n\na\n\narray([[0, 1],\n       [2, 3]])\n\n\n\nb\n\narray([[ 0, -1],\n       [-2, -3]])\n\n\n\nnp.concatenate([a,b])\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n\n위아래로 붙었네! 그럼 옆으로 붙이려면 어떻게 하지?\n\n- 옆으로 붙이려면?\n\nnp.concatenate([a,b], axis=1)\n\narray([[ 0,  1,  0, -1],\n       [ 2,  3, -2, -3]])\n\n\n- 위의 코드에서 axis=1 이 뭐지? axis=0,2 등을 치면 결과가 어떻게 될까?\n\nnp.concatenate([a,b],axis=0)\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n\n이건 그냥 np.concatenate([a,b])와 같다.\nnp.concatenate([a,b])는 np.concatenate([a,b],axis=0)의 생략버전이군?\n\n\nnp.concatenate([a,b],axis=2)\n\nAxisError: axis 2 is out of bounds for array of dimension 2\n\n\n\n이런건 없다.\n\n- axis의 의미가 뭔지 궁금함. 좀 더 예제를 살펴보자.\n\na = np.array(range(2*3*4)).reshape(2,3,4) # 3d array\na\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nb = -a\nb\n\narray([[[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=0)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=1)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b], axis=2)\n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\n이번에는 axis=2까지 된다?\n\n\nnp.concatenate([a,b], axis=3)\n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\naxis=3까지는 안된다?\n\n- 뭔가 나름의 방식으로 합쳐지는데 원리가 뭘까?\n(분석1) np.concatenate([a,b], axis=0)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=0).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\n첫번째 차원이 바뀌었다. \\(\\Rightarrow\\) 첫번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=0 (파이썬은 0부터 시작하니까!)\n\n(분석2) np.concatenate([a,b], axis=1)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n두번째 차원이 바뀌었다. \\(\\Rightarrow\\) 두번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=1\n\n(분석3) np.concatenate([a,b], axis=2)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n세번째 차원이 바뀌었다. \\(\\Rightarrow\\) 세번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=2\n\n(분석4) np.concatenate([a,b], axis=3)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=3).shape\n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\n네번째 차원이 없다. \\(\\Rightarrow\\) 세번째 축이 없다. \\(\\Rightarrow\\) axis=3으로 하면 에러가 난다.\n\n(보너스)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-1)\n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n마지막 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막 축이 바뀌었다. \\(\\Rightarrow\\) axis=-1\n\n(보너스2)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-2)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n마지막에서 2번째 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막에서 2번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=-2\n\n(보너스3)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-3)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-3).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\n마지막에서 3번째 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막에서 3번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=-3\n\n(보너스4)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-4)\n\nAxisError: axis -4 is out of bounds for array of dimension 3\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-4).shape\n\nAxisError: axis -4 is out of bounds for array of dimension 3\n\n\n\n마지막에서 4번째 차원은 없다. \\(\\Rightarrow\\) 마지막에서 4번째 축은 없다. \\(\\Rightarrow\\) axis=-4는 에러가 난다.\n\n- 0차원은 축이 없으므로 concatenate를 쓸 수 없다.\n\na = np.array(1)\nb = np.array(-1)\n\n\na.shape, b.shape\n\n((), ())\n\n\n\nnp.concatenate([a,b])\n\nValueError: zero-dimensional arrays cannot be concatenated\n\n\n이게 만약에 이렇게 바뀌면 1차원이니까 쓸 수 있다.\n\na = np.array([1])\nb = np.array([-1])\na.shape, b.shape\n\n((1,), (1,))\n\n\n\nnp.concatenate([a,b])\n\narray([ 1, -1])\n\n\n- 꼭 a,b가 같은 차원일 필요는 없다.\n\na = np.array(range(4)).reshape(2,2)\nb = np.array(range(2)).reshape(2,1)\n\n\nnp.concatenate([a,b], axis=1)\n\narray([[0, 1, 0],\n       [2, 3, 1]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=1).shape\n\n((2, 2), (2, 1), (2, 3))"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html#np.stack",
    "href": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html#np.stack",
    "title": "[IP2022] Numpy 4단계(concat, stack)",
    "section": "",
    "text": "- 혹시 아래가 가능할까?\n\n\\((3,)\\) 결합 : \\((3,) \\Rightarrow (3,2)\\)\n\n\na = np.array([1,2,3])\nb = -a\n\n\na,b\n\n(array([1, 2, 3]), array([-1, -2, -3]))\n\n\n\na.shape, b.shape\n\n((3,), (3,))\n\n\n\nnp.concatenate([a,b], axis=1)\n\nAxisError: axis 1 is out of bounds for array of dimension 1\n\n\n\n불가능\n\n- 아래와 같이 하면 해결 가능\n\na = np.array([1,2,3]).reshape(3,1)\nb = -a\n\n\na.shape, b.shape\n\n((3, 1), (3, 1))\n\n\n\na,b\n\n(array([[1],\n        [2],\n        [3]]),\n array([[-1],\n        [-2],\n        [-3]]))\n\n\n\nnp.concatenate([a,b], axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n\n분석: \\((3) (3) \\Rightarrow (3,1),(3,1)\\Rightarrow (3,1) \\space \\tt{concat} \\space (3,1)\\)\n\n- 위의 과정을 줄여서 아래와 같이 할 수 있다.\n\na = np.array([1,2,3])\nb = -a\n\n\nnp.stack([a,b], axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n- 아래도 가능\n\nnp.stack([a,b],axis=0)\n\narray([[ 1,  2,  3],\n       [-1, -2, -3]])\n\n\n- 분석해보고 외우자\n(분석1)\n\na = np.array([1,2,3])\nb = -a\n\n\na.shape, b.shape, np.stack([a,b],axis=0).shape\n\n((3,), (3,), (2, 3))\n\n\n\n\\((3)(3) \\Rightarrow \\text{첫 위치에 축을 추가 (axis=0)} \\Rightarrow (1,3)(1,3) \\Rightarrow (2,3)\\)\n\n(분석2)\n\na = np.array([1,2,3])\nb = -a\n\n\na.shape, b.shape, np.stack([a,b],axis=1).shape\n\n((3,), (3,), (3, 2))\n\n\n\\((3)(3)\\Rightarrow \\text{두번째 위치에 축을 추가 (axis=1)} \\Rightarrow (3,1)(3,1) \\Rightarrow (3,2)\\)\n- 고차원예제\n\na = np.arange(3*4*5).reshape(3,4,5)\nb = -a\n\n\na.shape, b.shape\n\n((3, 4, 5), (3, 4, 5))\n\n\n\nnp.stack([a,b], axis=0).shape # (3,4,5) =&gt; (1,3,4,5) // 첫 위치에 축이 추가되고 스택\n\n(2, 3, 4, 5)\n\n\n\nnp.stack([a,b], axis=1).shape # (3,4,5) =&gt; (3,1,4,5) // 두번째 위치에 축이 추가되고 스택\n\n(3, 2, 4, 5)\n\n\n\nnp.stack([a,b], axis=2).shape # (3,4,5) =&gt; (3,4,1,5) // 세번째 위치에 축이 추가되고 스택\n\n(3, 4, 2, 5)\n\n\n\nnp.stack([a,b], axis=3).shape # (3,4,5) =&gt; (3,4,5,1) // 네번째 위치에 축이 추가되고 스택\n\n(3, 4, 5, 2)\n\n\n\nnp.stack([a,b], axis=-1).shape # axis=-1 &lt;=&gt; axis=3\n\n(3, 4, 5, 2)\n\n\n\nnp.stack([a,b], axis=-2).shape # axis=-2 &lt;=&gt; axis=2\n\n(3, 4, 2, 5)\n\n\nnp.concatenate 는 축의 총 개수를 유지하면서 결합, np.stack은 축의 개수를 하나 증가시키면서 결합"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-13-pandas1.html",
    "href": "posts/1_IP2022/02_DataScience/2023-03-13-pandas1.html",
    "title": "[IP2022] Pandas 1단계",
    "section": "",
    "text": "데이터프레임 선언, 행\\(\\cdot\\)열 이름부여, 자료형, pd.Series"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-13-pandas1.html#pandas-공부-1단계",
    "href": "posts/1_IP2022/02_DataScience/2023-03-13-pandas1.html#pandas-공부-1단계",
    "title": "[IP2022] Pandas 1단계",
    "section": "pandas 공부 1단계",
    "text": "pandas 공부 1단계\n\nimport numpy as np\nimport pandas as pd\n\n\n데이터프레임 선언\n- 방법1: dictionary에서 만든다.\n\npd.DataFrame({'att':[30,40,50], 'mid':[50,60,70]}) # 리스트\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n50\n\n\n1\n40\n60\n\n\n2\n50\n70\n\n\n\n\n\n\n\n\npd.DataFrame({'att':(30,40,50),'mid':(50,60,70)}) # 튜플\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n50\n\n\n1\n40\n60\n\n\n2\n50\n70\n\n\n\n\n\n\n\n\npd.DataFrame({'att':np.array([30,40,50]),'mid':np.array([50,60,70])}) # numpy array\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n50\n\n\n1\n40\n60\n\n\n2\n50\n70\n\n\n\n\n\n\n\n- 방법2: 2차원 ndarray에서 만든다.\n\nnp.arange(2*3).reshape(2,3)\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\npd.DataFrame(np.arange(2*3).reshape(2,3))\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n\n\n열의 이름 부여\n- 방법1: 딕셔너리를 통하여 만들면 딕셔너리의 key가 자동으로 열의 이름이 된다.\n\npd.DataFrame({'att':np.array([30,40,50]), 'mid':np.array([50,60,70])})\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n50\n\n\n1\n40\n60\n\n\n2\n50\n70\n\n\n\n\n\n\n\n- 방법2: pd.DataFrame()의 옵션에 columns를 이용\n\npd.DataFrame(np.arange(2*3).reshape(2,3),columns=['X1','X2','X3'])\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n- 방법3: df.columns에 원하는 열이름 덮어씀 (1)\n\ndf=pd.DataFrame(np.arange(2*3).reshape(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n\ndf.columns = ['X1','X2','X3'] # columns 메소드 이용.\n\n\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n\ndf.columns, type(df.columns)\n\n(Index(['X1', 'X2', 'X3'], dtype='object'), pandas.core.indexes.base.Index)\n\n\n- 방법4: df.columns에 원하는 열이름 덮어씀 (2)\n\ndf=pd.DataFrame(np.arange(2*3).reshape(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n\ndf.columns = pd.Index(['X1','X2','X3'])\n\n\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n방법4 가 방법3 의 방식보다 컴퓨터가 이해하기 좋다. (=불필요한 에러를 방지할 수 있다.)\n\n## 방법3\ndf.columns, type(df.columns)  ## 내부적으로 list 타입을 pandas.core.indexes~~형태로 바꿔주긴 함.\n\n(Index(['X1', 'X2', 'X3'], dtype='object'), pandas.core.indexes.base.Index)\n\n\n\n['X1','X2','X3'], type(['X1','X2','X3'])\n\n(['X1', 'X2', 'X3'], list)\n\n\n\n처음부터 타입을 맞춰놓게 하는 게 좋다. (컴퓨터가 이해하기 명시적인 표현)\n\n\n## 방법4\npd.Index(['X1','X2','X3']), type(pd.Index(['X1','X2','X3']))\n\n(Index(['X1', 'X2', 'X3'], dtype='object'), pandas.core.indexes.base.Index)\n\n\n\n\n행의 이름 부여\n- 방법1: 중첩 dict이면 nested dic의 key가 알아서 행의 이름으로 된다.\n\n바깥쪽 딕셔너리의 키는 컬럼이름으로, 안쪽 딕셔너리의 키는 로우이름으로 들어간다.\n\n\npd.DataFrame({'att':{'guebin':30, 'iu':40, 'hynn':50} , 'mid':{'guebin':5, 'iu':45, 'hynn':90}})\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\nguebin\n30\n5\n\n\niu\n40\n45\n\n\nhynn\n50\n90\n\n\n\n\n\n\n\n- 방법2: pd.DataFrame()의 index옵션 이용\n\npd.DataFrame({'att':[30,40,50] , 'mid':[5,45,90]}, index=['guebin','iu','hynn'])\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\nguebin\n30\n5\n\n\niu\n40\n45\n\n\nhynn\n50\n90\n\n\n\n\n\n\n\n- 방법3: df.index에 덮어씌움.\n\ndf=pd.DataFrame({'att':[30,40,50] , 'mid':[5,45,90]})\ndf\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n5\n\n\n1\n40\n45\n\n\n2\n50\n90\n\n\n\n\n\n\n\n\ndf.index = pd.Index(['guebin','iu','hynn']) ## 좋은 코드!\n#df.index = ['guebin','iu','hynn'] &lt;- 이것도 실행 되기는 된다.\ndf\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\nguebin\n30\n5\n\n\niu\n40\n45\n\n\nhynn\n50\n90\n\n\n\n\n\n\n\n- 방법4: df.set_index()를 이용하여 덮어씌운다.\n\ndf=pd.DataFrame({'att':[30,40,50] , 'mid':[5,45,90]})\ndf\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n5\n\n\n1\n40\n45\n\n\n2\n50\n90\n\n\n\n\n\n\n\n\ndf.set_index(pd.Index(['guebin','iu','hynn'])) # set_index 메소드 이용\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\nguebin\n30\n5\n\n\niu\n40\n45\n\n\nhynn\n50\n90\n\n\n\n\n\n\n\n(주의) 아래는 에러가 난다.\n\ndf.set_index(['guebin','iu','hynn'])\n\nKeyError: \"None of ['guebin', 'iu', 'hynn'] are in the columns\"\n\n\n\ndf.set_index([['guebin','iu','hynn']]) # 꺽쇠를 한번 더 넣어주면 에러를 피할수 있다. \n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\nguebin\n30\n5\n\n\niu\n40\n45\n\n\nhynn\n50\n90\n\n\n\n\n\n\n\n\n\n자료형, len, shape, for문의 반복변수\n\ndf = pd.DataFrame({'att':[30,40,50],'mid':[5,45,90]})\ndf\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n5\n\n\n1\n40\n45\n\n\n2\n50\n90\n\n\n\n\n\n\n\n- type\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n- len\n\nlen(df) # row의 개수\n\n3\n\n\n- shape\n\ndf.shape\n\n(3, 2)\n\n\n- for문의 반복변수\n\nfor k in df:\n    print(k) # 딕셔너리 같죠?\n\natt\nmid\n\n\n\nfor k in {'att':[30,40,50],'mid':[5,45,90]}:\n    print(k)\n\natt\nmid\n\n\n\n\npd.Series\n- 2차원 ndarray가 데이터프레임에 대응한다면 1차원 ndarray는 pd.Series에 대응한다.\n\na=pd.Series(np.random.randn(10))\na\n\n0   -0.015761\n1    0.793164\n2   -0.194785\n3   -1.704138\n4    0.196202\n5   -0.542479\n6    0.134923\n7   -1.151843\n8    0.567016\n9    2.469013\ndtype: float64\n\n\n\ntype(a)\n\npandas.core.series.Series\n\n\n\nlen(a)\n\n10\n\n\n\na.shape\n\n(10,)\n\n\n\nfor value in a:\n    print(value)\n\n-0.01576052104052408\n0.7931636561267669\n-0.19478516128697446\n-1.7041378729481649\n0.19620173234455546\n-0.542479066364815\n0.13492305158609827\n-1.1518431416352932\n0.5670160023697828\n2.4690128371679556\n\n\n\nfor value in np.random.randn(10):\n    print(value)\n\n-0.0864801362204059\n-0.9294913581613311\n-0.4818729848296065\n2.1539740078272693\n0.5075567770278344\n0.6907204209585092\n0.2885924769916613\n-0.5636921329605091\n-0.9741967151982581\n1.8705475972066663"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-22-3wk-2.out.html",
    "href": "posts/1_IP2022/01_자료형/2023-03-22-3wk-2.out.html",
    "title": "03wk-2: 파이썬의 자료형 (5) – O",
    "section": "",
    "text": "슬기로운 튜플 사용법."
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-22-3wk-2.out.html#튜플을-왜-쓸까-1",
    "href": "posts/1_IP2022/01_자료형/2023-03-22-3wk-2.out.html#튜플을-왜-쓸까-1",
    "title": "03wk-2: 파이썬의 자료형 (5) – O",
    "section": "튜플을 왜 쓸까? (1)",
    "text": "튜플을 왜 쓸까? (1)\n- 책의 설명 (이 설명이 꼭 파이썬에 한정되는 것은 아님. 모든 언어에 존재하는 불변형 객체에 적용가능한 설명)\n\n실수방지\n빠르다, 다중작업에 유리하다, 여러사람과 작업하기에 유리하다, 깊은복사/얕은복사시 원하지않는 오류(side effect이라고 함)를 방지할 수 있다, 메모리관리에도 유리함…\n느낌: 불변형은 기능제한이 있는데 가볍고 빠른, 가변형은 기능은 풍부하지만 약간 느리고 무거운 느낌임 (불변형:라면사리, 가변형:라면)"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-22-3wk-2.out.html#슬기로운-튜플사용-starstarstarstarstar",
    "href": "posts/1_IP2022/01_자료형/2023-03-22-3wk-2.out.html#슬기로운-튜플사용-starstarstarstarstar",
    "title": "03wk-2: 파이썬의 자료형 (5) – O",
    "section": "슬기로운 튜플사용 (\\(\\star\\star\\star\\star\\star\\))",
    "text": "슬기로운 튜플사용 (\\(\\star\\star\\star\\star\\star\\))\n- 예제: 여러변수를 동시에 출력하고 싶을 경우 (다중출력?)\n변수를 아래와 같이 선언하였다고 하자.\n\na=1\nb=2\nc=3\n\n선언된 값을 확인하려면?\n\na\n\n1\n\n\n\nb\n\n2\n\n\n\nc\n\n3\n\n\n튜플을 이용하면?\n\na,b,c # 괄호하나 생략하는것이 이렇게 편하다..\n\n(1, 2, 3)\n\n\n- 예제: 다중할당1 (여러개의 변수를 동시에 선언하고 싶을 경우)\n\nname, age, sex, height, weight = 'Tom', 20, 'M', 180, 70 \n\n\nname, age, sex, height, weight\n\n('Tom', 20, 'M', 180, 70)\n\n\n\nheight\n\n180\n\n\n- 예제: 다중할당2, 위도와 경도\n\ncoor = (37,127) # 서울 \ncoor\n\n(37, 127)\n\n\n\nlat, long = coor\n\n\nlat \n\n37\n\n\n\nlong \n\n127\n\n\n- 잠깐만: 다중할당은 꼭 튜플에서만 가능한가?\n그건 아니다…\n\n[x,y,z] = [1,2,3] \nx,y,z # 다중출력 \n\n(1, 2, 3)\n\n\n\n[x,y] = 'hi'\nx,y \n\n('h', 'i')\n\n\n튜플과 같이 사용하면 가독성이 극대화 (그래서 다중할당은 거의 튜플과 세트로 사용함)\n\nx,y,z = 1,2,3\nx,y,z # 다중출력 \n\n(1, 2, 3)\n\n\n\nx,y = 'hi'\nx,y \n\n('h', 'i')\n\n\n- 예제: 임시변수 사용없이 두 변수의 값을 교환\n\na=10\nb=20\n\n\na,b = b,a \n\n\na\n\n20\n\n\n\nb\n\n10\n\n\n- 예제: for문과 튜플\n\nlst = [['guebin', 202112345, 'M'],\n       ['iu',202254321, 'F'],\n       ['hodong', 202011223, 'M']]\nlst\n\n[['guebin', 202112345, 'M'],\n ['iu', 202254321, 'F'],\n ['hodong', 202011223, 'M']]\n\n\n\nfor name,studentid,sex in lst: \n    print(name,sex)\n\nguebin M\niu F\nhodong M\n\n\n- 예제: for문과 튜플, dummy variable _\n\nfor name,studentid,sex in lst: \n    print(studentid)\n\n202112345\n202254321\n202011223\n\n\n\nfor _,studentid,_ in lst: \n    print(studentid)\n\n202112345\n202254321\n202011223\n\n\n\nfor _,_,sex in lst: \n    print(sex)\n\nM\nF\nM\n\n\n\nfor name,_,sex in lst: \n    print(name,sex)\n\nguebin M\niu F\nhodong M\n\n\n\nfor name,_  in lst: \n    print(name)\n\nValueError: too many values to unpack (expected 2)\n\n\n\nfor name,*args  in lst: \n    print(name)\n\nguebin\niu\nhodong\n\n\n- 예제: 튜플과 언패킹연산자 *\n\nhead, body, *tail = range(1,11) \nhead, body, tail\n\n(1, 2, [3, 4, 5, 6, 7, 8, 9, 10])\n\n\n\nhead1,head2, *body, tail1,tail2,tail3 = range(1,11) \nhead1,head2, body, tail1,tail2,tail3 \n\n(1, 2, [3, 4, 5, 6, 7], 8, 9, 10)\n\n\n\n*head, body, tail = range(1,11) \nhead, body, tail\n\n([1, 2, 3, 4, 5, 6, 7, 8], 9, 10)\n\n\n(관찰)\n그러고 보니까..\nhead1,head2, body, tail1,tail2,tail3  = (1, 2, [3,4,5,6,7], 8, 9, 10)\nhead1,head2, *body, tail1,tail2,tail3   = (1, 2, 3,4,5,6,7, 8, 9, 10)\n이렇다는 거잖아?\n*를 붙이면 1차원 자료구조가 풀린다..?\n\n*[1,2,3]\n\nSyntaxError: can't use starred expression here (386627056.py, line 1)\n\n\n\nprint([1,2,3])\n\n[1, 2, 3]\n\n\n\nprint(*[1,2,3]) ## 이런 느낌이란 말이지..\n\n1 2 3"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-22-3wk-2.out.html#튜플을-왜-쓸까-2",
    "href": "posts/1_IP2022/01_자료형/2023-03-22-3wk-2.out.html#튜플을-왜-쓸까-2",
    "title": "03wk-2: 파이썬의 자료형 (5) – O",
    "section": "튜플을 왜 쓸까? (2)",
    "text": "튜플을 왜 쓸까? (2)\n- 책의 설명 (이 설명이 꼭 파이썬에 한정되는 것은 아님. 모든 언어에 존재하는 불변형 객체에 적용가능한 설명)\n\n실수방지\n빠르다, 다중작업에 유리하다, 여러사람과 작업하기에 유리하다, 깊은복사/얕은복사시 원하지않는 오류(side effect이라고 함)를 방지할 수 있다, 메모리관리에도 유리함…\n느낌: 불변형은 기능제한이 있는데 가볍고 빠른, 가변형은 기능은 풍부하지만 약간 느리고 무거운 느낌임 (불변형:라면사리, 가변형:라면)\n\n- 내 설명: 소괄화 생략할 수 있어서 쓰는거야\n\n튜플의 장점은 소괄호의 생략에 있음 (이것은 파이썬과 줄리아만 가능)\n소괄호생략 + 언패킹 \\(\\Rightarrow\\) 엄청난 가독성\n컴공과 사람들 의견: 튜플 + 언패킹 \\(\\Rightarrow\\) 엄청난 가독성 \\(\\Rightarrow\\) 충격 \\(\\Rightarrow\\) “파이썬 편하더라고요..”\n\n\ndef mycal(a,b):\n    return a+b, a-b, a*b, a/b  #여러개의 값을 리턴하는듯 보임. -&gt; 사실은 길이가 4인 튜플 1개를 리턴\n\n\nmycal(2,3)\n\n(5, -1, 6, 0.6666666666666666)\n\n\n\nlen(mycal(2,3))\n\n4\n\n\n\n_, _, mulrslt, _ = mycal(2,3) # 병렬할당 \n\n\nmulrslt\n\n6\n\n\n- 의문: 왜 튜플만 괄호를 생략할 수 있지?\n답이 없는 문제인데 답을 해보겠습니다.\n\n튜플을 먼저 만들고, 괄호를 생략하는 문법을 추가한것은 아닐것임\n원래 괄호없이 컴마만 대충찍어서 선언가능한 아주간단한 타입의 벡터형을 만들고 싶었을 것임.\n왜? 괄호없는 벡터를 만들고, 언패킹을 사용하면 여러가지 구문들이 엄청나게 간단해짐.\n컴마컴마로 선언하는 벡터는 한 두번 쓰고 버리는 경우가 많으며 대부분 이름도 필요없음 \\(\\to\\) 원소에 접근해서 sorting하여 순서를 바꾸고 싶다던가 원소를 추가할 이유가 없음 \\(\\to\\) 비싼 가변형으로 만들 이유가 없다는 것..\n우리가 필요한 것: 데이터가 벡터의 형태로 모여있기만 하면 된다!"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "",
    "text": "인덱싱고급 (스트라이딩), if문이 포함된 컴프리헨션"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#인덱싱고급-스트라이딩",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#인덱싱고급-스트라이딩",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "인덱싱고급 (스트라이딩)",
    "text": "인덱싱고급 (스트라이딩)\n- 스트라이딩 [start:end:step]\n\nlst = list('abcdefghijk')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k']\n\n\n\nlst[0:9:3]\n\n['a', 'd', 'g']\n\n\n- 생략\n\nlst[0:9]\n#lst[0:9:]\n#lst[0:9:1]\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\n\n\n\nlst[0::3]\n\n['a', 'd', 'g', 'j']\n\n\n\nlst[:8:3]\n\n['a', 'd', 'g']\n\n\n- 예제1: 짝수/홀수 원소 추출\n아래와 같은 문자열이 있다고 하자.\n\nlst = list('abcdefghijk')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k']\n\n\nindex = 0,2,4, ... 에 해당하는 원소를 출력하라.\n\nlst[::2]\n\n['a', 'c', 'e', 'g', 'i', 'k']\n\n\nindex = 1,4,7 ... 에 해당하는 원소를 출력하라.\n\nlst[1::3]\n\n['b', 'e', 'h', 'k']\n\n\n- 예제2: 세로로..\n\n(예제2를 위한 예비학습) 문자열에서 \\n을 출력하면 출력시 줄바꿈이 일어난다.\n\nprint('1행\\n2행\\n3행')\n\n1행\n2행\n3행\n\n\n예비학습 끝\n\n아래와 같은 문자열이 있다고 하자.\n\ntxt = '너같이사랑스럽고\\n또예쁘고도멋지고\\n속훤히보이는너알\\n았어그동안고마웠\\n지정말정말사랑해'\nprint(txt)\n\n너같이사랑스럽고\n또예쁘고도멋지고\n속훤히보이는너알\n았어그동안고마웠\n지정말정말사랑해\n\n\n위 문자열을 세로로 읽는 코드를 작성하라. (9칸씩 점프하면서 읽으면 된다)\n(풀이)\n\ntxt[::9]\n\n'너또속았지'\n\n\n- step = -1 이면?\n\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k']\n\n\n\nlst[::-1]\n\n['k', 'j', 'i', 'h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n- 스트라이딩으로 step = -1 옵션 주기 vs 리스트의 .reverse() 메소드 이용하기\n관찰1: reverse 메소드는 리스트 자체를 변화시킴\n\nlst = list('abcdefgh')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst.reverse()  \nlst\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n관찰2: [::-1]는 리스트는 변화시키지 않음\n\nlst = list('abcdefgh')\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst[::-1]\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n- -step은 쓰기 까다롭다.\n(예제) 처음과 끝을 생략하지 않고 아래와 동일한 효과를 주는 코드를 만들어 보자.\n\nlst = list('abcdefgh')\nlst[::-1]\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n(풀이)\n결국 lst[?:?:-1]의 꼴에서 적당히 ?의 값을 채우면 된다. –&gt; 어려워\n\nlst\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n\n\n\nlst[::-1]\n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']\n\n\n\n\n\nNone\na\nb\nc\nd\ne\nf\ng\nh\nNone\n\n\n\n\n?\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n?\n\n\n\n\nlst[-1:-9:-1] \n\n['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#컴프리헨션-고급-if문이-포함된-컴프리헨션",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#컴프리헨션-고급-if문이-포함된-컴프리헨션",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "컴프리헨션 고급 (if문이 포함된 컴프리헨션)",
    "text": "컴프리헨션 고급 (if문이 포함된 컴프리헨션)\n- 예제: 제곱수중에서 12로 나누어 떨어지는 수만 원소로 가지는 리스트를 만들고 싶다.\n\n제곱수: 1,4,9,16,25,36, …\n12로 나누어 떨어지는 수: 36, …\n\n(예비학습1)\n\n12 % 4 # %는 나머지를 계산하는 연산자, 12를 4로 나누면 나머지가 0\n\n0\n\n\n\n12 % 5 # %는 나머지를 계산하는 연산자, 12를 5로 나누면 나머지가 2\n\n2\n\n\n(예비학습2)\n\na = 2 ## a에 2를 \"대입\" 하라. \n\n\na == 2 # a에 들어있는 값이 2인지 \"test\"하라.\n\nTrue\n\n\n\na == 3 # a에 들어있는 값이 3인지 \"test\"하라.\n\nFalse\n\n\n(예비학습3) if문\n\na= 3 \nif a%2 == 0: \n    a_is='even' ## a%2==0 이 true일 경우만 실행된다. \nelse:\n    a_is='odd' ## a%2==0 이 false일 경우만 실행된다. \n\n\na,a_is\n\n(3, 'odd')\n\n\n(풀이1) - 비어있는 리스트를 만들고 \\(\\to\\) for문 + if문\n\nlst = list()\nfor i in range(1,101): \n    if i**2 % 12 == 0:\n        lst.append(i**2)\n\n\nlst\n\n[36,\n 144,\n 324,\n 576,\n 900,\n 1296,\n 1764,\n 2304,\n 2916,\n 3600,\n 4356,\n 5184,\n 6084,\n 7056,\n 8100,\n 9216]\n\n\n(풀이2) - if문이 포함된 리스트컴프리헨션\n\n[i**2 for i in range(1,101) if i**2 % 12 == 0]\n\n[36,\n 144,\n 324,\n 576,\n 900,\n 1296,\n 1764,\n 2304,\n 2916,\n 3600,\n 4356,\n 5184,\n 6084,\n 7056,\n 8100,\n 9216]"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#함수고급-if문이-포함된-리턴",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#함수고급-if문이-포함된-리턴",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "함수고급 (if문이 포함된 리턴)",
    "text": "함수고급 (if문이 포함된 리턴)\n- 홀수/짝수를 판별하는 함수 만들기 1\n\ndef test(x): \n    if x % 2 == 0: \n        return 'even'\n    else:\n        return 'odd'\n\n\ntest(5)\n\n'odd'\n\n\n(사용)\n\n[test(l) for l in list(range(1,11))]\n\n['odd', 'even', 'odd', 'even', 'odd', 'even', 'odd', 'even', 'odd', 'even']\n\n\n- 홀수/짝수를 판별하는 함수 만들기 2\n\ndef test(x):\n    return 'even' if x % 2 == 0 else 'odd'\n\n\ntest(4)\n\n'even'\n\n\n(사용)\n\n[test(l) for l in list(range(1,11))]\n\n['odd', 'even', 'odd', 'even', 'odd', 'even', 'odd', 'even', 'odd', 'even']"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#len함수",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#len함수",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "len함수",
    "text": "len함수\n- 0차원 자료형은 len함수가 동작하지 않음\n\na=1 \nlen(a)\n\n\na=True\nlen(a)\n\n\na=3.14\nlen(a)\n\n\nnote: 이것이 어떠한 수학적인 의미를 가지거나 0차원의 본질적진리를 뜻하는 것은 안미. R에서는 1,3.14,TRUE의 길이가 1로 존재함.\n\n- 1차원 자료형은 len함수가 동작\n\na='guebin'\nlen(a)\n\n\na=[1,2,3,4,5,6]\nlen(a)\n\n\na=1,2,3,4,5,6 \nlen(a)\n\n\na=range(10)\nlen(a)\n\n- 길이가 1인 1차원 자료형과 0차원 자료형은 다른것임\n\na='g'\nlen(a)\n\n\na=[1]\nlen(a)\n\n\na=(1,)\nlen(a)\n\n\na=range(1)\nlen(a)\n\n- 길이가 0인 1차원 자료형도 존재함\n\na=''\nlen(a)\n\n\na=[]\nlen(a)\n\n\na=()\nlen(a)\n\n\na=range(0)\nlen(a)"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#intro-str-list-tuple-정리",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#intro-str-list-tuple-정리",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "intro: str, list, tuple 정리",
    "text": "intro: str, list, tuple 정리\n- str, list, tuple은 모두 시퀀스형이라는 공통점이 있다. \\(\\to\\) 원소의 위치번호로 인덱싱이 가능\n\nlst = [1,2,3,4]\n\n\nlst[0] # 위치번호=0\n\n\nlst[-1] # 위치번호=-1\n\n- str, list, tuple은 차이점도 존재함. 잠깐 정리해보자.\n시퀀스형의 카테고리\n\n컨테니어형: list, tuple\n균일형: str\n가변형: list\n불변형: tuple, str\n\n표로 정리하면\n\n\n\n\n컨테니어형\n균일형\n\n\n\n\n가변형\nlist\n.\n\n\n불변형\ntuple\nstr\n\n\n\n- 시퀀스형이 아닌 1차원 자료형도 있을까? 원소의 위치번호로 인덱싱이 불가능한 자료형\n- 왜 이런게 필요할까?\n\n벡터에서 원소를 뽑는것은 정보의 모임에서 정보를 검색하는 것과 같다.\n정보를 순서대로 나열한뒤에 그 순서를 이용하여 검색하는 방법은 유용하다.\n하지만 경우에 따라서는 키워드를 기억해서 그 키워드를 바탕으로 정보에 접근하는 방법이 유용할 수 있다.\n\n카카오톡 대화내용검색\n(상황1) 오늘아침에 와이프가 뭔가를 카톡으로 부탁했었음. 그런데 그 뭔가가 기억안남.\n(상황2) 개강전에 동료교수와 함께 저녁약속을 카톡으로 잡았었음. 그런데 그게 언제인지 기억안남.\n(상황3) 오늘아침 동료교수와 함께 점심약속을 카톡으로 잡았었음. 그런데 그 장소가 기억나지 않음.\n- 순서대로 정리된 자료를 검색할때는 시퀀스형이 유리하다. 그런데 키워드로 검색하고 싶을 경우는 딕셔너리 타입이 유리하다."
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#선언",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#선언",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "선언",
    "text": "선언\n- 방법1: 가장 일반적\n\ndct = {'guebin':49, 'hanni':80}\ndct\n\n- 방법2: dict() 이용\n\ndct = dict(guebin=49, hanni=80)\ndct\n\n- 방법3: 중첩된 리스트를 만든 뒤에 형태변환\n\n_lst = [['guebin',49],['hanni',80]]\n_lst \n\n\ndict(_lst)\n\n- 방법4: 중첩된 튜플을 만든 뒤에 형태변환\n\n_tpl = ('guebin',49), ('hanni',80)\n_tpl\n\n\ndict(_tpl)"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#원소추출",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#원소추출",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "원소추출",
    "text": "원소추출\n- 원소의 위치로 추출할 수 없고, key로 추출해야 한다.\n\ndct = {'guebin':49, 'hanni':80}\ndct\n\nguebin의 점수를 추출하고 싶다면?\n\ndct['guebin']\n\n- 만약에 dict가 아니라 list로 정보를 저장했다면?\n(예제) 아래와 같은 리스트에서 guebin의 점수를 추출하고 싶다면?\n\nlst=[['guebin',49],['hanni',80]]\nlst\n\n(풀이1)\n\nlst[0][1] # guebin의 점수를 출력하란 의미\n\n(풀이2) – 진짜 최악\n\n[lst[i][1] for i in range(len(lst)) if lst[i][0] == 'guebin']\n\n(풀이3) – 덜 최악\n\n[score for name,score in lst if name == 'guebin']\n\n- ’guebin’의 점수를 추출하는 코드 비교\n\ndct['guebin'] # 코드1: 단순하고, 가독성있음\n\n\nlst[0][1] # 코드2: 단순하지만, 가독성이 있는건 아님\n\n\n[lst[i][1] for i in range(len(lst)) if lst[i][0] =='guebin'] # 코드3: 단순하지도 않고, 가독성도 없음.\n\n\n[score for name,score in lst if name=='guebin' ] # 코드4: 단순하지 않지만, 가독성은 있음"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#원소추가-변경-삭제",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#원소추가-변경-삭제",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "원소추가, 변경, 삭제",
    "text": "원소추가, 변경, 삭제\n\ndct={'guebin':49, 'hanni':80}\ndct\n\n- 원소에 접근: guebin의 점수 출력\n\ndct['guebin']\n\n- 추가: hynn학생의 점수를 추가\n\ndct['hynn'] = 99\n\n\ndct\n\n- 변경: hanni의 점수를 변경\n\ndct['hanni'] = 100 \n\n\ndct\n\n- 삭제\n(방법1)\n\ndct={'guebin':49, 'hanni':80, 'hynn':99}\ndel dct['guebin']  \ndct\n\n(방법2)\n\ndct={'guebin':49, 'hanni':80, 'hynn':99} \ndct.pop('guebin')\n\n\ndct\n\n- 참고로 리스트였다면 이러한 삭제작업역시 비효율적이었을 것임\n\nlst = [['guebin',49],['hanni',80],['hynn',99]] \nlst\n\nguebin의 점수를 삭제하려면?\n\n[[name,score] for name,score in lst if name != 'guebin']"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#연산",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#연산",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "연산",
    "text": "연산\n- 하나있어요..\n\ndct = {'guebin':49, 'hanni':80} \ndct\n\n\n'guebin' in dct\n\n\n'hanni' in dct\n\n\n'hynn' in dct\n\n- in은 사실 다른자료형도 가능했음\n(관찰1)\n\n'a' in 'guebin' \n\n\n'b' in 'guebin' \n\n\n'c' in 'guebin' \n\n(관찰2)\n\ntpl = 1,2,3 \ntpl\n\n\n1 in tpl\n\n\n4 in tpl\n\n(관찰3)\n\nlst = [['guebin',49],['hanni',80],['hynn',99]] \nlst\n\n\n['guebin',49] in lst\n\n- in연산자가 dict형에 사용되면 key를 기준으로 True, False를 판단한다."
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#딕셔너리-특수기능",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#딕셔너리-특수기능",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "딕셔너리 특수기능",
    "text": "딕셔너리 특수기능\n(pop)\n\ndct = {'guebin':49, 'hanni':80} \ndct.pop('hanni')\ndct\n\n(get)\n\ndct = {'guebin':49, 'hanni':80} \ndct\n\n\ndct.get('guebin') \n\n아래와 같은 기능\n\ndct['guebin']\n\n미묘한 차이점이 존재함\n\ndct['hynn'] # hynn이 없어서 키에러 출력, 그런 key는 없다.. \n\n\ndct.get('hynn') # hynn이 없으면 아무것도 출력안함 \n\n(keys,values,items)\n- .keys()는 딕셔너리의 키를 리턴한다.\n\ndct = {'guebin':49, 'hanni':80} \ndct\n\n\n_keys=dct.keys()\n_keys\n\n\ntype(_keys) # 리턴된 자료형은 이상한것임\n\n\nlist(_keys) # 아무튼 그 이상한 자료형도 리스트화 가능 \n\n- .values()는 딕셔너리의 값들을 리턴한다.\n\n_values = dct.values()\n_values \n\n\ntype(_values)\n\n\nlist(_values)\n\n- .items()는 딕셔너리의 (키,값)을 리턴한다.\n\n_items = dct.items()\n_items \n\n\ntype(_items)\n\n\nlist(_items)"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#for문과-dict-star",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#for문과-dict-star",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "for문과 dict (\\(\\star\\))",
    "text": "for문과 dict (\\(\\star\\))\n\ndct = {'guebin': 49, 'hanni': 80}\ndct\n\n{'guebin': 49, 'hanni': 80}\n\n\n(예시1)\n\nfor k in dct.keys():\n    print(k)\n\nguebin\nhanni\n\n\n\nfor k in dct:\n    print(k)\n\nguebin\nhanni\n\n\n\n딕셔너리 그자체도 for문에 넣을 수 있다.\nk에는 value가 삭제되어 들어간다. (즉 key만)\n결과를 보면 dct 대신에 dct.keys()와 list(dct)를 넣었을때와 결과가 같다.\n\n\nNote: list(dct) 하면 key만 리턴된다.\n\n(예시2)\n\nfor v in dct.values():\n    print(v)\n\n49\n80\n\n\n(예시3)\n\nfor i in dct.items():\n    print(i)\n\n('guebin', 49)\n('hanni', 80)\n\n\n(예시4)\n\nfor k,v in dct.items():\n    print(k,v)\n\nguebin 49\nhanni 80\n\n\n(예시5) – {}의 중간고사 점수는 {}점 입니다.\n\nfor name,score in dct.items():\n    print('{}의 중간고사 점수는 {}점 입니다.'.format(name,score))\n\nguebin의 중간고사 점수는 49점 입니다.\nhanni의 중간고사 점수는 80점 입니다."
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#dict에서-key혹은-value만-뽑아내기",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#dict에서-key혹은-value만-뽑아내기",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "dict에서 key혹은 value만 뽑아내기",
    "text": "dict에서 key혹은 value만 뽑아내기\n- 예제: 아래의 dict에서 key만 뽑아내고 싶다.\n\ndct = {'guebin':49, 'hanni':80} \n\n(풀이1)\n\nlist(dct)\n\n['guebin', 'hanni']\n\n\n(풀이2)\n\nlist(dct.keys())\n\n['guebin', 'hanni']\n\n\n(풀이3)\n\n[k for k in dct]\n\n['guebin', 'hanni']\n\n\n(풀이4)\n\n[k for k,v in dct.items()]\n\n['guebin', 'hanni']\n\n\n- 예제: 아래의 dict에서 value만 뽑아내고 싶다.\n\ndct = {'guebin':49, 'hanni':80} \n\n(풀이1)\n\nlist(dct.values())\n\n[49, 80]\n\n\n(풀이2)\n\n[dct[k] for k in dct]\n\n[49, 80]\n\n\n(풀이3)\n\n[v for v in dct.values()]\n\n[49, 80]\n\n\n(풀이4)\n\n[v for k,v in dct.items()]\n\n[49, 80]"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#바꿔치기-1",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#바꿔치기-1",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "바꿔치기 (1)",
    "text": "바꿔치기 (1)\n- 예제1: 아래와 같은 리스트가 있다고 하자.\n\nlst = list('abcd'*2)\nlst\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd']\n\n\n아래의 규칙에 의하여 lst의 각 원소의 값을 바꾸고 싶다고 하자. 이를 구현하는 코드를 작성하라.\n\n\n\n변환전\n변환후\n\n\n\n\n‘a’\n[1,0,0,0]\n\n\n‘b’\n[0,1,0,0]\n\n\n‘c’\n[0,0,1,0]\n\n\n‘d’\n[0,0,0,1]\n\n\n\nhint: 아래의 dct를 이용할 것\n\nlst = list('abcd'*2)\nlst\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd']\n\n\n\ndct = {'a':[1,0,0,0], 'b':[0,1,0,0], 'c':[0,0,1,0], 'd':[0,0,0,1]}\ndct\n\n{'a': [1, 0, 0, 0], 'b': [0, 1, 0, 0], 'c': [0, 0, 1, 0], 'd': [0, 0, 0, 1]}\n\n\n(풀이)\n\n[dct[x] for x in lst]\n\n[[1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 0, 1],\n [1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 0, 1]]\n\n\n- 예제2: 예제1을 역변환하라.\n\ndct = {'a':[1,0,0,0], 'b':[0,1,0,0], 'c':[0,0,1,0], 'd':[0,0,0,1]}\n\n\nlst= [[1, 0, 0, 0],\n      [0, 1, 0, 0],\n      [0, 0, 1, 0],\n      [0, 0, 0, 1],\n      [1, 0, 0, 0],\n      [0, 1, 0, 0],\n      [0, 0, 1, 0],\n      [0, 0, 0, 1]]\nlst \n\n[[1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 0, 1],\n [1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 0, 1]]\n\n\n\nl = lst[0]\nl\n\n[1, 0, 0, 0]\n\n\n\n[k for k,v in dct.items() if v==l]\n\n['a']\n\n\n\n[k for l in lst for k,v in dct.items() if v==l]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd']\n\n\n(풀이)\n\n[x for l in lst for x,y in dct.items() if y == l]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd']"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#바꿔치기-2",
    "href": "posts/1_IP2022/01_자료형/2023-03-27-4wk-1.out.html#바꿔치기-2",
    "title": "04wk-1: 파이썬의 자료형 (6) – O",
    "section": "바꿔치기 (2)",
    "text": "바꿔치기 (2)\n- 예제1: 아래와 같은 리스트를 고려하자.\n\nlst = ['딸기','사과','바나나','딸기','사과','오토바이','자동차','버스','기차','오토바이','자동차']\n\n다음의 맵핑규칙에 따라서 위의 리스트의 원소를 바꾸어라.\n\n\n\n변환전\n변환후\n\n\n\n\n딸기\n과일\n\n\n사과\n과일\n\n\n바나나\n과일\n\n\n오토바이\n탈것\n\n\n자동차\n탈것\n\n\n버스\n탈것\n\n\n기차\n탈것\n\n\n\n(풀이)\n\ndct = {'과일':['딸기','사과','바나나'], '탈것':['오토바이','자동차','버스','기차']}\ndct\n\n{'과일': ['딸기', '사과', '바나나'], '탈것': ['오토바이', '자동차', '버스', '기차']}\n\n\n\n[x for l in lst for x,y in dct.items() if l in y]\n\n['과일', '과일', '과일', '과일', '과일', '탈것', '탈것', '탈것', '탈것', '탈것', '탈것']"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html",
    "href": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html",
    "title": "02wk-2: 파이썬의 자료형 (3) – O",
    "section": "",
    "text": "list 기본/고급 내용, 문자열메서드 format, replace"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#선언",
    "href": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#선언",
    "title": "02wk-2: 파이썬의 자료형 (3) – O",
    "section": "선언",
    "text": "선언\n- 리스트의 선언\n\na= [1,2,3,22] \n\n- 비어있는 리스트의 선언\n\na= []\na\n\n[]\n\n\n\na= list()\na\n\n[]"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#연산",
    "href": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#연산",
    "title": "02wk-2: 파이썬의 자료형 (3) – O",
    "section": "연산",
    "text": "연산\n- 더하기연산\n\n[1,2]+[-3,4,5]\n\n[1, 2, -3, 4, 5]\n\n\n\n우리의 예상과 다른 결과가 나옴 \\(\\to\\) 파이썬은 R처럼 자체적으로 좋은 계산기능을 내장하고 있지 않음.\n\n- 브로드캐스팅과 같이 R에서는 당연히 가능했던 기능을 사용할 수 없음.\n\n[1,2,3,4,5] + 1\n\nTypeError: can only concatenate list (not \"int\") to list\n\n\n- 뺄셈은 정의되지 않음\n\na= [1,2,1,2]\na-[1,2]\n\nTypeError: unsupported operand type(s) for -: 'list' and 'list'\n\n\n- 곱하기는 정의가능\n\n[1,2]*3\n\n[1, 2, 1, 2, 1, 2]\n\n\n- 나눗셈은 정의되지 않음\n\n[1,2,1,2,1,2] /3\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'\n\n\n- 더하기와 곱하기는 원소의 추가와 반복추가를 의미하지만 그렇다고 해서 뺄셈과 나눗셈이 원소의 삭제를 의미하는것은 아님\n- 더하기와 곱하기가 원소의 추가와 반복추가를 의미하여 편리할때도 있긴하지만, 우리는 산술적인 +, * 를 원하는 경우도 있다. 이럴 경우는 어떻게 할 수 있을까?\n(예제)\n\na=[1,2]\nb=[3,4]\n\na+b = [4,6] 이 되도록 하려면?\n(풀이1)\n\n[a[0]+b[0],a[1]+b[1]]\n\n[4, 6]\n\n\n풀이가 가능한 이유? a,b는 리스트이지만 a[0], a[1], b[0], b[1] 은 각각 인트형임. 인트형은 + 연산이 가능했음.\n(풀이2)\nnumpy 패키지 (파이썬의 여러 수치연산들을 담당하는 라이브러리)\n\n이러한 벡터연산은 누구나 필요로 하는 연산임.\n내가 아니더라도 누군가가 프로그램화 해놓았을 것임.\n그 누군가가 자신이 만든 코드를 잘 정리하여 무료로 배포했을 수도 있음. (패키지를 배포한다고 표현)\n그 패키지를 우리는 가져와서 설치한뒤 사용하기만 하면된다.\n\n패키지를 설치하는 방법\n\n!pip install numpy # 최신버전을 설치함\n!conda install -c conda-forge numpy -y # 안전한 버전을 설치함\n\n설치된 패키지를 사용하는 방법\n\nimport numpy 한뒤에 numpy.??로 기능을 사용\nimport numpy as np 한뒤에 np.??로 기능을 사용\n\n\nimport numpy ## 설치한패키지를 쓰겠다고 선언함 \n\n\na=[1,2]\nb=[3,4]\n\n\naa = numpy.array(a)\nbb = numpy.array(b)\n\n\naa+bb\n\narray([4, 6])\n\n\n여러가지 연산 가능 (마치 R처럼 쓸 수 있음)\n\n2*aa\n\narray([2, 4])\n\n\n\n2*aa+1\n\narray([3, 5])\n\n\n\n2*aa+1+bb\n\narray([6, 9])\n\n\n(풀이3)\n\nimport numpy as np ## 설치한 numpy라는 패키지를 쓰겠음. 그런데 numpy말고 np라는 이름으로 쓰겠음\n\n\nnp.array(a)+np.array(b)\n\narray([4, 6])"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#인덱싱",
    "href": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#인덱싱",
    "title": "02wk-2: 파이썬의 자료형 (3) – O",
    "section": "인덱싱",
    "text": "인덱싱\n- str형과 동일한 방식\n\na=[11,22,33,44,55] # 0 -4 -3 -2 -1\n\n\na[-2:] # 끝의 2개의 원소를 뽑음 \n\n[44, 55]"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#콘테이너형-객체",
    "href": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#콘테이너형-객체",
    "title": "02wk-2: 파이썬의 자료형 (3) – O",
    "section": "콘테이너형 객체",
    "text": "콘테이너형 객체\n- 리스트의 원소는 int, float 따위만 가능한 것이 아니다. (리스트는 컨테이너형 객체이므로)\n\nlst = [1,3.14,True,'a',[1,2], \n       (1,2),{'name':'iu','age':27},{1,2,3}]\n\n\nlst\n\n[1, 3.14, True, 'a', [1, 2], (1, 2), {'name': 'iu', 'age': 27}, {1, 2, 3}]\n\n\n각 원소의 타입을 알아보자.\n\ntype(lst[0])\n\nint\n\n\n\ntype(lst[1])\n\nfloat\n\n\n\ntype(lst[2])\n\nbool\n\n\n\ntype(lst[3])\n\nstr\n\n\n\ntype(lst[4])\n\nlist\n\n\n\ntype(lst[5])\n\ntuple\n\n\n\ntype(lst[6])\n\ndict\n\n\n\ntype(lst[7])\n\nset\n\n\n- str은 컨테이너형이 아니다.\n\n'abcd'[2]\n\n'c'\n\n\n\nstr의 모든 원소는 문자임"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#가변객체",
    "href": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#가변객체",
    "title": "02wk-2: 파이썬의 자료형 (3) – O",
    "section": "가변객체",
    "text": "가변객체\n- 리스트는 원소를 수정할 수 있다. (리스트는 가변객체이므로)\n\na=[11,22,33]\na\n\n[11, 22, 33]\n\n\n\na[0]\n\n11\n\n\n\na[0]=111\n\n\na\n\n[111, 22, 33]\n\n\n- 원소수정은 당연한 기능같은데 이것이 불가능한 경우도 있다.\n(가능한경우)\n\na=['g','u','e','b','i','n']\na\n\n['g', 'u', 'e', 'b', 'i', 'n']\n\n\n\na[0]\n\n'g'\n\n\n\na[0]='G'\n\n\na\n\n['G', 'u', 'e', 'b', 'i', 'n']\n\n\n(불가능한경우)\n\na='guebin'\na\n\n'guebin'\n\n\n\na[0]\n\n'g'\n\n\n\na[0]='G'\n\nTypeError: 'str' object does not support item assignment"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#리스트의-원소-삭제",
    "href": "posts/1_IP2022/01_자료형/2023-03-15-2wk-2.out.html#리스트의-원소-삭제",
    "title": "02wk-2: 파이썬의 자료형 (3) – O",
    "section": "리스트의 원소 삭제",
    "text": "리스트의 원소 삭제\n(예제1) del을 이용한 원소삭제\n아래와 같이 문자로 된 리스트를 선언하자.\n\na=['g','u','e','b','i','n']\na\n\n['g', 'u', 'e', 'b', 'i', 'n']\n\n\n사실 더 쉽게 선언할 수 있음\n\nlist('guebin')\n\n['g', 'u', 'e', 'b', 'i', 'n']\n\n\n첫번째 원소를 삭제하고 싶다면?\n\ndel a[0]\na\n\n['u', 'e', 'b', 'i', 'n']\n\n\n이 상태에서 다시 첫번째 원소를 삭제한다면?\n\ndel a[0]\na\n\n['e', 'b', 'i', 'n']\n\n\n(예제2) pop을 이용한 원소삭제\n\na=list('guebin')\na\n\n['g', 'u', 'e', 'b', 'i', 'n']\n\n\n\na.pop(0)\n\n'g'\n\n\n\na\n\n['u', 'e', 'b', 'i', 'n']\n\n\n\na.pop(0)\n\n'u'\n\n\n\na\n\n['e', 'b', 'i', 'n']"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html",
    "href": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html",
    "title": "02wk-1: 파이썬의 자료형 (2) – O",
    "section": "",
    "text": "str 선언, 연산, 인덱싱, 특수기능들(.lower(), .upper())"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html#선언",
    "href": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html#선언",
    "title": "02wk-1: 파이썬의 자료형 (2) – O",
    "section": "선언",
    "text": "선언\n- 예시1\n\na='guebin'\n\n\na\n\n'guebin'\n\n\n- 예시2\n\na=\"guebin\"\n\n\na\n\n'guebin'"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html#연산",
    "href": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html#연산",
    "title": "02wk-1: 파이썬의 자료형 (2) – O",
    "section": "연산",
    "text": "연산\n- 더하기(+)연산\n\na='X'\nb='2'\n\n\nc=a+b\nc\n\n'X2'\n\n\n- 빼기(-)연산\n\na='X2'\nb='2'\na-b\n\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\n\n\n\n이런건 없다.\n\n- 곱하기(*)연산\n\na='X'\n\n\na+a+a\n\n'XXX'\n\n\n\na*3 # a*3 = a+a+a = 'X'+'X+'X'\n\n'XXX'\n\n\n아래도 가능하다.\n\n3*a\n\n'XXX'\n\n\n그리고 아래도 가능하다.\n\na='X'\nb=3 \na*b\n\n'XXX'\n\n\n대신에 의미상 맞지 않는 것은 수행되지 않고 에러가 난다.\n\na='X'\nb='Y'\na+b\n\n'XY'\n\n\n\na*b\n\nTypeError: can't multiply sequence by non-int of type 'str'\n\n\n- 나눗셈(/)연산\n\na='XX'\n\n\na/2\n\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\n\n이런건 없다.."
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html#인덱싱",
    "href": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html#인덱싱",
    "title": "02wk-1: 파이썬의 자료형 (2) – O",
    "section": "인덱싱",
    "text": "인덱싱\n- str은 하나의 벡터 문자가 여러개 있는 형태라고 생각하면 된다.\n\na='guebin'\n\n\na\n\n'guebin'\n\n\n\n6개의 칸에 글씨가 하나씩 들어가 있음.\n\n- 대괄호 []안에 숫자를 넣는 방식으로 벡터의 원소를 호출할 수 있다. (주의: 인덱스가 0부터 시작함)\n\na[0] #첫번째원소\n\n'g'\n\n\n\na[1] #두번째원소 \n\n'u'\n\n\n마지막원소는 -1로 호출할 수도 있다.\n\na[-1]\n\n'n'\n\n\n마지막에서 2번째 원소는 -2로 호출가능하다.\n\na[-2]\n\n'i'\n\n\n- 요약하면 아래와 같은 방식으로 호출가능함.\n\n\n\ng\nu\ne\nb\ni\nn\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n0\n-5\n-4\n-3\n-2\n-1\n\n\n\n\na[4]\n\n'i'\n\n\n\na[-2]\n\n'i'\n\n\n\na[-4]\n\n'e'\n\n\n- :을 이용하여 여러개의 원소를 호출할 수 있음.\n\na='guebin'\n\n\na[0:3] # a[0],a[1],a[2],a[3]이 아니라 a[0],a[1],a[2]까지만 뽑힌다. 즉 마지막의 3은 호출되지 않는다. \n\n'gue'\n\n\n\na[1:3] # a[1], a[2] 만 호출 // start=1,  stop=3 \n\n'ue'\n\n\nindex=1부터 시작해서 마지막원소까지 호출하려면?\n\na='guebin'\n\n\na[5] # guebin의 마지막원소 'n'이 출려 \n\n'n'\n\n\n\na[1:5] # 5는 포함되지 않으므로 틀림\n\n'uebi'\n\n\n\na[1:6] # 정답\n\n'uebin'\n\n\n안 헷갈리는 방법은 없을까? 생략한다.\n\na[1:]\n\n'uebin'\n\n\n- 생략의 응용1\n\na='k-pop' \na\n\n'k-pop'\n\n\n\na[2:5]\n\n'pop'\n\n\n\na[2:]\n\n'pop'\n\n\n- 생략의 응용2\n\na='k-pop'\na\n\n'k-pop'\n\n\n\na[0:2] # a[0],a[1]\n\n'k-'\n\n\n\na[:2] # a[0],a[1] \n\n'k-'\n\n\n- 생략의 응용3\n\na='k-pop'\na\n\n'k-pop'\n\n\n\na[0:5] # a[0],...,a[4]\n\n'k-pop'\n\n\n\na[:]\n\n'k-pop'"
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html#str-특수기능",
    "href": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html#str-특수기능",
    "title": "02wk-1: 파이썬의 자료형 (2) – O",
    "section": "str 특수기능",
    "text": "str 특수기능\n- 파이썬의 변수는 단순히 정보를 담는 그릇이 아니다. 유용한 기능을 제공하는 경우가 있다.\n\na='ABCD' # a라는 변수는 'ABCD'라는 정보를 담는 그릇의 역할만 하지 않고, 특화된 어떠한 기능도 제공한다. \na\n\n'ABCD'\n\n\n\na.lower() # a.lower()를 쓰면 a의 모든 문자를 소문자로 바꾸는 기능을 제공, lower(a)라고 읽자!\n\n'abcd'\n\n\n여기에서 lower()는 문자열에 특화된 기능임. 따라서 당연히 아래는 불가능\n\na=3.14\na.lower() # lower(a)\n\nAttributeError: 'float' object has no attribute 'lower'\n\n\n- 자료형에 특화된 기능(=함수)을 확인하는 방법? a.+ tab 으로 목록 확인 가능\n\na='guebin'\n\n\na.upper?\n\n\nSignature: a.upper()\nDocstring: Return a copy of the string converted to uppercase.\nType:      builtin_function_or_method\n\n\n\n\na.upper() # upper(a) \n\n'GUEBIN'\n\n\n\na.capitalize() # capitalize(a) \n\n'Guebin'\n\n\n\na='asdf'\n\n- 문자열에 대한 다른 내용들은 추후에 다루겠음.\n- 마음의눈: a.f() 형태를 읽는 팁\n\na.f()는 f(a)로 생각하면 편리함.\na.f(2)는 f(a,2)로 생각하면 편리함.\n이런점에서 R %&gt;% 연산자와 비슷하다고 생각할 수 있다. (약간 다르긴함)\n\n- 사실 .은 좀 더 다양한 상황에서 쓰일 수 있다. 변수이름.함수이름() 의 형태가 아니라\n\n패지키이름.함수이름()\n패키지이름.변수이름\n패키지이름.패키지이름.함수이름()\n…\n\n와 같이 다양한 형태가 가능하다. 근본적인 공통점은 .을 기준으로 상위개념.하위개념 으로 이해하는 것이 좋다."
  },
  {
    "objectID": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html#len",
    "href": "posts/1_IP2022/01_자료형/2023-03-13-2wk-1.out.html#len",
    "title": "02wk-1: 파이썬의 자료형 (2) – O",
    "section": "len",
    "text": "len\n- len함수 소개: 원소의 갯수를 알려주는 함수.\n\na='ABCD' \nlen(a)\n\n4\n\n\n- 참고: len은 0차원 변수형에서는 동작하지 않고 1차원 변수형에서만 동작한다.\n(0차원) len 함수가 동작하지 않음.\n\na=3.14\n\n\nlen(a)\n\nTypeError: object of type 'float' has no len()\n\n\n\nb=True\n\n\nlen(b)\n\nTypeError: object of type 'bool' has no len()\n\n\n(1차원) len 함수가 잘 동작함.\n\na='3.14'\nlen(a)\n\n4\n\n\n\nb=[1,2,3]\n\n\nlen(b)\n\n3"
  },
  {
    "objectID": "6_note.html",
    "href": "6_note.html",
    "title": "NOTE",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 15, 2023\n\n\nAutoTS\n\n\nJiyunLim \n\n\n\n\nSep 14, 2023\n\n\nAutoGluon TS\n\n\nJiyunLim \n\n\n\n\nSep 9, 2023\n\n\n연습장2\n\n\nJiyunLim \n\n\n\n\nSep 3, 2023\n\n\n연습장1\n\n\nJiyunLim \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2023 study blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n11wk-043: 아이스크림 판매량 / 배깅\n\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n11wk-042: Weighted_Data / 의사결정나무 weights\n\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n11wk-041: Medical Cost / 의사결정나무 max_feature,random_state\n\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n11wk-040: Medical Cost / 의사결정나무의 시각화\n\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\nA3: 개발환경의 변천사\n\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n10wk-039: 의사결정나무 Discussion\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n10wk-038: 아이스크림 – 의사결정나무 원리\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n10wk-037: 아이스크림 – 의사결정나무, max_depth\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n10wk-036: 애니메이션\n\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[Vis] pheatmap\n\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n08wk-supp: 중간점검\n\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n07wk-033: 취업(다중공선성) / 의사결정나무\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n07wk-032: 아이스크림(교호작용) / 의사결정나무\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n07wk-031: 체중감량(교호작용) / 의사결정나무\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n07wk-030: 아이스크림(교호작용) / 선형회귀\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n07wk-029: 체중감량(교호작용) / 회귀분석\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n07wk-028: 선형모형의 적\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n07wk-027: 아이스크림(이상치) / 회귀분석\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n07wk-035: 아이스크림(이상치) / 의사결정나무\n\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n07wk-034: 취업(오버피팅) / 의사결정나무\n\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[DV2023] supp-1: FIFA23 자료의 시각화\n\n\n\n\n\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[DV2023] 07wk-1: Pandas – lambda df:의 활용, MultiIndex의 이해, tidydata의 이해, melt/stack\n\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n6wk-2 데이터 다루기\n\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n06wk-026: 취업+각종영어점수, LassoCV\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n06wk-025: 취업+각종영어점수, Lasso\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n06wk-024: 취업+각종영어점수, RidgeCV\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n06wk-023: 취업+각종영어점수, Ridge\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n06wk-022: 취업+각종영어점수, 다중공선성\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n05wk-021: 취업+밸런스게임, 오버피팅\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n05wk-020: StandardScaler를 이용한 전처리\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n05wk-019: MinMaxScaler를 이용한 전처리\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[DV2023] 05wk-2: Pandas – transform column (꿀팁)\n\n\n\n\n\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n4wk-1 선형대수\n\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n04wk-018: Predictor 깊은 이해 + 기호정리\n\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n04wk-017: 취업, 로지스틱을 더 깊게\n\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n04wk-016: 타이타닉, 결측치처리+로지스틱\n\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n04wk-015: 결측치 처리, sklearn.impute\n\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n04wk-014: 결측치 시각화, msno\n\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[DV2023] 04wk-1: Pandas – 기본기능, missing, query, 할당, transform column\n\n\n\n\n\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[DL] 4wk. Numerical Computation and Machine Learning Basics\n\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n04wk-2: 파이썬의 자료형 (7) – O\n\n\n\n\n\n\n\n파이썬의 자료형\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2023\n\n\njiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n03wk-013: 타이타닉, 로지스틱\n\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n03wk-012: 취업, 로지스틱\n\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n03wk-011: Medical Cost, 회귀분석\n\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n03wk-010: 아이스크림(초코/바닐라), 회귀분석\n\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n03wk-009: 아이스크림, 회귀분석\n\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n04wk-1: 파이썬의 자료형 (6) – O\n\n\n\n\n\n\n\n파이썬의 자료형\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\njiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[DV2023] 03wk-2: Pandas\n\n\n\n\n\n\n\nPandas\n\n\nIndexing\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n3wk-1 그래프2\n\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n03wk-2: 파이썬의 자료형 (5) – O\n\n\n\n\n\n\n\n파이썬의 자료형\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n03wk-1: 파이썬의 자료형 (4) – O\n\n\n\n\n\n\n\n파이썬의 자료형\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2023\n\n\njiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[DL] 3wk. Applied Math and Machine Learning Basics (2)\n\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n02wk-2: 파이썬의 자료형 (3) – O\n\n\n\n\n\n\n\n파이썬의 자료형\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n02wk-1: 파이썬의 자료형 (2) – O\n\n\n\n\n\n\n\n파이썬의 자료형\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\nAutoTS\n\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n01wk-2: 파이썬의 자료형 (1) – O\n\n\n\n\n\n\n\n파이썬의 자료형\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\nAutoGluon TS\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n2wk-2 그래프\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n3wk-1\n\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n2wk-2 웹 기초\n\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n2wk 환경설정 및 파이썬 기초\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n02wk-008: 타이타닉, Autogluon (best_quality)\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-007: 타이타닉, Autogluon (Fsize,Drop)\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-006: 타이타닉, Autogluon (Fsize)\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-005: 타이타닉, Autogluon\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-004: 타이타닉, Alexis Cook의 코드\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-003: 타이타닉, 첫 제출\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n2wk 파이썬\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[DL] 2wk. Applied Math and Machine Learning Basics\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n타이타닉 튜토리얼\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n연습장2\n\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n01wk-002: 타이타닉, 데이터의 이해\n\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n딥러닝\n\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2023\n\n\njiyunLim\n\n\n\n\n\n\n  \n\n\n\n\ntitanic\n\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n연습장1\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n트랜스포머 (전력사용량 데이터)\n\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Transformere (Stock Price)\n\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\nAttention is all you need\n\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[Fourier] 퓨리에변환(detailed)\n\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[Fourier] 푸리에변환 코드실습 (블로그)\n\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[Fourier] 퓨리에변환4jy\n\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\n신록예찬\n\n\n\n\n\n\n  \n\n\n\n\n[IP2023] 13wk-1: 깊은복사와 얕은복사\n\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n[수리통계학] 추정 for JY\n\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\n신록예찬\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 11wk. MaxPool2D, Conv2D\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 10wk. Softmax / 다양한 평가지표\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 9wk. Likelihood function\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 중간고사\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 7wk. Piece-wise LR / Logistic Regression\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 6wk. 회귀모형 적합 with keras\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 5wk. optimizer를 이용한 최적화\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 4wk. 미분 / 경사하강법\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 3wk. 텐서플로우 intro2 (tf.GradientTape())\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 2wk. 텐서플로우 intro1 (tf.constant선언, tnp사용법)\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 1wk. 강의소개 및 단순선형회귀\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)\n\n\n\n\n\n\n\n통계와 시각화\n\n\nplotnine\n\n\n아이스크림을 많이 먹으면 걸리는 병\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2023\n\n\njiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n10wk-2 심슨의 역설\n\n\n\n\n\n\n\n통계와 시각화\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\njiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] Pandas 2단계\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] Pandas 1단계\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] Pandas 0단계\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[2022 EXAM] 2022 final exam\n\n\n\n\n\n\n\npython\n\n\nclass\n\n\ntest\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] class 10단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] class 09단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] class 08단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] class 07단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] class 06단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] class 05단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] Numpy 4단계(concat, stack)\n\n\n\n\n\n\n\nPython\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nsimultaneous equation\n\n\n\n\n\n\n\nR\n\n\nlinear algebra\n\n\nbasic\n\n\n\n\nimplementation with R\n\n\n\n\n\n\nFeb 19, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] class 04단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] class 03단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] class 02단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n[IP2022] class 01단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nJupyter\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n05wk-2\n\n\n\n\n\n\n\n훌륭한 시각화\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2022\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n05wk-1\n\n\n\n\n\n\n\nseaborn\n\n\nmatplotlib\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n[2021 EXAM] 2021 final exam solution\n\n\n\n\n\n\n\npython\n\n\nclass\n\n\ntest\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2022\n\n\nGuebinChoi\n\n\n\n\n\n\n  \n\n\n\n\n07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)\n\n\n\n\n\n\n\npandas\n\n\n통계와 시각화\n\n\n아이스크림을 많이 먹으면 걸리는 병\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2022\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] Quarto Blog 만들기\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2000\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\nKaggle 리눅스 세팅\n\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2000\n\n\nJiyunLim\n\n\n\n\n\n\nNo matching items"
  }
]