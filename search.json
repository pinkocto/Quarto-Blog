[
  {
    "objectID": "7_study.html",
    "href": "7_study.html",
    "title": "STUDY",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n \n\n\nTransformers for Timeseries\n\n\n \n\n\n\n\nSep 14, 2023\n\n\n2wk-2\n\n\nJiyunLim \n\n\n\n\nSep 13, 2023\n\n\n2wk-2\n\n\nJiyunLim \n\n\n\n\nSep 12, 2023\n\n\n2wk\n\n\nJiyunLim \n\n\n\n\nSep 11, 2023\n\n\n2wk\n\n\nJiyunLim \n\n\n\n\nAug 10, 2023\n\n\nTime Series Transformere (Stock Price)\n\n\nJiyunLim \n\n\n\n\nAug 9, 2023\n\n\nAttention is all you need\n\n\nJiyunLim \n\n\n\n\nJul 11, 2023\n\n\n[Fourier] 퓨리에변환(detailed)\n\n\nJiyunLim \n\n\n\n\nJun 25, 2023\n\n\n[Fourier] 푸리에변환 코드실습 (블로그)\n\n\nJiyunLim \n\n\n\n\nJun 23, 2023\n\n\n[Fourier] 퓨리에변환4jy\n\n\n신록예찬 \n\n\n\n\nJun 14, 2023\n\n\n[수리통계학] 추정 for JY\n\n\n신록예찬 \n\n\n\n\nFeb 19, 2023\n\n\nsimultaneous equation\n\n\njiyun Lim\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "1_ip2022.html",
    "href": "1_ip2022.html",
    "title": "IP2022",
    "section": "",
    "text": "This page is organized based on the contents of the Introduction to Python (2022-1) and lecture notes of Professor Guebin Choi of Jeonbuk National University.\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 21, 2023\n\n\n13wk-1: 깊은복사와 얕은복사\n\n\n최규빈 \n\n\n\n\nMar 14, 2023\n\n\nPandas 2단계\n\n\njiyun Lim\n\n\n\n\nMar 13, 2023\n\n\nPandas 1단계\n\n\njiyun Lim\n\n\n\n\nMar 12, 2023\n\n\nPandas 0단계\n\n\njiyun Lim\n\n\n\n\nFeb 27, 2023\n\n\n2022 final exam\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\nclass 6단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\nclass 9단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\nclass 8단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\nclass 10단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\nclass 7단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\nclass 5단계\n\n\njiyun Lim\n\n\n\n\nFeb 23, 2023\n\n\nNumpy 4단계(concat, stack)\n\n\njiyun Lim\n\n\n\n\nFeb 15, 2023\n\n\nclass 3단계\n\n\njiyun Lim\n\n\n\n\nFeb 15, 2023\n\n\nclass 1단계\n\n\njiyun Lim\n\n\n\n\nFeb 15, 2023\n\n\nclass 2단계\n\n\njiyun Lim\n\n\n\n\nFeb 15, 2023\n\n\nclass 4단계\n\n\njiyun Lim\n\n\n\n\nJun 9, 2022\n\n\n2021 final exam solution\n\n\nGuebinChoi \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "5_stbda2023.html",
    "href": "5_stbda2023.html",
    "title": "STBDA2023",
    "section": "",
    "text": "This page is organized based on the contents of the 2023-2 Special Topics in Big Data Analysis lectures and lecture notes of Professor Guebin Choi of Jeonbuk National University.\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 12, 2023\n\n\n02wk-007: 타이타닉, Autogluon (Fsize,Drop)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-006: 타이타닉, Autogluon (Fsize)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n[STBDA] Quarto Blog 만들기\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-003: 타이타닉, 첫 제출\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-008: 타이타닉, Autogluon (best_quality)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-004: 타이타닉, Alexis Cook의 코드\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-005: 타이타닉, Autogluon\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-008: 타이타닉, Autogluon (best_quality)\n\n\n최규빈 \n\n\n\n\nSep 10, 2023\n\n\n타이타닉 튜토리얼\n\n\nJiyunLim \n\n\n\n\nSep 5, 2023\n\n\n01wk-002: 타이타닉, 데이터의 이해\n\n\n최규빈 \n\n\n\n\nSep 4, 2023\n\n\ntitanic\n\n\nJiyunLim \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-13-pandas1.html",
    "href": "posts/1_IP2022/02_DataScience/2023-03-13-pandas1.html",
    "title": "Pandas 1단계",
    "section": "",
    "text": "데이터프레임 선언, 행\\(\\cdot\\)열 이름부여, 자료형, pd.Series"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-13-pandas1.html#pandas-공부-1단계",
    "href": "posts/1_IP2022/02_DataScience/2023-03-13-pandas1.html#pandas-공부-1단계",
    "title": "Pandas 1단계",
    "section": "pandas 공부 1단계",
    "text": "pandas 공부 1단계\n\nimport numpy as np\nimport pandas as pd\n\n\n데이터프레임 선언\n- 방법1: dictionary에서 만든다.\n\npd.DataFrame({'att':[30,40,50], 'mid':[50,60,70]}) # 리스트\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n50\n\n\n1\n40\n60\n\n\n2\n50\n70\n\n\n\n\n\n\n\n\npd.DataFrame({'att':(30,40,50),'mid':(50,60,70)}) # 튜플\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n50\n\n\n1\n40\n60\n\n\n2\n50\n70\n\n\n\n\n\n\n\n\npd.DataFrame({'att':np.array([30,40,50]),'mid':np.array([50,60,70])}) # numpy array\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n50\n\n\n1\n40\n60\n\n\n2\n50\n70\n\n\n\n\n\n\n\n- 방법2: 2차원 ndarray에서 만든다.\n\nnp.arange(2*3).reshape(2,3)\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\npd.DataFrame(np.arange(2*3).reshape(2,3))\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n\n\n열의 이름 부여\n- 방법1: 딕셔너리를 통하여 만들면 딕셔너리의 key가 자동으로 열의 이름이 된다.\n\npd.DataFrame({'att':np.array([30,40,50]), 'mid':np.array([50,60,70])})\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n50\n\n\n1\n40\n60\n\n\n2\n50\n70\n\n\n\n\n\n\n\n- 방법2: pd.DataFrame()의 옵션에 columns를 이용\n\npd.DataFrame(np.arange(2*3).reshape(2,3),columns=['X1','X2','X3'])\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n- 방법3: df.columns에 원하는 열이름 덮어씀 (1)\n\ndf=pd.DataFrame(np.arange(2*3).reshape(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n\ndf.columns = ['X1','X2','X3'] # columns 메소드 이용.\n\n\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n\ndf.columns, type(df.columns)\n\n(Index(['X1', 'X2', 'X3'], dtype='object'), pandas.core.indexes.base.Index)\n\n\n- 방법4: df.columns에 원하는 열이름 덮어씀 (2)\n\ndf=pd.DataFrame(np.arange(2*3).reshape(2,3))\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n\ndf.columns = pd.Index(['X1','X2','X3'])\n\n\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\n0\n0\n1\n2\n\n\n1\n3\n4\n5\n\n\n\n\n\n\n\n방법4 가 방법3 의 방식보다 컴퓨터가 이해하기 좋다. (=불필요한 에러를 방지할 수 있다.)\n\n## 방법3\ndf.columns, type(df.columns)  ## 내부적으로 list 타입을 pandas.core.indexes~~형태로 바꿔주긴 함.\n\n(Index(['X1', 'X2', 'X3'], dtype='object'), pandas.core.indexes.base.Index)\n\n\n\n['X1','X2','X3'], type(['X1','X2','X3'])\n\n(['X1', 'X2', 'X3'], list)\n\n\n\n처음부터 타입을 맞춰놓게 하는 게 좋다. (컴퓨터가 이해하기 명시적인 표현)\n\n\n## 방법4\npd.Index(['X1','X2','X3']), type(pd.Index(['X1','X2','X3']))\n\n(Index(['X1', 'X2', 'X3'], dtype='object'), pandas.core.indexes.base.Index)\n\n\n\n\n행의 이름 부여\n- 방법1: 중첩 dict이면 nested dic의 key가 알아서 행의 이름으로 된다.\n\n바깥쪽 딕셔너리의 키는 컬럼이름으로, 안쪽 딕셔너리의 키는 로우이름으로 들어간다.\n\n\npd.DataFrame({'att':{'guebin':30, 'iu':40, 'hynn':50} , 'mid':{'guebin':5, 'iu':45, 'hynn':90}})\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\nguebin\n30\n5\n\n\niu\n40\n45\n\n\nhynn\n50\n90\n\n\n\n\n\n\n\n- 방법2: pd.DataFrame()의 index옵션 이용\n\npd.DataFrame({'att':[30,40,50] , 'mid':[5,45,90]}, index=['guebin','iu','hynn'])\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\nguebin\n30\n5\n\n\niu\n40\n45\n\n\nhynn\n50\n90\n\n\n\n\n\n\n\n- 방법3: df.index에 덮어씌움.\n\ndf=pd.DataFrame({'att':[30,40,50] , 'mid':[5,45,90]})\ndf\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n5\n\n\n1\n40\n45\n\n\n2\n50\n90\n\n\n\n\n\n\n\n\ndf.index = pd.Index(['guebin','iu','hynn']) ## 좋은 코드!\n#df.index = ['guebin','iu','hynn'] &lt;- 이것도 실행 되기는 된다.\ndf\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\nguebin\n30\n5\n\n\niu\n40\n45\n\n\nhynn\n50\n90\n\n\n\n\n\n\n\n- 방법4: df.set_index()를 이용하여 덮어씌운다.\n\ndf=pd.DataFrame({'att':[30,40,50] , 'mid':[5,45,90]})\ndf\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n5\n\n\n1\n40\n45\n\n\n2\n50\n90\n\n\n\n\n\n\n\n\ndf.set_index(pd.Index(['guebin','iu','hynn'])) # set_index 메소드 이용\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\nguebin\n30\n5\n\n\niu\n40\n45\n\n\nhynn\n50\n90\n\n\n\n\n\n\n\n(주의) 아래는 에러가 난다.\n\ndf.set_index(['guebin','iu','hynn'])\n\nKeyError: \"None of ['guebin', 'iu', 'hynn'] are in the columns\"\n\n\n\ndf.set_index([['guebin','iu','hynn']]) # 꺽쇠를 한번 더 넣어주면 에러를 피할수 있다. \n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\nguebin\n30\n5\n\n\niu\n40\n45\n\n\nhynn\n50\n90\n\n\n\n\n\n\n\n\n\n자료형, len, shape, for문의 반복변수\n\ndf = pd.DataFrame({'att':[30,40,50],'mid':[5,45,90]})\ndf\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n0\n30\n5\n\n\n1\n40\n45\n\n\n2\n50\n90\n\n\n\n\n\n\n\n- type\n\ntype(df)\n\npandas.core.frame.DataFrame\n\n\n- len\n\nlen(df) # row의 개수\n\n3\n\n\n- shape\n\ndf.shape\n\n(3, 2)\n\n\n- for문의 반복변수\n\nfor k in df:\n    print(k) # 딕셔너리 같죠?\n\natt\nmid\n\n\n\nfor k in {'att':[30,40,50],'mid':[5,45,90]}:\n    print(k)\n\natt\nmid\n\n\n\n\npd.Series\n- 2차원 ndarray가 데이터프레임에 대응한다면 1차원 ndarray는 pd.Series에 대응한다.\n\na=pd.Series(np.random.randn(10))\na\n\n0   -0.015761\n1    0.793164\n2   -0.194785\n3   -1.704138\n4    0.196202\n5   -0.542479\n6    0.134923\n7   -1.151843\n8    0.567016\n9    2.469013\ndtype: float64\n\n\n\ntype(a)\n\npandas.core.series.Series\n\n\n\nlen(a)\n\n10\n\n\n\na.shape\n\n(10,)\n\n\n\nfor value in a:\n    print(value)\n\n-0.01576052104052408\n0.7931636561267669\n-0.19478516128697446\n-1.7041378729481649\n0.19620173234455546\n-0.542479066364815\n0.13492305158609827\n-1.1518431416352932\n0.5670160023697828\n2.4690128371679556\n\n\n\nfor value in np.random.randn(10):\n    print(value)\n\n-0.0864801362204059\n-0.9294913581613311\n-0.4818729848296065\n2.1539740078272693\n0.5075567770278344\n0.6907204209585092\n0.2885924769916613\n-0.5636921329605091\n-0.9741967151982581\n1.8705475972066663"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html",
    "href": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html",
    "title": "Numpy 4단계(concat, stack)",
    "section": "",
    "text": "Numpy array를 결합하는 기능들에 대해 알아보자. (np.concatenate, np.concat)\n\n\n\n- 기본예제\n\nimport numpy as np\n\n\na = np.array([1,2])\nb = -a\n\n\nnp.concatenate([a,b])\n\narray([ 1,  2, -1, -2])\n\n\n- 응용\n\na = np.array([1,2])\nb = -a\nc = np.array([3,4,5])\n\n\nnp.concatenate([a,b,c])\n\narray([ 1,  2, -1, -2,  3,  4,  5])\n\n\n\n여기까진 딱히 concatenate의 메리트가 없어보임\n리스트였다면 a+b+c하면 되는 기능이니까?\n\n- 2d array에 적용해보자.\n\na = np.arange(4).reshape(2,2)\nb = -a\n\n\na\n\narray([[0, 1],\n       [2, 3]])\n\n\n\nb\n\narray([[ 0, -1],\n       [-2, -3]])\n\n\n\nnp.concatenate([a,b])\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n\n위아래로 붙었네! 그럼 옆으로 붙이려면 어떻게 하지?\n\n- 옆으로 붙이려면?\n\nnp.concatenate([a,b], axis=1)\n\narray([[ 0,  1,  0, -1],\n       [ 2,  3, -2, -3]])\n\n\n- 위의 코드에서 axis=1 이 뭐지? axis=0,2 등을 치면 결과가 어떻게 될까?\n\nnp.concatenate([a,b],axis=0)\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n\n이건 그냥 np.concatenate([a,b])와 같다.\nnp.concatenate([a,b])는 np.concatenate([a,b],axis=0)의 생략버전이군?\n\n\nnp.concatenate([a,b],axis=2)\n\nAxisError: axis 2 is out of bounds for array of dimension 2\n\n\n\n이런건 없다.\n\n- axis의 의미가 뭔지 궁금함. 좀 더 예제를 살펴보자.\n\na = np.array(range(2*3*4)).reshape(2,3,4) # 3d array\na\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nb = -a\nb\n\narray([[[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=0)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=1)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b], axis=2)\n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\n이번에는 axis=2까지 된다?\n\n\nnp.concatenate([a,b], axis=3)\n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\naxis=3까지는 안된다?\n\n- 뭔가 나름의 방식으로 합쳐지는데 원리가 뭘까?\n(분석1) np.concatenate([a,b], axis=0)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=0).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\n첫번째 차원이 바뀌었다. \\(\\Rightarrow\\) 첫번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=0 (파이썬은 0부터 시작하니까!)\n\n(분석2) np.concatenate([a,b], axis=1)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n두번째 차원이 바뀌었다. \\(\\Rightarrow\\) 두번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=1\n\n(분석3) np.concatenate([a,b], axis=2)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n세번째 차원이 바뀌었다. \\(\\Rightarrow\\) 세번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=2\n\n(분석4) np.concatenate([a,b], axis=3)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=3).shape\n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\n네번째 차원이 없다. \\(\\Rightarrow\\) 세번째 축이 없다. \\(\\Rightarrow\\) axis=3으로 하면 에러가 난다.\n\n(보너스)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-1)\n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n마지막 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막 축이 바뀌었다. \\(\\Rightarrow\\) axis=-1\n\n(보너스2)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-2)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n마지막에서 2번째 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막에서 2번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=-2\n\n(보너스3)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-3)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-3).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\n마지막에서 3번째 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막에서 3번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=-3\n\n(보너스4)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-4)\n\nAxisError: axis -4 is out of bounds for array of dimension 3\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-4).shape\n\nAxisError: axis -4 is out of bounds for array of dimension 3\n\n\n\n마지막에서 4번째 차원은 없다. \\(\\Rightarrow\\) 마지막에서 4번째 축은 없다. \\(\\Rightarrow\\) axis=-4는 에러가 난다.\n\n- 0차원은 축이 없으므로 concatenate를 쓸 수 없다.\n\na = np.array(1)\nb = np.array(-1)\n\n\na.shape, b.shape\n\n((), ())\n\n\n\nnp.concatenate([a,b])\n\nValueError: zero-dimensional arrays cannot be concatenated\n\n\n이게 만약에 이렇게 바뀌면 1차원이니까 쓸 수 있다.\n\na = np.array([1])\nb = np.array([-1])\na.shape, b.shape\n\n((1,), (1,))\n\n\n\nnp.concatenate([a,b])\n\narray([ 1, -1])\n\n\n- 꼭 a,b가 같은 차원일 필요는 없다.\n\na = np.array(range(4)).reshape(2,2)\nb = np.array(range(2)).reshape(2,1)\n\n\nnp.concatenate([a,b], axis=1)\n\narray([[0, 1, 0],\n       [2, 3, 1]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=1).shape\n\n((2, 2), (2, 1), (2, 3))\n\n\n\n\n\n- 혹시 아래가 가능할까?\n\n\\((3,)\\) 결합 : \\((3,) \\Rightarrow (3,2)\\)\n\n\na = np.array([1,2,3])\nb = -a\n\n\na,b\n\n(array([1, 2, 3]), array([-1, -2, -3]))\n\n\n\na.shape, b.shape\n\n((3,), (3,))\n\n\n\nnp.concatenate([a,b], axis=1)\n\nAxisError: axis 1 is out of bounds for array of dimension 1\n\n\n\n불가능\n\n- 아래와 같이 하면 해결 가능\n\na = np.array([1,2,3]).reshape(3,1)\nb = -a\n\n\na.shape, b.shape\n\n((3, 1), (3, 1))\n\n\n\na,b\n\n(array([[1],\n        [2],\n        [3]]),\n array([[-1],\n        [-2],\n        [-3]]))\n\n\n\nnp.concatenate([a,b], axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n\n분석: \\((3) (3) \\Rightarrow (3,1),(3,1)\\Rightarrow (3,1) \\space \\tt{concat} \\space (3,1)\\)\n\n- 위의 과정을 줄여서 아래와 같이 할 수 있다.\n\na = np.array([1,2,3])\nb = -a\n\n\nnp.stack([a,b], axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n- 아래도 가능\n\nnp.stack([a,b],axis=0)\n\narray([[ 1,  2,  3],\n       [-1, -2, -3]])\n\n\n- 분석해보고 외우자\n(분석1)\n\na = np.array([1,2,3])\nb = -a\n\n\na.shape, b.shape, np.stack([a,b],axis=0).shape\n\n((3,), (3,), (2, 3))\n\n\n\n\\((3)(3) \\Rightarrow \\text{첫 위치에 축을 추가 (axis=0)} \\Rightarrow (1,3)(1,3) \\Rightarrow (2,3)\\)\n\n(분석2)\n\na = np.array([1,2,3])\nb = -a\n\n\na.shape, b.shape, np.stack([a,b],axis=1).shape\n\n((3,), (3,), (3, 2))\n\n\n\\((3)(3)\\Rightarrow \\text{두번째 위치에 축을 추가 (axis=1)} \\Rightarrow (3,1)(3,1) \\Rightarrow (3,2)\\)\n- 고차원예제\n\na = np.arange(3*4*5).reshape(3,4,5)\nb = -a\n\n\na.shape, b.shape\n\n((3, 4, 5), (3, 4, 5))\n\n\n\nnp.stack([a,b], axis=0).shape # (3,4,5) =&gt; (1,3,4,5) // 첫 위치에 축이 추가되고 스택\n\n(2, 3, 4, 5)\n\n\n\nnp.stack([a,b], axis=1).shape # (3,4,5) =&gt; (3,1,4,5) // 두번째 위치에 축이 추가되고 스택\n\n(3, 2, 4, 5)\n\n\n\nnp.stack([a,b], axis=2).shape # (3,4,5) =&gt; (3,4,1,5) // 세번째 위치에 축이 추가되고 스택\n\n(3, 4, 2, 5)\n\n\n\nnp.stack([a,b], axis=3).shape # (3,4,5) =&gt; (3,4,5,1) // 네번째 위치에 축이 추가되고 스택\n\n(3, 4, 5, 2)\n\n\n\nnp.stack([a,b], axis=-1).shape # axis=-1 &lt;=&gt; axis=3\n\n(3, 4, 5, 2)\n\n\n\nnp.stack([a,b], axis=-2).shape # axis=-2 &lt;=&gt; axis=2\n\n(3, 4, 2, 5)\n\n\nnp.concatenate 는 축의 총 개수를 유지하면서 결합, np.stack은 축의 개수를 하나 증가시키면서 결합"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html#np.concatenate",
    "href": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html#np.concatenate",
    "title": "Numpy 4단계(concat, stack)",
    "section": "",
    "text": "- 기본예제\n\nimport numpy as np\n\n\na = np.array([1,2])\nb = -a\n\n\nnp.concatenate([a,b])\n\narray([ 1,  2, -1, -2])\n\n\n- 응용\n\na = np.array([1,2])\nb = -a\nc = np.array([3,4,5])\n\n\nnp.concatenate([a,b,c])\n\narray([ 1,  2, -1, -2,  3,  4,  5])\n\n\n\n여기까진 딱히 concatenate의 메리트가 없어보임\n리스트였다면 a+b+c하면 되는 기능이니까?\n\n- 2d array에 적용해보자.\n\na = np.arange(4).reshape(2,2)\nb = -a\n\n\na\n\narray([[0, 1],\n       [2, 3]])\n\n\n\nb\n\narray([[ 0, -1],\n       [-2, -3]])\n\n\n\nnp.concatenate([a,b])\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n\n위아래로 붙었네! 그럼 옆으로 붙이려면 어떻게 하지?\n\n- 옆으로 붙이려면?\n\nnp.concatenate([a,b], axis=1)\n\narray([[ 0,  1,  0, -1],\n       [ 2,  3, -2, -3]])\n\n\n- 위의 코드에서 axis=1 이 뭐지? axis=0,2 등을 치면 결과가 어떻게 될까?\n\nnp.concatenate([a,b],axis=0)\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n\n이건 그냥 np.concatenate([a,b])와 같다.\nnp.concatenate([a,b])는 np.concatenate([a,b],axis=0)의 생략버전이군?\n\n\nnp.concatenate([a,b],axis=2)\n\nAxisError: axis 2 is out of bounds for array of dimension 2\n\n\n\n이런건 없다.\n\n- axis의 의미가 뭔지 궁금함. 좀 더 예제를 살펴보자.\n\na = np.array(range(2*3*4)).reshape(2,3,4) # 3d array\na\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nb = -a\nb\n\narray([[[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=0)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=1)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b], axis=2)\n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\n이번에는 axis=2까지 된다?\n\n\nnp.concatenate([a,b], axis=3)\n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\naxis=3까지는 안된다?\n\n- 뭔가 나름의 방식으로 합쳐지는데 원리가 뭘까?\n(분석1) np.concatenate([a,b], axis=0)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=0).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\n첫번째 차원이 바뀌었다. \\(\\Rightarrow\\) 첫번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=0 (파이썬은 0부터 시작하니까!)\n\n(분석2) np.concatenate([a,b], axis=1)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n두번째 차원이 바뀌었다. \\(\\Rightarrow\\) 두번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=1\n\n(분석3) np.concatenate([a,b], axis=2)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n세번째 차원이 바뀌었다. \\(\\Rightarrow\\) 세번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=2\n\n(분석4) np.concatenate([a,b], axis=3)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\na.shape, b.shape, np.concatenate([a,b], axis=3).shape\n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\n네번째 차원이 없다. \\(\\Rightarrow\\) 세번째 축이 없다. \\(\\Rightarrow\\) axis=3으로 하면 에러가 난다.\n\n(보너스)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-1)\n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n마지막 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막 축이 바뀌었다. \\(\\Rightarrow\\) axis=-1\n\n(보너스2)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-2)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n마지막에서 2번째 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막에서 2번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=-2\n\n(보너스3)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-3)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-3).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\n마지막에서 3번째 차원이 바뀌었다. \\(\\Rightarrow\\) 마지막에서 3번째 축이 바뀌었다. \\(\\Rightarrow\\) axis=-3\n\n(보너스4)\n\n# a = np.array(range(2*3*4)).reshape(2,3,4)\na = np.arange(2*3*4).reshape(2,3,4)\nb = -a\n\n\nnp.concatenate([a,b], axis=-4)\n\nAxisError: axis -4 is out of bounds for array of dimension 3\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=-4).shape\n\nAxisError: axis -4 is out of bounds for array of dimension 3\n\n\n\n마지막에서 4번째 차원은 없다. \\(\\Rightarrow\\) 마지막에서 4번째 축은 없다. \\(\\Rightarrow\\) axis=-4는 에러가 난다.\n\n- 0차원은 축이 없으므로 concatenate를 쓸 수 없다.\n\na = np.array(1)\nb = np.array(-1)\n\n\na.shape, b.shape\n\n((), ())\n\n\n\nnp.concatenate([a,b])\n\nValueError: zero-dimensional arrays cannot be concatenated\n\n\n이게 만약에 이렇게 바뀌면 1차원이니까 쓸 수 있다.\n\na = np.array([1])\nb = np.array([-1])\na.shape, b.shape\n\n((1,), (1,))\n\n\n\nnp.concatenate([a,b])\n\narray([ 1, -1])\n\n\n- 꼭 a,b가 같은 차원일 필요는 없다.\n\na = np.array(range(4)).reshape(2,2)\nb = np.array(range(2)).reshape(2,1)\n\n\nnp.concatenate([a,b], axis=1)\n\narray([[0, 1, 0],\n       [2, 3, 1]])\n\n\n\na.shape, b.shape, np.concatenate([a,b], axis=1).shape\n\n((2, 2), (2, 1), (2, 3))"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html#np.stack",
    "href": "posts/1_IP2022/02_DataScience/2023-02-23-numpy4.html#np.stack",
    "title": "Numpy 4단계(concat, stack)",
    "section": "",
    "text": "- 혹시 아래가 가능할까?\n\n\\((3,)\\) 결합 : \\((3,) \\Rightarrow (3,2)\\)\n\n\na = np.array([1,2,3])\nb = -a\n\n\na,b\n\n(array([1, 2, 3]), array([-1, -2, -3]))\n\n\n\na.shape, b.shape\n\n((3,), (3,))\n\n\n\nnp.concatenate([a,b], axis=1)\n\nAxisError: axis 1 is out of bounds for array of dimension 1\n\n\n\n불가능\n\n- 아래와 같이 하면 해결 가능\n\na = np.array([1,2,3]).reshape(3,1)\nb = -a\n\n\na.shape, b.shape\n\n((3, 1), (3, 1))\n\n\n\na,b\n\n(array([[1],\n        [2],\n        [3]]),\n array([[-1],\n        [-2],\n        [-3]]))\n\n\n\nnp.concatenate([a,b], axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n\n분석: \\((3) (3) \\Rightarrow (3,1),(3,1)\\Rightarrow (3,1) \\space \\tt{concat} \\space (3,1)\\)\n\n- 위의 과정을 줄여서 아래와 같이 할 수 있다.\n\na = np.array([1,2,3])\nb = -a\n\n\nnp.stack([a,b], axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n- 아래도 가능\n\nnp.stack([a,b],axis=0)\n\narray([[ 1,  2,  3],\n       [-1, -2, -3]])\n\n\n- 분석해보고 외우자\n(분석1)\n\na = np.array([1,2,3])\nb = -a\n\n\na.shape, b.shape, np.stack([a,b],axis=0).shape\n\n((3,), (3,), (2, 3))\n\n\n\n\\((3)(3) \\Rightarrow \\text{첫 위치에 축을 추가 (axis=0)} \\Rightarrow (1,3)(1,3) \\Rightarrow (2,3)\\)\n\n(분석2)\n\na = np.array([1,2,3])\nb = -a\n\n\na.shape, b.shape, np.stack([a,b],axis=1).shape\n\n((3,), (3,), (3, 2))\n\n\n\\((3)(3)\\Rightarrow \\text{두번째 위치에 축을 추가 (axis=1)} \\Rightarrow (3,1)(3,1) \\Rightarrow (3,2)\\)\n- 고차원예제\n\na = np.arange(3*4*5).reshape(3,4,5)\nb = -a\n\n\na.shape, b.shape\n\n((3, 4, 5), (3, 4, 5))\n\n\n\nnp.stack([a,b], axis=0).shape # (3,4,5) =&gt; (1,3,4,5) // 첫 위치에 축이 추가되고 스택\n\n(2, 3, 4, 5)\n\n\n\nnp.stack([a,b], axis=1).shape # (3,4,5) =&gt; (3,1,4,5) // 두번째 위치에 축이 추가되고 스택\n\n(3, 2, 4, 5)\n\n\n\nnp.stack([a,b], axis=2).shape # (3,4,5) =&gt; (3,4,1,5) // 세번째 위치에 축이 추가되고 스택\n\n(3, 4, 2, 5)\n\n\n\nnp.stack([a,b], axis=3).shape # (3,4,5) =&gt; (3,4,5,1) // 네번째 위치에 축이 추가되고 스택\n\n(3, 4, 5, 2)\n\n\n\nnp.stack([a,b], axis=-1).shape # axis=-1 &lt;=&gt; axis=3\n\n(3, 4, 5, 2)\n\n\n\nnp.stack([a,b], axis=-2).shape # axis=-2 &lt;=&gt; axis=2\n\n(3, 4, 2, 5)\n\n\nnp.concatenate 는 축의 총 개수를 유지하면서 결합, np.stack은 축의 개수를 하나 증가시키면서 결합"
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022_06_09_2021년_파이썬입문_기말고사_(풀이포함).html",
    "href": "posts/1_IP2022/04_시험/2022_06_09_2021년_파이썬입문_기말고사_(풀이포함).html",
    "title": "2021 final exam solution",
    "section": "",
    "text": "2021년 1학기 파이썬입문 기말고사 (풀이포함)\nLINK HERE!\n\n# 1. (20점)\nN사에서 게임유저들에게 여름방학 기념이벤트로 진명왕의 집판검이라는 이름의 아이템을 선물했다고 하자. 진명왕의 집판검은 총 5회에 걸쳐서 강화(upgrade)될 수 있데 강화의 성공확률은 10%라고 하자. 강화가 5번성공하면 더 이상 강화가 진행되지 않는다고 하자. (따라서 더 이상 강화시도를 하지 않아도 무방하다) 아래는 이 아이템에 강화를 진행하였을때 각 강화상태를 설명한 예시이다.\n\n\n\n시도횟수\n강화성공여부\n강화상태\n비고\n\n\n\n\n1\n강화실패\n+0 \\(\\to\\) +0\n강화실패로 인하여 강화상태 변화없음\n\n\n2\n강화성공\n+0 \\(\\to\\) +1\n강화성공으로 인한 강화상태 변화\n\n\n3\n강화실패\n+1 \\(\\to\\) +1\n강화실패로 인하여 강화상태 변화없음\n\n\n4\n강화성공\n+1 \\(\\to\\) +2\n강화성공으로 인한 강화상태 변화\n\n\n5\n강화성공\n+2 \\(\\to\\) +3\n강화성공으로 인한 강화상태 변화\n\n\n6\n강화성공\n+3 \\(\\to\\) +4\n강화성공으로 인한 강화상태 변화\n\n\n7\n강화실패\n+4 \\(\\to\\) +4\n강화실패로 인하여 강화상태 변화없음\n\n\n8\n강화성공\n+4 \\(\\to\\) +5\n모든 강화 성공\n\n\n9\n-\n+5 \\(\\to\\) +5\n더 이상 강화시도 하지 않음\n\n\n10\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\n강화는 하루에 한 번씩만 시도할 수 있으며 시도가능한 기간은 7월1일부터 8월31일까지로 한정되어 있다고 하자. 따라서 방학동안 유저들은 총 62번 시도를 할 수 있다. 방학이 끝난이후 100명 유저중 대략 몇명정도 +5 강화상태에 있겠는가? 파이썬을 통한 시뮬레이션을 활용하여 추론하라. (단, +5강화에 성공하지 못한 모든 유저는 반드시 하루에 한번 강화를 시도해야 한다고 가정하자.)\n(풀이1)\n\nimport numpy as np\nnp.random.seed(1)\nsum(np.random.binomial(n=62, p=0.1, size=10000)&gt;=5)/10000\n\n0.7514\n\n\n(풀이2)\n\nclass ExecutionSword():\n    def __init__(self,prob):\n        self.nuser=100000\n        self.prob=prob\n        self.attemptresult=None\n        self.upgradestate=pd.DataFrame({'day0':[0]*self.nuser})\n        self.failstate=pd.DataFrame({'day0':[0]*self.nuser})\n        self.ratio=0\n        self.day=0\n    def addday(self):\n        self.day=self.day+1            \n    def attempt(self):\n        self.attemptresult = np.random.binomial(n=1, p=self.prob, size=self.nuser)\n    def update(self):\n        # 강화상태 업데이트\n        self.upgradestate['day%s' % self.day] = np.minimum(5,self.upgradestate['day%s' % (self.day-1)]+self.attemptresult)\n        # 강화실패누적횟수 업데이트 \n        self.failstate['day%s' % self.day]=self.failstate['day%s' % (self.day-1)]+(self.attemptresult==0)*1\n        # 강화상태==5 or 강화상태==0 일 경우 강화실패누적횟수 초기화 \n        self.failstate['day%s' % self.day][self.upgradestate['day%s' % self.day]== 0]=0\n        self.failstate['day%s' % self.day][self.upgradestate['day%s' % self.day]== 5]=0\n    def reset(self):\n        # 실패횟수 = 2 인것을 찾아 index_ 에 저장 -&gt; index_ 에 해당하는 유저의 강화횟수와 실패횟수를 모두 0으로 초기화 \n        index_= self.failstate['day%s' % self.day]==2\n        self.failstate['day%s' % self.day][index_] = 0\n        self.upgradestate['day%s' % self.day][index_] = 0\n    def arrangeprob(self):\n        self.ratio=sum(self.upgradestate['day%s' % self.day]==5) / self.nuser\n        if self.ratio &gt; 0.5:\n            self.prob = 0.9\n\n\n# 1 \nimport pandas as pd\ns1=ExecutionSword(0.1)\nfor i in range(62):\n    s1.addday()\n    s1.attempt()\n    s1.update()\n\n\nsum(s1.upgradestate.day62==5)/s1.nuser\n\n0.75551\n\n\n\n\n# 2. (70점)\n강화성공확률을 40%로 수정한다. 강화에 누적2회 실패하면 강화상태가 초기화 된다고 하자. (따라서 강화실패 누적횟수를 카운트하는 변수가 필요하다) 단, 강화실패 누적횟수는 누적2회 달성시 0으로 초기화 된다. 또한 강화상태가 +0인 경우는 실패하여도 강화실패 누적횟수가 추가되지 않는다.\n\n\n\n시도횟수\n강화성공여부\n강화상태\n강화실패누적\n비고\n\n\n\n\n1\n강화성공\n+0 \\(\\to\\) +1\n0 \\(\\to\\) 0\n-\n\n\n2\n강화성공\n+1 \\(\\to\\) +2\n0 \\(\\to\\) 0\n-\n\n\n3\n강화실패\n+2 \\(\\to\\) +2\n0 \\(\\to\\) 1\n-\n\n\n4\n강화성공\n+2 \\(\\to\\) +3\n1 \\(\\to\\) 1\n-\n\n\n5\n강화실패\n+3 \\(\\to\\) +0\n1 \\(\\to\\) 0\n강화실패로 누적2회로 인한 초기화\n\n\n6\n강화실패\n+0 \\(\\to\\) +0\n0 \\(\\to\\) 0\n강화실패 누적횟수 증가하지 않음\n\n\n7\n강화성공\n+0 \\(\\to\\) +1\n0 \\(\\to\\) 0\n-\n\n\n8\n강화성공\n+1 \\(\\to\\) +2\n0 \\(\\to\\) 0\n-\n\n\n9\n강화성공\n+2 \\(\\to\\) +3\n0 \\(\\to\\) 0\n-\n\n\n10\n강화성공\n+3 \\(\\to\\) +4\n0 \\(\\to\\) 0\n-\n\n\n11\n강화성공\n+4 \\(\\to\\) +5\n0 \\(\\to\\) 0\n모든 강화 성공\n\n\n12\n-\n+5 \\(\\to\\) +5\n0 \\(\\to\\) 0\n더 이상 강화시도 하지 않음\n\n\n13\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\n(1) 이 경우 62일의 방학뒤에 100명의 유저중 대략 몇명정도 +5 강화상태에 있겠는가? 시뮬레이션을 활용하여 추론하라. (단, +5강화에 성공하지 못한 모든 유저는 반드시 하루에 한번 강화를 시도해야 한다고 가정하자.)\n(2) 31번째 시도 이후 대략 몇명의 유저가 +5 강화상태에 있겠는가?\n\n# 2-1,2 \ns2=ExecutionSword(0.4)\n\n\nfor i in range(62):\n    s2.addday()\n    s2.attempt()\n    s2.update()\n    s2.reset() ## 초기화가 되는 조건이 있으므로 문제1에서 reset함수만 추가하면 된다. \n\n\n# 2-1\nsum(s2.upgradestate.day31==5)/s2.nuser\n\n0.36392\n\n\n\n# 2-2\nsum(s2.upgradestate.day62==5)/s2.nuser\n\n0.61803\n\n\n(3) 100명의 유저중 50명이상의 유저가 +5 강화상태에 도달하는 순간 모든 유저의 강화성공확률을 90%로 증가시킨다고 하자. 62일의 방학뒤에 100명의 유저 중 몇명 정도가 +5 강화상태에 있겠는가?\n\n# 2-3 \ns3=ExecutionSword(0.4)\n\n\nfor i in range(62):\n    s3.addday()\n    s3.attempt()\n    s3.update()\n    s3.reset() ## 초기화가 되는 조건이 있으므로 reset함수 추가\n    s3.arrangeprob() ## 전체유저의 50%가 강화성공하면 강화확률이 조정되는 조건이 있으므로 arragneprob 추가 \n\n\nsum(s3.upgradestate.day62==5)/s3.nuser\n\n0.9993"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class4.html",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class4.html",
    "title": "class 4단계",
    "section": "",
    "text": "프린트 가로채기 __str__, __repr__ (파이썬의 비밀2,3)\n\n\n\n\n\nmotivating example\n__str__, 파이썬의 비밀2\n__repr__, 파이썬의 비밀3\n주피터 노트북의 비밀 (_repr_html_), __repr__와 __str__의 우선적용 순위\n\n\n\n\n\n\nimport numpy as np\n\n\n\n\n\n\n\n\n\n# class1 hw's review\nclass RPC:\n    def throw(self):\n        print(np.random.choice(['가위','바위','보']))\n\n\na = RPC()\n\n\na.throw()\n\n가위\n\n\n\n\n\n[가위, 바위, 보] 말고 [가위, 보] 혹은 [바위, 보] 처럼 정해진 케이스가 아닌 입력으로 받고 싶을 수도 있다.\n\nclass RPC:\n    def throw(self, candidate):\n        print(np.random.choice(candidate))\n\n\na = RPC()\n\n\n# throw(a, ['가위','바위','보'])\na.throw(['가위','바위','보'])\n\n보\n\n\n\na.throw(['가위', '보']) # 보, 가위만.\n\n가위\n\n\n\n\n\n\nclass RPC:\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def throw(self):\n        print(np.random.choice(self.candidate))\n\n\na = RPC() # __init__ 는 암묵적으로 실행\n\n\na.throw()\n\n보\n\n\n\n\n\n위의 코드 3줄과 동일한 코드이며, 풀어써보면 다음과 같다.\n\nclass RPC2:\n    pass\n\n\nb = RPC2() # 아무것도 없음..\n\n\ndef initt(b, candidate = ['가위','바위','보']):\n    b.candidate = candidate\n\n\ninitt(b)\n\n\n# 던져서 화면에 보여주는 과정까지 추가\ndef throw(b):\n    print(np.random.choice(b.candidate))\n\n\nthrow(b)\n\n보\n\n\n\n\n\n풀어쓴 코드를 조합해보면?\n\nclass RPC2:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n    def throw(self):\n        print(np.random.choice(self.candidate))\n\n\nb = RPC2()\n\n\nb.candidate\n\n['가위', '바위', '보']\n\n\n\nb.throw()\n\n가위\n\n\n\n\n\n생각해보니까 throw는 choose + show의 결합인 것 같다.\n\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def choose(self):\n        self.actions = np.random.choice(self.candidate)\n    def show(self):\n        print(self.actions)\n\n\na = RPC()  ## 시점2\n\n\na.actions ## 시점3 (지금은 정의되지 않음, choose를 해야함)\n\nAttributeError: 'RPC' object has no attribute 'actions'\n\n\n\na.choose() # 뭔가 선택했겠지?    ## 시점4\n\n\na.actions # 바위를 선택했구만     ## 시점5 \n\n'바위'\n\n\n\na.show()   ## 시점6\n\n바위\n\n\n\n\n\n위와 같은 코드입니다.\n\nclass _RPC:  ## 시점1 \n    pass  # &lt;-- 이렇게하면 아무 기능이 없는 비어있는 클래스가 정의된다.\n\n\n_a  = _RPC()  ## 시점2\n\ndef _init(_a, candidate = ['가위','바위','보']):\n    _a.candidate = candidate\n    \n_init(_a)\n\n\n_a.actions ## 시점3\n\nAttributeError: '_RPC' object has no attribute 'actions'\n\n\n\n# choose 선언      ## 시점4\ndef _choose(_a):\n    _a.actions = np.random.choice(_a.candidate)\n_choose(_a)\n\n\n_a.actions  ## 시점5\n\n'바위'\n\n\n\n# show 선언    ## 시점6\ndef _show(_a):\n    print(_a.actions)\n_show(_a)\n\n바위\n\n\n\n\n\n\n또 다른 인스턴스 b를 만들자. b는 가위만 낼 수 있다.\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def choose(self):\n        self.actions = np.random.choice(self.candidate)\n    def show(self):\n        print(self.actions)\n        \n\nb = RPC()\n\n\nb.candidate\n\n['가위', '바위', '보']\n\n\n\n아무것도 없으면 b의 candidate이 가위, 가위, 보로 들어감\n\n\nb = RPC(['가위']) # 가위만 포함된 리스트 전달\n\n\nb.candidate\n\n['가위']\n\n\n\nb.choose()\nb.show()\n\n가위\n\n\n- a, b의 선택들을 모아서 기록하고 싶다.\n\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n        self.actions = list() ## 추가\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate)) ## 추가\n    def show(self):\n        print(self.actions)\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nnp.random.seed(123)\nfor i in range(5):\n    a.choose()\n    a.show()\n\n['보']\n['보', '바위']\n['보', '바위', '보']\n['보', '바위', '보', '보']\n['보', '바위', '보', '보', '가위']\n\n\n\nshow() 지난 히스토리까지 다 나오니까 보기 좀 불편하댜\n\n\nnp.random.seed(123)\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n        self.actions = list() ## 추가\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate)) ## 추가\n    def show(self):\n        print(self.actions[-1]) ### 추추가\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n바위\n보\n보\n가위\n\n\n\na.actions\n\n['보', '바위', '보', '보', '가위']\n\n\n\nfor i in range(5):\n    b.choose()\n    b.show()\n\n가위\n가위\n가위\n가위\n가위\n\n\n\nb.actions\n\n['가위', '가위', '가위', '가위', '가위']\n\n\n\na.candidate, a.actions # (낼 수 있는 패, 내가 낸 패)\n\n(['가위', '바위', '보'], ['보', '바위', '보', '보', '가위'])\n\n\n\nb.candidate, b.actions # (낼 수 있는 패, 내가 낸 패)\n\n(['가위'], ['가위', '가위', '가위', '가위', '가위'])\n\n\n- info라는 함수를 만들어서 a의 오브젝트가 가지고 있는 정보를 모두 보도록 하자.\n(예비학습) 문자열 \\n 이 포함된다면?\n\n'클래스\\n어렵네..'\n\n'클래스\\n어렵네..'\n\n\n\nprint('클래스\\n어렵네..')\n\n클래스\n어렵네..\n\n\n예비학습 끝\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])      \n    def info(self):\n        print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n바위\n가위\n보\n가위\n바위\n\n\n\nfor i in range(5):\n    b.choose()\n    b.show()\n\n가위\n가위\n가위\n가위\n가위\n\n\n\na.info()\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위', '가위', '보', '가위', '바위']\n\n\n\nb.info()\n\n낼 수 있는 패: ['가위']\n기록: ['가위', '가위', '가위', '가위', '가위']\n\n\n- 만들고보니까 info와 print의 기능이 거의 비슷함 \\(\\to\\) print(a)를 하면 a.info()와 동일한 효과를 내도록 만들 수 있을까?\n- 말도 안되는 소리같다. 왜? - 안될것 같은 이유1: print는 파이썬 내장기능, 내장기능을 우리가 맘대로 커스터마이징해서 쓰기는 어려울 것 같다. - 안될 것 같은 이유2: 이유1이 해결된다 해도 문제다. 그럼 지금까지 우리가 사용했던 수 많은 print()의 결과는 어떻게 되는가?\n결론은 가능하다\n- 그런데 a의 자료형(RPC 자료형)에 해당하는 오브젝트에 한정하여 print를 수정하는 방법이 가능하다면? (그럼 다른 오브젝트들은 수정된 print에 영향을 받지 않음)\n\n\n\n\n- 관찰1: 현재 print(a)의 결과는 아래와 같다.\n\nprint(a)\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n\na는 RPC클래스에서 만든 오브젝트이며 a가 저장된 메모리 주소는 0x7faaa7500850라는 의미\n\n- 관찰2: a에는 __str__ 이 있다.\n\ndir(a)\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n 'actions',\n 'candidate',\n 'choose',\n 'info',\n 'show']\n\n\n\nset(dir(a)) & {'__str__'}\n\n{'__str__'}\n\n\n이것을 함수처럼 사용하니까 아래와 같다.\n\na.__str__\n\n&lt;method-wrapper '__str__' of RPC object at 0x7faaa7500850&gt;\n\n\n\na.__str__() # 클래스 안에 있는 메소드, 문자열 리턴\n\n'&lt;__main__.RPC object at 0x7faaa7500850&gt;'\n\n\n\nprint(a.__str__()) # 이거 print(a)를 실행한 결과와 같다?\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n\nprint(a)\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n- 생각: 만약에 내가 a.__str__() 라는 함수를 재정의 하여 리턴값을 ’너는 해킹당했다’로 바꾸게 되면 print(a)해서 나오는 결과는 어떻게 될까? (약간 해커같죠)\n(예비학습) 함수 덮어씌우기\n\ndef f():\n    print('asdf')\n\n\nf()\n\nasdf\n\n\n\ndef f():\n    print('guebin hahaha')\n\n\nf()\n\nguebin hahaha\n\n\n이런식으로 함수가 이미 정의되어 있더라도, 내가 나중에 덮어씌우면 그 함수의 기능을 다시 정의한다.\n(해킹시작)\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return '너는 해킹당했다'\n    def info(self):\n        print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\nprint(a)\n\n너는 해킹당했다\n\n\n- __str__ 의 리턴값을 info에서 타이핑했던 문자열로 재정의한다면?\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    # def info(self):\n    #     print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n    def __str__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.choose()\na.show()\n\n바위\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위']\n\n\n\na.choose()\na.show()\n\n가위\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위', '가위']\n\n\n\n\n- print(a) 와 print(a.__str__()) 는 같은 문법이다.\n- 참고로 a.__str__() 와 str(a) 도 같은 방법이다.\n\nstr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: ['바위', '가위']\"\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: ['바위', '가위']\"\n\n\n- 지금까지 우리가 썼던 기능을 확인!\n(예제1)\n\na = [1,2,3]\n\n\nprint(a)\n\n[1, 2, 3]\n\n\n\na.__str__()\n\n'[1, 2, 3]'\n\n\n\nstr(a)\n\n'[1, 2, 3]'\n\n\n(예제2)\n\na = {1,2,3}\nprint(a)\n\n{1, 2, 3}\n\n\n\nstr(a)\n\n'{1, 2, 3}'\n\n\n\na.__str__()\n\n'{1, 2, 3}'\n\n\n(예제3)\n\na = np.array(1)\na.shape\n\n()\n\n\n\ntype(a.shape)\n\ntuple\n\n\n\nprint(a.shape)\n\n()\n\n\n\na.shape.__str__()\n\n'()'\n\n\n\nstr(a.shape)\n\n'()'\n\n\n(예제4)\n\na = range(10)\nprint(a)\n\nrange(0, 10)\n\n\n\na.__str__()\n\n'range(0, 10)'\n\n\n(예제5)\n\na = np.arange(100).reshape(10,10)\nprint(a)\n\n[[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]\n [20 21 22 23 24 25 26 27 28 29]\n [30 31 32 33 34 35 36 37 38 39]\n [40 41 42 43 44 45 46 47 48 49]\n [50 51 52 53 54 55 56 57 58 59]\n [60 61 62 63 64 65 66 67 68 69]\n [70 71 72 73 74 75 76 77 78 79]\n [80 81 82 83 84 85 86 87 88 89]\n [90 91 92 93 94 95 96 97 98 99]]\n\n\n\na.__str__()\n\n'[[ 0  1  2  3  4  5  6  7  8  9]\\n [10 11 12 13 14 15 16 17 18 19]\\n [20 21 22 23 24 25 26 27 28 29]\\n [30 31 32 33 34 35 36 37 38 39]\\n [40 41 42 43 44 45 46 47 48 49]\\n [50 51 52 53 54 55 56 57 58 59]\\n [60 61 62 63 64 65 66 67 68 69]\\n [70 71 72 73 74 75 76 77 78 79]\\n [80 81 82 83 84 85 86 87 88 89]\\n [90 91 92 93 94 95 96 97 98 99]]'\n\n\n\n\n\n\n- 생각해보니까 print를 써서 우리가 원하는 정보를 확인하는건 아니였음\n\na = [1,2,3]\n\n\na\n\n[1, 2, 3]\n\n\n\nprint(a) # print(a.__str__()) + enter ==&gt; a + enter\n\n[1, 2, 3]\n\n\n-`` a + 엔터를 하면 print(a) + 엔터를 하는 것과 같은 효과인가?\n(반례)\n\na = np.array([1,2,3,4]).reshape(2,2)\n\n\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nprint(a)\n\n[[1 2]\n [3 4]]\n\n\n- a + 엔터 는 print(a) + 엔터 가 다른 경우도 있다. \\(\\to\\) 추측: 서로 다른 숨겨진 기능이 있다! \\(\\to\\) 결론: 그 기능은 __repr__ 에 저장되어 있음.\n\n__repr__ 추가 전\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\na\n\n&lt;__main__.RPC at 0x7faaa6d821c0&gt;\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\n__repr__ 추가 후\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\na # print(a.__repr__())\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n- 그럼 우리가 지금까지 했던 것?\n\na = np.array([1,2,3])\n\n\na\n\narray([1, 2, 3])\n\n\n\nprint(a)\n\n[1 2 3]\n\n\n\na.__repr__()\n\n'array([1, 2, 3])'\n\n\n\na.__str__()\n\n'[1 2 3]'\n\n\n\n\n- 대화형콘솔에서 오브젝트이름 + 엔터를 쳐서 나오는 출력은 __repr__의 결과와 연관이 있다.\n\na = np.array(range(10000)).reshape(100,100)\na\n\narray([[   0,    1,    2, ...,   97,   98,   99],\n       [ 100,  101,  102, ...,  197,  198,  199],\n       [ 200,  201,  202, ...,  297,  298,  299],\n       ...,\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])\n\n\n\na.__repr__()\n\n'array([[   0,    1,    2, ...,   97,   98,   99],\\n       [ 100,  101,  102, ...,  197,  198,  199],\\n       [ 200,  201,  202, ...,  297,  298,  299],\\n       ...,\\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])'\n\n\n- 참고로 a.__repr__()은 repr(a)와 같다.\n\nrepr(a)\n\n'array([[   0,    1,    2, ...,   97,   98,   99],\\n       [ 100,  101,  102, ...,  197,  198,  199],\\n       [ 200,  201,  202, ...,  297,  298,  299],\\n       ...,\\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])'\n\n\n\n\n\n- 요즘에는 IDE 발전에 따라서 오브젝트 + 엔터 칠 때 나오는 출력의 형태도 다양해지고 있음.\n\nimport pandas as pd\n\n\ndf = pd.DataFrame({'a':[1,2,3],\n                   'b':[2,3,4]})\n\n\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\n예쁘게 나온다.\n\n- 위의 결과는 print(df.__repr__())의 결과와 조금 다르게 나온다?\n\nprint(df.__repr__())\n\n   a  b\n0  1  2\n1  2  3\n2  3  4\n\n\n- print(df.__repr__())는 예전 검은화면에서 코딩할 때 나오는 출력임\nPython 3.10.2 | packaged by conda-forge | (main, Feb  1 2022, 19:28:35) [GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt; &gt;&gt; import pandas as pd \n&gt;&gt;&gt; df = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})&gt;&gt;&gt; df\n   a  b\n0  1  2\n1  2  3\n2  3  4\n&gt;&gt;&gt;\n- 주피터에서는 ‘오브젝트이름 + 엔터’ 치면 HTML(df.__repr_html())이 실행되고 repr_html_()이 정의되어 있지 않으면 print(df.__rept__())이 실행된다.\n\ndf._repr_html_()\n\n'&lt;div&gt;\\n&lt;style scoped&gt;\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n&lt;/style&gt;\\n&lt;table border=\"1\" class=\"dataframe\"&gt;\\n  &lt;thead&gt;\\n    &lt;tr style=\"text-align: right;\"&gt;\\n      &lt;th&gt;&lt;/th&gt;\\n      &lt;th&gt;a&lt;/th&gt;\\n      &lt;th&gt;b&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;0&lt;/th&gt;\\n      &lt;td&gt;1&lt;/td&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;1&lt;/th&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;2&lt;/th&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n      &lt;td&gt;4&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;'\n\n\n\nhtml 코드!\n\n\nfrom IPython.core.display import HTML\n\n\nHTML(df._repr_html_())\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n- 물론 df._repr_html_()함수가 내부적으로 있어도 html이 지원되지 않는 환경이라면 print(df.__repr__())이 내부적으로 수행된다.\n\n\n\n\n(예제1)\n- 아래의 예제를 관찰하자.\n\nclass RPS:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPS()\na\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__repr__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\nrepr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n- 여기까지는 상식수준의 결과임. 이제 아래를 관찰하라.\n\nprint(a) # print(a.__repr__())\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\nstr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\n__str__()은 건드린적이 없는데?\n\n\na.__repr__??\n\n\nSignature: a.__repr__()\nDocstring: Return repr(self).\nSource:   \n    def __repr__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\nFile:      ~/Dropbox/Quarto-Blog/posts/Python/&lt;ipython-input-296-bcd76efb6380&gt;\nType:      method\n\n\n\n\na.__str__??\n\n\nSignature:      a.__str__()\nCall signature: a.__str__(*args, **kwargs)\nType:           method-wrapper\nString form:    &lt;method-wrapper '__str__' of RPS object at 0x7faaa47aae20&gt;\nDocstring:      Return str(self).\n\n\n\n\n__str__()은 건드린 적이 없는데 \\(\\to\\) 건드린적은 없는데 기능이 바뀌어있음.\n\n(예제2)\n- 아래의 예제를 관찰하자.\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\na.__repr__()\n\n'&lt;__main__.RPC object at 0x7f8f38ca22e0&gt;'\n\n\n\na.__str__??\n\n\nSignature: a.__str__()\nDocstring: Return str(self).\nSource:   \n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\nFile:      ~/Dropbox/Quarto-Blog/posts/Python/&lt;ipython-input-3-2e46ee18321f&gt;\nType:      method\n\n\n\n\na.__repr__??\n\n\nSignature:      a.__repr__()\nCall signature: a.__repr__(*args, **kwargs)\nType:           method-wrapper\nString form:    &lt;method-wrapper '__repr__' of RPC object at 0x7f8f38ca22e0&gt;\nDocstring:      Return repr(self).\n\n\n\n2번째 예제에서는 건드린 애만 바뀌었는데 첫번째 예제에서는 건드리지 않은 애들까지 기능이 바뀌었다.\n(예제3)\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return '너는 해킹당했다. 하하하'\n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\na\n\n너는 해킹당했다. 하하하\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n- __str__ 와 __repr__을 건드리지 않고 출력결과를 바꾸고 싶다면?\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        기록: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\nstr(a)\n\n'&lt;__main__.RPC object at 0x7f8f38bb7730&gt;'\n\n\n\nrepr(a)\n\n'&lt;__main__.RPC object at 0x7f8f38bb7730&gt;'\n\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: []\n        \n\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n바위\n가위\n바위\n보\n\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: ['보', '바위', '가위', '바위', '보']\n        \n\n\n\n\n\n아래의 클래스를 수정하여\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt; \n        기록: {}\n        \"\"\"\n        return html_str.format(self.candidate,self.actions)\n클래스에서 생성된 인스턴스의 출력결과가 아래와 같도록 하라.\n학번: 202143052 \n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['가위', '가위', '보', '보', '바위']\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        학번: {} &lt;br/&gt;\n        낼 수 있는 패: {} &lt;br/&gt; \n        기록: {}\n        \"\"\"\n        return html_str.format(202143052,self.candidate,self.actions)\n\n\na = RPS()\n\n\na\n\n\n        학번: 202143052 \n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: []\n        \n\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n가위\n바위\n바위\n가위\n\n\n\na\n\n\n        학번: 202143052 \n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: ['보', '가위', '바위', '바위', '가위']"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class4.html#contents",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class4.html#contents",
    "title": "class 4단계",
    "section": "",
    "text": "motivating example\n__str__, 파이썬의 비밀2\n__repr__, 파이썬의 비밀3\n주피터 노트북의 비밀 (_repr_html_), __repr__와 __str__의 우선적용 순위"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class4.html#imports",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class4.html#imports",
    "title": "class 4단계",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class4.html#motivating-example",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class4.html#motivating-example",
    "title": "class 4단계",
    "section": "",
    "text": "# class1 hw's review\nclass RPC:\n    def throw(self):\n        print(np.random.choice(['가위','바위','보']))\n\n\na = RPC()\n\n\na.throw()\n\n가위\n\n\n\n\n\n[가위, 바위, 보] 말고 [가위, 보] 혹은 [바위, 보] 처럼 정해진 케이스가 아닌 입력으로 받고 싶을 수도 있다.\n\nclass RPC:\n    def throw(self, candidate):\n        print(np.random.choice(candidate))\n\n\na = RPC()\n\n\n# throw(a, ['가위','바위','보'])\na.throw(['가위','바위','보'])\n\n보\n\n\n\na.throw(['가위', '보']) # 보, 가위만.\n\n가위\n\n\n\n\n\n\nclass RPC:\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def throw(self):\n        print(np.random.choice(self.candidate))\n\n\na = RPC() # __init__ 는 암묵적으로 실행\n\n\na.throw()\n\n보\n\n\n\n\n\n위의 코드 3줄과 동일한 코드이며, 풀어써보면 다음과 같다.\n\nclass RPC2:\n    pass\n\n\nb = RPC2() # 아무것도 없음..\n\n\ndef initt(b, candidate = ['가위','바위','보']):\n    b.candidate = candidate\n\n\ninitt(b)\n\n\n# 던져서 화면에 보여주는 과정까지 추가\ndef throw(b):\n    print(np.random.choice(b.candidate))\n\n\nthrow(b)\n\n보\n\n\n\n\n\n풀어쓴 코드를 조합해보면?\n\nclass RPC2:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n    def throw(self):\n        print(np.random.choice(self.candidate))\n\n\nb = RPC2()\n\n\nb.candidate\n\n['가위', '바위', '보']\n\n\n\nb.throw()\n\n가위\n\n\n\n\n\n생각해보니까 throw는 choose + show의 결합인 것 같다.\n\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def choose(self):\n        self.actions = np.random.choice(self.candidate)\n    def show(self):\n        print(self.actions)\n\n\na = RPC()  ## 시점2\n\n\na.actions ## 시점3 (지금은 정의되지 않음, choose를 해야함)\n\nAttributeError: 'RPC' object has no attribute 'actions'\n\n\n\na.choose() # 뭔가 선택했겠지?    ## 시점4\n\n\na.actions # 바위를 선택했구만     ## 시점5 \n\n'바위'\n\n\n\na.show()   ## 시점6\n\n바위\n\n\n\n\n\n위와 같은 코드입니다.\n\nclass _RPC:  ## 시점1 \n    pass  # &lt;-- 이렇게하면 아무 기능이 없는 비어있는 클래스가 정의된다.\n\n\n_a  = _RPC()  ## 시점2\n\ndef _init(_a, candidate = ['가위','바위','보']):\n    _a.candidate = candidate\n    \n_init(_a)\n\n\n_a.actions ## 시점3\n\nAttributeError: '_RPC' object has no attribute 'actions'\n\n\n\n# choose 선언      ## 시점4\ndef _choose(_a):\n    _a.actions = np.random.choice(_a.candidate)\n_choose(_a)\n\n\n_a.actions  ## 시점5\n\n'바위'\n\n\n\n# show 선언    ## 시점6\ndef _show(_a):\n    print(_a.actions)\n_show(_a)\n\n바위\n\n\n\n\n\n\n또 다른 인스턴스 b를 만들자. b는 가위만 낼 수 있다.\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n    def choose(self):\n        self.actions = np.random.choice(self.candidate)\n    def show(self):\n        print(self.actions)\n        \n\nb = RPC()\n\n\nb.candidate\n\n['가위', '바위', '보']\n\n\n\n아무것도 없으면 b의 candidate이 가위, 가위, 보로 들어감\n\n\nb = RPC(['가위']) # 가위만 포함된 리스트 전달\n\n\nb.candidate\n\n['가위']\n\n\n\nb.choose()\nb.show()\n\n가위\n\n\n- a, b의 선택들을 모아서 기록하고 싶다.\n\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n        self.actions = list() ## 추가\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate)) ## 추가\n    def show(self):\n        print(self.actions)\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nnp.random.seed(123)\nfor i in range(5):\n    a.choose()\n    a.show()\n\n['보']\n['보', '바위']\n['보', '바위', '보']\n['보', '바위', '보', '보']\n['보', '바위', '보', '보', '가위']\n\n\n\nshow() 지난 히스토리까지 다 나오니까 보기 좀 불편하댜\n\n\nnp.random.seed(123)\nclass RPC: ## 시점1\n    def __init__(self, candidate = ['가위', '바위', '보']):\n        self.candidate = candidate\n        self.actions = list() ## 추가\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate)) ## 추가\n    def show(self):\n        print(self.actions[-1]) ### 추추가\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n바위\n보\n보\n가위\n\n\n\na.actions\n\n['보', '바위', '보', '보', '가위']\n\n\n\nfor i in range(5):\n    b.choose()\n    b.show()\n\n가위\n가위\n가위\n가위\n가위\n\n\n\nb.actions\n\n['가위', '가위', '가위', '가위', '가위']\n\n\n\na.candidate, a.actions # (낼 수 있는 패, 내가 낸 패)\n\n(['가위', '바위', '보'], ['보', '바위', '보', '보', '가위'])\n\n\n\nb.candidate, b.actions # (낼 수 있는 패, 내가 낸 패)\n\n(['가위'], ['가위', '가위', '가위', '가위', '가위'])\n\n\n- info라는 함수를 만들어서 a의 오브젝트가 가지고 있는 정보를 모두 보도록 하자.\n(예비학습) 문자열 \\n 이 포함된다면?\n\n'클래스\\n어렵네..'\n\n'클래스\\n어렵네..'\n\n\n\nprint('클래스\\n어렵네..')\n\n클래스\n어렵네..\n\n\n예비학습 끝\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])      \n    def info(self):\n        print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\nb = RPC(['가위'])\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n바위\n가위\n보\n가위\n바위\n\n\n\nfor i in range(5):\n    b.choose()\n    b.show()\n\n가위\n가위\n가위\n가위\n가위\n\n\n\na.info()\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위', '가위', '보', '가위', '바위']\n\n\n\nb.info()\n\n낼 수 있는 패: ['가위']\n기록: ['가위', '가위', '가위', '가위', '가위']\n\n\n- 만들고보니까 info와 print의 기능이 거의 비슷함 \\(\\to\\) print(a)를 하면 a.info()와 동일한 효과를 내도록 만들 수 있을까?\n- 말도 안되는 소리같다. 왜? - 안될것 같은 이유1: print는 파이썬 내장기능, 내장기능을 우리가 맘대로 커스터마이징해서 쓰기는 어려울 것 같다. - 안될 것 같은 이유2: 이유1이 해결된다 해도 문제다. 그럼 지금까지 우리가 사용했던 수 많은 print()의 결과는 어떻게 되는가?\n결론은 가능하다\n- 그런데 a의 자료형(RPC 자료형)에 해당하는 오브젝트에 한정하여 print를 수정하는 방법이 가능하다면? (그럼 다른 오브젝트들은 수정된 print에 영향을 받지 않음)"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class4.html#str__",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class4.html#str__",
    "title": "class 4단계",
    "section": "",
    "text": "- 관찰1: 현재 print(a)의 결과는 아래와 같다.\n\nprint(a)\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n\na는 RPC클래스에서 만든 오브젝트이며 a가 저장된 메모리 주소는 0x7faaa7500850라는 의미\n\n- 관찰2: a에는 __str__ 이 있다.\n\ndir(a)\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n 'actions',\n 'candidate',\n 'choose',\n 'info',\n 'show']\n\n\n\nset(dir(a)) & {'__str__'}\n\n{'__str__'}\n\n\n이것을 함수처럼 사용하니까 아래와 같다.\n\na.__str__\n\n&lt;method-wrapper '__str__' of RPC object at 0x7faaa7500850&gt;\n\n\n\na.__str__() # 클래스 안에 있는 메소드, 문자열 리턴\n\n'&lt;__main__.RPC object at 0x7faaa7500850&gt;'\n\n\n\nprint(a.__str__()) # 이거 print(a)를 실행한 결과와 같다?\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n\nprint(a)\n\n&lt;__main__.RPC object at 0x7faaa7500850&gt;\n\n\n- 생각: 만약에 내가 a.__str__() 라는 함수를 재정의 하여 리턴값을 ’너는 해킹당했다’로 바꾸게 되면 print(a)해서 나오는 결과는 어떻게 될까? (약간 해커같죠)\n(예비학습) 함수 덮어씌우기\n\ndef f():\n    print('asdf')\n\n\nf()\n\nasdf\n\n\n\ndef f():\n    print('guebin hahaha')\n\n\nf()\n\nguebin hahaha\n\n\n이런식으로 함수가 이미 정의되어 있더라도, 내가 나중에 덮어씌우면 그 함수의 기능을 다시 정의한다.\n(해킹시작)\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return '너는 해킹당했다'\n    def info(self):\n        print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\nprint(a)\n\n너는 해킹당했다\n\n\n- __str__ 의 리턴값을 info에서 타이핑했던 문자열로 재정의한다면?\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    # def info(self):\n    #     print('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n    def __str__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.choose()\na.show()\n\n바위\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위']\n\n\n\na.choose()\na.show()\n\n가위\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['바위', '가위']\n\n\n\n\n- print(a) 와 print(a.__str__()) 는 같은 문법이다.\n- 참고로 a.__str__() 와 str(a) 도 같은 방법이다.\n\nstr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: ['바위', '가위']\"\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: ['바위', '가위']\"\n\n\n- 지금까지 우리가 썼던 기능을 확인!\n(예제1)\n\na = [1,2,3]\n\n\nprint(a)\n\n[1, 2, 3]\n\n\n\na.__str__()\n\n'[1, 2, 3]'\n\n\n\nstr(a)\n\n'[1, 2, 3]'\n\n\n(예제2)\n\na = {1,2,3}\nprint(a)\n\n{1, 2, 3}\n\n\n\nstr(a)\n\n'{1, 2, 3}'\n\n\n\na.__str__()\n\n'{1, 2, 3}'\n\n\n(예제3)\n\na = np.array(1)\na.shape\n\n()\n\n\n\ntype(a.shape)\n\ntuple\n\n\n\nprint(a.shape)\n\n()\n\n\n\na.shape.__str__()\n\n'()'\n\n\n\nstr(a.shape)\n\n'()'\n\n\n(예제4)\n\na = range(10)\nprint(a)\n\nrange(0, 10)\n\n\n\na.__str__()\n\n'range(0, 10)'\n\n\n(예제5)\n\na = np.arange(100).reshape(10,10)\nprint(a)\n\n[[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]\n [20 21 22 23 24 25 26 27 28 29]\n [30 31 32 33 34 35 36 37 38 39]\n [40 41 42 43 44 45 46 47 48 49]\n [50 51 52 53 54 55 56 57 58 59]\n [60 61 62 63 64 65 66 67 68 69]\n [70 71 72 73 74 75 76 77 78 79]\n [80 81 82 83 84 85 86 87 88 89]\n [90 91 92 93 94 95 96 97 98 99]]\n\n\n\na.__str__()\n\n'[[ 0  1  2  3  4  5  6  7  8  9]\\n [10 11 12 13 14 15 16 17 18 19]\\n [20 21 22 23 24 25 26 27 28 29]\\n [30 31 32 33 34 35 36 37 38 39]\\n [40 41 42 43 44 45 46 47 48 49]\\n [50 51 52 53 54 55 56 57 58 59]\\n [60 61 62 63 64 65 66 67 68 69]\\n [70 71 72 73 74 75 76 77 78 79]\\n [80 81 82 83 84 85 86 87 88 89]\\n [90 91 92 93 94 95 96 97 98 99]]'"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class4.html#repr__",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class4.html#repr__",
    "title": "class 4단계",
    "section": "",
    "text": "- 생각해보니까 print를 써서 우리가 원하는 정보를 확인하는건 아니였음\n\na = [1,2,3]\n\n\na\n\n[1, 2, 3]\n\n\n\nprint(a) # print(a.__str__()) + enter ==&gt; a + enter\n\n[1, 2, 3]\n\n\n-`` a + 엔터를 하면 print(a) + 엔터를 하는 것과 같은 효과인가?\n(반례)\n\na = np.array([1,2,3,4]).reshape(2,2)\n\n\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nprint(a)\n\n[[1 2]\n [3 4]]\n\n\n- a + 엔터 는 print(a) + 엔터 가 다른 경우도 있다. \\(\\to\\) 추측: 서로 다른 숨겨진 기능이 있다! \\(\\to\\) 결론: 그 기능은 __repr__ 에 저장되어 있음.\n\n__repr__ 추가 전\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\na\n\n&lt;__main__.RPC at 0x7faaa6d821c0&gt;\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\n__repr__ 추가 후\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return('낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions))\n\n\na = RPC()\n\n\na # print(a.__repr__())\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n- 그럼 우리가 지금까지 했던 것?\n\na = np.array([1,2,3])\n\n\na\n\narray([1, 2, 3])\n\n\n\nprint(a)\n\n[1 2 3]\n\n\n\na.__repr__()\n\n'array([1, 2, 3])'\n\n\n\na.__str__()\n\n'[1 2 3]'\n\n\n\n\n- 대화형콘솔에서 오브젝트이름 + 엔터를 쳐서 나오는 출력은 __repr__의 결과와 연관이 있다.\n\na = np.array(range(10000)).reshape(100,100)\na\n\narray([[   0,    1,    2, ...,   97,   98,   99],\n       [ 100,  101,  102, ...,  197,  198,  199],\n       [ 200,  201,  202, ...,  297,  298,  299],\n       ...,\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])\n\n\n\na.__repr__()\n\n'array([[   0,    1,    2, ...,   97,   98,   99],\\n       [ 100,  101,  102, ...,  197,  198,  199],\\n       [ 200,  201,  202, ...,  297,  298,  299],\\n       ...,\\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])'\n\n\n- 참고로 a.__repr__()은 repr(a)와 같다.\n\nrepr(a)\n\n'array([[   0,    1,    2, ...,   97,   98,   99],\\n       [ 100,  101,  102, ...,  197,  198,  199],\\n       [ 200,  201,  202, ...,  297,  298,  299],\\n       ...,\\n       [9700, 9701, 9702, ..., 9797, 9798, 9799],\\n       [9800, 9801, 9802, ..., 9897, 9898, 9899],\\n       [9900, 9901, 9902, ..., 9997, 9998, 9999]])'\n\n\n\n\n\n- 요즘에는 IDE 발전에 따라서 오브젝트 + 엔터 칠 때 나오는 출력의 형태도 다양해지고 있음.\n\nimport pandas as pd\n\n\ndf = pd.DataFrame({'a':[1,2,3],\n                   'b':[2,3,4]})\n\n\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\n예쁘게 나온다.\n\n- 위의 결과는 print(df.__repr__())의 결과와 조금 다르게 나온다?\n\nprint(df.__repr__())\n\n   a  b\n0  1  2\n1  2  3\n2  3  4\n\n\n- print(df.__repr__())는 예전 검은화면에서 코딩할 때 나오는 출력임\nPython 3.10.2 | packaged by conda-forge | (main, Feb  1 2022, 19:28:35) [GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt; &gt;&gt; import pandas as pd \n&gt;&gt;&gt; df = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})&gt;&gt;&gt; df\n   a  b\n0  1  2\n1  2  3\n2  3  4\n&gt;&gt;&gt;\n- 주피터에서는 ‘오브젝트이름 + 엔터’ 치면 HTML(df.__repr_html())이 실행되고 repr_html_()이 정의되어 있지 않으면 print(df.__rept__())이 실행된다.\n\ndf._repr_html_()\n\n'&lt;div&gt;\\n&lt;style scoped&gt;\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n&lt;/style&gt;\\n&lt;table border=\"1\" class=\"dataframe\"&gt;\\n  &lt;thead&gt;\\n    &lt;tr style=\"text-align: right;\"&gt;\\n      &lt;th&gt;&lt;/th&gt;\\n      &lt;th&gt;a&lt;/th&gt;\\n      &lt;th&gt;b&lt;/th&gt;\\n    &lt;/tr&gt;\\n  &lt;/thead&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;0&lt;/th&gt;\\n      &lt;td&gt;1&lt;/td&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;1&lt;/th&gt;\\n      &lt;td&gt;2&lt;/td&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;th&gt;2&lt;/th&gt;\\n      &lt;td&gt;3&lt;/td&gt;\\n      &lt;td&gt;4&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n&lt;/div&gt;'\n\n\n\nhtml 코드!\n\n\nfrom IPython.core.display import HTML\n\n\nHTML(df._repr_html_())\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n- 물론 df._repr_html_()함수가 내부적으로 있어도 html이 지원되지 않는 환경이라면 print(df.__repr__())이 내부적으로 수행된다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class4.html#repr__와-__str__의-우선적용-순위",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class4.html#repr__와-__str__의-우선적용-순위",
    "title": "class 4단계",
    "section": "",
    "text": "(예제1)\n- 아래의 예제를 관찰하자.\n\nclass RPS:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPS()\na\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__repr__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\nrepr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n- 여기까지는 상식수준의 결과임. 이제 아래를 관찰하라.\n\nprint(a) # print(a.__repr__())\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\nstr(a)\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\n__str__()은 건드린적이 없는데?\n\n\na.__repr__??\n\n\nSignature: a.__repr__()\nDocstring: Return repr(self).\nSource:   \n    def __repr__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\nFile:      ~/Dropbox/Quarto-Blog/posts/Python/&lt;ipython-input-296-bcd76efb6380&gt;\nType:      method\n\n\n\n\na.__str__??\n\n\nSignature:      a.__str__()\nCall signature: a.__str__(*args, **kwargs)\nType:           method-wrapper\nString form:    &lt;method-wrapper '__str__' of RPS object at 0x7faaa47aae20&gt;\nDocstring:      Return str(self).\n\n\n\n\n__str__()은 건드린 적이 없는데 \\(\\to\\) 건드린적은 없는데 기능이 바뀌어있음.\n\n(예제2)\n- 아래의 예제를 관찰하자.\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n\na.__str__()\n\n\"낼 수 있는 패: ['가위', '바위', '보']\\n기록: []\"\n\n\n\na.__repr__()\n\n'&lt;__main__.RPC object at 0x7f8f38ca22e0&gt;'\n\n\n\na.__str__??\n\n\nSignature: a.__str__()\nDocstring: Return str(self).\nSource:   \n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\nFile:      ~/Dropbox/Quarto-Blog/posts/Python/&lt;ipython-input-3-2e46ee18321f&gt;\nType:      method\n\n\n\n\na.__repr__??\n\n\nSignature:      a.__repr__()\nCall signature: a.__repr__(*args, **kwargs)\nType:           method-wrapper\nString form:    &lt;method-wrapper '__repr__' of RPC object at 0x7f8f38ca22e0&gt;\nDocstring:      Return repr(self).\n\n\n\n2번째 예제에서는 건드린 애만 바뀌었는데 첫번째 예제에서는 건드리지 않은 애들까지 기능이 바뀌었다.\n(예제3)\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def __repr__(self):\n        return '너는 해킹당했다. 하하하'\n    def __str__(self):\n        return '낼 수 있는 패: {}\\n기록: {}'.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\na\n\n너는 해킹당했다. 하하하\n\n\n\nprint(a)\n\n낼 수 있는 패: ['가위', '바위', '보']\n기록: []\n\n\n- __str__ 와 __repr__을 건드리지 않고 출력결과를 바꾸고 싶다면?\n\nclass RPC:\n    def __init__(self, candidate = ['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        기록: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions)\n\n\na = RPC()\n\n\nstr(a)\n\n'&lt;__main__.RPC object at 0x7f8f38bb7730&gt;'\n\n\n\nrepr(a)\n\n'&lt;__main__.RPC object at 0x7f8f38bb7730&gt;'\n\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: []\n        \n\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n바위\n가위\n바위\n보\n\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        기록: ['보', '바위', '가위', '바위', '보']"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class4.html#숙제",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class4.html#숙제",
    "title": "class 4단계",
    "section": "",
    "text": "아래의 클래스를 수정하여\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt; \n        기록: {}\n        \"\"\"\n        return html_str.format(self.candidate,self.actions)\n클래스에서 생성된 인스턴스의 출력결과가 아래와 같도록 하라.\n학번: 202143052 \n낼 수 있는 패: ['가위', '바위', '보']\n기록: ['가위', '가위', '보', '보', '바위']\n\nclass RPS: \n    def __init__(self,candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list() \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def show(self):\n        print(self.actions[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        학번: {} &lt;br/&gt;\n        낼 수 있는 패: {} &lt;br/&gt; \n        기록: {}\n        \"\"\"\n        return html_str.format(202143052,self.candidate,self.actions)\n\n\na = RPS()\n\n\na\n\n\n        학번: 202143052 \n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: []\n        \n\n\n\nfor i in range(5):\n    a.choose()\n    a.show()\n\n보\n가위\n바위\n바위\n가위\n\n\n\na\n\n\n        학번: 202143052 \n        낼 수 있는 패: ['가위', '바위', '보']  \n        기록: ['보', '가위', '바위', '바위', '가위']"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class2.html",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class2.html",
    "title": "class 2단계",
    "section": "",
    "text": "__init__\nself의 의미\n파이썬의 비밀1\n파이썬의 비밀2\n\n\n\n\n\n\n# 이미지 출력을 위한 패키지 불러오기\nfrom PIL import Image\nimport requests\n\n\n\n- STOOOP을 다시 복습\n\nurl1 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\nurl2 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop2.png?raw=true' \n\n\nclass STOOOP:\n    title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~'\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\ns1 = STOOOP() # STOOOP 이라는 클래스에서 s1이라는 인스턴스를 만드는 과정\n\n\ns1.title, s1.url, s1.end\n\n('학교폭력!',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰~~~')\n\n\n\ns1.stop()\n\n학교폭력!\n멈춰~~~\n\n\n\n\n\n- 왜 s1의 default title이 항상 ‘학교폭력’ 이어야 하는가? \\(\\to\\) __init__ 의 개발\n- 성능4: __init__() 함수를 이용하여 ‘클래스 \\(\\to\\) 인스턴스’ 의 시점에서 수행하는 일련의 동작들을 묶어서 수행할 수 있음.\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n- 잘못된 사용\n\ns1 = STOOOP() # 이 시점에서 __init__ 이 수행된다.\n\nTypeError: __init__() missing 1 required positional argument: 'title'\n\n\n- 올바른 사용\n\ns1 = STOOOP('수강신청매크로') # 이 시점에서 __init__ 이 수행된다!\n\n\ns1.title, s1.url, s1.end\n\n('수강신청매크로',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰~~~~')\n\n\n\ns1.stop()\n\n수강신청매크로\n멈춰~~~~\n\n\n\n\n\n- 잘못된 사용에서 에러가 발생한 이유는?\nTypeError: __init__() missing 1 required positional argument: 'title'\n\ns1 = STOOOP() 이 실행되는 순간 __init__() 이 내부적으로 실행된다.\n그런데 __init__() 의 첫번째 입력인 self는 입력안해도 무방했음. 그런데 두번째 입력은 title은 입력을 해야했음.\n그런데 title을 입력하지 않아서 발생하는 에러.\n\n- __init__(self, arg1, arg2,...) 함수에 대하여\n\n엄청나게 특별해 보이지만 사실 몇가지 특별한 점을 제외하고는 어떠한 마법도 없는 함수이다.\n특별한 점1: 첫번째 입력으로 반드시 self를 넣어야함. (이건 사실 클래스 내의 메소드 거의 다 그러함)\n특별한 점2: 클래스에서 인스턴스를 만드는 시점에 자동으로 실행된다.\n특별한 점3: __init(self, arg1, arg2,...)의 입력중 self 이외의 입력들은 ‘클래스 \\(\\to\\) 인스턴스’ 시점에서 ’인스턴스이름 = 클래스이름(arg1, arg2,…)’와 같이 사용한다. (이 예제의 경우 STOOOP(title) 와 같이 사용해야함)\n\n- title이 디폴트로 들어가는 상황도 불편했지만, title을 명시적으로 넣지 않으면 에러가 발생하는 것도 불편하다?\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title=None):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\ns2 = STOOOP()\ns3 = STOOOP('KOSPI 하락')\n\n\ns2.stop() # title 없는 경우\n\nNone\n멈춰~~~~\n\n\n\n\n\n\n제목이 없으면 없는대로 잘 출력이 된다.\n\n\ns3.stop() # title = 'KOSPI 하락'\n\nKOSPI 하락\n멈춰~~~~\n\n\n\n\n\n\n\n\n\n- 이전 예제를 복습\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(\"당신은 이 이미지를 {}번 보았습니다\".format(self.n))\n        self.n = self.n+1 \n\n\nk4 = Klass4()\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 2번 보았습니다\n\n\n- 위의 예제는 아래와 같이 구현할 수도 있다.\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 이미지를 {}번 보았습니다.'.format(self.n))\n        # slef.n = self.n + 1\n\n\nk4 = Klass4()\n\n\nk4.n\n\n1\n\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 1번 보았습니다.\n\n\n\nk4.n = k4.n + 1\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 2번 보았습니다.\n\n\n\nk4.n = k4.n + 1\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 3번 보았습니다.\n\n\n\n결국에는 k4.n = k4.n + 1의 기능을 구현하여 넣은 것이 self.n = self.n + 1 이다.\n따라서 self는 k4에 대응한다. 즉, self는 인스턴스 이름에 대응한다.\n\n우리가 하고 싶은 것은 클래스를 선언하는 시점에서 인스턴스가 생성된 이후 시점에 대한 어떠한 동작들을 정의하고 싶다.\n그런데 클래스가 설계하는 시점에서 인스턴스의 이름이 정해지지 않았으므로 이러한 동작들을 정의하기에 불편하다.\n그래서 클래스를 설계하는 시점에 그 클래스로부터 만들어지는 인스턴스는 그냥 self라는 가칭으로 부른다.\n\n굳이 비유를 하자면 self는 인스턴스의 태명 같은 것이다.\n\n\n요약: self의 의미는 (후에 만들어질 ) 인스턴스의 이름이다. (즉, self는 인스턴스의 태명같은 것!)\n\n\n\n탐구: 인스턴스의 자료형이 무엇인지 탐구해보자.\n- 아래의 두 클래스를 선언해보자.\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title=None):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 이미지를 {}번 보았습니다.'.format(self.n))\n        # self.n = self.n + 1\n\n- 인스턴스를 생성해보자.\n\nk4 = Klass4()\ns1 = STOOOP()\n\n\n\n\nk4?\n\n\nType:        Klass4\nString form: &lt;__main__.Klass4 object at 0x7fb4956082b0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\ns1?\n\n\nType:        STOOOP\nString form: &lt;__main__.STOOOP object at 0x7fb495608310&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n- ??? 타입은 자료형 즉, int, float, list 이런 것 아니었나?\n\na = [1,2,3]\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n- 그런데 지금 k4, s1의 타입은 Klass4, STOOOP이다.\n\n가설1 : 사실 파이썬 내부에 Klass4, STOOOP이라는 자료형이 있었다. 그런데 내가 만든 k4, s1이 우연히 그 자료형을 따르는 것! (이건 너무 억지스럽다.)\n가설2: type이 list인 것은 사실 list라는 클래스에서 생긴 인스턴스이다. \\(\\to\\) 리스트 자료형을 찍어낼 수 있는 어떤 클래스가 파이썬에 내부적으로 존재할 것이다. (이게 맞는 것 같다.)\n\n꺠달음1\n- 가설2가 맞다? 그렇다면 아래는 모두 어딘가에서 찍혀진 인스턴스이다.\n\na = [1,2,3]\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\na = 1,2,3\na\n\n(1, 2, 3)\n\n\n\na = 1\na?\n\n\nType:        int\nString form: 1\nDocstring:  \nint([x]) -&gt; integer\nint(x, base=10) -&gt; integer\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n&gt;&gt;&gt; int('0b100', base=0)\n4\n\n\n\n\na = '1'\na?\n\n\nType:        str\nString form: 1\nLength:      1\nDocstring:  \nstr(object='') -&gt; str\nstr(bytes_or_buffer[, encoding[, errors]]) -&gt; str\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\n\n- 그리고 위의 a=[1,2,3] 과 같은 것들은 모두 ‘클래스\\(\\to\\) 인스턴스’ 에 해당하는 과정이었다.\n깨달음2\n- 생각해보니까 아래와 같이 list를 선언하는 방식도 있었음\n\na = list()\na\n\n[]\n\n\n\n이거 지금 생각해보니까 list라는 이름의 클래스에서 a라는 인스턴스를 찍어내는 문법이다?!\n\n- 아래도 가능함\n\na = list((1,2,3))\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\n이것도 지금 보니까 list라는 이름의 클래스에서 a라는 인스턴스를 찍어내는 문법이다. 여기에서 (1,2,3)은 __init__() 의 입력이다.\n\n깨달음3\n- 그러고보니까 각 자료형마다 특수한 기능들이 있었음.\n- a. + tab을 하면 append, clear 등등이 나온다.\n- 이러한 기능은 지금까지 우리가 ‘list자료형 특수기능들’ 이라고 부르면서 사용했었다. 그런데 a가 list 클래스에서 생성된 인스턴스라는 관점에서 보면 이러한 기능들은 list 클래스에서 정의된 메소드라고 볼 수 있다.\n깨달음4 - a.f() 는 f(a) 로 해석 가능하다고 하였다. 이 해석에 따르면 메소드의 첫번째 입력은 메소드가 소속된 인스턴스라고 해석할 수 있다.\n- 동일한 논리로 아래의 코드는 stop() 의 입력에서 s1을 넣는다는 의미이다.\n\ns1.stop()\n\nNone\n멈춰~~~~\n\n\n\n\n\n\n\n\n\n\n아래의 조건에 맞는 클래스를 생성하라.\n\n['가위', '바위'] 와 같은 리스트를 입력으로 받아 인스턴스를 생성한다.\n위의 리스트에서 하나의 값을 뽑는 메소드 f를 가지고 있다.\n\n# 사용예시\na = Klass(['가위', '바위'])\na.f() # 가위가 1/2 바위가 1/2의 확률로 출력\nb = Klass(['가위', '바위', '보'])\nb.f() # 가위, 바위, 보가 1/3의 확률로 출력"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class2.html#contents",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class2.html#contents",
    "title": "class 2단계",
    "section": "",
    "text": "__init__\nself의 의미\n파이썬의 비밀1\n파이썬의 비밀2"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class2.html#imports",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class2.html#imports",
    "title": "class 2단계",
    "section": "",
    "text": "# 이미지 출력을 위한 패키지 불러오기\nfrom PIL import Image\nimport requests\n\n\n\n- STOOOP을 다시 복습\n\nurl1 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\nurl2 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop2.png?raw=true' \n\n\nclass STOOOP:\n    title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~'\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\ns1 = STOOOP() # STOOOP 이라는 클래스에서 s1이라는 인스턴스를 만드는 과정\n\n\ns1.title, s1.url, s1.end\n\n('학교폭력!',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰~~~')\n\n\n\ns1.stop()\n\n학교폭력!\n멈춰~~~\n\n\n\n\n\n- 왜 s1의 default title이 항상 ‘학교폭력’ 이어야 하는가? \\(\\to\\) __init__ 의 개발\n- 성능4: __init__() 함수를 이용하여 ‘클래스 \\(\\to\\) 인스턴스’ 의 시점에서 수행하는 일련의 동작들을 묶어서 수행할 수 있음.\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n- 잘못된 사용\n\ns1 = STOOOP() # 이 시점에서 __init__ 이 수행된다.\n\nTypeError: __init__() missing 1 required positional argument: 'title'\n\n\n- 올바른 사용\n\ns1 = STOOOP('수강신청매크로') # 이 시점에서 __init__ 이 수행된다!\n\n\ns1.title, s1.url, s1.end\n\n('수강신청매크로',\n 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true',\n '멈춰~~~~')\n\n\n\ns1.stop()\n\n수강신청매크로\n멈춰~~~~\n\n\n\n\n\n- 잘못된 사용에서 에러가 발생한 이유는?\nTypeError: __init__() missing 1 required positional argument: 'title'\n\ns1 = STOOOP() 이 실행되는 순간 __init__() 이 내부적으로 실행된다.\n그런데 __init__() 의 첫번째 입력인 self는 입력안해도 무방했음. 그런데 두번째 입력은 title은 입력을 해야했음.\n그런데 title을 입력하지 않아서 발생하는 에러.\n\n- __init__(self, arg1, arg2,...) 함수에 대하여\n\n엄청나게 특별해 보이지만 사실 몇가지 특별한 점을 제외하고는 어떠한 마법도 없는 함수이다.\n특별한 점1: 첫번째 입력으로 반드시 self를 넣어야함. (이건 사실 클래스 내의 메소드 거의 다 그러함)\n특별한 점2: 클래스에서 인스턴스를 만드는 시점에 자동으로 실행된다.\n특별한 점3: __init(self, arg1, arg2,...)의 입력중 self 이외의 입력들은 ‘클래스 \\(\\to\\) 인스턴스’ 시점에서 ’인스턴스이름 = 클래스이름(arg1, arg2,…)’와 같이 사용한다. (이 예제의 경우 STOOOP(title) 와 같이 사용해야함)\n\n- title이 디폴트로 들어가는 상황도 불편했지만, title을 명시적으로 넣지 않으면 에러가 발생하는 것도 불편하다?\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title=None):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\ns2 = STOOOP()\ns3 = STOOOP('KOSPI 하락')\n\n\ns2.stop() # title 없는 경우\n\nNone\n멈춰~~~~\n\n\n\n\n\n\n제목이 없으면 없는대로 잘 출력이 된다.\n\n\ns3.stop() # title = 'KOSPI 하락'\n\nKOSPI 하락\n멈춰~~~~"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class2.html#self의-의미",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class2.html#self의-의미",
    "title": "class 2단계",
    "section": "",
    "text": "- 이전 예제를 복습\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(\"당신은 이 이미지를 {}번 보았습니다\".format(self.n))\n        self.n = self.n+1 \n\n\nk4 = Klass4()\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 2번 보았습니다\n\n\n- 위의 예제는 아래와 같이 구현할 수도 있다.\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 이미지를 {}번 보았습니다.'.format(self.n))\n        # slef.n = self.n + 1\n\n\nk4 = Klass4()\n\n\nk4.n\n\n1\n\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 1번 보았습니다.\n\n\n\nk4.n = k4.n + 1\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 2번 보았습니다.\n\n\n\nk4.n = k4.n + 1\n\n\nk4.show()\n\n\n\n\n당신은 이 이미지를 3번 보았습니다.\n\n\n\n결국에는 k4.n = k4.n + 1의 기능을 구현하여 넣은 것이 self.n = self.n + 1 이다.\n따라서 self는 k4에 대응한다. 즉, self는 인스턴스 이름에 대응한다.\n\n우리가 하고 싶은 것은 클래스를 선언하는 시점에서 인스턴스가 생성된 이후 시점에 대한 어떠한 동작들을 정의하고 싶다.\n그런데 클래스가 설계하는 시점에서 인스턴스의 이름이 정해지지 않았으므로 이러한 동작들을 정의하기에 불편하다.\n그래서 클래스를 설계하는 시점에 그 클래스로부터 만들어지는 인스턴스는 그냥 self라는 가칭으로 부른다.\n\n굳이 비유를 하자면 self는 인스턴스의 태명 같은 것이다.\n\n\n요약: self의 의미는 (후에 만들어질 ) 인스턴스의 이름이다. (즉, self는 인스턴스의 태명같은 것!)\n\n\n\n탐구: 인스턴스의 자료형이 무엇인지 탐구해보자.\n- 아래의 두 클래스를 선언해보자.\n\nclass STOOOP:\n    # title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def __init__(self, title=None):\n        self.title = title\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n\n\nclass Klass4:\n    n = 1\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 이미지를 {}번 보았습니다.'.format(self.n))\n        # self.n = self.n + 1\n\n- 인스턴스를 생성해보자.\n\nk4 = Klass4()\ns1 = STOOOP()\n\n\n\n\nk4?\n\n\nType:        Klass4\nString form: &lt;__main__.Klass4 object at 0x7fb4956082b0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\ns1?\n\n\nType:        STOOOP\nString form: &lt;__main__.STOOOP object at 0x7fb495608310&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n- ??? 타입은 자료형 즉, int, float, list 이런 것 아니었나?\n\na = [1,2,3]\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n- 그런데 지금 k4, s1의 타입은 Klass4, STOOOP이다.\n\n가설1 : 사실 파이썬 내부에 Klass4, STOOOP이라는 자료형이 있었다. 그런데 내가 만든 k4, s1이 우연히 그 자료형을 따르는 것! (이건 너무 억지스럽다.)\n가설2: type이 list인 것은 사실 list라는 클래스에서 생긴 인스턴스이다. \\(\\to\\) 리스트 자료형을 찍어낼 수 있는 어떤 클래스가 파이썬에 내부적으로 존재할 것이다. (이게 맞는 것 같다.)\n\n꺠달음1\n- 가설2가 맞다? 그렇다면 아래는 모두 어딘가에서 찍혀진 인스턴스이다.\n\na = [1,2,3]\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\na = 1,2,3\na\n\n(1, 2, 3)\n\n\n\na = 1\na?\n\n\nType:        int\nString form: 1\nDocstring:  \nint([x]) -&gt; integer\nint(x, base=10) -&gt; integer\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n&gt;&gt;&gt; int('0b100', base=0)\n4\n\n\n\n\na = '1'\na?\n\n\nType:        str\nString form: 1\nLength:      1\nDocstring:  \nstr(object='') -&gt; str\nstr(bytes_or_buffer[, encoding[, errors]]) -&gt; str\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\n\n- 그리고 위의 a=[1,2,3] 과 같은 것들은 모두 ‘클래스\\(\\to\\) 인스턴스’ 에 해당하는 과정이었다.\n깨달음2\n- 생각해보니까 아래와 같이 list를 선언하는 방식도 있었음\n\na = list()\na\n\n[]\n\n\n\n이거 지금 생각해보니까 list라는 이름의 클래스에서 a라는 인스턴스를 찍어내는 문법이다?!\n\n- 아래도 가능함\n\na = list((1,2,3))\na?\n\n\nType:        list\nString form: [1, 2, 3]\nLength:      3\nDocstring:  \nBuilt-in mutable sequence.\nIf no argument is given, the constructor creates a new empty list.\nThe argument must be an iterable if specified.\n\n\n\n\n이것도 지금 보니까 list라는 이름의 클래스에서 a라는 인스턴스를 찍어내는 문법이다. 여기에서 (1,2,3)은 __init__() 의 입력이다.\n\n깨달음3\n- 그러고보니까 각 자료형마다 특수한 기능들이 있었음.\n- a. + tab을 하면 append, clear 등등이 나온다.\n- 이러한 기능은 지금까지 우리가 ‘list자료형 특수기능들’ 이라고 부르면서 사용했었다. 그런데 a가 list 클래스에서 생성된 인스턴스라는 관점에서 보면 이러한 기능들은 list 클래스에서 정의된 메소드라고 볼 수 있다.\n깨달음4 - a.f() 는 f(a) 로 해석 가능하다고 하였다. 이 해석에 따르면 메소드의 첫번째 입력은 메소드가 소속된 인스턴스라고 해석할 수 있다.\n- 동일한 논리로 아래의 코드는 stop() 의 입력에서 s1을 넣는다는 의미이다.\n\ns1.stop()\n\nNone\n멈춰~~~~"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class2.html#homework",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class2.html#homework",
    "title": "class 2단계",
    "section": "",
    "text": "아래의 조건에 맞는 클래스를 생성하라.\n\n['가위', '바위'] 와 같은 리스트를 입력으로 받아 인스턴스를 생성한다.\n위의 리스트에서 하나의 값을 뽑는 메소드 f를 가지고 있다.\n\n# 사용예시\na = Klass(['가위', '바위'])\na.f() # 가위가 1/2 바위가 1/2의 확률로 출력\nb = Klass(['가위', '바위', '보'])\nb.f() # 가위, 바위, 보가 1/3의 확률로 출력"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class10.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class10.html",
    "title": "class 10단계",
    "section": "",
    "text": "문자열 join, matplotlib, 참조와 에일리어싱\n\n\n\n- 예제\n\n'abcd'\n\n'abcd'\n\n\n\nlst = list('abcd')\nlst\n\n['a', 'b', 'c', 'd']\n\n\n\n['a','b','c','d']를 붙여서 'abcd'로 하고 싶은데?\n\n\n''.join(lst)\n\n'abcd'\n\n\n\na='' # string object\n\n\na?\n\n\nType:        str\nString form: \nLength:      0\nDocstring:  \nstr(object='') -&gt; str\nstr(bytes_or_buffer[, encoding[, errors]]) -&gt; str\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\n\n\na.join? # iterable이 와야함.\n\n\nSignature: a.join(iterable, /)\nDocstring:\nConcatenate any number of strings.\nThe string whose method is called is inserted in between each given string.\nThe result is returned as a new string.\nExample: '.'.join(['ab', 'pq', 'rs']) -&gt; 'ab.pq.rs'\nType:      builtin_function_or_method\n\n\n\n\na.join(lst) # lst도 일단 리스트니까 iterable object\n\n'abcd'\n\n\n\nset(dir(lst)) & {'__iter__' , '__next__'} # iterable object임을 확인\n\n{'__iter__'}\n\n\n- 해설: ''는 string object이고, .join는 string object에 소속된 메서드이다.\n\na = ''\na.join(lst) # join(a,lst) 와 같은 효과\n\n'abcd'\n\n\n- join의 간단한 사용방법\n\n'-'.join(lst)\n\n'a-b-c-d'\n\n\n\n\n\n- 파이썬의 모든것은 객체이다:matplotlib의 다른 사용 (객체지향적 언어로 그림그리기!)\n- 그림오브젝트 생성\n\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure() # plt라는 모듈안에서 figure()라는 함수 실행\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n그림오브젝트가 실행되고 fig라는 이름이 붙음\n\nid(fig)\n\n140529470770096\n\n\n\nfig\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n- 그림오브젝트의 액시즈를 확인 -&gt; 아무것도 없음\n\nfig.axes\n\n[]\n\n\n- (0,0) 자리에 (가로=1, 세로=1) 크기의 액시즈를 넣어보자.\n\nfig.add_axes([0,0,1,1])\n\n&lt;Axes: &gt;\n\n\n\nfig.axes\n\n[&lt;Axes: &gt;]\n\n\n\n아까는 빈 리스트였는데 뭔가 추가되어 있다.\n\n\nfig\n\n\n\n\n- (0,1.2) 위치에 (가로=1,세로=1) 크기의 엑시즈 추가\n\nfig.add_axes([0,1.2, 1,1])\nfig\n\n\n\n\n- (0.5,0.5) 위치에 (가로=1, 세로=1) 크기의 그림 추가\n\nfig.add_axes([0.5,0.5,1,1])\n\n&lt;Axes: &gt;\n\n\n\nfig\n\n\n\n\n- fig의 세번째 엑시즈에 접근\n\na3 = fig.axes[2] # 이것역시 오브젝트임.\na3\n\n&lt;Axes: &gt;\n\n\n\nid(fig.axes[2]) # 어딘가에 저장이 되어있으니까 오브젝트!\n\n140529466106976\n\n\n- 엑시즈의 메소드 중에 plot이 있음 \\(\\to\\) 이것으로 그림을 그려봄.\n\na3.plot([1,2,3],[4,5,3],'--r')\n\n\nfig\n\n\n\n\n- 다시 세번째 축에 접근하여 다른 그림을 그려보자.\n\nfig.axes[-1].plot([1,2,3],[5,4,3],':o')\nfig\n\n\n\n\n- 이제 첫번째 축에 접근하여 새로운 그림을 그려보자.\n\nfig.axes[0].plot([1,2,3],[4,1,4],'--b')\nfig\n\n\n\n\n- 클래스에 대한 이해가 없다면 위와 같은 그림을 그리기도 힘들고 코드를 해석하기도 힘듬\n\n\n\n\n# !conda install -c conda-forge rise -y\n\n- 아래의 코드를 관찰하자.\n\na = [1,2,3]\nb = a\n\n\na, b\n\n([1, 2, 3], [1, 2, 3])\n\n\n\nid(a), id(b)\n\n(140529440320192, 140529440320192)\n\n\n같은 방문 앞에 a라는 포스트잇과, b라는 포스트잇이 같이 붙어있었음.\n\na = a + [4] ## 추가\na,b\n\n([1, 2, 3, 4], [1, 2, 3])\n\n\n\nid(a), id(b) # id 추적 -&gt; a의 id 달라짐.\n\n(140529450518400, 140529440320192)\n\n\n새로운 공간(다른방)에 a라는 포스트잇을 붙인것 (방이 바뀐것)\n- 이제 다시 아래의 코드를 관찰하자.\n\na = [1,2,3]\nb = a\na.append(4)\n\n현재 a,b의 출력결과는?\n\na, b\n\n([1, 2, 3, 4], [1, 2, 3, 4])\n\n\n- 아래의 코드를 다시 살펴보자.\n\na = [1,2,3]\nb = a\na.append(4)\n\na,b라는 변수들은 메모리에 어떻게 저장이 되어있을까?\n상상력을 조금 발휘하면 아래와 같이 여길 수 있다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\n\na = [1,2,3]\n\n\n메모리주소1에 존재하는 방을 a라고 하고, 그 방에 [1,2,3]을 넣는다.\n\n\n아래를 실행하였을 경우\n\n\nb = a\n\n\n메모리주소 38에 존재하는 방을 b라고 하고, 그 방에 a를 넣어야하는데 a는 [1,2,3]이니까 [1,2,3]을 넣는다.\n\n\n아래를 실행하면\n\n\na.append(4)\n\n\n방 a로 가서 [1,2,3]을 [1,2,3,4]로 바꾼다.\n그리고 방 b에는 아무것도 하지 않는다.\n\n- R에서는 맞는 비유인데, 파이썬은 적절하지 않은 비유이다.\n\nid(a)\n\n140529439072192\n\n\n\nid(b)\n\n140529439072192\n\n\n실제로는 a,b가 저장된 메모리 주소가 동일함\n- 파이썬에서는 아래가 더 적절한 비유이다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\n\na = [1,2,3]\n\n\n메모리주소 140529439072192에서 [1,2,3]을 생성한다.\n방 140529439072192의 방문에 a라는 포스트잇을 붙인다.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방중에서 a라는 포스트잇이 붙은 방을 찾아가면 된다.\n\n\n아래를 실행하였을 경우\n\n\nb=a\n\n\na 라는 포스트잇이 있는데, a라는 포스트잇이랑 b라는 포스트잇과 같은 효과를 주도록 한다.\n쉽게말하면, b라는 포스트잇을 방 140529439072192의 방문에 붙인다는 이야기.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방 중에서 a라는 포스트잇이 붙어있거나, b라는 포스트잇이 붙어있는 방을 찾아가면 된다.\n\n\n아래를 실행하면\n\n\na.append(4)\n\n\na라는 포스트잇이 붙어있는 방으로 가서, 그 내용물 append함수를 써서 4를 추가하라. 즉 내용물 [1,2,3]을 [1,2,3,4]로 바꾸라.\n같은방에 a,b라는 포스트잇이 모두 붙어있음. 따라서 b라는 포스트잇이 붙은 방을 찾아가서 내용물을 열어보면 [1,2,3,4]가 나온다.\n\n- 결론: 파이썬의 모든것은 오브젝트이다. 그리고 모든 오브젝트는 메모리주소 위에 올라간다. 하지만 그 메모리 주소에 붙어있는 포스트잇이 하나라는 보장은 없다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class10.html#문자열-join",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class10.html#문자열-join",
    "title": "class 10단계",
    "section": "",
    "text": "- 예제\n\n'abcd'\n\n'abcd'\n\n\n\nlst = list('abcd')\nlst\n\n['a', 'b', 'c', 'd']\n\n\n\n['a','b','c','d']를 붙여서 'abcd'로 하고 싶은데?\n\n\n''.join(lst)\n\n'abcd'\n\n\n\na='' # string object\n\n\na?\n\n\nType:        str\nString form: \nLength:      0\nDocstring:  \nstr(object='') -&gt; str\nstr(bytes_or_buffer[, encoding[, errors]]) -&gt; str\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.\n\n\n\n\na.join? # iterable이 와야함.\n\n\nSignature: a.join(iterable, /)\nDocstring:\nConcatenate any number of strings.\nThe string whose method is called is inserted in between each given string.\nThe result is returned as a new string.\nExample: '.'.join(['ab', 'pq', 'rs']) -&gt; 'ab.pq.rs'\nType:      builtin_function_or_method\n\n\n\n\na.join(lst) # lst도 일단 리스트니까 iterable object\n\n'abcd'\n\n\n\nset(dir(lst)) & {'__iter__' , '__next__'} # iterable object임을 확인\n\n{'__iter__'}\n\n\n- 해설: ''는 string object이고, .join는 string object에 소속된 메서드이다.\n\na = ''\na.join(lst) # join(a,lst) 와 같은 효과\n\n'abcd'\n\n\n- join의 간단한 사용방법\n\n'-'.join(lst)\n\n'a-b-c-d'"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class10.html#matplotlib",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class10.html#matplotlib",
    "title": "class 10단계",
    "section": "",
    "text": "- 파이썬의 모든것은 객체이다:matplotlib의 다른 사용 (객체지향적 언어로 그림그리기!)\n- 그림오브젝트 생성\n\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure() # plt라는 모듈안에서 figure()라는 함수 실행\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n그림오브젝트가 실행되고 fig라는 이름이 붙음\n\nid(fig)\n\n140529470770096\n\n\n\nfig\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n- 그림오브젝트의 액시즈를 확인 -&gt; 아무것도 없음\n\nfig.axes\n\n[]\n\n\n- (0,0) 자리에 (가로=1, 세로=1) 크기의 액시즈를 넣어보자.\n\nfig.add_axes([0,0,1,1])\n\n&lt;Axes: &gt;\n\n\n\nfig.axes\n\n[&lt;Axes: &gt;]\n\n\n\n아까는 빈 리스트였는데 뭔가 추가되어 있다.\n\n\nfig\n\n\n\n\n- (0,1.2) 위치에 (가로=1,세로=1) 크기의 엑시즈 추가\n\nfig.add_axes([0,1.2, 1,1])\nfig\n\n\n\n\n- (0.5,0.5) 위치에 (가로=1, 세로=1) 크기의 그림 추가\n\nfig.add_axes([0.5,0.5,1,1])\n\n&lt;Axes: &gt;\n\n\n\nfig\n\n\n\n\n- fig의 세번째 엑시즈에 접근\n\na3 = fig.axes[2] # 이것역시 오브젝트임.\na3\n\n&lt;Axes: &gt;\n\n\n\nid(fig.axes[2]) # 어딘가에 저장이 되어있으니까 오브젝트!\n\n140529466106976\n\n\n- 엑시즈의 메소드 중에 plot이 있음 \\(\\to\\) 이것으로 그림을 그려봄.\n\na3.plot([1,2,3],[4,5,3],'--r')\n\n\nfig\n\n\n\n\n- 다시 세번째 축에 접근하여 다른 그림을 그려보자.\n\nfig.axes[-1].plot([1,2,3],[5,4,3],':o')\nfig\n\n\n\n\n- 이제 첫번째 축에 접근하여 새로운 그림을 그려보자.\n\nfig.axes[0].plot([1,2,3],[4,1,4],'--b')\nfig\n\n\n\n\n- 클래스에 대한 이해가 없다면 위와 같은 그림을 그리기도 힘들고 코드를 해석하기도 힘듬"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class10.html#참조와-에일리어싱",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class10.html#참조와-에일리어싱",
    "title": "class 10단계",
    "section": "",
    "text": "# !conda install -c conda-forge rise -y\n\n- 아래의 코드를 관찰하자.\n\na = [1,2,3]\nb = a\n\n\na, b\n\n([1, 2, 3], [1, 2, 3])\n\n\n\nid(a), id(b)\n\n(140529440320192, 140529440320192)\n\n\n같은 방문 앞에 a라는 포스트잇과, b라는 포스트잇이 같이 붙어있었음.\n\na = a + [4] ## 추가\na,b\n\n([1, 2, 3, 4], [1, 2, 3])\n\n\n\nid(a), id(b) # id 추적 -&gt; a의 id 달라짐.\n\n(140529450518400, 140529440320192)\n\n\n새로운 공간(다른방)에 a라는 포스트잇을 붙인것 (방이 바뀐것)\n- 이제 다시 아래의 코드를 관찰하자.\n\na = [1,2,3]\nb = a\na.append(4)\n\n현재 a,b의 출력결과는?\n\na, b\n\n([1, 2, 3, 4], [1, 2, 3, 4])\n\n\n- 아래의 코드를 다시 살펴보자.\n\na = [1,2,3]\nb = a\na.append(4)\n\na,b라는 변수들은 메모리에 어떻게 저장이 되어있을까?\n상상력을 조금 발휘하면 아래와 같이 여길 수 있다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\n\na = [1,2,3]\n\n\n메모리주소1에 존재하는 방을 a라고 하고, 그 방에 [1,2,3]을 넣는다.\n\n\n아래를 실행하였을 경우\n\n\nb = a\n\n\n메모리주소 38에 존재하는 방을 b라고 하고, 그 방에 a를 넣어야하는데 a는 [1,2,3]이니까 [1,2,3]을 넣는다.\n\n\n아래를 실행하면\n\n\na.append(4)\n\n\n방 a로 가서 [1,2,3]을 [1,2,3,4]로 바꾼다.\n그리고 방 b에는 아무것도 하지 않는다.\n\n- R에서는 맞는 비유인데, 파이썬은 적절하지 않은 비유이다.\n\nid(a)\n\n140529439072192\n\n\n\nid(b)\n\n140529439072192\n\n\n실제로는 a,b가 저장된 메모리 주소가 동일함\n- 파이썬에서는 아래가 더 적절한 비유이다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\n\na = [1,2,3]\n\n\n메모리주소 140529439072192에서 [1,2,3]을 생성한다.\n방 140529439072192의 방문에 a라는 포스트잇을 붙인다.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방중에서 a라는 포스트잇이 붙은 방을 찾아가면 된다.\n\n\n아래를 실행하였을 경우\n\n\nb=a\n\n\na 라는 포스트잇이 있는데, a라는 포스트잇이랑 b라는 포스트잇과 같은 효과를 주도록 한다.\n쉽게말하면, b라는 포스트잇을 방 140529439072192의 방문에 붙인다는 이야기.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방 중에서 a라는 포스트잇이 붙어있거나, b라는 포스트잇이 붙어있는 방을 찾아가면 된다.\n\n\n아래를 실행하면\n\n\na.append(4)\n\n\na라는 포스트잇이 붙어있는 방으로 가서, 그 내용물 append함수를 써서 4를 추가하라. 즉 내용물 [1,2,3]을 [1,2,3,4]로 바꾸라.\n같은방에 a,b라는 포스트잇이 모두 붙어있음. 따라서 b라는 포스트잇이 붙은 방을 찾아가서 내용물을 열어보면 [1,2,3,4]가 나온다.\n\n- 결론: 파이썬의 모든것은 오브젝트이다. 그리고 모든 오브젝트는 메모리주소 위에 올라간다. 하지만 그 메모리 주소에 붙어있는 포스트잇이 하나라는 보장은 없다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class9.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class9.html",
    "title": "class 9단계",
    "section": "",
    "text": "global/local 변수, 인스턴스/클래스 변수, 인스턴스/클래스 메서드\n\n\n\n커널을 재시작하고 아래를 관찰하자.\n\n\n- 관찰1: 함수내의 변수 출력\n\ndef f():\n    x = 10\n    print(x)\n\n\nf()\n\n10\n\n\n- 관찰2: 함수내의 변수가 없을 경우 출력이 되지 않음\n\ndef g():\n    print(x)\n\n\ng()\n\nNameError: name 'x' is not defined\n\n\n- 관찰3: 동일한 이름의 변수가 global에 있다면 함수내에 (local) 그 이름의 변수가 선언되지 않아도 global 변수를 빌려서 사용함.\n\nx = 20\ndef g():\n    print(x)\n\n\ng()\n\n20\n\n\n- 관찰4: f()가 실행되면서 x=10이 함수내에(=local에) 실행되지만 이 결과가 외부의 x=20에(=global에) 영향을 미치지는 못함.\n\nf()\n\n10\n\n\n\nx\n\n20\n\n\n\n\n\n(코드1)\n\nx = 38\ndef nextyear():\n    y = x+1\n    print(x,y)\nnextyear()\n\n38 39\n\n\n(코드2)\n\nx = 38\ndef nextyear():\n    y = x+1\n    print(x,y)\n    x = 0\nnextyear()\n\nUnboundLocalError: local variable 'x' referenced before assignment\n\n\n- 해석: - 잘못된해석: 코드1은 실행되었고, 코드2에서 에러가 났다. 코드1과 2의 차이점은 x=0 이라는 코드가 코드2에 추가로 포함되어있다는 것이다. 따라서 x=0이 잘못된 코드이고 이걸 실행하는 과정에서 에러가 발생했다.\n\n올바른해석: 코드1에서는 x가 global variable이고 코드2에서는 x가 local variable이어서 생기는 문제\n\n- 코드2의 올바른 수정\n\nx = 38\ndef nextyear():\n    x = 0\n    y = x+1\n    print(x,y)\nnextyear()\n\n0 1\n\n\n\n\n\n\n- 예비학습이 주는 교훈\n(원칙1) global에서 정의된 이름은 local에서 정의된 이름이 없을 경우 그를 대신할 수 있다. (local은 경우에 따라서 global에 있는 변수를 빌려 쓸 수 있다.)\n(원칙2) local과 global에서 같은 이름 ’x’가 각각 정의되어 있는 경우? global의 변수와 local의 변수는 각각 따로 행동하여 서로 영향을 주지 않는다. (독립적이다)\n\n만약에 local에 global의 변수를 같이 쓰고 있었다고 할지라도, 추후 새롭게 local에 이름이 새롭게 정의된다면 그 순간 local과 global의 변수를 각자 따로 행동하며 서로 영향을 주지 않는다.\\(\\to\\) 아래예제 확인\n\n\nx = 10\ndef f():\n    print(x)\n\n\nf() # x를 빌려쓰는 신세\n\n10\n\n\n\ndef f():\n    x = 20 # 이제 새롭게 x를 정의했으니까\n    print(x)\n\n\nf() # 다른길을 간다.\n\n20\n\n\n- 이전에 공부하였던 인스턴스변수와 클래스변수 역시 비슷한 행동을 보인다.\n\nclass Moo:\n    x = 0 # 클래스 변수\n\n\nmoo=Moo()\n\n(관찰1)\n\nMoo.x, moo.x\n\n(0, 0)\n\n\n\nmoo.x는 사실 정의한적이 없지만 Moo.x를 빌려쓰고 있다. (원칙1)\n\n(관찰2)\n\nMoo.x = 100\n\n\nMoo.x, moo.x\n\n(100, 100)\n\n\n\nMoo.x를 변화시키면 moo.x도 변화한다. (빌려쓰고 있는 것이니까, 원칙1의 재확인)\n\n(관찰3)\n\nmoo.x = 200\n\n\nMoo.x, moo.x\n\n(100, 200)\n\n\n\nmoo.x=200을 하는 순간 새롭게 인스턴스변수를 선언한 셈이된다. 따라서 원칙2가 적용되어 이제부터 Moo.x와 moo.x는 서로 독립적으로 행동한다.\n\n(관찰4)\n\nMoo.x = -99\n\n\nMoo.x, moo.x\n\n(-99, 200)\n\n\n\nmoo.x = 99\n\n\nMoo.x, moo.x\n\n(-99, 99)\n\n\n\nMoo.x를 바꾼다고 해서 moo.x가 영향받지 않고 moo.x를 바꿔도 Moo.x가 영향받지 않음. (완전히 독립, 원칙2의 재확인)\n\n\n\n\n\n클래스변수와 인스턴스 변수의 구분\n\n\n인스턴스 변수가 정의되지 않으면 클래스변수를 빌려쓸 수 있음(클래스변수가 상위개념)\n\n\n인스턴스변수와 클래스변수가 같은 이름으로 저장되어 있으면 각각 독립적으로 행동\n\n\n\n\n\n\n- self 비밀: 사실 클래스에서 정의된 함수의 첫번째 인자의 이름이 꼭 self일 필요는 없다. (무엇으로 전달하든 클래스 안에서 정의된 메소드의 첫번째 인자는 기본적으로 태명역할을 한다.)\n\nclass Moo:\n    def __init__(self):\n        self.name = 'jordy'\n    def f(self):\n        print(self.name)\n\n\nmoo = Moo()\n\n\nmoo.name\n\n'jordy'\n\n\n\nmoo.f()\n\njordy\n\n\n\n꼭 위와 같이 할 필요는 없다.\n\n\nclass Moo:\n    def __init__(abab):\n        abab.name = 'jordy'\n    def f(cdcd):\n        print(cdcd.name)\n\n\nmoo = Moo()\n\n\nmoo.name\n\n'jordy'\n\n\n\nmoo.f()\n\njordy\n\n\n- 인스턴스 메서드: 위의 __init__와 f와 같이 첫번째 인자를 인스턴스의 태명으로 받는 함수를 인스턴스 메서드 (간단히 메서드) 라고 한다.\n\n인스턴스 메소드는 self.f()와 같이 사용한다. 의미는 f(self) 이다.\n\n\nmoo.name = 'chunsik'\n\n\nmoo.name\n\n'chunsik'\n\n\n\nmoo.__init__()\n\n\nmoo.name # 인스턴스 메서드의 사용예시: self.__init__()의 꼴로 사용\n\n'jordy'\n\n\n\n오 신기하다.\n\n- 아래와 같이 사용할 수 없다.\n\nMoo.__init__() # 인스턴스가 들어와야하는데 클래스가 들어와버려서 이렇게 쓸순 없다.\n\nTypeError: __init__() missing 1 required positional argument: 'abab'\n\n\n\n인스턴스 메소드이기때문에 에러가 난다. 즉, 첫번째 입력 (.__init__()앞에)에 인스턴스가 들어가야 하는데 클래스가 들어와버렸다.\n\n\n\n\n- 클래스 메서드: 함수의 첫 인자로 클래스오브젝트를 받는 메서드를 클래스메서드라고 한다.\n- 목표: Moo.f() 와 같은 형태로 사용할 수 있는 함수를 만들어 보자. \\(\\to\\) 클래스메서드를 만들어보자.\n\nclass Moo:\n    def f(self): # 클래스 안에서 함수를 선언하면 디폴트로 인스턴스 메서드가 만들어진다.\n        print('인스턴스 메서드') \n\n\nmoo = Moo()\n\n\nmoo.f()\n\n인스턴스 메서드\n\n\n\nMoo.f() # 인스턴스 메서드니까 안되는게 당연\n\nTypeError: f() missing 1 required positional argument: 'self'\n\n\n\nclass Moo:\n    @classmethod\n    def f(cls): # 함수의 첫 인자로 클래스오브젝트를 받는다. cls는 클래스 Moo의 별명? 이라고 생각하면 된다.\n        print('클래스 메서드')\n\n\nmoo = Moo()\n\n\nMoo.f()\n\n클래스 메서드\n\n\n\nmoo.f() # 인스턴스 메서드를 따로 정의한적은 없지만 같은 이름의 클래스 메서드가 있으므로 빌려와서 씀!\n\n클래스 메서드\n\n\n- 예제\n\nclass Moo:\n    @classmethod\n    def set_class_x(cls, value): # 클래스 메서드\n        cls.x = value # 클래스변수선언, Moo.x = value와 같은 코드!\n    def set_instance_x(self, value): # 인스턴스 메서드\n        self.x = value # 인스턴스 변수선언\n\n\nmoo = Moo()\n\n\nMoo.set_class_x(10) # 클래스메서드로 클래스변수에 10을 설정\n\n\nMoo.x\n\n10\n\n\n\nMoo.set_instance_x(10) # 클래스에서 인스턴스 메서드 사용 -&gt; 사용불가\n\nTypeError: set_instance_x() missing 1 required positional argument: 'value'\n\n\n\nMoo.x, moo.x # 인스턴스변수는 따로 설정하지 않았지만 클래스 변수값을 빌려쓰고 있음\n\n(10, 10)\n\n\n\nmoo.set_class_x(20) # 인스턴스에서는 원래 set_class_x 라는 메서드는 없지만 클래스에서 빌려씀\n\n\nMoo.x, moo.x # 현재 moo.x(인스턴스)는 클래스 변수를 빌려쓰고 있는 상황이므로 같이 바뀜\n\n(20, 20)\n\n\n\nmoo.set_instance_x(-20) \n# 인스턴스에서 인스턴스 메서드를 사용하여 인스턴스 변수값을 -20으로 설정 \n# -&gt; 이때부터 인스턴스 변수와 클래스 변수는 서로 독립적인 노선을 간다.\n\n\nMoo.set_class_x(30) # 독립적인 노선을 가기로 헀으므로 클래스변수만 30으로 바뀜.\nMoo.x, moo.x\n\n(30, -20)\n\n\n\nmoo.set_class_x(-40) # 여전히 인스턴스에서 set_class_x라는 함수는 없으므로 클래스메소드를 빌려쓰고 있음.\n\n\n\n\n- 스태틱 메서드: 첫 인자로 인스턴스와 클래스 모두 받지 않음. (클래스안에 정의되어 있지만 그냥 함수와 같음)\n\nclass Cals:\n    @staticmethod\n    def add(a,b):\n        return a+b\n    @staticmethod\n    def sub(a,b):\n        return a-b\n\n\nfs = Cals()\n\n\nfs.add(1,2)\n\n3\n\n\n\nfs.sub(1,2)\n\n-1\n\n\n\nfs는 그냥 함수들을 묶어놓은 느낌? 정리하기 편하게?"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class9.html#예비학습-변수의-범위",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class9.html#예비학습-변수의-범위",
    "title": "class 9단계",
    "section": "",
    "text": "커널을 재시작하고 아래를 관찰하자.\n\n\n- 관찰1: 함수내의 변수 출력\n\ndef f():\n    x = 10\n    print(x)\n\n\nf()\n\n10\n\n\n- 관찰2: 함수내의 변수가 없을 경우 출력이 되지 않음\n\ndef g():\n    print(x)\n\n\ng()\n\nNameError: name 'x' is not defined\n\n\n- 관찰3: 동일한 이름의 변수가 global에 있다면 함수내에 (local) 그 이름의 변수가 선언되지 않아도 global 변수를 빌려서 사용함.\n\nx = 20\ndef g():\n    print(x)\n\n\ng()\n\n20\n\n\n- 관찰4: f()가 실행되면서 x=10이 함수내에(=local에) 실행되지만 이 결과가 외부의 x=20에(=global에) 영향을 미치지는 못함.\n\nf()\n\n10\n\n\n\nx\n\n20\n\n\n\n\n\n(코드1)\n\nx = 38\ndef nextyear():\n    y = x+1\n    print(x,y)\nnextyear()\n\n38 39\n\n\n(코드2)\n\nx = 38\ndef nextyear():\n    y = x+1\n    print(x,y)\n    x = 0\nnextyear()\n\nUnboundLocalError: local variable 'x' referenced before assignment\n\n\n- 해석: - 잘못된해석: 코드1은 실행되었고, 코드2에서 에러가 났다. 코드1과 2의 차이점은 x=0 이라는 코드가 코드2에 추가로 포함되어있다는 것이다. 따라서 x=0이 잘못된 코드이고 이걸 실행하는 과정에서 에러가 발생했다.\n\n올바른해석: 코드1에서는 x가 global variable이고 코드2에서는 x가 local variable이어서 생기는 문제\n\n- 코드2의 올바른 수정\n\nx = 38\ndef nextyear():\n    x = 0\n    y = x+1\n    print(x,y)\nnextyear()\n\n0 1"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class9.html#인스턴스-변수-클래스-변수",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class9.html#인스턴스-변수-클래스-변수",
    "title": "class 9단계",
    "section": "",
    "text": "- 예비학습이 주는 교훈\n(원칙1) global에서 정의된 이름은 local에서 정의된 이름이 없을 경우 그를 대신할 수 있다. (local은 경우에 따라서 global에 있는 변수를 빌려 쓸 수 있다.)\n(원칙2) local과 global에서 같은 이름 ’x’가 각각 정의되어 있는 경우? global의 변수와 local의 변수는 각각 따로 행동하여 서로 영향을 주지 않는다. (독립적이다)\n\n만약에 local에 global의 변수를 같이 쓰고 있었다고 할지라도, 추후 새롭게 local에 이름이 새롭게 정의된다면 그 순간 local과 global의 변수를 각자 따로 행동하며 서로 영향을 주지 않는다.\\(\\to\\) 아래예제 확인\n\n\nx = 10\ndef f():\n    print(x)\n\n\nf() # x를 빌려쓰는 신세\n\n10\n\n\n\ndef f():\n    x = 20 # 이제 새롭게 x를 정의했으니까\n    print(x)\n\n\nf() # 다른길을 간다.\n\n20\n\n\n- 이전에 공부하였던 인스턴스변수와 클래스변수 역시 비슷한 행동을 보인다.\n\nclass Moo:\n    x = 0 # 클래스 변수\n\n\nmoo=Moo()\n\n(관찰1)\n\nMoo.x, moo.x\n\n(0, 0)\n\n\n\nmoo.x는 사실 정의한적이 없지만 Moo.x를 빌려쓰고 있다. (원칙1)\n\n(관찰2)\n\nMoo.x = 100\n\n\nMoo.x, moo.x\n\n(100, 100)\n\n\n\nMoo.x를 변화시키면 moo.x도 변화한다. (빌려쓰고 있는 것이니까, 원칙1의 재확인)\n\n(관찰3)\n\nmoo.x = 200\n\n\nMoo.x, moo.x\n\n(100, 200)\n\n\n\nmoo.x=200을 하는 순간 새롭게 인스턴스변수를 선언한 셈이된다. 따라서 원칙2가 적용되어 이제부터 Moo.x와 moo.x는 서로 독립적으로 행동한다.\n\n(관찰4)\n\nMoo.x = -99\n\n\nMoo.x, moo.x\n\n(-99, 200)\n\n\n\nmoo.x = 99\n\n\nMoo.x, moo.x\n\n(-99, 99)\n\n\n\nMoo.x를 바꾼다고 해서 moo.x가 영향받지 않고 moo.x를 바꿔도 Moo.x가 영향받지 않음. (완전히 독립, 원칙2의 재확인)\n\n\n\n\n\n클래스변수와 인스턴스 변수의 구분\n\n\n인스턴스 변수가 정의되지 않으면 클래스변수를 빌려쓸 수 있음(클래스변수가 상위개념)\n\n\n인스턴스변수와 클래스변수가 같은 이름으로 저장되어 있으면 각각 독립적으로 행동"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class9.html#인스턴스-메서드",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class9.html#인스턴스-메서드",
    "title": "class 9단계",
    "section": "",
    "text": "- self 비밀: 사실 클래스에서 정의된 함수의 첫번째 인자의 이름이 꼭 self일 필요는 없다. (무엇으로 전달하든 클래스 안에서 정의된 메소드의 첫번째 인자는 기본적으로 태명역할을 한다.)\n\nclass Moo:\n    def __init__(self):\n        self.name = 'jordy'\n    def f(self):\n        print(self.name)\n\n\nmoo = Moo()\n\n\nmoo.name\n\n'jordy'\n\n\n\nmoo.f()\n\njordy\n\n\n\n꼭 위와 같이 할 필요는 없다.\n\n\nclass Moo:\n    def __init__(abab):\n        abab.name = 'jordy'\n    def f(cdcd):\n        print(cdcd.name)\n\n\nmoo = Moo()\n\n\nmoo.name\n\n'jordy'\n\n\n\nmoo.f()\n\njordy\n\n\n- 인스턴스 메서드: 위의 __init__와 f와 같이 첫번째 인자를 인스턴스의 태명으로 받는 함수를 인스턴스 메서드 (간단히 메서드) 라고 한다.\n\n인스턴스 메소드는 self.f()와 같이 사용한다. 의미는 f(self) 이다.\n\n\nmoo.name = 'chunsik'\n\n\nmoo.name\n\n'chunsik'\n\n\n\nmoo.__init__()\n\n\nmoo.name # 인스턴스 메서드의 사용예시: self.__init__()의 꼴로 사용\n\n'jordy'\n\n\n\n오 신기하다.\n\n- 아래와 같이 사용할 수 없다.\n\nMoo.__init__() # 인스턴스가 들어와야하는데 클래스가 들어와버려서 이렇게 쓸순 없다.\n\nTypeError: __init__() missing 1 required positional argument: 'abab'\n\n\n\n인스턴스 메소드이기때문에 에러가 난다. 즉, 첫번째 입력 (.__init__()앞에)에 인스턴스가 들어가야 하는데 클래스가 들어와버렸다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class9.html#클래스-메서드",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class9.html#클래스-메서드",
    "title": "class 9단계",
    "section": "",
    "text": "- 클래스 메서드: 함수의 첫 인자로 클래스오브젝트를 받는 메서드를 클래스메서드라고 한다.\n- 목표: Moo.f() 와 같은 형태로 사용할 수 있는 함수를 만들어 보자. \\(\\to\\) 클래스메서드를 만들어보자.\n\nclass Moo:\n    def f(self): # 클래스 안에서 함수를 선언하면 디폴트로 인스턴스 메서드가 만들어진다.\n        print('인스턴스 메서드') \n\n\nmoo = Moo()\n\n\nmoo.f()\n\n인스턴스 메서드\n\n\n\nMoo.f() # 인스턴스 메서드니까 안되는게 당연\n\nTypeError: f() missing 1 required positional argument: 'self'\n\n\n\nclass Moo:\n    @classmethod\n    def f(cls): # 함수의 첫 인자로 클래스오브젝트를 받는다. cls는 클래스 Moo의 별명? 이라고 생각하면 된다.\n        print('클래스 메서드')\n\n\nmoo = Moo()\n\n\nMoo.f()\n\n클래스 메서드\n\n\n\nmoo.f() # 인스턴스 메서드를 따로 정의한적은 없지만 같은 이름의 클래스 메서드가 있으므로 빌려와서 씀!\n\n클래스 메서드\n\n\n- 예제\n\nclass Moo:\n    @classmethod\n    def set_class_x(cls, value): # 클래스 메서드\n        cls.x = value # 클래스변수선언, Moo.x = value와 같은 코드!\n    def set_instance_x(self, value): # 인스턴스 메서드\n        self.x = value # 인스턴스 변수선언\n\n\nmoo = Moo()\n\n\nMoo.set_class_x(10) # 클래스메서드로 클래스변수에 10을 설정\n\n\nMoo.x\n\n10\n\n\n\nMoo.set_instance_x(10) # 클래스에서 인스턴스 메서드 사용 -&gt; 사용불가\n\nTypeError: set_instance_x() missing 1 required positional argument: 'value'\n\n\n\nMoo.x, moo.x # 인스턴스변수는 따로 설정하지 않았지만 클래스 변수값을 빌려쓰고 있음\n\n(10, 10)\n\n\n\nmoo.set_class_x(20) # 인스턴스에서는 원래 set_class_x 라는 메서드는 없지만 클래스에서 빌려씀\n\n\nMoo.x, moo.x # 현재 moo.x(인스턴스)는 클래스 변수를 빌려쓰고 있는 상황이므로 같이 바뀜\n\n(20, 20)\n\n\n\nmoo.set_instance_x(-20) \n# 인스턴스에서 인스턴스 메서드를 사용하여 인스턴스 변수값을 -20으로 설정 \n# -&gt; 이때부터 인스턴스 변수와 클래스 변수는 서로 독립적인 노선을 간다.\n\n\nMoo.set_class_x(30) # 독립적인 노선을 가기로 헀으므로 클래스변수만 30으로 바뀜.\nMoo.x, moo.x\n\n(30, -20)\n\n\n\nmoo.set_class_x(-40) # 여전히 인스턴스에서 set_class_x라는 함수는 없으므로 클래스메소드를 빌려쓰고 있음."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class9.html#스태틱-메서드",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class9.html#스태틱-메서드",
    "title": "class 9단계",
    "section": "",
    "text": "- 스태틱 메서드: 첫 인자로 인스턴스와 클래스 모두 받지 않음. (클래스안에 정의되어 있지만 그냥 함수와 같음)\n\nclass Cals:\n    @staticmethod\n    def add(a,b):\n        return a+b\n    @staticmethod\n    def sub(a,b):\n        return a-b\n\n\nfs = Cals()\n\n\nfs.add(1,2)\n\n3\n\n\n\nfs.sub(1,2)\n\n-1\n\n\n\nfs는 그냥 함수들을 묶어놓은 느낌? 정리하기 편하게?"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class3.html",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class3.html",
    "title": "class 3단계",
    "section": "",
    "text": "이 단계에서는 클래스오브젝트에 소속된 변수와 인스턴스오브젝트에 소속된 변수를 설명한다.\n\n\n\n- 파이썬은 모든 것이 오브젝트로 이루어져 있다. \\(\\leftarrow\\) 우선 그냥 외우기!\n- 오브젝트는 메모리 주소에 저장되는 모든 것을 의미한다.\n\na = 1\nid(a) # 메모리주소를 보는 명령어\n\n7618240\n\n\n\na = 'asdf'\nid(a)\n\n140366991918512\n\n\n\na = [1,2,3]\nid(a)\n\n140366923845376\n\n\n- 클래스와 인스턴스도 오브젝트다.\n\nclass A:\n    x = 0\n    def f(self):\n        print(self.x)\n\n\nid(A)\n\n39987760\n\n\n\nA는 오브젝트\n\n\nb = A()\n\n\nid(b)\n\n140366932540960\n\n\n\nb는 오브젝트\n\n- 앞으로는 A를 클래스 오브젝트, a,b를 인스턴스 오브젝트라고 부르자.\n\n\n- 시점0\n\n# 클래스 선언 시점\nclass A:\n    x = 0\n    y = 0\n    def f(self):\n        self.x = self.x + 1\n        A.y = A.y + 1\n        print('현재 인스턴스에서 f가 {}번 실행'.format(self.x))\n        print('A클래스에서 만들어진 모든 인스턴스들에서 f가 {}번 실행'.format(self.y))\n\n\nid(A) # A라는게 메모리 어딘가에 저장되어 있음.\n\n53014736\n\n\n\nA.x, A.y\n\n(0, 0)\n\n\n- 시점1\n\n# a라는 인스턴스\na = A()\n\n\n# b라는 인스턴스\nb = A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 0], [0, 0], [0, 0])\n\n\n- 시점2\n\na.f() # a에서 f라는 메소드 사용\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 1번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 1], [1, 1], [0, 1])\n\n\n\n여기서 현재 인스턴스라 함은 a를 의미한다.\n\n[1,1] 에서 첫번째 1은 현재 인스턴스(a)에서 f가 1번 실행되었다는 것을 의미하고\n[1,1] 에서 두번째 1은 A클래스에서 만들어진 모든 인스턴스들에서 f가 1번 실행되었음을 의미한다.\n\n\n- 시점3\n\nb.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 2번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 2], [1, 2], [1, 2])\n\n\n\n여기서 현재 인스턴스라 함은 b를 의미한다.\n\n[1,2] 에서 첫번째 1은 현재 인스턴스(b)에서 f가 1번 실행되었다는 것을 의미하고\n[1,2] 에서 두번째 2는 A클래스에서 만들어진 모든 인스턴스들에서 f가 2번 실행되었음을 의미한다 (왜냐면, 위에서 이미 한번 실행을 했기 때문)\n\n\n- 시점4\n\nb.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 3번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 3], [1, 3], [2, 3])\n\n\n- 시점5\n\na.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 4번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 4], [2, 4], [2, 4])\n\n\n- 시점6\n\n# c라는 인스턴스를 만들어보자.\nc = A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y], [c.x, c.y]\n\n([0, 4], [2, 4], [2, 4], [0, 4])\n\n\n- 시점7\n\nc.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 5번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y], [c.x, c.y]\n\n([0, 5], [2, 5], [2, 5], [1, 5])\n\n\n- 신기한 점: 각 인스턴스에서 인스턴스이름.f()를 실행한 횟수를 서로 공유하는 듯 하다. (마치 A가 관리하는 것 처럼 느껴진다.)"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class3.html#오브젝트의-개념",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class3.html#오브젝트의-개념",
    "title": "class 3단계",
    "section": "",
    "text": "- 파이썬은 모든 것이 오브젝트로 이루어져 있다. \\(\\leftarrow\\) 우선 그냥 외우기!\n- 오브젝트는 메모리 주소에 저장되는 모든 것을 의미한다.\n\na = 1\nid(a) # 메모리주소를 보는 명령어\n\n7618240\n\n\n\na = 'asdf'\nid(a)\n\n140366991918512\n\n\n\na = [1,2,3]\nid(a)\n\n140366923845376\n\n\n- 클래스와 인스턴스도 오브젝트다.\n\nclass A:\n    x = 0\n    def f(self):\n        print(self.x)\n\n\nid(A)\n\n39987760\n\n\n\nA는 오브젝트\n\n\nb = A()\n\n\nid(b)\n\n140366932540960\n\n\n\nb는 오브젝트\n\n- 앞으로는 A를 클래스 오브젝트, a,b를 인스턴스 오브젝트라고 부르자.\n\n\n- 시점0\n\n# 클래스 선언 시점\nclass A:\n    x = 0\n    y = 0\n    def f(self):\n        self.x = self.x + 1\n        A.y = A.y + 1\n        print('현재 인스턴스에서 f가 {}번 실행'.format(self.x))\n        print('A클래스에서 만들어진 모든 인스턴스들에서 f가 {}번 실행'.format(self.y))\n\n\nid(A) # A라는게 메모리 어딘가에 저장되어 있음.\n\n53014736\n\n\n\nA.x, A.y\n\n(0, 0)\n\n\n- 시점1\n\n# a라는 인스턴스\na = A()\n\n\n# b라는 인스턴스\nb = A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 0], [0, 0], [0, 0])\n\n\n- 시점2\n\na.f() # a에서 f라는 메소드 사용\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 1번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 1], [1, 1], [0, 1])\n\n\n\n여기서 현재 인스턴스라 함은 a를 의미한다.\n\n[1,1] 에서 첫번째 1은 현재 인스턴스(a)에서 f가 1번 실행되었다는 것을 의미하고\n[1,1] 에서 두번째 1은 A클래스에서 만들어진 모든 인스턴스들에서 f가 1번 실행되었음을 의미한다.\n\n\n- 시점3\n\nb.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 2번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 2], [1, 2], [1, 2])\n\n\n\n여기서 현재 인스턴스라 함은 b를 의미한다.\n\n[1,2] 에서 첫번째 1은 현재 인스턴스(b)에서 f가 1번 실행되었다는 것을 의미하고\n[1,2] 에서 두번째 2는 A클래스에서 만들어진 모든 인스턴스들에서 f가 2번 실행되었음을 의미한다 (왜냐면, 위에서 이미 한번 실행을 했기 때문)\n\n\n- 시점4\n\nb.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 3번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 3], [1, 3], [2, 3])\n\n\n- 시점5\n\na.f()\n\n현재 인스턴스에서 f가 2번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 4번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y]\n\n([0, 4], [2, 4], [2, 4])\n\n\n- 시점6\n\n# c라는 인스턴스를 만들어보자.\nc = A()\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y], [c.x, c.y]\n\n([0, 4], [2, 4], [2, 4], [0, 4])\n\n\n- 시점7\n\nc.f()\n\n현재 인스턴스에서 f가 1번 실행\nA클래스에서 만들어진 모든 인스턴스들에서 f가 5번 실행\n\n\n\n[A.x, A.y], [a.x, a.y], [b.x, b.y], [c.x, c.y]\n\n([0, 5], [2, 5], [2, 5], [1, 5])\n\n\n- 신기한 점: 각 인스턴스에서 인스턴스이름.f()를 실행한 횟수를 서로 공유하는 듯 하다. (마치 A가 관리하는 것 처럼 느껴진다.)"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-23-퓨리에변환4jy.html",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-23-퓨리에변환4jy.html",
    "title": "[Fourier] 퓨리에변환4jy",
    "section": "",
    "text": "회귀분석 느낌으로\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n회귀모형 (1)\n\nx = np.linspace(-10,10,1000)\nx0 = x*0+1\nx1 = x \nbeta0 = 3 \nbeta1 = 2\ny = x0*beta0+x1*beta1+np.random.randn(1000)\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n회귀모형 (2)\n\\[f_i = 2\\times \\sin(2\\pi t_i) + 1 \\times \\sin(4\\pi t_i) + 3\\times\\sin(6\\pi t_i)+\\epsilon_i, \\quad t_i = \\frac{i}{1000}\\]\n회귀분석 느낌의 표현은 아래와 같다.\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3} + \\epsilon_i\\]\n여기서 \\(x_{i1} = \\sin{2\\pi t_i}\\) 이고, \\(x_{i2} = \\sin(4\\pi t_i), \\quad x_{i3} = \\sin(6\\pi t_i)\\).\n- 관측한자료\n\nN=1000\nx=np.linspace(0,1,N)\neps = np.random.randn(N)\nX0 = np.sin(x*0*np.pi)\nX1 = np.sin(x*2*np.pi)\nX2 = np.sin(x*4*np.pi)\nX3 = np.sin(x*6*np.pi)\n\ny_true = 2*X1+1*X2+3*X3\ny= y_true + eps\n\n\nplt.plot(x,y,'o',alpha=0.1)\n# plt.plot(x,y_true)\n\n\n\n\n\nobserved signal\n\n- 위의 자료를 해석하는 방법\n\ndef spec(y):\n    N= len(y)\n    return abs(np.fft.fft(y)/N)*2 \n\n\nreal symmetric frequency\n\n\ny=2*X1+1*X2+3*X3+eps\nyfft =spec(y) \ny1=2*X1\ny2=1*X2\ny3=3*X3\nyfft1=spec(y1)\nyfft2=spec(y2)\nyfft3=spec(y3)\nepsfft=spec(eps)\n\n\nplt.plot(yfft[:20],'o',alpha=0.5)\nplt.plot(yfft1[:20],'x',alpha=1,)\nplt.plot(yfft2[:20],'x',alpha=1)\nplt.plot(yfft3[:20],'x',alpha=1)\nplt.plot(epsfft[:20],'x',alpha=1)\n\n\n\n\n- 퓨리에변환 -&gt; threshold -&gt; 역퓨리에변환을 이용한 스킬\n\nyfft=np.fft.fft(y)\n\n\nplt.plot(abs(yfft[1:50]),'o')\n\n\n\n\n\nyfft[abs(yfft)&lt;100] = 0\n\n\nplt.plot(y,'o',alpha=0.1)\nyhat=np.fft.ifft(yfft)\nplt.plot(yhat,'--')\nplt.plot(y-eps,'-')\n\n/home/jy/anaconda3/envs/torch/lib/python3.8/site-packages/matplotlib/cbook/__init__.py:1335: ComplexWarning: Casting complex values to real discards the imaginary part\n  return np.asarray(x, float)\n\n\n\n\n\n\nplt.plot(spec(y)[:50],'o')\nplt.plot(spec(yhat)[:50],'x')\n\n\n\n\n\n- Fast Fourier Transfors (FFT)\n\\[X(k) = \\frac{1}{N}\\sum_{n=0}^{N-1} x(n) \\cdot e^{-j \\frac{2\\pi}{N}kn}\\]\n- np.fft.fft() 함수 이용\n\nnp.fft.fft(y)[:10]\n\narray([ 28.92715005   +0.j        ,  34.05878195-1028.97157083j,\n        17.69086431 -530.11724055j,   7.37432739-1494.50531202j,\n        29.03159355  -33.71928164j,  20.2513632    -3.83449785j,\n         4.98960501   -9.5019045j , -18.28763105   +8.27271731j,\n       -43.73889873  +19.33055512j,  32.90966965  -34.52773359j])\n\n\n\nplt.plot(abs(np.fft.fft(y))[1:50], 'o')\n\n\n\n\n- 직접 구현\n\nN = len(y)\nfft_y_ = 0\nfor i in range(N):\n    fft_y_ += y[i]*np.exp(-2j*np.pi/N*np.arange(N))\n\n\nplt.plot(abs(np.fft.fft(y))[1:50],'o')\n\n\n\n\n- 비교\n\nfig, axes = plt.subplots(2,1)\naxes[0].plot(abs(np.fft.fft(y))[1:50], 'o')\naxes[0].set_title('Method1')\naxes[1].plot(abs(np.fft.fft(y))[1:50],'o')\naxes[1].set_title('Method2')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n삼성전자 주가자료를 스무딩해보기\n- 삼성전자 자료\n\nimport yfinance as yf\n\n\nstart_date = \"2023-01-01\"\nend_date = \"2023-05-02\"\ny = yf.download(\"005930.KS\", start=start_date, end=end_date)['Adj Close'].to_numpy()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\nplt.plot(y)\n\n\n\n\n- 스펙트럼\n\nyfft = np.fft.fft(y)\n\n\nplt.plot(abs(yfft))\n\n\n\n\n- 처음 50개정도만 관찰\n\nplt.plot(abs(yfft[:50]),'o')\n\n\n\n\n\n첫값이 너무커서 나머지는 잘안보임\n\n- 2번째부터 50번째까지만 관찰\n\nplt.plot(abs(yfft)[2:50],'o')\nplt.axhline(y=22500, color='r', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7f85162b57c0&gt;\n\n\n\n\n\n\n대충 이정도 짜르면 될것같음\n\n- thresholded value\n\ntresh_value = 22500\n\n\nyfft[abs(yfft)&lt;tresh_value] =0 \n\n- 퓨리에역변환\n\nyhat = np.fft.ifft(yfft)\nyhat[:5]\n\narray([59664.72193044+8.87311904e-14j, 58572.98839934+8.87311904e-14j,\n       58066.07369126+3.39894326e-14j, 58169.18671667-6.87747670e-14j,\n       58706.41986821-1.14383435e-13j])\n\n\n실수화\n\nyhat = np.real(yhat)\nyhat[:5]\n\narray([59664.72193044, 58572.98839934, 58066.07369126, 58169.18671667,\n       58706.41986821])\n\n\n- 적합결과 시각화\n\nplt.plot(y)\nplt.plot(yhat,'--')\n\n\n\n\n\n- 숙제: treshold value를 관찰하며 시각화해볼것\n\n# 스펙트럼\nyfft1 = yfft.copy()\nyfft2 = yfft.copy()\nyfft3 = yfft.copy()\n\n\nplt.plot(abs(yfft)[2:50],'o')\nplt.axhline(y=30000, color='r', linestyle='--')\nplt.axhline(y=50000, color='g', linestyle='--')\nplt.axhline(y=100000, color='y', linestyle='--')\nplt.show()\n\n\n\n\n- thresholded value\n\ntresh_value1 = 30000\ntresh_value2 = 50000\ntresh_value3 = 100000\n\n\nyfft1[abs(yfft)&lt;tresh_value1] =0 \nyfft2[abs(yfft)&lt;tresh_value2] =0 \nyfft3[abs(yfft)&lt;tresh_value3] =0 \n\n- 퓨리에역변환\n\nyhat1 = np.real(np.fft.ifft(yfft1))\nyhat2 = np.real(np.fft.ifft(yfft2))\nyhat3 = np.real(np.fft.ifft(yfft3))\nyhat1[:5], yhat2[:5], yhat3[:5] \n\n(array([60302.63175219, 59674.04944237, 59176.52517726, 58830.97854078,\n        58648.54453033]),\n array([60610.76706766, 60334.04540323, 60094.64069051, 59898.16437304,\n        59749.22168918]),\n array([61926.12309451, 61926.12309451, 61926.12309451, 61926.12309451,\n        61926.12309451]))\n\n\n- 적합결과 시각화\n\nplt.plot(y)\nplt.plot(yhat1, color='r', linestyle='--', label='thresh=30000')\nplt.plot(yhat2,color='g', linestyle='--', label='thresh=50000')\nplt.plot(yhat3, color='y', linestyle='--', label='thresh=100000')\nplt.legend()\nplt.show()\n\n\n\n\n\nthreshold 값이 커질수록 스무딩 되는 느낌\n\\(\\text{thresh}=100,000\\) \\(\\to\\) underfitting\n\\(\\text{thresh}=30,000\\) 으로 잡았을 때 원자료와 비슷\n\n\n\n\nminor topics\n- y의 FFT 결과는 항상 y와 같은길이임\n\nlen(y)\n\n82\n\n\n\nlen(np.fft.fft(y))\n\n82\n\n\n- 에일리어싱: number of observation은 얼마나 세밀한 주파수까지 측정가능하냐를 결정함\n예시1: 에일리어싱\n\nx = np.linspace(-3.14,3.14,10)\n\n\nx1 = np.sin(8*x)\nx2 = np.sin(10*x)\n\n\nnp.corrcoef([x1,x2])\n\narray([[ 1.        , -0.99975131],\n       [-0.99975131,  1.        ]])\n\n\n\nplt.plot(x1,label='x1')\nplt.plot(x2,label='x2')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f8514838190&gt;\n\n\n\n\n\n\n실제로는 x2가 더 고주파인데, 같은 주파수처럼 보임\n\n예시2: 에일리어싱이 없는 경우\n\nx = np.linspace(-3.14,3.14,100000)\n\n\nx1 = np.sin(8*x)\nx2 = np.sin(10*x)\n\n\nnp.corrcoef([x1,x2])\n\narray([[ 1.00000000e+00, -6.45767105e-08],\n       [-6.45767105e-08,  1.00000000e+00]])\n\n\n\nplt.plot(x1)\nplt.plot(x2)\n\n\n\n\n\n주파수 왜곡떄문에 실제로는 corr ceof = 0 일지라도 관측되는건 corr coef &gt;0 일 수 있음\n\n\n\n에일리어싱: https://moonnote.tistory.com/133\n나이퀴스트 이론: https://ralasun.github.io/signal%20analysis/2021/07/01/nyq/\nkernel : https://sonsnotation.blogspot.com/2020/11/11-1-kernel.html\nFFT: https://towardsdatascience.com/fourier-transform-the-practical-python-implementation-acdd32f1b96a"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#이론-및-예시",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#이론-및-예시",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "이론 및 예시",
    "text": "이론 및 예시\n- 이론: real-valued signal은 무조건 \\(|X[k]|^2\\)의 그래프가 대칭으로 나옴 (단, \\(X[0]\\)은 제외)\n- 예시1:\n\nx = np.array([1,2,3,4,5])\nX = np.fft.fft(x) \n\n\n# plt.plot(abs(X)**2,'o')\nplt.plot(abs(X)**2,'o')\nplt.plot((abs(X)**2)[0],'x')\nplt.axvline(x=2.5, linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7f5fc5098040&gt;\n\n\n\n\n\n\n첫항을 제외하고 대칭임\n\n\n# 확인\nabs(X)**2\n\narray([225.        ,  18.09016994,   6.90983006,   6.90983006,\n        18.09016994])\n\n\n첫 항인 \\(X[0]=225.\\)을 제외하고 대칭, 그래프 역시 대칭인 그래프가 나옴.\n- 예시2:\n\nx = np.array([1,2,3,-3,-2,-1]) # 이산신호\nX = np.fft.fft(x) # 퓨리에 변환된 이산신호\n\n\nplt.plot(abs(X)**2,'o')\nplt.plot((abs(X)**2)[0], 'x')\nplt.axvline(x=3, linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7f5fc4861dc0&gt;\n\n\n\n\n\n\n첫항을 제외하고 대칭임\n\n\nabs(X)**2\n\narray([ 0., 64., 12., 16., 12., 64.])\n\n\n- 예시3: \\({\\bf x}\\)가 복소수일 경우는 첫항을 제외하고 대칭이 되지 않음\n\nx = np.array([1+1j,2+2j,3+3j,-3-3j,-2-2j,1-1j]) \nX = np.fft.fft(x) \n\n\nplt.plot(abs(X)**2,'o')\n\n\n\n\n직관적으로, 첫항을 제외하고 대칭이 아님을 알 수 있음."
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#왜-xn이-실수일-경우만-xk2이-대칭으로-나올까",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#왜-xn이-실수일-경우만-xk2이-대칭으로-나올까",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "왜 \\(x[n]\\)이 실수일 경우만 \\(|X[k]|^2\\)이 대칭으로 나올까?",
    "text": "왜 \\(x[n]\\)이 실수일 경우만 \\(|X[k]|^2\\)이 대칭으로 나올까?\n- 예비학습1\n임의의 \\(0 \\leq \\alpha \\leq 1\\)에 대하여 \\(\\cos(2\\pi \\alpha) =\\cos(2\\pi (1-\\alpha))\\) 가 성립함\n\nalpha = 0.2\nnp.cos(2*np.pi*alpha),np.cos(2*np.pi*(1-alpha))\n\n(0.30901699437494745, 0.30901699437494723)\n\n\n\n그래프를 잘 그려보세여\n\n\nN = 30\nalpha_ = np.linspace(0,1,N)\nplt.plot(alpha_, np.cos(2*np.pi*alpha_), 'o', alpha=0.6, label='cos(2*pi*alpha)')\nplt.plot(alpha_, np.cos(2*np.pi*(1-alpha_)), 'x', color='red', label='cos(2*pi*(1-alpha))')\nplt.legend(loc=1)\n\n&lt;matplotlib.legend.Legend at 0x7f5fc44edee0&gt;\n\n\n\n\n\n- 예비학습2\n임의의 \\(0 \\leq \\alpha \\leq 1\\)에 대하여 \\(\\sin(2\\pi \\alpha) = -\\sin(2\\pi (1-\\alpha))\\) 가 성립함\n\nalpha = 0.2\nnp.sin(2*np.pi*alpha),np.sin(2*np.pi*(1-alpha))\n\n(0.9510565162951535, -0.9510565162951536)\n\n\n\n그래프를 잘 그려보세여\n\n\nN = 30\nalpha_ = np.linspace(0,1,N)\nplt.plot(alpha_, np.sin(2*np.pi*alpha_), 'o', alpha=0.6, label='sin(2*pi*alpha)')\nplt.plot(alpha_, -np.sin(2*np.pi*(1-alpha_)), 'x', color='red', label='-sin(2*pi*(1-alpha))')\nplt.legend(loc=1)\n\n&lt;matplotlib.legend.Legend at 0x7f5fc6533df0&gt;\n\n\n\n\n\n\n# 잘못된 그림..\nN = 30\nalpha_ = np.linspace(0,1,N)\nplt.plot(alpha_, np.sin(2*np.pi*alpha_), 'o', alpha=0.6, label='sin(2*pi*alpha)')\nplt.plot(alpha_, np.sin(2*np.pi*(1-alpha_)), 'x', color='red', label='sin(2*pi*(1-alpha))')\nplt.legend(loc=1)\n\n&lt;matplotlib.legend.Legend at 0x7f5fc63676d0&gt;\n\n\n\n\n\n- 왜 실수일경우만 대칭인지? (어디 정리된걸 아무리 찾아도 못찾겠어서 그냥 직접 수식을 썼는데요, 이걸 기억할 필요는 없어요.. 아마 제가 쓴것보다 쉽게 설명하는 방법이 있을겁니다)\n(해설) \\(k=0,1,2,\\dots,N-1\\)에 대하여 \\(X[k]\\)는 아래와 같이 표현가능하다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]e^{-\\frac{j2\\pi kn}{N}}\\]\n오일러공식1을 사용하면 아래와 같이 정리할 수 있다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(-\\frac{2\\pi kn}{N}\\right) + i \\sum_{n=0}^{N-1}x[n]\\sin\\left(-\\frac{2\\pi kn}{N}\\right)\\]\n\\(\\cos\\)은 짝함수2, \\(\\sin\\)은 홀함수3임을 이용하여 다시정리하면\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\ncase1 \\(k=1\\) 인 경우와 \\(k=N-1\\)인 경우는 서로 \\(|X[k]|^2\\)이 같음을 보이자.\n\\[X[1] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi n}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi n}{N}\\right)\\]\n\\[X[N-1] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\]\n여기에서 예비학습1,2를 떠올리면 \\(\\cos\\left(\\frac{2\\pi n}{N}\\right)=\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right)\\) 이고 \\(\\sin\\left(\\frac{2\\pi n}{N}\\right)=-\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\) 임을 알 수 있다. 따라서 \\(X[1]\\)와 \\(X[N-1]\\)은 켤레복소수이다. 즉\n\\[X[1] = X[N-1]^\\ast, \\quad X[1]^\\ast = X[N-1]\\]\n이다. 그런데 임의의 복소수 \\(x=a+bi\\)에 대하여 \\(|x|^2 = a^2+b^2 = (a+bi)(a-bi)= x x^\\ast\\) 가 성립하므로\n\\[|X[1]|^2 = |X[N-1]|^2\\]\n이 성립한다.\n\n만약에 \\(x[n]\\)이 실수가 아닌경우는 \\(\\cos\\left(\\frac{2\\pi n}{N}\\right)=\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right)\\) 이고 \\(\\sin\\left(\\frac{2\\pi n}{N}\\right)=-\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\) 이라고 하여도 \\(X[1]\\)와 \\(X[N-1]\\)은 켤레복소수라고 주장할수 없다.\n\ncase2 \\(k=2\\) 인 경우와 \\(k=N-2\\)인 경우는 서로 \\(|X[k]|^2\\)이 같음도 비슷한 논리로 보일 수 있다."
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#허수파트-해석",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#허수파트-해석",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "허수파트 해석",
    "text": "허수파트 해석\n관찰1: 모든 \\(k\\)에 대하여 \\(X[k]\\)의 허수파트는 항상 \\(0\\)이다.\nk=0\n\nk=0\nsin_part_0 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nx*sin_part_0\n\narray([ 0.,  0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,\n        0.,  0.])\n\n\n\nsum(x*sin_part_0)\n\n0.0\n\n\n\n\\(X[k]\\)의 허수파트는 항상 \\(0\\)\n\nk=1\n\nk=1\nsin_part_1 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nx*sin_part_1\n\narray([ 0.        ,  0.37157241,  0.49726095,  0.29389263, -0.10395585,\n       -0.4330127 , -0.47552826, -0.20336832,  0.20336832,  0.47552826,\n        0.4330127 ,  0.10395585, -0.29389263, -0.49726095, -0.37157241])\n\n\n\nsum(x*sin_part_1)\n\n1.0547118733938987e-15\n\n\n\n거의 \\(0\\)\n\n약간을 직관을 위해서 그림을 그려보자.\n\nplt.plot(x,'--o')\nplt.plot(sin_part_1,'--o')\n\n\n\n\n\\(x\\)가 \\(\\mathbb{R}\\)에서 정의된 연속함수라고 상상하면 \\(\\sum_{n=0}^{N-1}\\cos\\left(\\frac{2\\pi n}{N} \\right)\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\)에 대응하는 식은 \\(\\int_0^{2\\pi}\\cos(t)\\sin(t)dt\\)라고 볼 수 있어서 sum(x*sin_part_1)=0임을 더 쉽게 이해할 수 있다.\nk=2\n\nk=2\nsin_part_2 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o', label='cos(t)')\nplt.plot(sin_part_2,'--o', label='sin(2t)')\nplt.legend(loc=3)\n\n&lt;matplotlib.legend.Legend at 0x7f5fc6129190&gt;\n\n\n\n\n\n\\(x\\)가 \\(\\mathbb{R}\\)에서 정의된 연속함수라고 상상하면 파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(\\sin(2t)\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\nsum(x*sin_part_2)\n\n-1.2212453270876722e-15\n\n\n\\(\\cos(t)\\)는 임의의 \\(\\sin(kt)\\)와 항상 직교하므로, 임의의 \\(k\\)에 대하여 허수파트는 항상 0이다.\n따라서 이 경우 \\(X[k]\\)는 아래와 같이 써도 무방하다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right)\\]"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#실수파트-해석",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#실수파트-해석",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "실수파트 해석",
    "text": "실수파트 해석\n관찰2: \\(X[k]\\)의 실수파트는 \\(k=1\\)혹은 \\(k=N-1\\)일때 아래와 같이 정리된다.\n\\[\\sum_{n=0}^{N-1}\\cos\\left(\\frac{2\\pi n}{N} \\right)^2\\]\n그외의 경우에는 아래와 같이 된다.\nk=0\n\nk=0\ncos_part_0 = np.array([np.cos(2*np.pi*k*n/N) for n in range(N)])\n\n\ncos_part_0\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nplt.plot(x,'--o')\nplt.plot(cos_part_0,'--o')\n\n\n\n\n\n파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(1\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\nk=2\n\nk=2\ncos_part_2 = np.array([np.cos(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o', label='cos(t)')\nplt.plot(cos_part_2,'--o', label='cos(2t)')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f5fc5fc2fd0&gt;\n\n\n\n\n\n\n파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(\\cos(2t)\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\n임의의 \\(k\\)에 대하여 \\(\\cos(t)\\)와 \\(\\cos(kt)\\)는 항상 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n- 요약: 만약에 \\(x[n]\\)이 아래와 같은 꼴이라고 하자.\n\\[x[n] = \\cos\\left(\\frac{2\\pi n}{N} \\right)\\]\n이때 퓨리에변환 \\(X[k]\\)는 아래와 같이 정리된다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\n\\(X[k]\\)의 허수파트는 항상 0이 되고, 실수 파트는 \\(k=1,N-1\\)일 경우에만 값을 가지고 나머지는 0의 값을 가진다.\n\nX = np.fft.fft(x) # 퓨리에 변환된 이산신호\nfig, ax = plt.subplots(1,3)\nax[0].plot(x,'o--'); ax[0].set_title('x[n]') \nax[1].plot(np.real(X),'x'); ax[1].set_title('real(X[k])') # 실수파트\nax[2].plot(np.imag(X),'x'); ax[2].set_title('imag(X[k])') # 허수파트\nfig.set_figwidth(15)\n\n\n\n\n- 응용: \\(x[n]\\)이 아래와 같은 꼴이라고 하자.\n\\[x[n] = \\cos\\left(\\frac{6\\pi n}{N} \\right)\\]\n퓨리에변환 \\(X[k]\\)는 아래와 같이 정리된다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\n\\(X[k]\\)의 허수파트는 항상 0이 되고, 실수 파트는 \\(k=3,N-3\\)일 경우에만 값을 가지고 나머지는 0의 값을 가진다.\n\nN = 15 \nx = np.array([np.cos(6*np.pi*n/N) for n in range(N)]) # 이산신호\nX = np.fft.fft(x) # 퓨리에 변환된 이산신호\nfig, ax = plt.subplots(1,3)\nax[0].plot(x,'o--'); ax[0].set_title('x[n]')\nax[1].plot(np.real(X),'x'); ax[1].set_title('real(X[k])') # 실수파트\nax[2].plot(np.imag(X),'x'); ax[2].set_title('imag(X[k])') # 허수파트\nfig.set_figwidth(15)\n\n\n\n\n\n\\(X[k]\\)의 허수파트는 항상 \\(0\\)\n\\(X[k]\\)의 실수파트는 \\(X[3], X[12]\\)일 경우에만 값을 가지고 나머지는 \\(0\\)"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#책갈피",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-28-퓨리에변환(detailed).html#책갈피",
    "title": "[Fourier] 퓨리에변환(detailed)",
    "section": "책갈피",
    "text": "책갈피\n\n\n\n\n\n\nImportant\n\n\n\n이것은 \\(x[n]\\)을 아래와 같이 나눈뒤\n\n\\(x_1[n] = \\cos\\left(\\frac{2\\pi n}{N} \\right)\\)\n\\(x_2[n] = 2\\times\\cos\\left(\\frac{6\\pi n}{N} \\right)\\)\n\n각각 퓨리에변환한 결과를 합친것과 같다.\n\n\n\nN = 15 \nx1 = np.array([np.cos(2*np.pi*n/N) for n in range(N)]) # signal1\nx2 = np.array([2*np.cos(6*np.pi*n/N) for n in range(N)]) # signal2\nX1 = np.fft.fft(x1) # fft(signal1)\nX2 = np.fft.fft(x2) # fft(signal2)\nfig, ax = plt.subplots(2,3)\nax[0][0].plot(x1,'o--'); ax[0][0].set_title('x1[n]')\nax[0][1].plot(np.real(X1),'x'); ax[0][1].set_title('real(X1[k])')\nax[0][2].plot(np.imag(X1),'x'); ax[0][2].set_title('imag(X1[k])')\nax[1][0].plot(x2,'o--'); ax[1][0].set_title('x2[n]')\nax[1][1].plot(np.real(X2),'x'); ax[1][1].set_title('real(X2[k])')\nax[1][2].plot(np.imag(X2),'x'); ax[1][2].set_title('imag(X2[k])')\nfig.set_figwidth(15)\nfig.set_figheight(8)"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "",
    "text": "import this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\n\n파이썬은 명확하고 쉽게 표현하기 위한 언어."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#the-zen-of-python",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#the-zen-of-python",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "",
    "text": "import this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\n\n파이썬은 명확하고 쉽게 표현하기 위한 언어."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#가상환경",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#가상환경",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "가상환경",
    "text": "가상환경\n\n!는 시스템 쉘에서 쓰는 명령어. (컴퓨터 자체한테 일을 시키는 것.)\n\n\n# 현재 환경에 설치된 패키지 리스트 출력\n!pip freeze | grep pandas\n\npandas==1.3.5\n\n\n\nfreeze는 깔려있는 패키지를 동결시킨다. (동결시킨 패키지 목록을 필요에 따라 복붙하면 됨.)\n\n\n# 파이썬 버전 확인\n!python --version\n\nPython 3.8.16\n\n\n\n3번째 버전은 신경안써도 됨. 중간 숫자는 1년에 한번씩 업데이트 된다고 보면 됨.\n\n\n# 사용중인 GPU / 드라이브 버전 / CUDA 버전 등 GPU 상태 확인\n!nvidia-smi\n\nTue Sep 12 17:28:14 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |\n|  0%   33C    P8    31W / 420W |      2MiB / 24576MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n\n# 플랫폼 사양 확인\nimport platform as p\np.platform()\n\n'Linux-6.2.0-26-generic-x86_64-with-glibc2.17'\n\n\n\n# 리눅스 종류 확인\n!cat /etc/issue.net\n\nUbuntu 22.04.2 LTS\n\n\n\n# CPU 정보 확인\n# !cat /proc/cpuinfo\n\n\n# Memory 확인\n!cat /proc/meminfo\n\nMemTotal:       131818196 kB\nMemFree:        15323916 kB\nMemAvailable:   37475372 kB\nBuffers:          407604 kB\nCached:         22186364 kB\nSwapCached:            0 kB\nActive:         11569860 kB\nInactive:       103288084 kB\nActive(anon):       6120 kB\nInactive(anon): 92263696 kB\nActive(file):   11563740 kB\nInactive(file): 11024388 kB\nUnevictable:           0 kB\nMlocked:               0 kB\nSwapTotal:       2097148 kB\nSwapFree:        2097148 kB\nZswap:                 0 kB\nZswapped:              0 kB\nDirty:                64 kB\nWriteback:             0 kB\nAnonPages:      92263116 kB\nMapped:          1037044 kB\nShmem:              5832 kB\nKReclaimable:     810380 kB\nSlab:            1055988 kB\nSReclaimable:     810380 kB\nSUnreclaim:       245608 kB\nKernelStack:       28640 kB\nPageTables:       246096 kB\nSecPageTables:         0 kB\nNFS_Unstable:          0 kB\nBounce:                0 kB\nWritebackTmp:          0 kB\nCommitLimit:    68006244 kB\nCommitted_AS:   116634948 kB\nVmallocTotal:   34359738367 kB\nVmallocUsed:      129356 kB\nVmallocChunk:          0 kB\nPercpu:            29696 kB\nHardwareCorrupted:     0 kB\nAnonHugePages:   6203392 kB\nShmemHugePages:        0 kB\nShmemPmdMapped:        0 kB\nFileHugePages:         0 kB\nFilePmdMapped:         0 kB\nHugePages_Total:       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nHugetlb:               0 kB\nDirectMap4k:      881700 kB\nDirectMap2M:    39938048 kB\nDirectMap1G:    93323264 kB\n\n\n\n# KB -&gt; GB\n!awk '/MemFree/ { printf \"%.3f \\n\", $2/1024/1024 }' /proc/meminfo\n\n14.612 \n\n\n\n# 디스크 사용량 확인\n!df -h\n\nFilesystem      Size  Used Avail Use% Mounted on\ntmpfs            13G  2.0M   13G   1% /run\n/dev/nvme0n1p3  916G  234G  635G  27% /\ntmpfs            63G  1.6M   63G   1% /dev/shm\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\n/dev/nvme0n1p2  512M  6.1M  506M   2% /boot/efi\ntmpfs            13G   80K   13G   1% /run/user/127\ntmpfs            13G   68K   13G   1% /run/user/1000"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#modules",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#modules",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Modules",
    "text": "Modules\n모듈/패키지/프레임워크\n\nimport re\nmy_regex = re.compile(\"[0-9]+\", re.I)\nmy_regex.findall(\"010-1234-5678abd 23af\")\n\n['010', '1234', '5678', '23']\n\n\n\nfrom collections import defaultdict, Counter\nlookup = defaultdict(int)\nmy_counter = Counter()\n\n\nmatch = 30\nfrom re import *\nprint(match)\n\n&lt;function match at 0x7f83e4b4be50&gt;\n\n\n\nsum([x for x in range(1,11)])\n\n55\n\n\n\nsum = 0\nfor i in range(1, 10+1):\n    sum += i\nprint(sum)\n\n55\n\n\n\nsum # 지금 sum에는 55가 저장되어 있음.\n\n55\n\n\n\nsum([x for x in range(1,11)]) # 따라서 함수는 작동하지 X\n\nTypeError: 'int' object is not callable\n\n\n\nimport해서 가져오면 변수이름과 동일할 경우 변수 이름이 덮어씌워진다."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#functions",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#functions",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Functions",
    "text": "Functions\n\ndef double(x):\n  \"\"\"주어진 값을 두배로 만들어서 돌려줍니다.\"\"\"\n  return x*2\n\n\n따옴표 3개쓰는 이유? 여러 줄 쓰기 위해..\n함수 선언은 메모리에만 올라가는 것이고 실행되지는 않는다.\n\n\ndouble?\n\n\nSignature: double(x)\nDocstring: 주어진 값을 두배로 만들어서 돌려줍니다.\nFile:      ~/Dropbox/임지윤/Quarto-Blog/posts/5_study/7_ds2023/&lt;ipython-input-15-7ee74bb62645&gt;\nType:      function\n\n\n\n\ndef apply_to_one(f): # sinx, cosx 이든 어떤 식이 와도 1을 집어넣는 함수.\n  \"\"\"주어진 함수에 1을 적용합니다.\"\"\"\n  return f(1)\n\n\napply_to_one(double)\n\n2\n\n\n\napply_to_one(lambda x: x + 5)\n\n6\n\n\n\nadd_five = lambda x: x + 5     # 이런형태로는 람다를 쓰지마세요.\n\ndef add_five(n):               # 차라리 이렇게 쓰는 게 낫습니다.\n  \"\"\"주어진 숫자에 5를 더합니다.\"\"\"\n  return n + 5\n\n\ndef hello(name=\"고객\"): # default값을 줄 수 있음.\n  print(f\"안녕하세요. {name}님\")\n\n\nhello() # 덮어씌워 줄 게 없으니까 디폴트값.\nhello('개굴') # 개굴을 덮어씌워 줌.\n\n안녕하세요. 고객님\n안녕하세요. 개굴님"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#strings",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#strings",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Strings",
    "text": "Strings\n\nsingle_quoted_string = 'data science'\ndouble_quoted_string = \"data science\"\n\n\ntab_string = \"\\t\"\nlen(tab_string)\n\n1\n\n\n\nnot_tab_string = r\"\\t\"      # raw string, 역슬래시 자체를 써야 하는 경우. 정규표현식.\nlen(not_tab_string)\n\n2"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#exceptions",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#exceptions",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Exceptions",
    "text": "Exceptions\n\ntry:\n  print(0 / 0)\nexcept ZeroDivisionError:\n  print(\"0으로 나눌 수 없습니다.\")\n\n0으로 나눌 수 없습니다."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#lists",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#lists",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Lists",
    "text": "Lists\n\ninteger_list = [1, 2, 3]\nheterogeneous_list = [\"string\", 0.1, True]\nlist_of_lists = [integer_list, heterogeneous_list, []]\n\n\nlist_of_lists\n\n[[1, 2, 3], ['string', 0.1, True], []]\n\n\n\n# integer_list 리스트의 길이(갯수)는?\nprint('integet_list의 길이?:', len(integer_list))\n\n# integer_list 리스트 안 모든 값의 합은?\nsum_ = 0\nfor i in range(len(integer_list)):\n    sum_ += integer_list[i]\nprint('integer_list 안 모든 값의 합은?:', sum_)\n\ninteget_list의 길이?: 3\ninteger_list 안 모든 값의 합은?: 6\n\n\n\nx = [x for x in range(10)]\nx\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nzero = x[0]\nnine = x[-1]\nx[0] = -1\n\n\nx\n\n[-1, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nfirst_three = x[:3]    # 첫 3개 항목\nthree_to_end = x[3:]   # 숫자 3부터 끝까지\none_to_four = x[1:5]    # 숫자 1부터 4까지\nlast_three = x[-3:]     # 마지막 3개 항목\nwithout_first_and_last =  x[1:-1]    # 첫항목과 끝항목 빼고 나머지\ncopy_of_x =  x[:]     # 리스트를 복사\n\n\nfirst_three, three_to_end, one_to_four, last_three, without_first_and_last, copy_of_x\n\n([-1, 1, 2],\n [3, 4, 5, 6, 7, 8, 9],\n [1, 2, 3, 4],\n [7, 8, 9],\n [1, 2, 3, 4, 5, 6, 7, 8],\n [-1, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nevery_third = x[::3]\nfive_to_three = x[5:2:-1]\n\n\nevery_third, five_to_three\n\n([-1, 3, 6, 9], [5, 4, 3])\n\n\n\nprint(1 in [1, 2, 3])\nprint(0 in [1, 2, 3])\n\nTrue\nFalse\n\n\n\nx = [1,2,3]\n\n\nx.append(4)\n# x.append(5)\n# x.append(6)\n\n\nx\n\n[1, 2, 3, 4]\n\n\n\nx.append(5)\nx\n\n[1, 2, 3, 4, 5]\n\n\n\nx.append(6)\nx\n\n[1, 2, 3, 4, 5, 6]\n\n\n\nx.append(4,5,6) # append는 하나만 들어간다.\n\nAttributeError: 'int' object has no attribute 'append'\n\n\n\nx = [1, 2, 3]\nx.append([4, 5, 6]) # 리스트를 한 덩어리로 취급.\nx\n\n[1, 2, 3, [4, 5, 6]]\n\n\n\nx = [1,2,3]\nx.extend([4,5,6])\nx\n\n[1, 2, 3, 4, 5, 6]\n\n\n\nx = [1, 2, 3]\ny = x + [4,5,6]\ny\n\n[1, 2, 3, 4, 5, 6]\n\n\n\nx, y = [1, 2]\n\n\n_, y = [1, 2]\n\n\n_, y, _ = [1, 2, 3]"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#tuples",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#tuples",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Tuples",
    "text": "Tuples\n\nmy_list = [1, 2]\nmy_tuple = (1, 2)\nother_tuple = 3, 4\n\n\nother_tuple\n\n(3, 4)\n\n\n\nx, y = 1, 2\n\n\nx, y = y, x # swap x, y. 다른 언어에서는 지원하지 않음. 주의.\n\n\nprint(x)\nprint(y)\n\n2\n1\n\n\n\nmy_list[0] = 5\nmy_list\n\n[5, 2]\n\n\n\nmy_tuple[0] = 5\nmy_tuple\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n프로그램을 만들 때 tuple을 많이 사용한다. (누군가 맘대로 못바꾸게)"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#dictionaries",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#dictionaries",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Dictionaries",
    "text": "Dictionaries\n\nempty_dict = {}\nempty_dict2 = dict()\ngrades = {'홍길동': 80, \"임꺽정\": 95}\n\n\nhong_grade = grades['홍길동']\n\n\nhong_grade\n\n80\n\n\n\nkim_grade = grades[\"김철수\"]\n\nKeyError: '김철수'\n\n\n\nkim_grade = grades.get('김철수',0)\nkim_grade\n\n0\n\n\n\nkim_grade = grades.get('김철수')\n\n\nkim_grade # 아무것도 출력이 안됨.\n\n\nprint(kim_grade)\n\nNone\n\n\n\ngrades.keys(), grades.values(), grades.items() # 반복문 쓸때\n\n(dict_keys(['홍길동', '임꺽정']),\n dict_values([80, 95]),\n dict_items([('홍길동', 80), ('임꺽정', 95)]))\n\n\n\n18일"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#defaultdict",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#defaultdict",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "defaultdict",
    "text": "defaultdict\n\ndocument = [\"동해물\", \"백두산\", \"하느님\", \"우리나라\", \"만세\", \"백두산\", \"우리나라\", \"우리나라\"]\n\nword_counts = {}\nfor word in document: # 사전 안에 그 단어가 있으면\n    if word in word_counts:\n        word_counts[word] += 1\n    else:\n        word_counts[word] = 1 # step1: \"동해물\" 없으니까 1 / \"백두산\" 없으니까 1 / ...\n\n\nword_counts\n\n{'동해물': 1, '백두산': 2, '하느님': 1, '우리나라': 3, '만세': 1}\n\n\n\n\"forgiveness is better than permission\"\n\nword_counts = {}\nfor word in document:\n    try:\n        word_counts[word] += 1\n    except KeyError:\n        word_counts[word] = 1\n\n\nword_counts\n\n{'동해물': 1, '백두산': 2, '하느님': 1, '우리나라': 3, '만세': 1}\n\n\n\nword_counts = {}\nfor word in document:\n    previous_count = word_counts.get(word, 0)\n    word_counts[word] = previous_count + 1\n\n\nword_counts\n\n{'동해물': 1, '백두산': 2, '하느님': 1, '우리나라': 3, '만세': 1}\n\n\n\nfrom collections import defaultdict\n\nword_counts = defaultdict(int)\nfor word in document:\n    word_counts[word] += 1\n\n\nword_counts\n\ndefaultdict(int, {'동해물': 1, '백두산': 2, '하느님': 1, '우리나라': 3, '만세': 1})"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#counters",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#counters",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Counters",
    "text": "Counters\n\nfrom collections import Counter\nc = Counter([0, 1, 2, 0])\n\n\nword_counts = Counter(document)\n\n\nfor word, count in word_counts.most_common(10):\n    print(word, count)\n\n우리나라 3\n백두산 2\n동해물 1\n하느님 1\n만세 1\n\n\n\nword_counts.most_common(10)\n\n[('우리나라', 3), ('백두산', 2), ('동해물', 1), ('하느님', 1), ('만세', 1)]"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#sets",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#sets",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Sets",
    "text": "Sets\n\nprimes_below_10 = {2, 3, 5, 7}\n\n\ns = set()\ns.add(1)\ns.add(2)\ns.add(2)\ns\n\n{1, 2}\n\n\n\nx = len(s)\ny = 2 in s\nz = 3 in s\n\n\nx, y, z\n\n(2, True, False)\n\n\n\n# list\nstopwords_list = [\"a\", \"an\", \"at\", \"yet\", \"you\"]    # \"a\", \"an\", \"at\", ...., \"yet\", \"you\"\n\"zip\" in stopwords_list\n\nFalse\n\n\n\n# set\nstopwords_set = set(stopwords_list)\n\"zip\" in stopwords_set\n\nFalse\n\n\n\n리스트도 한개씩 돌아가면서 체크를 하기 때문에 구현은 가능하지만 비효율적. 데이터 타입을 set 으로만 바꿔줘도 엄청 빨리 계산됨."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#sorting",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#sorting",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Sorting",
    "text": "Sorting\n\nx = [4, 1, 2, 3]\ny = sorted(x)\nprint(x)\nprint(y)\n\n[4, 1, 2, 3]\n[1, 2, 3, 4]\n\n\n\nprint(x.sort())\n\nNone\n\n\n\nx\n\n[1, 2, 3, 4]\n\n\n\nsorted는 리턴되서 나오는게 정렬된 리스트.\nsort자체는 그 자체를 바꿔버리고 리턴 값이 없다.\n그래서 sort는 할당을 하면 None이 나옴을 주의하자."
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#list-comprehensions",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#list-comprehensions",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "List Comprehensions",
    "text": "List Comprehensions\n\neven_numbers = [x for x in range(10) if x % 2 == 0]\nsquares = [x**2 for x in range(10)]\neven_squares = [x**2 for x in range(10) if x % 2 == 0]\n\n\n[x for x in range(10)]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n[x**2 for x in range(10)]\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\n\n[x**2 for x in range(10) if x % 2 == 0]\n\n[0, 4, 16, 36, 64]\n\n\n\nsquare_dict = {x: x**2 for x in range(10)}\nsquare_set = {x**2 for x in [1, -1]}\n\n\n{x: x**2 for x in range(10)}\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81}\n\n\n\n{x**2 for x in [1, -1]}\n\n{1}\n\n\n\nzeros = [0 for _ in even_numbers] # 관심없는 것은 _\nzeros\n\n[0, 0, 0, 0, 0]\n\n\n\npairs = [(x, y)\n         for x in range(10)\n         for y in range(10)]\n\n\nlen(pairs)\n\n100"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#automated-testing-and-assert",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#automated-testing-and-assert",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Automated Testing and assert",
    "text": "Automated Testing and assert\n\nassert 1 + 1 == 2\nassert 1 + 1 == 2, \"1+1은 2여야 하는데 결과가 이와 다릅니다.\""
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#randomness",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#randomness",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Randomness",
    "text": "Randomness\n\nimport random\nrandom.seed(42)\n\n\nfour_uniform_randoms = [random.random() for _ in range(4)]\n\n\nfour_uniform_randoms\n\n[0.6394267984578837,\n 0.025010755222666936,\n 0.27502931836911926,\n 0.22321073814882275]\n\n\n\nrandom.seed(42)\nprint(random.random())\n\n0.6394267984578837\n\n\n\nrandom.seed(42)\nprint(random.random())\n\n0.6394267984578837\n\n\n\nrandom.randrange(10) # range(10)\n\n2\n\n\n\nrandom.randrange(3, 6) # range(3, 6)\n\n3\n\n\n\nlottery_numbers = range(1, 45 + 1)\nwinning_numbers = random.sample(lottery_numbers, 6)\nwinning_numbers\n\n[44, 35, 6, 38, 28, 3]\n\n\n\nrandom.seed(11)\nfour_with_replacement = [random.choice(range(10)) for _ in range(4)]\nprint(four_with_replacement)\n\n[7, 8, 7, 7]"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#zip-and-argument-unpacking",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#zip-and-argument-unpacking",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "zip and Argument Unpacking",
    "text": "zip and Argument Unpacking\n\nlist1 =  ['a', 'b', 'c']\nlist2 = [1, 2, 3]\n\npairs = [pairs for pair in zip(list1, list2)]\n# pairs"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#args-and-kwargs",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#args-and-kwargs",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "args and kwargs",
    "text": "args and kwargs\n\ndef method_multiple_parameters(*args, **kwargs):\n  print(\"unnamed args:\", args)\n  print(\"keyword args:\", kwargs)\n\n\nmethod_multiple_parameters(1, 2, key=\"word\", key2=\"word2\")\n\nunnamed args: (1, 2)\nkeyword args: {'key': 'word', 'key2': 'word2'}"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-12-python.html#type-annotations",
    "href": "posts/7_study/7_ds2023/2023-09-12-python.html#type-annotations",
    "title": "2wk 환경설정 및 파이썬 기초",
    "section": "Type Annotations",
    "text": "Type Annotations\n\ndef add(a: int, b: int) -&gt; int:\n  return a + b\n\n\nvalues = []\nbest_so_far = None\n\n\nfrom typing import Optional, List\nvalues: List[int] = [] # 정수만 들어가는 리스트\nbest_so_far: Optional[float] = None   # float or None"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-13.html",
    "href": "posts/7_study/6_as2023/2023-09-13.html",
    "title": "2wk-2 웹 기초",
    "section": "",
    "text": "meta: ~을 넘어/ 상위 \\(\\to\\) 메타정보란 정보의 정보\n보이기 위해 필요한 자잘한 정보\ntag\n\n&lt;h&gt;: Heading의 약자\n&lt;p&gt;: paragraph 문단\n&lt;div&gt; : division\n&lt;br&gt; : 줄바꿈\n\n요새는 html은 내용만 적고 스타일은 css로 잡아준다.\n- 마크업 랭귀지?\n예를들어 Markdown도 마크업 랭귀지.\n\n\n\n\n\n\nNote\n\n\n\n마크업 언어(Markup Language)는 문서나 데이터를 구조화하고 표현하는 데 사용되는 특수한 종류의 언어입니다. 주로 텍스트 데이터를 태그와 같은 특별한 표시로 둘러싸서 그 데이터의 의미와 구조를 정의합니다. 마크업 언어를 사용하면 컴퓨터가 데이터를 이해하고 처리할 수 있도록 도와줍니다.\n가장 널리 사용되는 마크업 언어 중 하나는 HTML(HyperText Markup Language)입니다. HTML은 웹 페이지의 구조를 정의하고 웹 브라우저에서 웹 페이지를 표시하는 데 사용됩니다. HTML 문서는 여러 태그로 구성되며, 각 태그는 텍스트, 이미지, 링크 등 다양한 요소를 정의하고 배치합니다.\n또한 XML(Extensible Markup Language)은 데이터 저장 및 교환에 사용되는 다목적 마크업 언어로, 데이터베이스, 설정 파일, 웹 서비스 등 다양한 응용 분야에서 활용됩니다.\n마크다운(Markdown)은 더 간단한 마크업 언어로, 문서 작성을 쉽게 만들어주는데 주로 사용됩니다. 마크다운은 텍스트로 작성되며, 간단한 기호와 형식을 사용하여 문서 구조와 스타일을 정의합니다.\n\n\n- 메모장에 실행코드를 복붙하여 txt -&gt; html로 변경하여 실행 (안쓰는 방법)\n\n\n\n다음과 같은 코드 실행 결과를 메모장에 복붙\n\n\n- 확인사항\n\n\n\n파일탐색기-우상단(점점점)-옵션-보기-해당박스에 체크표시 해제\n\n\n\nctrl+r : 한꺼번에 바꿀때\n\n- 파일 내에서 모두 처리 (보통 쓰는 방법)\n\n\n\n\n\n\n\n웹상에 변경사항이 반영이 안될 때\n\n\n\n마우스 오르쪽버튼 &gt; open in(다음에서 열기) &gt; 탐색기 에서 열면 웹상에서 잘 반영되서 보임.\n\n\n파이썬 \\(\\overset{(1)}{\\Longrightarrow}\\) html코드(index.html) \\(\\overset{(2)}{\\Longrightarrow}\\) 웹브라우저로 가는 두 단계가 있다.\n\n첫번째는 \\n\n두번째는 &lt;br&gt; tag\n\n- 배포는?\nflask, jango, fastai\n\nflask: 한땀한땀\njango: 한번에\nfastai: api를 빨리 만들 수 있는 패키지\n\n\n단축키 정리\n\nctrl+r : 한꺼번에 바꿀때"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-02-20-simul_eq.html",
    "href": "posts/7_study/5_etc/2023-02-20-simul_eq.html",
    "title": "simultaneous equation",
    "section": "",
    "text": "파이썬에서 Numpy는 행렬 계산을 쉽게하기 위해 사용하는 패키지이다. R로도 행렬과 매트릭스를 구현해보자.\n- 예를 들어 아래와 같은 문제가 있다고 하자.\n\\[\\begin{cases}w+2x+ey+4z = 1 \\\\2w+2x+y=9 \\\\x-y = 4 \\\\3w+x-y+3y=7\\end{cases}\\]\n- 매트릭스 형태로 위의 식을 표현하면 아래와 같다.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nw \\\\ x \\\\ y \\\\z\n\\end{bmatrix}=\\begin{bmatrix}\n1 \\\\ 9 \\\\ 4 \\\\7\n\\end{bmatrix}\n\\]\n- 양변에\n\\[\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}\\]\n의 역행렬을 취하면\n\\[\\begin{bmatrix}\nw \\\\ x \\\\ y \\\\z\n\\end{bmatrix}=\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}^{-1}\\begin{bmatrix}\n1 \\\\ 9 \\\\ 4 \\\\7\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-02-20-simul_eq.html#행렬과-연립일차-방정식",
    "href": "posts/7_study/5_etc/2023-02-20-simul_eq.html#행렬과-연립일차-방정식",
    "title": "simultaneous equation",
    "section": "",
    "text": "파이썬에서 Numpy는 행렬 계산을 쉽게하기 위해 사용하는 패키지이다. R로도 행렬과 매트릭스를 구현해보자.\n- 예를 들어 아래와 같은 문제가 있다고 하자.\n\\[\\begin{cases}w+2x+ey+4z = 1 \\\\2w+2x+y=9 \\\\x-y = 4 \\\\3w+x-y+3y=7\\end{cases}\\]\n- 매트릭스 형태로 위의 식을 표현하면 아래와 같다.\n\\[\n\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}\n\\begin{bmatrix}\nw \\\\ x \\\\ y \\\\z\n\\end{bmatrix}=\\begin{bmatrix}\n1 \\\\ 9 \\\\ 4 \\\\7\n\\end{bmatrix}\n\\]\n- 양변에\n\\[\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}\\]\n의 역행렬을 취하면\n\\[\\begin{bmatrix}\nw \\\\ x \\\\ y \\\\z\n\\end{bmatrix}=\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 2 & 1 & 0 \\\\\n0 & 1 &-1 & 0 \\\\\n3 & 1 &-1 & 3\n\\end{bmatrix}^{-1}\\begin{bmatrix}\n1 \\\\ 9 \\\\ 4 \\\\7\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-02-20-simul_eq.html#r로-구현",
    "href": "posts/7_study/5_etc/2023-02-20-simul_eq.html#r로-구현",
    "title": "simultaneous equation",
    "section": "R로 구현",
    "text": "R로 구현\n\n- 방법1\n\nA=rbind(c(1,2,3,4),c(2,2,1,0),c(0,1,-1,0),c(3,1,-1,3))\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    2    2    1    0\n[3,]    0    1   -1    0\n[4,]    3    1   -1    3\n\n\n\nb=c(1,9,4,7)\ndim(b)=c(4,1)\nb\n\n     [,1]\n[1,]    1\n[2,]    9\n[3,]    4\n[4,]    7\n\n\n\nsolve(A) %*% b \n\n     [,1]\n[1,]    2\n[2,]    3\n[3,]   -1\n[4,]   -1\n\n\n따라서 \\((w,x,y,z) = (2,3,-1,-1)\\) 이다.\n\n\n- 방법2\n\nA = rbind(c(1,2,3,4),c(2,2,1,0),c(0,1,-1,0),c(3,1,-1,3))\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    2    2    1    0\n[3,]    0    1   -1    0\n[4,]    3    1   -1    3\n\n\n\nb = c(1,9,4,7)\nb\n\n[1] 1 9 4 7\n\n\n\nsolve(A) %*% b\n\n     [,1]\n[1,]    2\n[2,]    3\n[3,]   -1\n[4,]   -1"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-08-09-transformer.html",
    "href": "posts/7_study/5_etc/2023-08-09-transformer.html",
    "title": "Attention is all you need",
    "section": "",
    "text": "ref: https://arxiv.org/abs/1706.03762"
  },
  {
    "objectID": "posts/6_note/2023-09-09_baseline_code.html",
    "href": "posts/6_note/2023-09-09_baseline_code.html",
    "title": "연습장2",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n\n# 데이터 불러오기\ninputs = pd.read_csv('./farm/train_input.csv')\noutputs = pd.read_csv('./farm/train_output.csv')\n\n\ninputs.head()\n\n\n\n\n\n\n\n\nSample_no\n시설ID\n일\n주차\n내부CO2\n내부습도\n내부온도\n지온\n강우감지\n일사량\n외부온도\n외부풍향\n외부풍속\n지습\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n품종\n재배형태\n\n\n\n\n0\n0\nfarm25\n20220323\n30주차\n517.041667\n84.985417\n20.610833\n0.0\nNaN\n1879\n11.166667\n195.0\n0.083333\n0.0\n14\n2.68\n4.42\n88\ntomato09\nNaN\n\n\n1\n0\nfarm25\n20220324\n30주차\n514.416667\n88.291250\n20.695000\n0.0\nNaN\n1411\n12.708333\n142.5\n0.000000\n0.0\n14\n2.78\n5.63\n97\ntomato09\nNaN\n\n\n2\n0\nfarm25\n20220326\n30주차\n471.875000\n83.514583\n20.402500\n0.0\nNaN\n1955\n8.791667\n202.5\n0.000000\n0.0\n14\n2.69\n4.25\n101\ntomato09\nNaN\n\n\n3\n0\nfarm25\n20220327\n30주차\n469.250000\n80.916250\n20.139167\n0.0\nNaN\n2231\n8.041667\n180.0\n0.000000\n0.0\n14\n2.70\n4.25\n99\ntomato09\nNaN\n\n\n4\n0\nfarm25\n20220328\n30주차\n465.750000\n82.026250\n17.653333\n0.0\nNaN\n2284\n9.000000\n97.5\n0.041667\n0.0\n13\n2.66\n4.21\n94\ntomato09\nNaN\n\n\n\n\n\n\n\n\noutputs.head()\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n0\n20220330\n30주차\n208.0\n6.9\n16.67\n\n\n1\n1\n20220330\n30주차\n172.0\n6.8\n17.33\n\n\n2\n2\n20220330\n30주차\n150.0\n9.3\n16.00\n\n\n3\n3\n20220330\n30주차\n121.0\n5.9\n16.20\n\n\n4\n4\n20220406\n31주차\n175.0\n5.8\n17.40\n\n\n\n\n\n\n\n\ninputs.shape, outputs.shape\n\n((10112, 20), (1518, 6))\n\n\n\ninputs.isna().sum().sort_values(ascending=False)\n\n품종            7114\n외부풍향          6993\n지습            5873\n재배형태          2408\n지온            1749\n강우감지          1505\n외부풍속           670\n외부온도           201\n내부온도             0\n일사량              0\n시설ID             0\n내부습도             0\n내부CO2            0\n주차               0\n급액횟수             0\n급액EC(dS/m)       0\n급액pH             0\n급액량(회당)          0\n일                0\nSample_no        0\ndtype: int64\n\n\n\n# nan 제거  -- 베이스라인이므로 간단한 처리를 위해 nan 항목 보간 없이 학습\ninputs = inputs.dropna(axis=1)\n\n\n# 주차 정보 수치 변환\ninputs['주차'] = [int(i.replace('주차', \"\")) for i in inputs['주차']]\n\n\n# scaler\ninput_scaler = MinMaxScaler()\noutput_scaler = MinMaxScaler()\n\n\ninputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n주차\n내부CO2\n내부습도\n내부온도\n일사량\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n\n\n\n\n0\n30\n517.041667\n84.985417\n20.610833\n1879\n14\n2.68\n4.42\n88\n\n\n1\n30\n514.416667\n88.291250\n20.695000\n1411\n14\n2.78\n5.63\n97\n\n\n2\n30\n471.875000\n83.514583\n20.402500\n1955\n14\n2.69\n4.25\n101\n\n\n3\n30\n469.250000\n80.916250\n20.139167\n2231\n14\n2.70\n4.25\n99\n\n\n4\n30\n465.750000\n82.026250\n17.653333\n2284\n13\n2.66\n4.21\n94\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10107\n7\n334.684002\n65.565417\n21.985833\n979\n26\n2.06\n5.80\n81\n\n\n10108\n7\n333.726601\n61.144167\n22.530833\n2515\n28\n2.43\n4.42\n32\n\n\n10109\n7\n344.862883\n72.867917\n20.397917\n1972\n21\n2.71\n5.88\n27\n\n\n10110\n7\n372.708516\n66.672917\n24.401667\n1314\n18\n2.50\n5.39\n82\n\n\n10111\n7\n372.612192\n59.257083\n28.352500\n1310\n16\n2.50\n5.39\n82\n\n\n\n\n10112 rows × 9 columns\n\n\n\n\noutputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n208.0\n6.90\n16.67\n\n\n1\n172.0\n6.80\n17.33\n\n\n2\n150.0\n9.30\n16.00\n\n\n3\n121.0\n5.90\n16.20\n\n\n4\n175.0\n5.80\n17.40\n\n\n...\n...\n...\n...\n\n\n1513\n150.0\n6.95\n2.20\n\n\n1514\n140.0\n10.13\n1.40\n\n\n1515\n200.0\n9.61\n1.40\n\n\n1516\n210.0\n8.47\n2.20\n\n\n1517\n150.0\n9.16\n3.20\n\n\n\n\n1518 rows × 3 columns\n\n\n\n\n# scaling\ninput_sc = input_scaler.fit_transform(inputs.iloc[:,3:].to_numpy())\noutput_sc = output_scaler.fit_transform(outputs.iloc[:,3:].to_numpy())\n\n\nlen(inputs['Sample_no'].unique()) \n\n1518\n\n\n\n# 입력 시계열화\ninput_ts = []\nfor i in outputs['Sample_no']:\n    sample = input_sc[inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    input_ts.append(sample)\ninput_ts = np.concatenate(input_ts, axis=0)\n\n\ninput_ts.shape\n\n(1518, 7, 9)\n\n\n\n# 셋 분리\ntrain_x, val_x, train_y, val_y = train_test_split(input_ts, output_sc, test_size=0.2,\n                                                  shuffle=True, random_state=0)\n\n\ntrain_x.shape, val_x.shape, train_y.shape, val_y.shape\n\n((1214, 7, 9), (304, 7, 9), (1214, 3), (304, 3))\n\n\n\n# 모델 정의\ndef create_model():\n    x = Input(shape=[7, 9])\n    l1 = LSTM(64)(x)\n    out = Dense(3, activation='tanh')(l1)\n    return Model(inputs=x, outputs=out)\n\nmodel = create_model()\nmodel.summary()\ncheckpointer = ModelCheckpoint(monitor='val_loss', filepath='baseline.h5',\n                               verbose=1, save_best_only=True, save_weights_only=True)\n\nmodel.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['mse'])\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 7, 9)]            0         \n                                                                 \n lstm (LSTM)                 (None, 64)                18944     \n                                                                 \n dense (Dense)               (None, 3)                 195       \n                                                                 \n=================================================================\nTotal params: 19,139\nTrainable params: 19,139\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n\n\n\n# 학습\nhist = model.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[checkpointer])\n\nEpoch 1/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0170 - mse: 0.0170 \nEpoch 1: val_loss improved from inf to 0.00995, saving model to baseline.h5\n38/38 [==============================] - 1s 9ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0099 - val_mse: 0.0099\nEpoch 2/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0117 - mse: 0.0117\nEpoch 2: val_loss improved from 0.00995 to 0.00839, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0084 - val_mse: 0.0084\nEpoch 3/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0109 - mse: 0.0109\nEpoch 3: val_loss improved from 0.00839 to 0.00777, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 4/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0106 - mse: 0.0106\nEpoch 4: val_loss did not improve from 0.00777\n38/38 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 5/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0101 - mse: 0.0101\nEpoch 5: val_loss improved from 0.00777 to 0.00757, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0076 - val_mse: 0.0076\nEpoch 6/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0105 - mse: 0.0105\nEpoch 6: val_loss improved from 0.00757 to 0.00751, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0075 - val_mse: 0.0075\nEpoch 7/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0098 - mse: 0.0098\nEpoch 7: val_loss improved from 0.00751 to 0.00738, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0074 - val_mse: 0.0074\nEpoch 8/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 8: val_loss improved from 0.00738 to 0.00709, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0071 - val_mse: 0.0071\nEpoch 9/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 9: val_loss improved from 0.00709 to 0.00702, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 10/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 10: val_loss did not improve from 0.00702\n38/38 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0072 - val_mse: 0.0072\nEpoch 11/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 11: val_loss improved from 0.00702 to 0.00696, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 12/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 12: val_loss improved from 0.00696 to 0.00682, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 13/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 13: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 14/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 14: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 15/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0094 - mse: 0.0094\nEpoch 15: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 16/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 16: val_loss improved from 0.00682 to 0.00674, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 17/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 17: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 18/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 18: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 19/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 19: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 20/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 20: val_loss improved from 0.00674 to 0.00670, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 21/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 21: val_loss improved from 0.00670 to 0.00659, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 22/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 22: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 23/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 23: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 24/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0090 - mse: 0.0090\nEpoch 24: val_loss improved from 0.00659 to 0.00650, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 25/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 25: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 26/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 26: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 27/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 27: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 28/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 28: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 29/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 29: val_loss improved from 0.00650 to 0.00637, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0064 - val_mse: 0.0064\nEpoch 30/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0084 - mse: 0.0084\nEpoch 30: val_loss improved from 0.00637 to 0.00632, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0063 - val_mse: 0.0063\nEpoch 31/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 31: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 32/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0077 - mse: 0.0077\nEpoch 32: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 33/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 33: val_loss improved from 0.00632 to 0.00623, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 34/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0082 - mse: 0.0082\nEpoch 34: val_loss improved from 0.00623 to 0.00614, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0061 - val_mse: 0.0061\nEpoch 35/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0080 - mse: 0.0080\nEpoch 35: val_loss improved from 0.00614 to 0.00604, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 36/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 36: val_loss did not improve from 0.00604\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 37/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 37: val_loss improved from 0.00604 to 0.00601, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 38/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 38: val_loss improved from 0.00601 to 0.00586, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0059 - val_mse: 0.0059\nEpoch 39/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0078 - mse: 0.0078\nEpoch 39: val_loss did not improve from 0.00586\n38/38 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 40/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 40: val_loss improved from 0.00586 to 0.00551, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 41/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 41: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 42/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0068 - mse: 0.0068\nEpoch 42: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 43/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 43: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 44/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0071 - mse: 0.0071\nEpoch 44: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 45/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 45: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 46/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 46: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 47/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 47: val_loss improved from 0.00551 to 0.00509, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 48/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 48: val_loss did not improve from 0.00509\n38/38 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0054 - val_mse: 0.0054\nEpoch 49/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0062 - mse: 0.0062\nEpoch 49: val_loss improved from 0.00509 to 0.00507, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 50/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 50: val_loss did not improve from 0.00507\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0056 - val_mse: 0.0056\n\n\n\n# loss 히스토리 확인\nfig, loss_ax = plt.subplots()\nloss_ax.plot(hist.history['loss'], 'r', label='loss')\nloss_ax.plot(hist.history['val_loss'], 'g', label='val_loss')\nloss_ax.set_xlabel('epoch')\nloss_ax.set_ylabel('loss')\nloss_ax.legend()\nplt.title('Training loss - Validation loss plot')\nplt.show()\n\n\n\n\n\n# 저장된 가중치 불러오기\nmodel.load_weights('baseline.h5')\n\n\n# 테스트셋 전처리 및 추론\ntest_inputs = pd.read_csv('./farm/test_input.csv')\noutput_sample = pd.read_csv('./farm/answer_sample.csv')\n\ntest_inputs = test_inputs[inputs.columns]\ntest_inputs['주차'] = [int(i.replace('주차', \"\")) for i in test_inputs['주차']]\ntest_input_sc = input_scaler.transform(test_inputs.iloc[:,3:].to_numpy())\n\ntest_input_ts = []\nfor i in output_sample['Sample_no']:\n    sample = test_input_sc[test_inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    test_input_ts.append(sample)\ntest_input_ts = np.concatenate(test_input_ts, axis=0)\n\n\ntest_input_ts.shape\n\n(506, 7, 9)\n\n\n\nprediction = model.predict(test_input_ts)\n\nprediction = output_scaler.inverse_transform(prediction)\noutput_sample[['생장길이', '줄기직경', '개화군']] = prediction\n\n16/16 [==============================] - 0s 793us/step\n\n\n\noutput_sample\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n9\n20220413\n32주차\n47.151882\n7.198693\n13.229403\n\n\n1\n12\n20170312\n30주차\n420.956116\n3.321363\n10.489825\n\n\n2\n19\n20170319\n31주차\n589.641235\n4.079537\n7.553223\n\n\n3\n23\n20170326\n32주차\n281.593994\n4.563877\n7.977988\n\n\n4\n27\n20170430\n37주차\n89.870880\n7.789731\n4.768530\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n501\n2015\n20160508\n14주차\n188.253265\n12.274371\n5.467629\n\n\n502\n2016\n20160529\n17주차\n1998.461670\n4.776089\n1.905745\n\n\n503\n2024\n20160828\n7주차\n49.577644\n13.211569\n2.025992\n\n\n504\n2025\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n505\n2026\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n\n\n506 rows × 6 columns\n\n\n\n\n# 제출할 추론 결과 저장\noutput_sample.to_csv('prediction.csv', index=False)\n\n- 텐서보드\n\n# 학습\ncb1 = tf.keras.callbacks.TensorBoard()\nmodel.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[cb1])\n\nEpoch 1/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 2/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 3/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 4/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 5/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 6/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 7/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 8/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 9/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 10/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 11/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 12/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 13/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 14/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 15/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 16/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 17/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0044 - val_mse: 0.0044\nEpoch 18/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 19/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 20/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0050 - val_mse: 0.0050\nEpoch 21/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 22/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 23/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 24/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 25/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 26/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 27/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 28/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 29/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 30/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 31/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 32/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 33/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 34/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 35/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 36/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 37/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 38/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 39/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 40/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 41/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 42/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 43/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 44/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 45/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 46/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0037 - val_mse: 0.0037\nEpoch 47/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 48/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 49/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 50/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n\n\n&lt;keras.callbacks.History at 0x7f2df8e88b80&gt;\n\n\n\n%load_ext tensorboard\n%tensorboard --logdir logs --host \n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\nReusing TensorBoard on port 6006 (pid 109833), started 0:04:21 ago. (Use '!kill 109833' to kill it.)"
  },
  {
    "objectID": "posts/6_note/2023-09-09_baseline_code.html#import",
    "href": "posts/6_note/2023-09-09_baseline_code.html#import",
    "title": "연습장2",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n\n# 데이터 불러오기\ninputs = pd.read_csv('./farm/train_input.csv')\noutputs = pd.read_csv('./farm/train_output.csv')\n\n\ninputs.head()\n\n\n\n\n\n\n\n\nSample_no\n시설ID\n일\n주차\n내부CO2\n내부습도\n내부온도\n지온\n강우감지\n일사량\n외부온도\n외부풍향\n외부풍속\n지습\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n품종\n재배형태\n\n\n\n\n0\n0\nfarm25\n20220323\n30주차\n517.041667\n84.985417\n20.610833\n0.0\nNaN\n1879\n11.166667\n195.0\n0.083333\n0.0\n14\n2.68\n4.42\n88\ntomato09\nNaN\n\n\n1\n0\nfarm25\n20220324\n30주차\n514.416667\n88.291250\n20.695000\n0.0\nNaN\n1411\n12.708333\n142.5\n0.000000\n0.0\n14\n2.78\n5.63\n97\ntomato09\nNaN\n\n\n2\n0\nfarm25\n20220326\n30주차\n471.875000\n83.514583\n20.402500\n0.0\nNaN\n1955\n8.791667\n202.5\n0.000000\n0.0\n14\n2.69\n4.25\n101\ntomato09\nNaN\n\n\n3\n0\nfarm25\n20220327\n30주차\n469.250000\n80.916250\n20.139167\n0.0\nNaN\n2231\n8.041667\n180.0\n0.000000\n0.0\n14\n2.70\n4.25\n99\ntomato09\nNaN\n\n\n4\n0\nfarm25\n20220328\n30주차\n465.750000\n82.026250\n17.653333\n0.0\nNaN\n2284\n9.000000\n97.5\n0.041667\n0.0\n13\n2.66\n4.21\n94\ntomato09\nNaN\n\n\n\n\n\n\n\n\noutputs.head()\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n0\n20220330\n30주차\n208.0\n6.9\n16.67\n\n\n1\n1\n20220330\n30주차\n172.0\n6.8\n17.33\n\n\n2\n2\n20220330\n30주차\n150.0\n9.3\n16.00\n\n\n3\n3\n20220330\n30주차\n121.0\n5.9\n16.20\n\n\n4\n4\n20220406\n31주차\n175.0\n5.8\n17.40\n\n\n\n\n\n\n\n\ninputs.shape, outputs.shape\n\n((10112, 20), (1518, 6))\n\n\n\ninputs.isna().sum().sort_values(ascending=False)\n\n품종            7114\n외부풍향          6993\n지습            5873\n재배형태          2408\n지온            1749\n강우감지          1505\n외부풍속           670\n외부온도           201\n내부온도             0\n일사량              0\n시설ID             0\n내부습도             0\n내부CO2            0\n주차               0\n급액횟수             0\n급액EC(dS/m)       0\n급액pH             0\n급액량(회당)          0\n일                0\nSample_no        0\ndtype: int64\n\n\n\n# nan 제거  -- 베이스라인이므로 간단한 처리를 위해 nan 항목 보간 없이 학습\ninputs = inputs.dropna(axis=1)\n\n\n# 주차 정보 수치 변환\ninputs['주차'] = [int(i.replace('주차', \"\")) for i in inputs['주차']]\n\n\n# scaler\ninput_scaler = MinMaxScaler()\noutput_scaler = MinMaxScaler()\n\n\ninputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n주차\n내부CO2\n내부습도\n내부온도\n일사량\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n\n\n\n\n0\n30\n517.041667\n84.985417\n20.610833\n1879\n14\n2.68\n4.42\n88\n\n\n1\n30\n514.416667\n88.291250\n20.695000\n1411\n14\n2.78\n5.63\n97\n\n\n2\n30\n471.875000\n83.514583\n20.402500\n1955\n14\n2.69\n4.25\n101\n\n\n3\n30\n469.250000\n80.916250\n20.139167\n2231\n14\n2.70\n4.25\n99\n\n\n4\n30\n465.750000\n82.026250\n17.653333\n2284\n13\n2.66\n4.21\n94\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10107\n7\n334.684002\n65.565417\n21.985833\n979\n26\n2.06\n5.80\n81\n\n\n10108\n7\n333.726601\n61.144167\n22.530833\n2515\n28\n2.43\n4.42\n32\n\n\n10109\n7\n344.862883\n72.867917\n20.397917\n1972\n21\n2.71\n5.88\n27\n\n\n10110\n7\n372.708516\n66.672917\n24.401667\n1314\n18\n2.50\n5.39\n82\n\n\n10111\n7\n372.612192\n59.257083\n28.352500\n1310\n16\n2.50\n5.39\n82\n\n\n\n\n10112 rows × 9 columns\n\n\n\n\noutputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n208.0\n6.90\n16.67\n\n\n1\n172.0\n6.80\n17.33\n\n\n2\n150.0\n9.30\n16.00\n\n\n3\n121.0\n5.90\n16.20\n\n\n4\n175.0\n5.80\n17.40\n\n\n...\n...\n...\n...\n\n\n1513\n150.0\n6.95\n2.20\n\n\n1514\n140.0\n10.13\n1.40\n\n\n1515\n200.0\n9.61\n1.40\n\n\n1516\n210.0\n8.47\n2.20\n\n\n1517\n150.0\n9.16\n3.20\n\n\n\n\n1518 rows × 3 columns\n\n\n\n\n# scaling\ninput_sc = input_scaler.fit_transform(inputs.iloc[:,3:].to_numpy())\noutput_sc = output_scaler.fit_transform(outputs.iloc[:,3:].to_numpy())\n\n\nlen(inputs['Sample_no'].unique()) \n\n1518\n\n\n\n# 입력 시계열화\ninput_ts = []\nfor i in outputs['Sample_no']:\n    sample = input_sc[inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    input_ts.append(sample)\ninput_ts = np.concatenate(input_ts, axis=0)\n\n\ninput_ts.shape\n\n(1518, 7, 9)\n\n\n\n# 셋 분리\ntrain_x, val_x, train_y, val_y = train_test_split(input_ts, output_sc, test_size=0.2,\n                                                  shuffle=True, random_state=0)\n\n\ntrain_x.shape, val_x.shape, train_y.shape, val_y.shape\n\n((1214, 7, 9), (304, 7, 9), (1214, 3), (304, 3))\n\n\n\n# 모델 정의\ndef create_model():\n    x = Input(shape=[7, 9])\n    l1 = LSTM(64)(x)\n    out = Dense(3, activation='tanh')(l1)\n    return Model(inputs=x, outputs=out)\n\nmodel = create_model()\nmodel.summary()\ncheckpointer = ModelCheckpoint(monitor='val_loss', filepath='baseline.h5',\n                               verbose=1, save_best_only=True, save_weights_only=True)\n\nmodel.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['mse'])\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 7, 9)]            0         \n                                                                 \n lstm (LSTM)                 (None, 64)                18944     \n                                                                 \n dense (Dense)               (None, 3)                 195       \n                                                                 \n=================================================================\nTotal params: 19,139\nTrainable params: 19,139\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n\n\n\n# 학습\nhist = model.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[checkpointer])\n\nEpoch 1/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0170 - mse: 0.0170 \nEpoch 1: val_loss improved from inf to 0.00995, saving model to baseline.h5\n38/38 [==============================] - 1s 9ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0099 - val_mse: 0.0099\nEpoch 2/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0117 - mse: 0.0117\nEpoch 2: val_loss improved from 0.00995 to 0.00839, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0084 - val_mse: 0.0084\nEpoch 3/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0109 - mse: 0.0109\nEpoch 3: val_loss improved from 0.00839 to 0.00777, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 4/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0106 - mse: 0.0106\nEpoch 4: val_loss did not improve from 0.00777\n38/38 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 5/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0101 - mse: 0.0101\nEpoch 5: val_loss improved from 0.00777 to 0.00757, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0076 - val_mse: 0.0076\nEpoch 6/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0105 - mse: 0.0105\nEpoch 6: val_loss improved from 0.00757 to 0.00751, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0075 - val_mse: 0.0075\nEpoch 7/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0098 - mse: 0.0098\nEpoch 7: val_loss improved from 0.00751 to 0.00738, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0074 - val_mse: 0.0074\nEpoch 8/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 8: val_loss improved from 0.00738 to 0.00709, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0071 - val_mse: 0.0071\nEpoch 9/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 9: val_loss improved from 0.00709 to 0.00702, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 10/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 10: val_loss did not improve from 0.00702\n38/38 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0072 - val_mse: 0.0072\nEpoch 11/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 11: val_loss improved from 0.00702 to 0.00696, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 12/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 12: val_loss improved from 0.00696 to 0.00682, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 13/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 13: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 14/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 14: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 15/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0094 - mse: 0.0094\nEpoch 15: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 16/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 16: val_loss improved from 0.00682 to 0.00674, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 17/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 17: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 18/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 18: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 19/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 19: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 20/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 20: val_loss improved from 0.00674 to 0.00670, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 21/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 21: val_loss improved from 0.00670 to 0.00659, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 22/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 22: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 23/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 23: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 24/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0090 - mse: 0.0090\nEpoch 24: val_loss improved from 0.00659 to 0.00650, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 25/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 25: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 26/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 26: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 27/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 27: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 28/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 28: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 29/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 29: val_loss improved from 0.00650 to 0.00637, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0064 - val_mse: 0.0064\nEpoch 30/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0084 - mse: 0.0084\nEpoch 30: val_loss improved from 0.00637 to 0.00632, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0063 - val_mse: 0.0063\nEpoch 31/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 31: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 32/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0077 - mse: 0.0077\nEpoch 32: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 33/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 33: val_loss improved from 0.00632 to 0.00623, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 34/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0082 - mse: 0.0082\nEpoch 34: val_loss improved from 0.00623 to 0.00614, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0061 - val_mse: 0.0061\nEpoch 35/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0080 - mse: 0.0080\nEpoch 35: val_loss improved from 0.00614 to 0.00604, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 36/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 36: val_loss did not improve from 0.00604\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 37/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 37: val_loss improved from 0.00604 to 0.00601, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 38/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 38: val_loss improved from 0.00601 to 0.00586, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0059 - val_mse: 0.0059\nEpoch 39/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0078 - mse: 0.0078\nEpoch 39: val_loss did not improve from 0.00586\n38/38 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 40/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 40: val_loss improved from 0.00586 to 0.00551, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 41/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 41: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 42/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0068 - mse: 0.0068\nEpoch 42: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 43/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 43: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 44/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0071 - mse: 0.0071\nEpoch 44: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 45/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 45: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 46/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 46: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 47/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 47: val_loss improved from 0.00551 to 0.00509, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 48/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 48: val_loss did not improve from 0.00509\n38/38 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0054 - val_mse: 0.0054\nEpoch 49/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0062 - mse: 0.0062\nEpoch 49: val_loss improved from 0.00509 to 0.00507, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 50/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 50: val_loss did not improve from 0.00507\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0056 - val_mse: 0.0056\n\n\n\n# loss 히스토리 확인\nfig, loss_ax = plt.subplots()\nloss_ax.plot(hist.history['loss'], 'r', label='loss')\nloss_ax.plot(hist.history['val_loss'], 'g', label='val_loss')\nloss_ax.set_xlabel('epoch')\nloss_ax.set_ylabel('loss')\nloss_ax.legend()\nplt.title('Training loss - Validation loss plot')\nplt.show()\n\n\n\n\n\n# 저장된 가중치 불러오기\nmodel.load_weights('baseline.h5')\n\n\n# 테스트셋 전처리 및 추론\ntest_inputs = pd.read_csv('./farm/test_input.csv')\noutput_sample = pd.read_csv('./farm/answer_sample.csv')\n\ntest_inputs = test_inputs[inputs.columns]\ntest_inputs['주차'] = [int(i.replace('주차', \"\")) for i in test_inputs['주차']]\ntest_input_sc = input_scaler.transform(test_inputs.iloc[:,3:].to_numpy())\n\ntest_input_ts = []\nfor i in output_sample['Sample_no']:\n    sample = test_input_sc[test_inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    test_input_ts.append(sample)\ntest_input_ts = np.concatenate(test_input_ts, axis=0)\n\n\ntest_input_ts.shape\n\n(506, 7, 9)\n\n\n\nprediction = model.predict(test_input_ts)\n\nprediction = output_scaler.inverse_transform(prediction)\noutput_sample[['생장길이', '줄기직경', '개화군']] = prediction\n\n16/16 [==============================] - 0s 793us/step\n\n\n\noutput_sample\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n9\n20220413\n32주차\n47.151882\n7.198693\n13.229403\n\n\n1\n12\n20170312\n30주차\n420.956116\n3.321363\n10.489825\n\n\n2\n19\n20170319\n31주차\n589.641235\n4.079537\n7.553223\n\n\n3\n23\n20170326\n32주차\n281.593994\n4.563877\n7.977988\n\n\n4\n27\n20170430\n37주차\n89.870880\n7.789731\n4.768530\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n501\n2015\n20160508\n14주차\n188.253265\n12.274371\n5.467629\n\n\n502\n2016\n20160529\n17주차\n1998.461670\n4.776089\n1.905745\n\n\n503\n2024\n20160828\n7주차\n49.577644\n13.211569\n2.025992\n\n\n504\n2025\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n505\n2026\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n\n\n506 rows × 6 columns\n\n\n\n\n# 제출할 추론 결과 저장\noutput_sample.to_csv('prediction.csv', index=False)\n\n- 텐서보드\n\n# 학습\ncb1 = tf.keras.callbacks.TensorBoard()\nmodel.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[cb1])\n\nEpoch 1/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 2/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 3/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 4/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 5/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 6/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 7/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 8/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 9/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 10/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 11/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 12/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 13/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 14/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 15/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 16/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 17/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0044 - val_mse: 0.0044\nEpoch 18/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 19/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 20/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0050 - val_mse: 0.0050\nEpoch 21/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 22/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 23/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 24/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 25/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 26/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 27/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 28/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 29/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 30/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 31/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 32/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 33/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 34/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 35/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 36/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 37/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 38/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 39/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 40/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 41/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 42/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 43/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 44/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 45/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 46/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0037 - val_mse: 0.0037\nEpoch 47/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 48/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 49/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 50/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n\n\n&lt;keras.callbacks.History at 0x7f2df8e88b80&gt;\n\n\n\n%load_ext tensorboard\n%tensorboard --logdir logs --host \n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\nReusing TensorBoard on port 6006 (pid 109833), started 0:04:21 ago. (Use '!kill 109833' to kill it.)"
  },
  {
    "objectID": "posts/6_note/2023-09-13-autogluon_ts.html",
    "href": "posts/6_note/2023-09-13-autogluon_ts.html",
    "title": "AutoGluon TS",
    "section": "",
    "text": "Autogluon Timeseries는 AWS에서의 연구 및 개발을 기반으로 하며, 오픈 소스 커뮤니티와 공개 소프트웨어로 제공되고 있습니다. 이 라이브러리는 Python 언어로 작성되었으며, 주로 주피터 노트북과 함께 사용되며, 다양한 시계열 데이터에 대한 모델을 빠르게 실험하고 평가하는 데 도움이 됩니다.\n\nautogluon.timeseries 라이브러리를 이용하여 시계열 예측을 하려면 아래와 같은 두개의 클래스를 이용하면 된다.\n1. : TimeSeriesDataFrame\n\n시계열 데이터를 로드하고, 데이터의 전처리, 탐색 및 준비를 수행하는 단계.\n각 시계열은 “item_id,” “timestamp,” 그리고 “target” 열과 같은 기본 열 구조를 따라야 한다.\n\n2. : TimeSeriesPredictor (1-학습/2-예측/3-평가)\n\nfit/predict/leaderboard\n시계열 데이터에 대한 모델을 훈련하고 예측을 하는 단계.\n시계열 모델을 자동으로 피팅(fitting)하고 모델 선택 및 하이퍼파라미터 튜닝을 수행한다.\n다양한 시계열 모델 및 예측 알고리즘을 자동으로 적용하고 최적의 모델을 선택하여 사용자에게 최상의 예측 결과를 제공한다.\n\n위를 요약하면, autogluon.timeseries 라이브러리를 사용하여 시계열 데이터를 저장/관리(TimeSeriesDataFrame)를 하고, 이러한 데이터를 기반으로 모델을 훈련하고 예측을 하는(TimeSeriesPredictor) 자동화된 시계열 예측 작업을 수행할 수 있다."
  },
  {
    "objectID": "posts/6_note/2023-09-13-autogluon_ts.html#autogluon.timeseries",
    "href": "posts/6_note/2023-09-13-autogluon_ts.html#autogluon.timeseries",
    "title": "AutoGluon TS",
    "section": "",
    "text": "Autogluon Timeseries는 AWS에서의 연구 및 개발을 기반으로 하며, 오픈 소스 커뮤니티와 공개 소프트웨어로 제공되고 있습니다. 이 라이브러리는 Python 언어로 작성되었으며, 주로 주피터 노트북과 함께 사용되며, 다양한 시계열 데이터에 대한 모델을 빠르게 실험하고 평가하는 데 도움이 됩니다.\n\nautogluon.timeseries 라이브러리를 이용하여 시계열 예측을 하려면 아래와 같은 두개의 클래스를 이용하면 된다.\n1. : TimeSeriesDataFrame\n\n시계열 데이터를 로드하고, 데이터의 전처리, 탐색 및 준비를 수행하는 단계.\n각 시계열은 “item_id,” “timestamp,” 그리고 “target” 열과 같은 기본 열 구조를 따라야 한다.\n\n2. : TimeSeriesPredictor (1-학습/2-예측/3-평가)\n\nfit/predict/leaderboard\n시계열 데이터에 대한 모델을 훈련하고 예측을 하는 단계.\n시계열 모델을 자동으로 피팅(fitting)하고 모델 선택 및 하이퍼파라미터 튜닝을 수행한다.\n다양한 시계열 모델 및 예측 알고리즘을 자동으로 적용하고 최적의 모델을 선택하여 사용자에게 최상의 예측 결과를 제공한다.\n\n위를 요약하면, autogluon.timeseries 라이브러리를 사용하여 시계열 데이터를 저장/관리(TimeSeriesDataFrame)를 하고, 이러한 데이터를 기반으로 모델을 훈련하고 예측을 하는(TimeSeriesPredictor) 자동화된 시계열 예측 작업을 수행할 수 있다."
  },
  {
    "objectID": "posts/6_note/2023-09-13-autogluon_ts.html#loading-time-series-data-as-a-timeseriesdataframe",
    "href": "posts/6_note/2023-09-13-autogluon_ts.html#loading-time-series-data-as-a-timeseriesdataframe",
    "title": "AutoGluon TS",
    "section": "1. Loading time series data as a TimeSeriesDataFrame",
    "text": "1. Loading time series data as a TimeSeriesDataFrame\n\nimport pandas as pd\nfrom autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n\n\ndf = pd.read_csv(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly_subset/train.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nitem_id\ntimestamp\ntarget\n\n\n\n\n0\nH1\n1750-01-01 00:00:00\n605.0\n\n\n1\nH1\n1750-01-01 01:00:00\n586.0\n\n\n2\nH1\n1750-01-01 02:00:00\n586.0\n\n\n3\nH1\n1750-01-01 03:00:00\n559.0\n\n\n4\nH1\n1750-01-01 04:00:00\n511.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAutoGluon을 사용하려면 시계열 데이터가 long format 이어야 한다.\n\n\n\ntrain_data = TimeSeriesDataFrame.from_data_frame(\n    df,\n    id_column=\"item_id\",\n    timestamp_column=\"timestamp\"\n)\n\n\ntrain_data\n\n\n\n\n\n\n\n\n\ntarget\n\n\nitem_id\ntimestamp\n\n\n\n\n\nH1\n1750-01-01 00:00:00\n605.0\n\n\n1750-01-01 01:00:00\n586.0\n\n\n1750-01-01 02:00:00\n586.0\n\n\n1750-01-01 03:00:00\n559.0\n\n\n1750-01-01 04:00:00\n511.0\n\n\n...\n...\n...\n\n\nH200\n1750-02-09 19:00:00\n24.2\n\n\n1750-02-09 20:00:00\n22.6\n\n\n1750-02-09 21:00:00\n20.8\n\n\n1750-02-09 22:00:00\n19.6\n\n\n1750-02-09 23:00:00\n18.8\n\n\n\n\n148060 rows × 1 columns\n\n\n\n\ntype(train_data)\n\nautogluon.timeseries.dataset.ts_dataframe.TimeSeriesDataFrame\n\n\n- item\nTimeSeriesDataFrame 클래스를 사용하여 여러 개의 시계열 데이터를 저장하며, 이러한 각각의 시계열 데이터를 “item” 이라고 부른다.\n예를들어, 수요예측에서 각 “item”은 각각 다른 제품을 나타낼 수도 있고, 금융 데이터셋에서는 각 “item”이 주식을 나타낼 수 있음.\n- 다변량 예측과의 차이\n“multivariate forecasting”과 다르다는 것을 주의하자. “Multivariate forecasting”은 여러 변수 간의 상호작용을 모델링하고 다른 시계열 간의 관계를 고려하여 예측하는 것을 의미하는 반면 AutoGluon은 각 시계열을 개별적으로 처리하므로 “item” 간의 상호작용을 고려하지 않는다.\n- pd.DataFrame을 상속\n\n# TimeSeriesDataFrame??\n\n\n\n\nTimeSeriesDataFrame은 pandas.DataFrame을 상속하고 있어 pandas.DataFrame의 모든 속성 및 메서드를 사용할 수 있다."
  },
  {
    "objectID": "posts/6_note/2023-09-13-autogluon_ts.html#training-time-series-models-with-timeseriespredictor.fit",
    "href": "posts/6_note/2023-09-13-autogluon_ts.html#training-time-series-models-with-timeseriespredictor.fit",
    "title": "AutoGluon TS",
    "section": "2. Training time series models with TimeSeriesPredictor.fit",
    "text": "2. Training time series models with TimeSeriesPredictor.fit\n시계열 예측을 위해 TimeSeriesPredictor 오브젝트를 생성하자.\n\n- predictor 생성\n\npredictor = TimeSeriesPredictor(\n    prediction_length=48, # 시간별 자료 // 미래의 2일을 예측하고 싶어!\n    path=\"autogluon-m4-hourly\", # 학습된 모델 저장폴더 경로\n    target=\"target\",\n    eval_metric=\"MASE\",\n)\n\n\n\n- 적합\n\npredictor.fit(\n    train_data,\n    presets=\"medium_quality\", # 훈련시간 10분 제한 (medium_quality / fast_training / high_quality / best_quality)\n    time_limit=600,\n)\n\n================ TimeSeriesPredictor ================\nTimeSeriesPredictor.fit() called\nSetting presets to: medium_quality\nFitting with arguments:\n{'enable_ensemble': True,\n 'evaluation_metric': 'MASE',\n 'excluded_model_types': None,\n 'hyperparameter_tune_kwargs': None,\n 'hyperparameters': 'medium_quality',\n 'num_val_windows': 1,\n 'prediction_length': 48,\n 'random_seed': None,\n 'target': 'target',\n 'time_limit': 600,\n 'verbosity': 2}\nProvided training data set with 148060 rows, 200 items (item = single time series). Average time series length is 740.3. Data frequency is 'H'.\n=====================================================\nAutoGluon will save models to autogluon-m4-hourly/\nAutoGluon will gauge predictive performance using evaluation metric: 'MASE'\n    This metric's sign has been flipped to adhere to being 'higher is better'. The reported score can be multiplied by -1 to get the metric value.\n\nProvided dataset contains following columns:\n    target:           'target'\n\nStarting training. Start time is 2023-09-15 00:47:18\nModels that will be trained: ['Naive', 'SeasonalNaive', 'Theta', 'AutoETS', 'RecursiveTabular', 'DeepAR']\nTraining timeseries model Naive. Training for up to 599.88s of the 599.88s of remaining time.\n    -6.6629       = Validation score (-MASE)\n    0.06    s     = Training runtime\n    4.91    s     = Validation (prediction) runtime\nTraining timeseries model SeasonalNaive. Training for up to 594.90s of the 594.90s of remaining time.\n    -1.2169       = Validation score (-MASE)\n    0.05    s     = Training runtime\n    0.10    s     = Validation (prediction) runtime\nTraining timeseries model Theta. Training for up to 594.74s of the 594.74s of remaining time.\n    -2.1425       = Validation score (-MASE)\n    0.06    s     = Training runtime\n    29.74   s     = Validation (prediction) runtime\nTraining timeseries model AutoETS. Training for up to 564.94s of the 564.94s of remaining time.\n    -1.9400       = Validation score (-MASE)\n    0.06    s     = Training runtime\n    31.95   s     = Validation (prediction) runtime\nTraining timeseries model RecursiveTabular. Training for up to 532.92s of the 532.92s of remaining time.\n    -0.8988       = Validation score (-MASE)\n    5.95    s     = Training runtime\n    1.25    s     = Validation (prediction) runtime\nTraining timeseries model DeepAR. Training for up to 525.71s of the 525.71s of remaining time.\n    -1.5221       = Validation score (-MASE)\n    47.08   s     = Training runtime\n    0.77    s     = Validation (prediction) runtime\nFitting simple weighted ensemble.\n    -0.8823       = Validation score (-MASE)\n    2.68    s     = Training runtime\n    34.07   s     = Validation (prediction) runtime\nTraining complete. Models trained: ['Naive', 'SeasonalNaive', 'Theta', 'AutoETS', 'RecursiveTabular', 'DeepAR', 'WeightedEnsemble']\nTotal runtime: 124.87 s\nBest model: WeightedEnsemble\nBest model score: -0.8823\n\n\n&lt;autogluon.timeseries.predictor.TimeSeriesPredictor at 0x7f5246b844c0&gt;\n\n\n- presets\n프리셋이란, 말그대로 미리 정의해둔 설정이다. 즉, AutoGluon에서 사용할 모델과 훈련설정을 미리 정의해둔 설정을 말한다. 이 설정은 모델 선택과 하이퍼파라미터 튜닝을 자동화하여 더 빠르게 원하는 예측 모델을 찾을 수 있도록 도와준다.\n\n옵션: medium_quality, fast_training, high_quality, best_quality 등을 제공\n\n더 높은 품질의 프리셋은 일반적으로 더 정확한 예측 결과를 생성하지만 훈련에 더 많은 시간이 소요될 수 있다.\n\n모델: medium_quality는 간단한 기본모델인 Naive, SeasonalNaive와 통계모델 (AutoETS, Theta), 트리기반 모델(LightGBM), 심층학습모델(DeepAR) 및 이러한 모델을 결합한 가중앙상블이 포함된다.\n\n여기서 medium_quality는 훈련시간 10분 내로 제한하고, 이 모델 중 가장 좋은 예측 성능을 얻으려고 노력한다."
  },
  {
    "objectID": "posts/6_note/2023-09-13-autogluon_ts.html#generating-forecasts-with-timeseriespredictor.predict",
    "href": "posts/6_note/2023-09-13-autogluon_ts.html#generating-forecasts-with-timeseriespredictor.predict",
    "title": "AutoGluon TS",
    "section": "3. Generating forecasts with TimeSeriesPredictor.predict",
    "text": "3. Generating forecasts with TimeSeriesPredictor.predict\n\npredictions = predictor.predict(train_data)\npredictions.head()\n\nGlobal seed set to 123\nModel not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n\n\n\n\n\n\n\n\n\n\nmean\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\n\nitem_id\ntimestamp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH1\n1750-01-30 04:00:00\n624.225400\n592.533253\n603.438425\n611.274422\n617.975393\n624.239736\n630.498044\n637.191945\n645.042920\n655.908431\n\n\n1750-01-30 05:00:00\n558.038120\n515.235197\n530.005515\n540.575718\n549.627181\n558.071079\n566.513349\n575.543314\n586.115937\n600.829335\n\n\n1750-01-30 06:00:00\n515.495213\n464.240430\n481.872256\n494.568026\n505.378544\n515.514013\n525.643002\n536.460744\n549.135748\n566.750336\n\n\n1750-01-30 07:00:00\n481.701016\n423.316455\n443.383962\n457.847898\n470.179675\n481.696121\n493.234340\n505.589204\n520.044493\n540.152060\n\n\n1750-01-30 08:00:00\n459.156714\n394.461819\n416.640338\n432.661205\n446.361193\n459.170291\n471.982521\n485.641357\n501.591228\n523.913884\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# TimeSeriesDataFrame can also be loaded directly from a file\ntest_data = TimeSeriesDataFrame.from_path(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly_subset/test.csv\")\n\nplt.figure(figsize=(20, 3))\n\nitem_id = \"H1\"\ny_past = train_data.loc[item_id][\"target\"]\ny_pred = predictions.loc[item_id]\ny_test = test_data.loc[item_id][\"target\"][-48:]\n\nplt.plot(y_past[-200:], label=\"Past time series values\")\nplt.plot(y_pred[\"mean\"], label=\"Mean forecast\")\nplt.plot(y_test, label=\"Future time series values\")\n\nplt.fill_between(\n    y_pred.index, y_pred[\"0.1\"], y_pred[\"0.9\"], color=\"red\", alpha=0.1, label=f\"10%-90% confidence interval\"\n)\nplt.legend();"
  },
  {
    "objectID": "posts/6_note/2023-09-13-autogluon_ts.html#evaluating-the-performance-of-different-models",
    "href": "posts/6_note/2023-09-13-autogluon_ts.html#evaluating-the-performance-of-different-models",
    "title": "AutoGluon TS",
    "section": "4. Evaluating the performance of different models",
    "text": "4. Evaluating the performance of different models\n- 리더보드 확인\n\n# The test score is computed using the last\n# prediction_length=48 timesteps of each time series in test_data\npredictor.leaderboard(test_data, silent=True)\n\nAdditional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time_marginal\nfit_order\n\n\n\n\n0\nWeightedEnsemble\n-0.850095\n-0.882350\n48.495575\n34.074169\n2.684069\n7\n\n\n1\nRecursiveTabular\n-0.870271\n-0.898770\n0.802261\n1.253653\n5.947196\n5\n\n\n2\nSeasonalNaive\n-1.022854\n-1.216909\n0.107641\n0.095930\n0.053485\n2\n\n\n3\nDeepAR\n-1.586545\n-1.522113\n0.808109\n0.772847\n47.082557\n6\n\n\n4\nAutoETS\n-1.778462\n-1.939952\n46.754016\n31.951739\n0.059222\n4\n\n\n5\nTheta\n-1.905365\n-2.142531\n34.860988\n29.740138\n0.058667\n3\n\n\n6\nNaive\n-6.696079\n-6.662942\n0.103503\n4.909723\n0.061440\n1"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-zZrOGpLc8spPa9L39RiNhR&si=TFl5m9-VohYT_47L"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#a.-데이터",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#a.-데이터",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\n# import os\n# os.getcwd()\n\n\n# tr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\n# tst = TabularDataset(\"/kaggle/input/titanic/test.csv\")\n\ntr = TabularDataset(\"./titanic/train.csv\")\ntst = TabularDataset(\"./titanic/test.csv\")\n\n\ntype(tr), type(tst)\n\n(autogluon.core.dataset.TabularDataset, autogluon.core.dataset.TabularDataset)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#b.-predictor-생성",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230913_055133/\""
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#c.-적합fit",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#c.-적합fit",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\npredictr은 인스턴스니까 숨겨진 메소드가 있다.\n\npredictr.fit(tr) \n# 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230913_055133/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.16\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   675.53 GB / 982.82 GB (68.7%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    37551.07 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.21s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.23s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.14s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.29s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8324   = Validation score   (accuracy)\n    1.05s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8101   = Validation score   (accuracy)\n    0.11s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8212   = Validation score   (accuracy)\n    1.18s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.35s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8324   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 5.26s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230913_055133/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fb971df4220&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0         LightGBMLarge   0.832402       0.002752  0.347409                0.002752           0.347409            1       True         13\n1       NeuralNetFastAI   0.832402       0.007613  1.051858                0.007613           1.051858            1       True         10\n2   WeightedEnsemble_L2   0.832402       0.008036  1.318412                0.000423           0.266554            2       True         14\n3              CatBoost   0.826816       0.003309  0.381638                0.003309           0.381638            1       True          7\n4              LightGBM   0.821229       0.002449  0.144130                0.002449           0.144130            1       True          4\n5        NeuralNetTorch   0.821229       0.007525  1.182258                0.007525           1.182258            1       True         12\n6            LightGBMXT   0.815642       0.002653  0.232583                0.002653           0.232583            1       True          3\n7      RandomForestGini   0.815642       0.021465  0.288951                0.021465           0.288951            1       True          5\n8      RandomForestEntr   0.815642       0.021691  0.250421                0.021691           0.250421            1       True          6\n9        ExtraTreesGini   0.815642       0.022456  0.255295                0.022456           0.255295            1       True          8\n10              XGBoost   0.810056       0.003953  0.111502                0.003953           0.111502            1       True         11\n11       ExtraTreesEntr   0.810056       0.022516  0.263453                0.022516           0.263453            1       True          9\n12       KNeighborsDist   0.653631       0.004180  0.007864                0.004180           0.007864            1       True          2\n13       KNeighborsUnif   0.653631       0.031115  0.007931                0.031115           0.007931            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nLightGBMLarge\n0.832402\n0.002752\n0.347409\n0.002752\n0.347409\n1\nTrue\n13\n\n\n1\nNeuralNetFastAI\n0.832402\n0.007613\n1.051858\n0.007613\n1.051858\n1\nTrue\n10\n\n\n2\nWeightedEnsemble_L2\n0.832402\n0.008036\n1.318412\n0.000423\n0.266554\n2\nTrue\n14\n\n\n3\nCatBoost\n0.826816\n0.003309\n0.381638\n0.003309\n0.381638\n1\nTrue\n7\n\n\n4\nLightGBM\n0.821229\n0.002449\n0.144130\n0.002449\n0.144130\n1\nTrue\n4\n\n\n5\nNeuralNetTorch\n0.821229\n0.007525\n1.182258\n0.007525\n1.182258\n1\nTrue\n12\n\n\n6\nLightGBMXT\n0.815642\n0.002653\n0.232583\n0.002653\n0.232583\n1\nTrue\n3\n\n\n7\nRandomForestGini\n0.815642\n0.021465\n0.288951\n0.021465\n0.288951\n1\nTrue\n5\n\n\n8\nRandomForestEntr\n0.815642\n0.021691\n0.250421\n0.021691\n0.250421\n1\nTrue\n6\n\n\n9\nExtraTreesGini\n0.815642\n0.022456\n0.255295\n0.022456\n0.255295\n1\nTrue\n8\n\n\n10\nXGBoost\n0.810056\n0.003953\n0.111502\n0.003953\n0.111502\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.022516\n0.263453\n0.022516\n0.263453\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.004180\n0.007864\n0.004180\n0.007864\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.031115\n0.007931\n0.031115\n0.007931\n1\nTrue\n1\n\n\n\n\n\n\n\n\n모형에 대한 accuracy가 나타난다.\nvalidation: 실제 test에서 잘하기 위한 자체적 test라고 보면된다."
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#d.-예측-predict",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.8810325476992144\n\n\n\n(tr.Survived == (tr.Sex == \"female\")).mean() # 예전점수와 비교\n\n0.7867564534231201\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"./titanic_sub/autogluon_submission.csv\",index=False)\n\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n0\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n0\n\n\n...\n...\n...\n\n\n413\n1305\n0\n\n\n414\n1306\n1\n\n\n415\n1307\n0\n\n\n416\n1308\n0\n\n\n417\n1309\n0\n\n\n\n\n418 rows × 2 columns\n\n\n\n- TabularDataset?\n\n# pd.DataFrame &lt;-- 클래스\nTabularDataset?? # 클래스를 상속 --&gt; 상속받은 클래스의 기능을 물려받음.\n\n\nInit signature: TabularDataset(data, **kwargs)\nSource:        \nclass TabularDataset(pd.DataFrame):\n    \"\"\"\n    A dataset in tabular format (with rows = samples, columns = features/variables).\n    This object is essentially a pandas DataFrame (with some extra attributes) and all existing pandas methods can be applied to it.\n    For full list of methods/attributes, see pandas Dataframe documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n    Parameters\n    ----------\n    data : :class:`pd.DataFrame` or str\n        If str, path to data file (CSV or Parquet format).\n        If you already have your data in a :class:`pd.DataFrame`, you can specify it here.\n    Attributes\n    ----------\n    file_path: (str)\n        Path to data file from which this `TabularDataset` was created.\n        None if `data` was a :class:`pd.DataFrame`.\n    Note: In addition to these attributes, `TabularDataset` also shares all the same attributes and methods of a pandas Dataframe.\n    For a detailed list, see:  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n    Examples\n    --------\n    &gt;&gt;&gt; from autogluon.core.dataset import TabularDataset\n    &gt;&gt;&gt; train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n    &gt;&gt;&gt; train_data.head(30)\n    &gt;&gt;&gt; train_data.columns\n    \"\"\"\n    _metadata = [\"file_path\"]  # preserved properties that will be copied to a new instance of TabularDataset\n    @property\n    def _constructor(self):\n        return TabularDataset\n    @property\n    def _constructor_sliced(self):\n        return pd.Series\n    def __init__(self, data, **kwargs):\n        if isinstance(data, str):\n            file_path = data\n            data = load_pd.load(file_path)\n        else:\n            file_path = None\n        super().__init__(data, **kwargs)\n        self.file_path = file_path\nFile:           ~/anaconda3/envs/torch/lib/python3.8/site-packages/autogluon/core/dataset.py\nType:           type\nSubclasses:"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#result",
    "href": "posts/5_STBDA2023/02wk-005-타이타닉, Autogluon.out.html#result",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "Result",
    "text": "Result\n\npublic Score&gt; 0.75358"
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html",
    "href": "posts/5_STBDA2023/01wk-hw1.html",
    "title": "titanic",
    "section": "",
    "text": "- 변수 설명\n“Embarked” 변수는 승객이 타이타닉호에 어느 항구에서 탑승했는지를 나타냅니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#sibsp와-parch-에-대한-피처엔지니어링의-아이디어",
    "href": "posts/5_STBDA2023/01wk-hw1.html#sibsp와-parch-에-대한-피처엔지니어링의-아이디어",
    "title": "titanic",
    "section": "SibSp와 Parch 에 대한 피처엔지니어링의 아이디어",
    "text": "SibSp와 Parch 에 대한 피처엔지니어링의 아이디어\n- 두 변수 SibSp와 Parch를 더한 family_size라는 새로운 변수를 만들어 y를 예측할 수 있다.\n(함께 탑승한) 가족 구성원 수와 생존 여부 사이에는 일부 관련성이 있을 수 있으며, 다음과 같은 방식으로 관련성을 고려할 수 있습니다:\n\n가족 구성원 수와 생존 여부의 관련성: 가족과 함께 여행한 승객은 가족 구성원 간에 서로 도움을 주고 받을 수 있으므로 생존 가능성이 높을 수 있습니다. 특히 부모와 어린 자녀, 형제자매와 함께 여행한 경우에 이러한 경향이 뚜렷할 수 있습니다.\n가족 구성원 수와 구명보트 할당: 가족 구성원 수는 구명보트 할당에 영향을 미칠 수 있습니다. 가족 구성원이 함께 있을 경우, 함께 탑승하려는 경향이 있을 수 있으며, 구명보트의 공간이 한정되어 있기 때문에 가족 구성원 수에 따라 구명보트 할당이 결정될 수 있습니다.\n성별과 연령과의 조합: 가족 구성원 수와 함께 승객의 성별과 연령을 고려하는 경우, 가족 구성원과 함께 여행한 여성과 어린이의 생존 가능성이 높을 수 있습니다.\n데이터 분석과 모델링: 데이터 분석 및 머신 러닝 모델을 사용하여 가족 구성원 수와 생존 여부 간의 관계를 더 자세히 이해할 수 있습니다. 가족 구성원 수를 고려한 변수를 만들거나, 통계 분석을 통해 가족 구성원 수와 생존 여부 간의 상관 관계를 확인할 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#age",
    "href": "posts/5_STBDA2023/01wk-hw1.html#age",
    "title": "titanic",
    "section": "Age",
    "text": "Age\n- Age 변수를 연령대로 나눈 범주형 변수 Age_cate 사용하게 되면 해당 연령대 내에서의 패턴을 감지하기 위해 모델에 더 적합할 수 있다.\nAge변수를 연령대로 범주화한 경우 vs. 그렇지 않은 경우\n1.정보 손실 vs. 성능 개선:\n\n연령대로 나누는 경우: 연령대로 구분하면 연령 정보가 연속적인 값에서 범주형 값으로 변환됩니다. 이렇게 하면 연령 정보의 정확한 값을 잃게 되며, 대신 각 연령대에 대한 패턴과 차이를 분석할 수 있습니다. 이는 특정 연령대에서의 생존율 차이 등을 파악하는 데 유용할 수 있습니다.\n그대로 적용하는 경우: 연령을 그대로 사용하면 연속적인 값으로 정보를 보존합니다. 이는 모델에 연령에 따른 선형적인 관계를 고려할 수 있게 합니다. 모델이 연령 정보의 정확한 값을 활용하여 예측을 수행할 수 있지만, 노이즈와 이상치에 민감할 수 있습니다.\n\n2.모델의 복잡성:\n\n연령대로 나누는 경우: 연령대로 나눈 범주형 변수는 모델의 복잡성을 줄일 수 있으며, 해당 연령대 내에서의 패턴을 감지하기 위해 모델에 더 적합한 경우가 있습니다.\n그대로 적용하는 경우: 연령을 그대로 사용하면 모델이 더 복잡한 관계를 고려할 수 있으며, 연령 정보의 세부적인 영향을 고려할 수 있습니다. 그러나 데이터의 노이즈나 이상치에 민감할 수 있습니다.\n\n3.해석 가능성:\n\n연령대로 나누는 경우: 연령대는 그룹별로 패턴을 파악하기 용이하므로, 모델의 결과를 해석하거나 시각화하기에 유용합니다.\n그대로 적용하는 경우: 연령을 그대로 사용하면 모델 결과가 연령에 따른 선형적인 영향을 나타낼 수 있어 해석이 다소 복잡할 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#fare",
    "href": "posts/5_STBDA2023/01wk-hw1.html#fare",
    "title": "titanic",
    "section": "Fare",
    "text": "Fare\nFare 혹은 Age 등 수치형 변수는 구간화를 통해 범주형 변수로 만들어 y를 예측할 수 있다.\n\n비선형 관계 고려: 일부 모델은 변수와 반응 사이의 비선형 관계를 잘 처리할 수 있습니다. 구간화를 통해 비선형성을 모델링할 수 있습니다.\n이상치 관리: 구간화를 사용하면 이상치를 처리하는 데 도움이 될 수 있습니다. 이상치가 있는 경우 해당 값은 특정 구간에 할당될 수 있으며, 모델은 이상치에 대한 영향을 완화할 수 있습니다.\n해석력 향상: 수치형 변수를 범주형 변수로 변환하면 모델의 해석력을 향상시킬 수 있습니다. 각 구간(버킷)은 특정 범주의 의미를 가지므로 결과를 더 쉽게 해석할 수 있습니다.\n설명변수 추가: 구간화를 통해 범주형 변수를 생성하면 데이터에 추가적인 설명변수를 도입할 수 있습니다. 이는 모델이 변수 간의 상호작용을 고려하고 예측 성능을 향상시킬 수 있습니다.\n다중 공선성 제거: 구간화를 통해 수치형 변수를 범주형으로 변환하면 다중 공선성 문제를 일부 해결할 수 있습니다. 다중 공선성은 모델에서 변수 간의 높은 상관성으로 인해 발생할 수 있는 문제입니다.\n\n구간화를 적용할 때는 구간의 수, 경계값의 선택, 각 구간의 크기 등을 신중하게 고려해야 합니다. 또한 구간화의 효과는 데이터와 모델에 따라 다를 수 있으므로 실험을 통해 최적의 구간화 전략을 찾는 것이 중요합니다.\n- 구간화 기준\n\n등간 구간 (Equal Interval): 변수의 값 범위를 동일한 크기의 구간으로 나눕니다. 예를 들어, 연령을 0-10, 11-20, 21-30, … 등의 동일한 크기의 구간으로 분할합니다. 이 방법은 간단하고 균형잡힌 구간을 생성하지만, 데이터의 분포가 불균형할 때는 부적합할 수 있습니다.\n분위수 구간 (Quantile Interval): 데이터의 분위수(4분위, 5분위 등)를 사용하여 구간을 나눕니다. 분위수는 데이터를 동일한 백분율로 나눈 값입니다. 예를 들어, 중앙값(2분위)을 기준으로 데이터를 나눌 수 있습니다. 이 방법은 데이터 분포의 특성을 반영하며 이상치에 민감하지 않습니다.\n도메인 지식 활용: 분석 대상 데이터와 관련된 도메인 지식을 활용하여 구간을 정할 수 있습니다. 예를 들어, 연령대를 나눌 때 어린이, 청소년, 성인, 노인과 같이 도메인 지식을 활용하여 구간을 설정할 수 있습니다.\n연구 목적에 따른 최적화: 분석 목적에 따라 최적의 구간을 찾는 것이 중요합니다. 예를 들어, 데이터를 구간화할 때 생존 여부를 예측하는 모델을 만든다면 구간을 나눌 때 예측 성능을 고려하여 최적화할 수 있습니다.\n시각적 탐색: 데이터를 히스토그램 또는 상자 그림과 같은 시각화 도구를 사용하여 시각적으로 탐색한 후, 구간을 나누는 데 도움이 될 수 있는 경향성을 확인할 수 있습니다. 구간을 나눌 때 주의해야 할 점은 너무 많은 구간을 만들면 모델이 과적합될 수 있으며, 너무 적은 구간을 만들면 데이터의 패턴을 잃을 수 있습니다. 따라서 적절한 구간 수와 경계값을 선택하는 것이 중요합니다. 실험을 통해 최적의 구간화 전략을 찾고 모델의 성능을 지속적으로 평가하는 것이 좋습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#cabin",
    "href": "posts/5_STBDA2023/01wk-hw1.html#cabin",
    "title": "titanic",
    "section": "Cabin",
    "text": "Cabin\n객실번호는 객실위치와 관련이 있을 수 있다. 예를 들어 탈출하기 좋은 객실위치에 있는 승객이라면 생존확률이 더 높지 않을까?\n\n그런데, Cabin변수에 결측치가 많았던 것 같다.\n\n객실 번호는 탈출하기에 좋은 배의 위치와 관련하여 정보를 제공할 수 있습니다. 타이타닉의 침몰 시, 객실 위치는 생존 가능성에 영향을 미칠 수 있었습니다. 다음은 객실 번호와 탈출 가능성 간의 관련성을 고려할 수 있는 몇 가지 요소입니다:\n\n객실 위치: 객실 번호를 통해 어디에 위치한 객실인지를 파악할 수 있습니다. 일반적으로 상부 갑판에 위치한 객실은 비상 구명 보트에 더 가깝게 위치했을 가능성이 높습니다.\n침몰 지점: 타이타닉 침몰 시 어떤 부분이 먼저 가라앉았는지를 고려해야 합니다. 일부 객실은 침몰 시 빠르게 가라앉았을 수 있으며, 이로 인해 해당 객실에 있던 승객들이 탈출하기 어려웠을 것입니다.\n구명 보트 배치: 객실 위치와 가까운 구명 보트가 어떻게 배치되었는지를 고려해야 합니다. 특정 객실 위치에서 구명 보트가 가까이 있었다면 해당 승객들의 탈출 확률이 높았을 것입니다.\n승객 분포: 특정 객실 번호에 얼마나 많은 승객이 있었는지를 고려할 필요가 있습니다. 만약 특정 객실 번호에 많은 승객이 있었다면 그 객실에서 탈출하기 어려울 수 있었습니다.\n\n객실 번호와 생존 여부 간의 관련성을 분석하고 모델링에 활용할 수 있습니다. 이를 통해 객실 위치가 생존에 미치는 영향을 확인하고 예측할 수 있습니다. 데이터 분석을 통해 객실 위치가 생존 가능성에 어떤 영향을 미칠 수 있는지를 조사하고 모델에 포함하는 것이 중요합니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#age-와-pclass에-대한-피처엔지니어링의-아이디어",
    "href": "posts/5_STBDA2023/01wk-hw1.html#age-와-pclass에-대한-피처엔지니어링의-아이디어",
    "title": "titanic",
    "section": "Age 와 Pclass에 대한 피처엔지니어링의 아이디어",
    "text": "Age 와 Pclass에 대한 피처엔지니어링의 아이디어\n- 두 변수 Age_cate와 Pclass를 곱해서 Age_Pclass 라는 변수를 만들고 이를 활용하여 y를 예측할 수 있다.\nAge와 Pclass를 곱하는 것은 다양한 변수 간의 상호작용을 고려하는 한 가지 방법일 수 있습니다. 이렇게 하면 각 연령 그룹에서의 객실 등급에 따른 영향을 조사할 수 있습니다.예를 들어, 연령대가 어린 승객이 고급 객실 등급에 탔을 때와 연령대가 높은 승객이 저급 객실 등급에 탔을 때와 같이 특정 연령대와 객실 등급 조합이 생존 확률에 미치는 영향을 더 잘 나타낼 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-hw1.html#pclass와-embarked에-대한-피처엔지니어링의-아이디어",
    "href": "posts/5_STBDA2023/01wk-hw1.html#pclass와-embarked에-대한-피처엔지니어링의-아이디어",
    "title": "titanic",
    "section": "Pclass와 Embarked에 대한 피처엔지니어링의 아이디어",
    "text": "Pclass와 Embarked에 대한 피처엔지니어링의 아이디어\n탑승 항구는 승객의 경제적 요인 / 여행 목적/ 문화적 영향을 반영할 수 있고 이는 Pclass에 영향을 미칠 수 있으며 두 변수를 조합한 Em_Pclass라는 변수를 만들어 y를 예측할 수 있다.\n\n경제적 요인: 특정 항구에서 탑승한 승객들 중 높은 등급의 객실을 선택하는 비율이 높을 수 있습니다. 경제적으로 부유한 지역에서 출발한 승객들이 고급 객실을 더 많이 선택할 가능성이 높습니다.\n여행 목적: 승객의 여행 목적에 따라 객실 선택이 달라질 수 있습니다. 휴양지로 가는 승객들은 편안한 환경을 선호하고 고급 객실을 선택할 가능성이 높을 수 있습니다.\n가족 규모: 가족이 함께 여행하는 경우, 가족의 크기와 연령에 따라 객실 등급을 선택하는 경향이 달라질 수 있습니다. 대가족은 더 큰 객실이 필요할 수 있으며, 이로 인해 고급 객실을 선택할 가능성이 높을 수 있습니다.\n객실 가용성: 특정 항구에서 고급 객실의 가용성이 높았을 경우, 해당 항구에서 높은 등급의 객실을 더 많이 선택할 수 있습니다.\n문화적 영향: 승객의 문화적, 사회적 배경은 객실 선택에 영향을 미칠 수 있습니다. 어떤 문화나 국가에서는 특정 등급의 객실을 선호하는 경향이 있을 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html",
    "href": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-zw88937ohvzYyltCjK-_mg&si=gQE8ICf9c9TvFaxy"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html#a.-test-답을-모름-제출해야-알-수-있음",
    "href": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html#a.-test-답을-모름-제출해야-알-수-있음",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "A. test – 답을 모름, 제출해야 알 수 있음",
    "text": "A. test – 답을 모름, 제출해야 알 수 있음\n- 제출결과는 리더보드에서 확인할 수 있음."
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "href": "posts/5_STBDA2023/02wk-003-타이타닉, 첫 제출.out.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "B. train – 스스로 풀어보고 채점할 수 있음",
    "text": "B. train – 스스로 풀어보고 채점할 수 있음\n- 캐글에서 code \\(\\to\\) New Notebook 클릭\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n\nimport pandas as pd\n\n\n# tr=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n# tst=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntr = pd.read_csv('./titanic/train.csv')\ntst = pd.read_csv('./titanic/test.csv')\n\n# 예비학습 – accuracy의 계산\n\ndf = pd.DataFrame({'Surv':[1,0,1,1,0],'Sex':['f','m','f','m','m']})\n\n\ndf\n\n\n\n\n\n\n\n\nSurv\nSex\n\n\n\n\n0\n1\nf\n\n\n1\n0\nm\n\n\n2\n1\nf\n\n\n3\n1\nm\n\n\n4\n0\nm\n\n\n\n\n\n\n\n- Surv 열의 선택\n\ndf.Surv\n\n0    1\n1    0\n2    1\n3    1\n4    0\nName: Surv, dtype: int64\n\n\n- Sex 열의 선택\n\ndf.Sex\n\n0    f\n1    m\n2    f\n3    m\n4    m\nName: Sex, dtype: object\n\n\n- Sex == f이면 생존(1), 그렇지 않으면 사망(0)이라고 예측\n\n(df.Sex == 'f')*1\n\n0    1\n1    0\n2    1\n3    0\n4    0\nName: Sex, dtype: int64\n\n\n- 결과를 정리하면 아래와 같음\n\npd.DataFrame({'real': df.Surv, 'estimate': (df.Sex == 'f')*1})\n\n\n\n\n\n\n\n\nreal\nestimate\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n1\n0\n\n\n4\n0\n0\n\n\n\n\n\n\n\n- accuracy를 손으로 계산하면 \\(\\frac{4}{5}=0.8\\).\n- 컴퓨터로 accuracy를 계산한다면\n\n(df.Surv == (df.Sex == 'f')*1).sum()/5 # 방법1\n\n0.8\n\n\n\n(df.Surv == (df.Sex == 'f')*1).mean() # 방법2\n\n0.8\n\n\n#\n- 실제자료의 accuracy를 구해보자.\n\ntr = pd.read_csv('./titanic/train.csv')\n\n\n(tr.Survived == (tr.Sex == 'female')).mean()\n\n0.7867564534231201"
  },
  {
    "objectID": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html",
    "href": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html",
    "title": "[STBDA] Quarto Blog 만들기",
    "section": "",
    "text": "- 윈도우 컴퓨터인 경우 새로운 계정을 만들어 준다.\n\n한글계정명일 경우는 필수\n영어계정이더라도 계정을 분리하는게 관리에 유리\n\n- 최종적으로는\n\ngithub 로그인 + repository 생성완료\nanaconda 설치완료\ngit 설치완료\nquarto 설치완료\n\n이어야 한다.\n\n\n\ngithub 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리.\n\n\n\n\n\n아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨\n\n\n\n\n\n\n\ngit 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시\n\n\n\n\n\n\nquarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#a.-github",
    "href": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#a.-github",
    "title": "[STBDA] Quarto Blog 만들기",
    "section": "",
    "text": "github 로그인\nrepository 생성 (저는 asdf로 만들었어요)\n\n\n주의: repository 만들때 readme.md 파일을 생성 할 것. 그래야 이후의 작업이 편리."
  },
  {
    "objectID": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#b.-anaconda",
    "href": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#b.-anaconda",
    "title": "[STBDA] Quarto Blog 만들기",
    "section": "",
    "text": "아나콘다 다운로드: https://www.anaconda.com/download\n아나콘다 설치\n그림1처럼 terminal을 열었을 경우 (base)로 표시되는지 확인\n그림2처럼 conda env list등의 명령어가 잘 동작하는지 확인\n\n\n\n\n\n\n그림1: 정상적으로 설치된 경우 (base)가 보임\n\n\n\n\n\n\n\n\n그림2: 정상적으로 설치된 경우 cond env list 등의 명령이 잘 동작함. 현재환경은 *로 표시됨"
  },
  {
    "objectID": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#c.-git",
    "href": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#c.-git",
    "title": "[STBDA] Quarto Blog 만들기",
    "section": "",
    "text": "git 다운로드: https://git-scm.com/downloads\n설치\nterminal에서 git을 입력하여 정상적으로 설치되었는지 확인. 그림3 참고.\n\n\n본인이 편리한 경우 github desktop 혹은 sourcetree 등에서 작업해도 무방함. 하지만 수업에서는 다루지 않음.\n\n\n\n\n그림3: git이 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#d.-quarto",
    "href": "posts/5_STBDA2023/2000-09-12-Quarto-Blog.html#d.-quarto",
    "title": "[STBDA] Quarto Blog 만들기",
    "section": "",
    "text": "quarto 다운로드: https://quarto.org/docs/download/ 에서 pre-release 버전을 다운로드 할 것 (기능이 좀 더 많음)\n설치\n터미널에서 quarto help를 입력하여 정상적으로 설치되었는지 확인. 그림4 참고.\n\n\n\n\n그림4: quarto가 정상적으로 설치된 경우 출력예시"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-zEjYbXMD4e-nS0_-nx9Zac&si=eA9ME6Pe4ecmtIgi"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#a.-데이터",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#a.-데이터",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"./titanic/train.csv\")\ntst = TabularDataset(\"./titanic/test.csv\")\n\n- 피처엔지니어링\n\ntr.eval('Fsize = SibSp + Parch')\ntst.eval('Fsize = SibSp + Parch')\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n0\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n1\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n0\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n0\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\nSpector, Mr. Woolf\nmale\nNaN\n0\n0\nA.5. 3236\n8.0500\nNaN\nS\n0\n\n\n414\n1306\n1\nOliva y Ocana, Dona. Fermina\nfemale\n39.0\n0\n0\nPC 17758\n108.9000\nC105\nC\n0\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\n0\n0\nSOTON/O.Q. 3101262\n7.2500\nNaN\nS\n0\n\n\n416\n1308\n3\nWare, Mr. Frederick\nmale\nNaN\n0\n0\n359309\n8.0500\nNaN\nS\n0\n\n\n417\n1309\n3\nPeter, Master. Michael J\nmale\nNaN\n1\n1\n2668\n22.3583\nNaN\nC\n2\n\n\n\n\n418 rows × 12 columns"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#b.-predictor-생성",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230913_062514/\""
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#c.-적합fit",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#c.-적합fit",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr.eval('Fsize = SibSp + Parch'))\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230913_062514/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.16\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   675.50 GB / 982.82 GB (68.7%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    37202.13 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.1s = Fit runtime\n    12 features in original data used to generate 29 features in processed data.\n    Train Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.15s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.17s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8324   = Validation score   (accuracy)\n    1.03s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.11s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8324   = Validation score   (accuracy)\n    1.26s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.32s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8547   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230913_062514/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f32015ceb20&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.854749       0.039301  2.819397                0.000427           0.273874            2       True         14\n1         LightGBMLarge   0.832402       0.002563  0.319035                0.002563           0.319035            1       True         13\n2       NeuralNetFastAI   0.832402       0.006479  1.026765                0.006479           1.026765            1       True         10\n3        NeuralNetTorch   0.832402       0.007522  1.262704                0.007522           1.262704            1       True         12\n4              CatBoost   0.826816       0.003330  0.395544                0.003330           0.395544            1       True          7\n5               XGBoost   0.826816       0.003958  0.108880                0.003958           0.108880            1       True         11\n6              LightGBM   0.821229       0.002757  0.170070                0.002757           0.170070            1       True          4\n7      RandomForestEntr   0.821229       0.021223  0.248606                0.021223           0.248606            1       True          6\n8            LightGBMXT   0.815642       0.003093  0.200616                0.003093           0.200616            1       True          3\n9      RandomForestGini   0.815642       0.021101  0.267509                0.021101           0.267509            1       True          5\n10       ExtraTreesEntr   0.810056       0.022094  0.251629                0.022094           0.251629            1       True          9\n11       ExtraTreesGini   0.804469       0.020723  0.259421                0.020723           0.259421            1       True          8\n12       KNeighborsDist   0.648045       0.003651  0.007448                0.003651           0.007448            1       True          2\n13       KNeighborsUnif   0.648045       0.029592  0.008329                0.029592           0.008329            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.854749\n0.039301\n2.819397\n0.000427\n0.273874\n2\nTrue\n14\n\n\n1\nLightGBMLarge\n0.832402\n0.002563\n0.319035\n0.002563\n0.319035\n1\nTrue\n13\n\n\n2\nNeuralNetFastAI\n0.832402\n0.006479\n1.026765\n0.006479\n1.026765\n1\nTrue\n10\n\n\n3\nNeuralNetTorch\n0.832402\n0.007522\n1.262704\n0.007522\n1.262704\n1\nTrue\n12\n\n\n4\nCatBoost\n0.826816\n0.003330\n0.395544\n0.003330\n0.395544\n1\nTrue\n7\n\n\n5\nXGBoost\n0.826816\n0.003958\n0.108880\n0.003958\n0.108880\n1\nTrue\n11\n\n\n6\nLightGBM\n0.821229\n0.002757\n0.170070\n0.002757\n0.170070\n1\nTrue\n4\n\n\n7\nRandomForestEntr\n0.821229\n0.021223\n0.248606\n0.021223\n0.248606\n1\nTrue\n6\n\n\n8\nLightGBMXT\n0.815642\n0.003093\n0.200616\n0.003093\n0.200616\n1\nTrue\n3\n\n\n9\nRandomForestGini\n0.815642\n0.021101\n0.267509\n0.021101\n0.267509\n1\nTrue\n5\n\n\n10\nExtraTreesEntr\n0.810056\n0.022094\n0.251629\n0.022094\n0.251629\n1\nTrue\n9\n\n\n11\nExtraTreesGini\n0.804469\n0.020723\n0.259421\n0.020723\n0.259421\n1\nTrue\n8\n\n\n12\nKNeighborsDist\n0.648045\n0.003651\n0.007448\n0.003651\n0.007448\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.648045\n0.029592\n0.008329\n0.029592\n0.008329\n1\nTrue\n1"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#d.-예측-predict",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr.eval('Fsize = SibSp + Parch'))).mean()\n\n0.9438832772166106\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst.eval('Fsize = SibSp + Parch'))).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"./titanic_sub/autogluon(Fsize)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#submission-result",
    "href": "posts/5_STBDA2023/02wk-006-타이타닉, Autogluon (Fsize).out.html#submission-result",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "Submission Result",
    "text": "Submission Result\n\n\n\nSub and Description\nPublic Score\n\n\n\n\nautogulon_sub\n0.75358\n\n\nautogulon(Fsize)_sub\n0.77272"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_04_(5주차)_4월4일.html",
    "href": "posts/3_STBDA2022/2022_04_04_(5주차)_4월4일.html",
    "title": "[STBDA] 5wk. optimizer를 이용한 최적화",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-wVWUAZ5xT35INvWbNOXpBx\n\n옵티마이저를 이용하면 이전의 그 수식들을 다 기억하고 있지 않아도 된다는 장점!\n\n\nimports\n\n#\n#!conda install -c conda-forge python-graphviz -y\n\n\nimport tensorflow as tf \nimport numpy as np\nimport matplotlib.pyplot as plt \n\n\nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior() \n\n\n\n최적화의 문제\n- \\(loss=(\\frac{1}{2}\\beta-1)^2\\)\n- 기존에 했던 방법은 수식을 알고 있어야 한다는 단점이 있음\n\n\ntf.keras.optimizers를 이용한 최적화방법\n\n방법1: opt.apply_gradients()를 이용\n\nalpha= 0.01/6 # 학습률설정\n\n\nbeta= tf.Variable(-10.0)  # 초깃값 설정\n\n\nopt = tf.keras.optimizers.SGD(alpha)\n\n- iter1\n\nwith tf.GradientTape() as tape:  # 미분을 위한 tape object 생성\n    tape.watch(beta) \n    loss=(beta/2-1)**2 \nslope = tape.gradient(loss,beta)\n\n\n# 얠 안하고 싶다는 거임.\n# beta.assign_sub(slope*alpha) # beta update\n# beta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.969999&gt;\n\n\n\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) \nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n\n\n\n\n\n\nWarning\n\n\n\nopt.apply_gradients()의 입력은 pair의 list\n\n\n- iter2\n\nwith tf.GradientTape() as tape: \n    tape.watch(beta) \n    loss=(beta/2-1)**2 \nslope = tape.gradient(loss,beta)\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) \nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.980008&gt;\n\n\n\n\\(-10 \\to -9.99 \\to -9.98\\to \\dots\\)\n\n- for문으로 정리\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0) \nopt = tf.keras.optimizers.SGD(alpha)\n\n\nfor epoc in range(10000): \n    with tf.GradientTape() as tape: \n        tape.watch(beta) \n        loss=(beta/2-1)**2 \n    slope = tape.gradient(loss,beta)\n    opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) \n    beta\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251&gt;\n\n\n\nopt.lr\n\n&lt;tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0016666667&gt;\n\n\n\nopt.learning_rate\n\n&lt;tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0016666667&gt;\n\n\n\nid(opt.lr), id(opt.learning_rate) # 똑같음...\n\n(140176929947312, 140176929947312)\n\n\n\nopt.apply_gradients()의 입력은 pair 의 list\n지난시간에 했던 것이 optimizer로 완벽히 구현되었다!\n\n\n\n방법2: opt.minimize()\n\n방법2는 GradientTape()를 안써도 된다.\n\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0)\nopt = tf.keras.optimizers.SGD(alpha)\n\n\nloss_fn = lambda: (beta/2-1)**2\n\n\nlambda x: x**2 &lt;=&gt; lambda(x)=x^2\nlambda x,y: x+y &lt;=&gt; lambda(x,y)=x+y\nlambda: y &lt;=&gt; lambda()=y, 입력이 없으며 출력은 항상 y인 함수\n\n\nloss_fn() # 입력은 없고 출력은 뭔가 계산되는 함수 \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=36.0&gt;\n\n\n- iter 1\n\nopt.minimize(loss_fn, [beta])\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n- iter2\n\nopt.minimize(loss_fn, [beta])\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.980008&gt;\n\n\n- for문으로 정리하면\n\nalpha= 0.01/6\nbeta= tf.Variable(-10.0) \nopt = tf.keras.optimizers.SGD(alpha)\nloss_fn = lambda: (beta/2-1)**2\nfor epoc in range(10000): \n    opt.minimize(loss_fn, [beta])\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251&gt;\n\n\n\n\n\n회귀분석 문제\n- \\({\\bf y} \\approx 2.5 + 4.0 {\\bf x}\\)\n\ntnp.random.seed(43052)\nN = 200\nx = tnp.linspace(0,1,N) \nepsilon = tnp.random.randn(N)*0.5 # 오차항.\ny = 2.5+4*x + epsilon\ny_true = 2.5+4*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_true,'r--')\n\n\n\n\n\n\n이론적 풀이\n\n풀이1: 스칼라버전\n- 포인트 - \\(S_{xx}=\\), \\(S_{xy}=\\) - \\(\\hat{\\beta}_0=\\), \\(\\hat{\\beta}_1=\\)\n- 풀이\n\nSxx = sum((x-x.mean())**2)\nSxy = sum((x-x.mean())*(y-y.mean()))\n\n\nbeta1_hat = Sxy/Sxx \nbeta1_hat # true: 4.0\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=3.9330345167331697&gt;\n\n\n\nbeta0_hat = y.mean() - x.mean()*beta1_hat\nbeta0_hat # true: 2.5\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867&gt;\n\n\n\n\n풀이2: 벡터버전\n- 포인트 - \\(\\hat{\\beta}=(X'X)^{-1}X'y\\)\n- 풀이\n\ny=y.reshape(N,1)\nX=tf.stack([tf.ones(N,dtype=tf.float64),x],axis=1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\ntf.linalg.inv(X.T @ X ) @ X.T @ y \n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366721],\n       [3.93303452]])&gt;\n\n\n\n\n풀이3: 벡터버전, 손실함수의 도함수이용\n- 포인트\n\n\\(loss'(\\beta)=-2X'y +2X'X\\beta\\)\n\\(\\beta_{new} = \\beta_{old} - \\alpha \\times loss'(\\beta_{old})\\)\n\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tnp.array([-5,10]).reshape(2,1)\nbeta_hat\n\n&lt;tf.Tensor: shape=(2, 1), dtype=int64, numpy=\narray([[-5],\n       [10]])&gt;\n\n\n\n# slope = (-2*X.T @ y + 2*X.T @ X @ beta_hat) / N # SSE\nslope = (-2*X.T @ y + 2*X.T @ X @ beta_hat) / N   # MSE \nslope\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-9.10036894],\n       [-3.52886113]])&gt;\n\n\n\n앞으로는 MSE 버전으로 할 것임. (sample 수가 커질때마다 alpha 값을 조정하기가 너무 귀찮으니까..)\n\n\nalpha= 0.1 \n\n\nstep = slope*alpha\nstep\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-0.91003689],\n       [-0.35288611]])&gt;\n\n\n\nfor epoc in range(1000): \n    slope = (-2*X.T @ y + 2*X.T @ X @ beta_hat)/N \n    beta_hat = beta_hat - alpha* slope\n\n\nbeta_hat\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n\nGradientTape를 이용\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드1: 그레디언트 테입  \nwith tf.GradientTape() as tape: \n    loss = \n## 포인트코드2: 미분 \nslope = tape.gradient(loss,beta_hat) \n## 포인트코드3: update \nbeta_hat.assign_sub(slope*alph) \n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\nalpha=0.1\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= X@beta_hat\n        loss= (y-yhat).T @ (y-yhat) / N\n    slope = tape.gradient(loss,beta_hat) \n    beta_hat.assign_sub(alpha*slope) \n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n위에서 구한것과 같음!\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 미분\nslope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat])\n- 풀이\n\ny=y.reshape(-1) # 길이가 200인 벡터.\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0)\n\n\nalpha=0.1\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= beta0_hat + x*beta1_hat \n        loss= tf.reduce_sum((y-yhat)**2)/N #loss= sum((y-yhat)**2)/N\n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) \n    beta0_hat.assign_sub(alpha*slope0)\n    beta1_hat.assign_sub(alpha*slope1)\n\n\nbeta0_hat,beta1_hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048&gt;)\n\n\n\nloss를 tf.reduce_sum((y-yhat)**2)/N 이렇게 해야 속도가 훨씬 빠름..\nloss= sum((y-yhat)**2)/N 얘와 같지만 속도는 다름!\n\n\n\n\nGradientTape + opt.apply_gradients\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드: 업데이트\nopt.apply_gradients([(slope,beta_hat)])  ## pair의 list가 입력 \n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\n# tf.keras.optimizers       ## tf.optimizers와 동일..\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= X@beta_hat\n        loss= (y-yhat).T @ (y-yhat) / N\n    slope = tape.gradient(loss,beta_hat)\n    opt.apply_gradients([(slope,beta_hat)])\n    #beta_hat.assign_sub(alpha*slope) \n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 업데이트 \nopt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) ## pair의 list가 입력 \n- 풀이\n\ny=y.reshape(-1)\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0)\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000):\n    with tf.GradientTape() as tape: \n        yhat= beta0_hat + beta1_hat*x #X@beta_hat\n        loss= tf.reduce_sum((y-yhat)**2) / N\n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat])\n    opt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)])\n\n\npair의 list [(slope0,beta0_hat),(slope1,beta1_hat)]이런 형태가 필요함.\n미분하는 애들이 여러개 있을 때는 리스트로 전달함으로써 연산을 용이하게 함. (리스트로 전달하는 이유.)\n\n\nbeta0_hat,beta1_hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048&gt;)\n\n\n\n\n\nopt.minimize\n\n풀이1: 벡터버전, 사용자정의 손실함수 with lambda\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\n# 손실함수 정의\nloss_fn = lambda: (y-X@beta_hat).T @ (y-X@beta_hat) / N \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,[beta_hat])\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366721],\n       [3.93303452]])&gt;\n\n\n\n\\(\\hat{\\beta}\\)이 적절히 잘 추론됨.\n\n\n\n풀이2: 스칼라버전, 사용자정의 손실함수 with lambda\n- 포인트\n## 포인트코드: 미분 & 업데이트 = minimize \nopt.minimize(loss_fn,[beta0_hat,beta1_hat])\n- 풀이\n\ny=y.reshape(-1)\ny.shape,x.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0) \n\n\nloss_fn = lambda: tf.reduce_sum((y-beta0_hat-beta1_hat*x )**2) / N \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,[beta0_hat,beta1_hat])\n\n\nbeta0_hat,beta1_hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048&gt;)\n\n\n\n결과는 동일.\n\n\n\n풀이3: 벡터버전, 사용자정의 (짧은) 손실함수\n- 포인트\n## 포인트코드: 손실함수정의 \ndef loss_fn():\n    return ??\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\ndef loss_fn():\n    return (y-X@beta_hat).T @ (y-X@beta_hat) / N \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn, [beta_hat])\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n풀이4: 벡터버전, 사용자정의 (긴) 손실함수\n- 포인트\n## 포인트코드: 손실함수정의 \ndef loss_fn():\n    ??\n    ??\n    return ??\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = (y-yhat).T @ (y-yhat) / N # 컴퓨터한테 전달할 수식 2 \n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 \n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,[beta_hat])\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n풀이5: 벡터버전, 사용자정의 손실함수 &lt;- tf.losses.MSE\n- 포인트\n## 포인트코드: 미리구현되어있는 손실함수 이용 \ntf.losses.MSE(y,yhat)\n- 풀이\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\n# 알아서 MSE 잘 구해줌.\ntf.keras.losses.MSE(tnp.array([0.0,0.0,0.0]),tnp.array([1.0,2.0,3.0]))\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=4.666666666666667&gt;\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = tf.keras.losses.MSE(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2 \n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 \n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,[beta_hat])\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n풀이6: 벡터버전, 사용자정의 손실함수 &lt;- tf.losses.MeaSquaredError\n- 포인트\n## 포인트코드: 클래스로부터 손실함수 오브젝트 생성 (함수를 찍어내는 클래스) \nmse_fn = tf.losses.MeanSquaredError()\nmse_fn(y,yhat)\n- 풀이\n\nmseloss_fn = tf.losses.MeanSquaredError()\n\n\nmseloss_fn = tf.keras.losses.MSE 라고 보면된다.\n\n\nmseloss_fn(tnp.array([0.0,0.0]), tnp.array([1.0,2.0]))\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=2.5&gt;\n\n\n\ndir(mseloss_fn) # callable object.\n\n['__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_allow_sum_over_batch_size',\n '_fn_kwargs',\n '_get_reduction',\n '_keras_api_names',\n '_keras_api_names_v1',\n '_name_scope',\n '_set_name_scope',\n 'call',\n 'fn',\n 'from_config',\n 'get_config',\n 'name',\n 'reduction']\n\n\n\ny=y.reshape(N,1)\ny.shape,X.shape\n\n(TensorShape([200, 1]), TensorShape([200, 2]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])&gt;\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat # 컴퓨터한테 전달할 수식1\n    loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2 \n    return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 \n\n\nalpha=0.1 \nopt = tf.optimizers.SGD(alpha)\n\n\nfor epoc in range(1000): \n    opt.minimize(loss_fn,[beta_hat])\n\n\nbeta_hat\n\n&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])&gt;\n\n\n\n\n\ntf.keras.Sequential\n- \\(\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1x_i\\) 의 서로다른 표현\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\ngv(''' \n    \"1\" -&gt; \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta0_hat\"]\n    \"x\" -&gt; \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta1_hat\"]\n    \"beta0_hat + x*beta1_hat,    bias=False\" -&gt; \"yhat\"[label=\"indentity\"]\n    ''')\n\n\n\n\n\ngv('''\n\"x\" -&gt; \"x*beta1_hat,    bias=True\"[label=\"*beta1_hat\"] ;\n\"x*beta1_hat,    bias=True\" -&gt; \"yhat\"[label=\"indentity\"] ''')\n\n\n\n\n\ngv('''\n\"X=[1 x]\" -&gt; \"X@beta_hat,    bias=False\"[label=\"@beta_hat\"] ;\n\"X@beta_hat,    bias=False\" -&gt; \"yhat\"[label=\"indentity\"] ''')\n\n\n\n\n\n풀이1: 벡터버전, 사용자정의 손실함수\n- 포인트\n## 포인트코드1: 네트워크 생성 \nnet = tf.keras.Sequential()\n\n## 포인트코드2: 네트워크의 아키텍처 설계 \nnet.add(tf.keras.layers.Dense(1,input_shape=(2,),use_bias=False)) \n\n## 포인트코드3: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저\nnet.compile(opt,loss=loss_fn2)\n\n## 포인트코드4: 미분 & update \nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n- 레이어: 입력 -&gt; 레이어 -&gt; 출력\n- 네트워크: 레이어들의 집합\n- 풀이\n\nnet = tf.keras.Sequential() # 아무것도 없음..\n\n\nnetwork 안에 layer를 설계를 해야함.\n\n\nnet.add(tf.keras.layers.Dense(units=1,input_shape=(2,),use_bias=False)) ## yhat을 구하는 방법정의 = 아키텍처가 설계 \n\n\nunits는 layer의 출력의 차원, 이 경우는 yhat의 차원, yhat은 (200,1) 이므로 1임.\ninput_shape는 layer의 입력의 차원, 이 경우는 X의 차원, X는 (200,2) 이므로 2임.\n\n\nnet.summary() # 뭔가 만들어짐.\n\nModel: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n네트워크 안에 레이어가 만들어졌다.\n\n\nnet.weights # 뭔가 weight도 만들어져 있음!\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 1.2421309 ],\n        [-0.71216303]], dtype=float32)&gt;]\n\n\n\ndef loss_fn2(y,yhat):\n    return (y-yhat).T @ (y-yhat) / N \n\n\nalpha=0.1\nopt =tf.optimizers.SGD(alpha)\n\n\n[np.array([[-5.0],[10.0]],dtype=np.float32)]\n\n[array([[-5.],\n        [10.]], dtype=float32)]\n\n\n\nnet.set_weights([np.array([[-5.0],[10.0]],dtype=np.float32)])\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-5.],\n        [10.]], dtype=float32)&gt;]\n\n\n\nnet.compile(opt,loss=tf.losses.MSE)\n# 아키텍처 + 손실함수 + 옵티마이저 =&gt; 네트워크에 다 합치자 =&gt; 네트워크를 컴파일한다. \n\n\nnet.fit(X,y,epochs=1000,batch_size=N,verbose=0) # 미분 + 파라메터업데이트 = net.fit \n\n&lt;keras.callbacks.History at 0x7f7d0bd6cc70&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.58366 ],\n        [3.933048]], dtype=float32)&gt;]"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_11_(6주차)_4월11일.html",
    "href": "posts/3_STBDA2022/2022_04_11_(6주차)_4월11일.html",
    "title": "[STBDA] 6wk. 회귀모형 적합 with keras",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-zueMdNhXiDTIMD-Dz5sbBD\n\n\n\nimports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\n\n\\(x \\to \\hat{y}\\) 가 되는 과정을 그림으로 그리기\n- 단순회귀분석의 예시 - \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i, \\quad i=1,2,\\dots,n\\)\n(표현1)\n\n#collapse\ngv(''' \n    \"1\" -&gt; \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xₙ\" -&gt; \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xₙ*β̂₁,    bias=False\" -&gt; \"ŷₙ\"[label=\"identity\"]\n\n    \".\" -&gt; \"....................................\"[label=\"* β̂₀\"]\n    \"..\" -&gt; \"....................................\"[label=\"* β̂₁\"]\n    \"....................................\" -&gt; \"...\"[label=\" \"]\n\n    \"1 \" -&gt; \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₂\" -&gt; \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₂*β̂₁,    bias=False\" -&gt; \"ŷ₂\"[label=\"identity\"]\n    \n    \"1  \" -&gt; \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₁\" -&gt; \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₁*β̂₁,    bias=False\" -&gt; \"ŷ₁\"[label=\"identity\"]\n''')\n\n\n\n\n- 표현1의 소감? - 교수님이 고생해서 만든것 같음 - 그런데 그냥 다 똑같은 그림의 반복이라 사실 고생한 의미가 없음.\n(표현2)\n- 그냥 아래와 같이 그리고 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다”고 하면 될것 같다.\n\n#collapse\ngv(''' \n    \"1\" -&gt; \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xᵢ\" -&gt; \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xᵢ*β̂₁,    bias=False\" -&gt; \"ŷᵢ\"[label=\"identity\"]\n\n''')\n\n\n\n\n(표현3)\n- 그런데 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다” 라는 언급자체도 반복할 필요가 없을 것 같다. (어차피 당연히 그럴테니까) 그래서 단순히 아래와 같이 그려도 무방할듯 하다.\n\ngv(''' \n    \"1\" -&gt; \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x\" -&gt; \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x*β̂₁,    bias=False\" -&gt; \"ŷ\"[label=\"identity\"]\n\n''')\n\n\n\n\n(표현4)\n- 위의 모델은 아래와 같이 쓸 수 있다. (\\(\\beta_0\\)를 바이어스로 표현)\n\n#collapse\ngv('''\n\"x\" -&gt; \"x*β̂₁,    bias=True\"[label=\"*β̂₁\"] ;\n\"x*β̂₁,    bias=True\" -&gt; \"ŷ\"[label=\"indentity\"] ''')\n\n\n\n\n\n실제로는 이 표현을 많이 사용함\n\n(표현5)\n- 벡터버전으로 표현하면 아래와 같다. 이 경우에는 \\({\\bf X}=[1,x]\\)에 포함된 1이 bias의 역할을 해주므로 bias = False 임.\n\n#collapse\ngv('''\n\"X\" -&gt; \"X@β̂,    bias=False\"[label=\"@β̂\"] ;\n\"X@β̂,    bias=False\" -&gt; \"ŷ\"[label=\"indentity\"] ''')\n\n\n\n\n\n저는 이걸 좋아해요\n\n(표현5)’\n- 딥러닝에서는 \\(\\hat{\\boldsymbol{\\beta}}\\) 대신에 \\(\\hat{{\\bf W}}\\)을 라고 표현한다.\n\n#collapse\ngv('''\n\"X\" -&gt; \"X@Ŵ,    bias=False\"[label=\"@Ŵ\"] ;\n\"X@Ŵ,    bias=False\" -&gt; \"ŷ\"[label=\"identity\"] ''')\n\n\n\n\n- 실제로는 표현4 혹은 표현5를 외우면 된다.\n\n\nLayer의 개념\n- (표현4) 혹은 (표현5)의 그림은 레이어로 설명할 수 있다.\n- 레이어는 항상 아래와 같은 규칙을 가진다. - 첫 동그라미는 레이어의 입력이다. - 첫번째 화살표는 선형변환을 의미한다. - 두번째 동그라미는 선형변환의 결과이다. (이때 bias가 false인지 true인지에 따라서 실제 수식이 조금 다름) - 두번째 화살표는 두번째 동그라미에 어떠한 함수 \\(f\\)를 취하는 과정을 의미한다. (우리의 그림에서는 \\(f(x)=x\\)) - 세번째 동그라미는 레이어의 최종출력이다.\n- 엄청 복잡한데, 결국 레이어를 만들때 위의 그림들을 의미하도록 하려면 아래의 4개의 요소만 필요하다. 1. 레이어의 입력차원 2. 선형변환의 결과로 얻어지는 차원 3. 선형변환에서 바이어스를 쓸지? 안쓸지? 4. 함수 \\(f\\)\n- 주목: 1,2가 결정되면 자동으로 \\(\\hat{{\\bf W}}\\)의 차원이 결정된다.\n(예시) - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=1: \\(\\hat{\\bf W}\\)는 (2,1) 매트릭스 - 레이어의 입력차원=20, 선형변환의 결과로 얻어지는 차원=5: \\(\\hat{\\bf W}\\)는 (20,5) 매트릭스 - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=50: \\(\\hat{\\bf W}\\)는 (2,50) 매트릭스\n- 주목2: 이중에서 절대 생략불가능 것은 “2. 선형변환의 결과로 얻어지는 차원” 이다. - 레이어의 입력차원: 실제 레이어에 데이터가 들어올 때 데이터의 입력차원을 컴퓨터 스스로 체크하여 \\(\\hat{\\bf W}\\)의 차원을 결정할 수 있음. - 바이어스를 쓸지? 안쓸지? 기본적으로 쓴다고 가정한다. - 함수 \\(f\\): 기본적으로 항등함수를 가정하면 된다.\n\n\nKeras를 이용한 풀이\n- 기본뼈대: net생성 \\(\\to\\) add(layer) \\(\\to\\) compile(opt,loss) \\(\\to\\) fit(data,epochs)\n- 데이터정리\n\\[{\\bf y}\\approx 2.5 +4*x\\]\n\ntnp.random.seed(43052)\nN= 200 \nx= tnp.linspace(0,1,N)\nepsilon= tnp.random.randn(N)*0.5 \ny= 2.5+4*x +epsilon\n\n\nX=tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\n\n\n풀이1: 스칼라버전\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1)  \n# 입력차원? 데이터를 넣어보고 결정, 바이어스=디폴드값을 쓰겠음 (use_bias=true), 함수도 디폴트값을 쓰겠음 (f(x)=x)\nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) \n\n\n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 \n\n&lt;keras.callbacks.History at 0x7fb265125b20&gt;\n\n\n\nbatch_size=N : 경사하강법\nbatch_size!=N : 확률적 경사하강법\n\n(결과확인)\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.9330256]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([2.583672], dtype=float32)&gt;]\n\n\n\n스칼라버전 끝!\n\n\n\n풀이2: 벡터버전\n(0단계) 데이터정리\n\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False) ## 주의!  use_bias=False\nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) \n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 \n\n&lt;keras.callbacks.History at 0x7fb2650ead00&gt;\n\n\n(결과확인)\n\nnet.weights\n\n[&lt;tf.Variable 'dense_1/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)&gt;]\n\n\n\n\n\n\n\n\n잠시문법정리\n\n\n\n- 잠깐 Dense layer를 만드는 코드를 정리해보자.\n\n아래는 모두 같은 코드이다.\n\n\ntf.keras.layers.Dense(1)\ntf.keras.layers.Dense(units=1)\ntf.keras.layers.Dense(units=1,activation=‘linear’) // identity 가 더 맞는것 같은데..\ntf.keras.layers.Dense(units=1,activation=‘linear’,use_bias=True)\n\n\n아래의 코드1,2는 (1)의 코드들과 살짝 다른코드이다. (코드1과 코드2는 같은코드임)\n\n\ntf.keras.layers.Dense(1,input_dim=2) # 코드1\ntf.keras.layers.Dense(1,input_shape=(2,)) # 코드2\n\n\n아래는 사용불가능한 코드이다.\n\n\ntf.keras.layers.Dense(1,input_dim=(2,)) # 코드1\ntf.keras.layers.Dense(1,input_shape=2) # 코드2\n\n\n\n- 왜 input_dim이 필요한가?\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Dense(1,use_bias=False)) \n\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Dense(1,use_bias=False,input_dim=2))\n\n\nnet1.weights\n\nValueError: Weights for model 'sequential_3' have not yet been created. Weights are created when the model is first called on inputs or `build()` is called with an `input_shape`.\n\n\n\nnet2.weights\n\n[&lt;tf.Variable 'dense_4/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-1.4113412],\n        [-1.3562037]], dtype=float32)&gt;]\n\n\n\nnet1.summary()\n\nValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.\n\n\n\nnet2.summary()\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_4 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n풀이3: 스칼라버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,input_dim=1)\n\n\nnet.add(layer)\n\n\nnet.get_weights()\n\n[array([[-0.80770403]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nweight, bias 순으로 출력\n\n\n초기값을 설정\n\nnet.weights\n\n[&lt;tf.Variable 'dense_8/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-0.80770403]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_8/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]\n\n\n\nnet.get_weights()\n\n[array([[-0.80770403]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nweight, bias순으로 출력\n\n\nnet.set_weights?\n\n\nSignature: net.set_weights(weights)\nDocstring:\nSets the weights of the layer, from NumPy arrays.\nThe weights of a layer represent the state of the layer. This function\nsets the weight values from numpy arrays. The weight values should be\npassed in the order they are created by the layer. Note that the layer's\nweights must be instantiated before calling this function, by calling\nthe layer.\nFor example, a `Dense` layer returns a list of two values: the kernel\nmatrix and the bias vector. These can be used to set the weights of\nanother `Dense` layer:\n&gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1,\n...   kernel_initializer=tf.constant_initializer(1.))\n&gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n&gt;&gt;&gt; layer_a.get_weights()\n[array([[1.],\n       [1.],\n       [1.]], dtype=float32), array([0.], dtype=float32)]\n&gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1,\n...   kernel_initializer=tf.constant_initializer(2.))\n&gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n&gt;&gt;&gt; layer_b.get_weights()\n[array([[2.],\n       [2.],\n       [2.]], dtype=float32), array([0.], dtype=float32)]\n&gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights())\n&gt;&gt;&gt; layer_b.get_weights()\n[array([[1.],\n       [1.],\n       [1.]], dtype=float32), array([0.], dtype=float32)]\nArgs:\n  weights: a list of NumPy arrays. The number\n    of arrays and their shape must match\n    number of the dimensions of the weights\n    of the layer (i.e. it should match the\n    output of `get_weights`).\nRaises:\n  ValueError: If the provided weights list does not match the\n    layer's specifications.\nFile:      ~/anaconda3/envs/torch/lib/python3.8/site-packages/keras/engine/base_layer.py\nType:      method\n\n\n\n\nlayer_b.set_weights(layer_a.get_weights()) 와 같은방식으로 쓴다는 것이군?\n\n- 한번따라해보자.\n\n_w = net.get_weights()\n_w\n\n[array([[-0.80770403]], dtype=float32), array([0.], dtype=float32)]\n\n\n\n길이가 2인 리스트이고, 각 원소는 numpy array 임\n\n\nnet.set_weights(\n    [np.array([[10.0]],dtype=np.float32), # weight, β1_hat\n     np.array([-5.0],dtype=np.float32)] # bias, β0_hat \n)\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_8/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[10.]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_8/bias:0' shape=(1,) dtype=float32, numpy=array([-5.], dtype=float32)&gt;]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1),tf.losses.MSE) \n\n(4단계) net.fit()\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7fb26550b130&gt;\n\n\n결과확인\n\nnet.weights\n\n[&lt;tf.Variable 'dense_8/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.933048]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_8/bias:0' shape=(1,) dtype=float32, numpy=array([2.58366], dtype=float32)&gt;]\n\n\n\n\n풀이4: 벡터버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False,input_dim=2) \n\n\nnet.add(layer)\n\n\nnet.summary()\n\nModel: \"sequential_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_9 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n초기값을 설정하자\n\nnet.set_weights([np.array([[-5.0],[10.0]], dtype=np.float32)])\n\n\nnet.get_weights()\n\n[array([[-5.],\n        [10.]], dtype=float32)]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.losses.MSE) # optimizer, loss 전달하여 compile \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) # verbose=0: 귀찮은 메시지X, batch_size=N: 경사하강법\n\n&lt;keras.callbacks.History at 0x7fb28c5bf550&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_9/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.58366 ],\n        [3.933048]], dtype=float32)&gt;]\n\n\n\n똑같이 잘 나옴.\n\n- 사실 실전에서는 초기값을 설정할 필요가 별로 없음.\n\n\n풀이5: 벡터버전 사용자정의 손실함수\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False)  # 출력1, bias사용X\n\n\nnet.add(layer)\n\n(3단계) net.compile()\n\nloss_fn = lambda y,yhat: (y-yhat).T @ (y-yhat) / N\n\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss_fn)\n\n\n사용자가 직접 loss function을 정의해서 컴파일해도 상관없다.\n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7fb26591f4c0&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_11/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)&gt;]\n\n\n\n잘 수렴한다…\n\n\n\n풀이6: 벡터버전, net.compile의 옵션으로 손실함수 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss='mse')  # 알아서 내장되어있는 mse 찾아서 컴파일해줌. \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7fb265183d60&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_13/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836723],\n        [3.9330251]], dtype=float32)&gt;]\n\n\n\n\n풀이7: 벡터버전, net.compile의 옵션으로 손실함수 지정 + 옵티마이저 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(optimizer='sgd', loss='mse') \n#net.optimizer.lr = tf.Variable(0.1,dtype=tf.float32)\n#net.optimizer.lr = 0.1\n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=5000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7fb2645af7c0&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_15/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5849245],\n        [3.9306912]], dtype=float32)&gt;]\n\n\n\n아까보다 에폭을 좀 더 늘리면 \\(2.58, 3.93\\)으로 수렴!\n\n\n\n\n여러가지 회귀모형의 적합과 학습과정의 모니터링\n\n예제1\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 x_i\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.random.randn(N) \nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*x +epsilon\n\n\nX= np.stack([np.ones(N),x],axis=1)\ny= y.reshape(N,1)\n\n\nplt.plot(x,y,'o') # 관측한 자료 \n\n\n\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\n\n\nyhat = X@beta_hat \n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-') \n\n\n\n\n더 좋은 적합선을 얻기위해서!\n\n\\(loss'(\\beta)=-2X'y +2X'X\\beta\\)\n\n\nslope = (2*X.T@X@beta_hat - 2*X.T@y)/ N \nbeta_hat2 = beta_hat - 0.1*slope  \nyhat2 = X@beta_hat2\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-') \nplt.plot(x,yhat2.reshape(-1),'-') \n\n\n\n\n초록색이 좀 더 나아보인다.\n\nbeta_hat = np.array([-3,-2]).reshape(2,1) \nbeta_hats = beta_hat # beta_hats = beta_hat.copy() 가 더 안전한 코드입니다. \nfor i in range(1,30):\n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat - 2*X.T@y) / N \n    beta_hat = beta_hat - 0.1*slope # 0.1은 적당, 0.3은 쪼금빠르지만 그래도 적당, 0.9는 너무 나간것같음, 1.0 은 수렴안함, 1.2 \n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) \n\n\n좀 안정적인 건 \\(0.1\\)\n\\(0.3, 0.9\\)는 수렴은 하는데 좀 더 안전한건 \\(0.1\\)\n\\(1\\sim\\) 는 수렴하는척하면서 터짐…\n\n\nbeta_hats\n\narray([[-3.        , -1.98776175, -1.16054651, -0.48448286,  0.06808892,\n         0.51975837,  0.88897604,  1.19081334,  1.43758253,  1.63934274,\n         1.80431301,  1.93920945,  2.04952049,  2.13973166,  2.2135091 ,\n         2.27384947,  2.32320238,  2.36357035,  2.39659058,  2.42360161,\n         2.44569792,  2.46377445,  2.47856304,  2.49066214,  2.50056123,\n         2.5086606 ,  2.51528766,  2.52071021,  2.52514732,  2.52877816],\n       [-2.        , -0.929175  , -0.05089843,  0.66940348,  1.26010705,\n         1.74449975,  2.14169103,  2.46736052,  2.73437247,  2.95328049,\n         3.13274182,  3.27985761,  3.40045219,  3.4993023 ,  3.58032529,\n         3.64673351,  3.70116104,  3.74576767,  3.78232418,  3.81228235,\n         3.83683236,  3.85694989,  3.87343472,  3.88694243,  3.89801039,\n         3.90707901,  3.91450928,  3.92059703,  3.92558472,  3.92967104]])\n\n\n\nbeta_hats[0]\n\narray([-3.        , -1.98776175, -1.16054651, -0.48448286,  0.06808892,\n        0.51975837,  0.88897604,  1.19081334,  1.43758253,  1.63934274,\n        1.80431301,  1.93920945,  2.04952049,  2.13973166,  2.2135091 ,\n        2.27384947,  2.32320238,  2.36357035,  2.39659058,  2.42360161,\n        2.44569792,  2.46377445,  2.47856304,  2.49066214,  2.50056123,\n        2.5086606 ,  2.51528766,  2.52071021,  2.52514732,  2.52877816])\n\n\n\nb0hats = beta_hats[0].tolist()\nb1hats = beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.5451404 ],\n       [3.94818596]])\n\n\n\nfrom matplotlib import animation \nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n&lt;Figure size 864x360 with 0 Axes&gt;\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 (data, 적합된 직선)\nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*x) \n# ax2: 오른쪽그림 (loss function)\nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*x)**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02) \nax2.scatter(2.5451404,3.94818596,loss_fn(2.5451404,3.94818596),s=200,marker='*') # loss값이 제일 작은 지점. \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*x) \n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 참고\n\nβ0,β1 = np.meshgrid(np.array([0,1,2]), np.array([4,5,6]), indexing='ij')\nβ0 = β0.reshape(-1)\nβ1 = β1.reshape(-1)\n\n\nβ0, β1\n\n(array([0, 0, 0, 1, 1, 1, 2, 2, 2]), array([4, 5, 6, 4, 5, 6, 4, 5, 6]))\n\n\n\n_a=list(map(lambda x,y: x*y, β0, β1))\n\n\n_a\n\n[0, 0, 0, 4, 5, 6, 8, 10, 12]\n\n\n\n\n예제2\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX= np.stack([np.ones(N),np.exp(-x)],axis=1)\ny= y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\nbeta_hats = beta_hat.copy() # shallow copy, deep copy &lt;--- 여름 방학 특강 \nfor i in range(1,30): \n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat - 2*X.T@y) /N \n    beta_hat = beta_hat - 0.05*slope\n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) \n\n\nbeta_hats\n\narray([[-3.        , -1.74671631, -0.82428979, -0.14453919,  0.35720029,\n         0.72834869,  1.0036803 ,  1.20869624,  1.36209751,  1.47759851,\n         1.56525696,  1.63244908,  1.68458472,  1.72563174,  1.75850062,\n         1.78532638,  1.80767543,  1.82669717,  1.84323521,  1.85790889,\n         1.8711731 ,  1.88336212,  1.89472176,  1.90543297,  1.91562909,\n         1.92540859,  1.93484428,  1.94399023,  1.9528867 ,  1.96156382],\n       [-2.        , -0.25663415,  1.01939241,  1.95275596,  2.63488171,\n         3.13281171,  3.49570765,  3.75961951,  3.95098231,  4.08918044,\n         4.18842797,  4.2591476 ,  4.30898175,  4.34353413,  4.36691339,\n         4.38213187,  4.39139801,  4.39633075,  4.39811673,  4.3976256 ,\n         4.3954946 ,  4.3921905 ,  4.38805511,  4.3833386 ,  4.37822393,\n         4.37284482,  4.36729887,  4.36165718,  4.35597148,  4.35027923]])\n\n\n\nb0hats= beta_hats[0].tolist()\nb1hats= beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X)@X.T@y\n\narray([2.46307644, 3.99681332])\n\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n&lt;Figure size 864x360 with 0 Axes&gt;\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x))\n# ax2: 오른쪽그림 \nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02) \nax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x))\n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n예제3\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) \ny=y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2,-1]).reshape(3,1) \nbeta_hats = beta_hat.copy()\nfor i in range(1,30):\n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat -2*X.T@y) /N \n    beta_hat = beta_hat - 0.1 * slope \n    beta_hats= np.concatenate([beta_hats,beta_hat],axis=1)\n\n\nbeta_hats\n\narray([[-3.        , -0.71767532,  0.36255782,  0.89072137,  1.16423101,\n         1.31925078,  1.41819551,  1.48974454,  1.54713983,  1.59655416,\n         1.64091846,  1.68167278,  1.71956758,  1.75503084,  1.78833646,\n         1.81968188,  1.84922398,  1.877096  ,  1.90341567,  1.92828934,\n         1.95181415,  1.97407943,  1.99516755,  2.01515463,  2.0341111 ,\n         2.05210214,  2.06918818,  2.08542523,  2.10086524,  2.11555643],\n       [-2.        ,  1.16947474,  2.64116513,  3.33411605,  3.66880042,\n         3.83768856,  3.92897389,  3.98315095,  4.01888831,  4.04486085,\n         4.06516144,  4.08177665,  4.09571971,  4.10754954,  4.1176088 ,\n         4.12613352,  4.13330391,  4.13926816,  4.14415391,  4.14807403,\n         4.15112966,  4.1534121 ,  4.15500404,  4.15598045,  4.15640936,\n         4.15635249,  4.15586584,  4.15500014,  4.15380139,  4.1523112 ],\n       [-1.        , -0.95492718, -0.66119313, -0.27681968,  0.12788212,\n         0.52254445,  0.89491388,  1.24088224,  1.55993978,  1.85310654,\n         2.12199631,  2.36839745,  2.59408948,  2.8007666 ,  2.99000967,\n         3.16327964,  3.32192026,  3.46716468,  3.60014318,  3.72189116,\n         3.83335689,  3.93540864,  4.02884144,  4.11438316,  4.19270026,\n         4.26440288,  4.33004965,  4.39015202,  4.44517824,  4.49555703]])\n\n\n\nb0hats,b1hats,b2hats = beta_hats # unpacking\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.46597526],\n       [4.00095138],\n       [5.04161877]])\n\n\n\ntrue가 잘 찾아짐.\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n&lt;Figure size 864x360 with 0 Axes&gt;\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x) + b2hats[0]*np.cos(5*x))\n# ax2: 오른쪽그림 \n# β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\n# β0=β0.reshape(-1)\n# β1=β1.reshape(-1)\n# loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\n# loss = list(map(loss_fn, β0,β1))\n# ax2.scatter(β0,β1,loss,alpha=0.02) \n# ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x) + b2hats[i]*np.cos(5*x))\n    # ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n예제3: 케라스로 해보자!\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) \ny=y.reshape(N,1)\n\n\nnet = tf.keras.Sequential() # 1: 네트워크 생성\nnet.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer \nnet.compile(tf.optimizers.SGD(0.1), loss='mse') # 3: compile\nnet.fit(X,y,epochs=30, batch_size=N) # 4: fit \n\nEpoch 1/30\n1/1 [==============================] - 0s 77ms/step - loss: 45.8620\nEpoch 2/30\n1/1 [==============================] - 0s 1ms/step - loss: 20.5386\nEpoch 3/30\n1/1 [==============================] - 0s 1ms/step - loss: 13.5033\nEpoch 4/30\n1/1 [==============================] - 0s 1ms/step - loss: 10.5736\nEpoch 5/30\n1/1 [==============================] - 0s 1ms/step - loss: 8.7382\nEpoch 6/30\n1/1 [==============================] - 0s 1ms/step - loss: 7.3295\nEpoch 7/30\n1/1 [==============================] - 0s 1ms/step - loss: 6.1758\nEpoch 8/30\n1/1 [==============================] - 0s 1ms/step - loss: 5.2144\nEpoch 9/30\n1/1 [==============================] - 0s 1ms/step - loss: 4.4098\nEpoch 10/30\n1/1 [==============================] - 0s 1ms/step - loss: 3.7355\nEpoch 11/30\n1/1 [==============================] - 0s 1ms/step - loss: 3.1704\nEpoch 12/30\n1/1 [==============================] - 0s 1ms/step - loss: 2.6967\nEpoch 13/30\n1/1 [==============================] - 0s 1ms/step - loss: 2.2996\nEpoch 14/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.9667\nEpoch 15/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.6877\nEpoch 16/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.4539\nEpoch 17/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.2578\nEpoch 18/30\n1/1 [==============================] - 0s 1ms/step - loss: 1.0935\nEpoch 19/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.9557\nEpoch 20/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.8403\nEpoch 21/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.7435\nEpoch 22/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.6623\nEpoch 23/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.5943\nEpoch 24/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.5373\nEpoch 25/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.4895\nEpoch 26/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.4494\nEpoch 27/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.4158\nEpoch 28/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3877\nEpoch 29/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3641\nEpoch 30/30\n1/1 [==============================] - 0s 1ms/step - loss: 0.3443\n\n\n&lt;keras.callbacks.History at 0x7fb265509dc0&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_16/kernel:0' shape=(3, 1) dtype=float32, numpy=\n array([[2.4614503],\n        [3.925311 ],\n        [4.5953326]], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'o') \nplt.plot(x,(X@net.weights).reshape(-1),'--')\n\n\n\n\n\n\n\n숙제\n\n예제2: 케라스를 이용하여 아래를 만족하는 적절한 \\(\\beta_0\\)와 \\(\\beta_1\\)을 구하라. 적합결과를 시각화하라. (애니메이션 시각화 X)\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon\n\n\nX = np.stack([np.ones(N), np.exp(-x)],axis=1)\ny = y.reshape(N,1)\n\n\nX.shape, y.shape\n\n((100, 2), (100, 1))\n\n\n\nnet = tf.keras.Sequential() # 1. 네트워크 생성\nnet.add(tf.keras.layers.Dense(1,use_bias=False)) # 2. add layer\nnet.compile(tf.optimizers.SGD(0.1), loss='mse') # 3. compile\nnet.fit(X,y,epochs=100, batch_size=N, verbose=0) # 4. fit\n\n&lt;keras.callbacks.History at 0x7fb1ef048940&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_21/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5061626],\n        [3.966341 ]], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,(X@net.weights).reshape(-1),'--')"
  },
  {
    "objectID": "posts/3_STBDA2022/2023-01-01.html",
    "href": "posts/3_STBDA2022/2023-01-01.html",
    "title": "Jupyter",
    "section": "",
    "text": "ref: STBDA 10주차 (2번째 동영상)\n- 파이썬 / 주피터 설치\nconda create -n test python=3.9\n\nconda env list\n\nconda activate test\n\n# conda라는 앱스토어에서 jupyterlab을 깐다. (conda-forge라는 커뮤니티에서 검증이 된 (추천목록) 패키지들을 깔겠다..)\n# 여러 솔루션 중 하나\nconda install -c conda-forge jupyterlab \n\njupyter lab\n\n주피터 설치 완료\n\n- 주피터에서 슬라이드 쇼\n\n## test 환경 활성화\nconda activte test\n\n## jupyter notebook slide show\nconda install -c conda-forge rise\njupyter notebook"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_03_28_(4주차)_3월28일.html",
    "href": "posts/3_STBDA2022/2022_03_28_(4주차)_3월28일.html",
    "title": "[STBDA] 4wk. 미분 / 경사하강법",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-xGxAT-3Sq_jpD-eBxpsT8L"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_03_28_(4주차)_3월28일.html#경사하강법",
    "href": "posts/3_STBDA2022/2022_03_28_(4주차)_3월28일.html#경사하강법",
    "title": "[STBDA] 4wk. 미분 / 경사하강법",
    "section": "경사하강법",
    "text": "경사하강법\n\n최적화문제\n- \\(loss=(\\frac{1}{2}\\beta-1)^2\\)를 최소하는 \\(\\beta\\)를 컴퓨터를 활용하여 구하는 문제를 생각해보자. - 답은 \\(\\beta=2\\)임을 알고 있다.\n\n\n방법1: grid search\n\n알고리즘\n\nbeta = [-10.00,-9.99,…,10.00] 와 같은 리스트를 만든다.\n(1)의 리스트의 각원소에 해당하는 loss를 구한다.\n(2)에서 구한 loss를 제일 작게 만드는 beta를 찾는다.\n\n\n\n구현코드\n\nbeta = np.linspace(-10,10,100) \nloss = (beta/2 -1)**2 \n\n\n# argmin()\ntnp.argmin([1,2,-3,3,4]) # 가장작은 인덱스 리턴\n\n&lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt;\n\n\n\ntnp.argmin([1,2,3,-3,4])\n\n&lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt;\n\n\n\ntnp.argmin(loss)\n\n&lt;tf.Tensor: shape=(), dtype=int64, numpy=59&gt;\n\n\n\nbeta[59]\n\n1.9191919191919187\n\n\n\n\n그리드서치의 문제점\n- 비판1: [-10,10]이외에 해가 존재하면? - 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 - 하지만 임의의 고정된 \\(x,y\\)에 대하여 \\(loss(\\beta)=(x\\beta-y)^2\\) 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 - 해결책: 더 넓게 많은 범위를 탐색하자?\n- 비판2: 효율적이지 않음 - 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 - \\(\\to\\) 생각해보니까 \\(\\beta=2\\)인 순간 \\(loss=(\\frac{1}{2}\\beta-1)^2=0\\)이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) - \\(\\to\\) 따라서 \\(\\beta=2\\) 이후로는 탐색할 필요가 없다\n\n\n\n방법2: gradient descent\n\\(loss=(\\frac{1}{2}\\beta-1)^2\\)\n\n알고리즘!\n\nbeta = -5 로 셋팅한다.\n\n\n(-5/2-1)**2 # 아무거나 던지면 됨..\n\n12.25\n\n\n\nbeta=-5 근처에서 조금씩 이동하여 loss를 조사해본다.\n\n\n(-4.99/2-1)**2 ## 오른쪽으로 0.01 이동하고 loss조사\n\n12.215025\n\n\n\n(-5.01/2-1)**2 ## 왼쪽으로 0.01 이동하고 loss조사\n\n12.285025\n\n\n\n(2)의 결과를 잘 해석하고 더 유리한 쪽으로 이동\n위의 과정을 반복하고 왼쪽, 오른쪽 어느쪽으로 움직여도 이득이 없다면 멈춘다.\n\n\n\n알고리즘 분석\n- (2)-(3)의 과정은 beta=-5 에서 미분계수를 구하고 미분계수가 양수이면 왼쪽으로 움직이고 음수이면 오른쪽으로 움직인다고 해석가능. 아래그림을 보면 더 잘 이해가 된다.\n\nplt.plot(beta,loss)\n\n\n\n\n\n\n왼쪽/오른쪽중에 어디로 갈지 어떻게 판단하는 과정을 수식화?\n- 아래와 같이 해석가능\n\n오른쪽으로 0.01 간다 = beta_old에 0.01을 더함. (if, 미분계수가 음수)\n왼쪽으로 0.01 간다. = beta_old에 0.01을 뺀다. (if, 미분계수가 양수)\n\n- 그렇다면 $_{new} =\n\\[\\begin{cases}\n\\beta_{old} + 0.01, & loss'(\\beta_{old})&lt; 0  \\\\\n\\beta_{old} - 0.01, & loss'(\\beta_{old})&gt; 0\n\\end{cases}\\]\n$\n\n\n혹시 알고리즘을 좀 개선할수 있을까?\n- 항상 0.01씩 움직여야 하는가?\n\nplt.plot(beta,loss)\n\n\n\n\n- \\(\\beta=-10\\) 일 경우의 접선의 기울기? \\(\\beta=-4\\) 일때 접선의 기울기?\n\n\\(\\beta=-10\\) =&gt; 기울기는 -6\n\\(\\beta=-4\\) =&gt; 기울기는 -3\n\n\n(-10/2-1), (-4/2-1)\n\n(-6.0, -3.0)\n\n\n- 실제로 6,3씩 이동할순 없으니 적당한 \\(\\alpha\\) (예를들면 \\(\\alpha=0.01\\)) 를 잡아서 곱한만큼 이동하자.\n- 수식화하면\n\n\\(\\beta_{new} = \\beta_{old} - \\alpha~ loss'(\\beta_{old})\\)\n\\(\\beta_{new} = \\beta_{old} - \\alpha~ \\left[\\frac{\\partial}{\\partial \\beta }loss(\\beta)\\right]_{\\beta=\\beta_{old}}\\)\n\n- \\(\\alpha\\)의 의미 - \\(\\alpha\\)가 크면 크게크게 움직이고 작으면 작게작게 움직인다. - \\(\\alpha&gt;0\\) 이어야 한다.\n\n\n구현코드\n- iter 1\n\\(\\beta=-10\\)이라고 하자.\n\nbeta = tf.Variable(-10.0) \n\n\nwith tf.GradientTape(persistent=True) as tape: \n    loss = (beta/2-1)**2 \n\n\ntape.gradient(loss,beta)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt;\n\n\n\\(\\beta = -10\\) 에서 0.01만큼 움직이고 싶음\n\nalpha= 0.01/6\n\n\nalpha * tape.gradient(loss,beta)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.01&gt;\n\n\n\nbeta.assign_sub(alpha * tape.gradient(loss,beta))\n\n&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n\n한번 더 움직이자.\n\n- iter2\n\nwith tf.GradientTape(persistent=True) as tape: \n    loss = (beta/2-1)**2 \n\n\nbeta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.980008&gt;\n\n\n\n더 나은 \\(\\beta\\)가 되었음! 이렇게 가다보면 \\(2\\)까지 잘 갈 수 있을 것 같다!\n\n- for 문을 이용하자.\n(강의용)\n\nbeta = tf.Variable(-10.0) \n\n\nfor k in range(10000): \n    with tf.GradientTape(persistent=True) as tape: \n        loss = (beta/2-1)**2 \n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251&gt;\n\n\n\n잘 찾았네!\n\n(시도1) beta 다시 초기화 해서 iter 100번 돌려보자.\n\nbeta = tf.Variable(-10.0) \n\n\nfor k in range(100): \n    with tf.GradientTape(persistent=True) as tape: \n        loss = (beta/2-1)**2 \n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-8.157076&gt;\n\n\n(시도2) beta 다시 초기화 해서 iter 1000번 돌려보자.\n\nbeta = tf.Variable(-10.0)  # 초기화\n\n\nfor k in range(1000): \n    with tf.GradientTape(persistent=True) as tape: \n        loss = (beta/2-1)**2 \n    beta.assign_sub(tape.gradient(loss,beta)*alpha)\n\n\nbeta\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-3.2133684&gt;\n\n\n\n10000번 돌렸을 때는 답과 근사했는데 iter 수를 줄였더니 잘 못맞추네… 그럼 이건 알고리즘상 문제가 아니라 보폭이 너무 작아서..느린 것 뿐..\n\n- 너무 느린 것 같다? \\(\\to\\) \\(\\alpha\\)를 키워보자!\n\n\n학습률\n- 목표: \\(\\alpha\\)에 따라서 수렴과정이 어떻게 달라지는 시각화해보자.\n\n[시각화 코드 예비학습]\n\nfig = plt.figure() # 도화지가 만들어지고 fig라는 이름을 붙인다. \n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nax = fig.add_subplot() # fig는 ax라는 물체를 만든다. \n\n\nid(fig.axes[0])\n\n140572441475776\n\n\n\nid(ax)\n\n140572441475776\n\n\n\npnts, = ax.plot([1,2,3],[4,5,6],'or')\npnts\n\n&lt;matplotlib.lines.Line2D at 0x7fd9926c52e0&gt;\n\n\n\npnts.get_xdata()\n\narray([1, 2, 3])\n\n\n\npnts.get_ydata()\n\narray([4, 5, 6])\n\n\n\nfig\n\n\n\n\n\npnts.set_ydata([5,5,5])\n\n\npnts.get_ydata() # 값이 수정됨!\n\n[5, 5, 5]\n\n\n\nfig\n\n\n\n\n- 응용\n\nplt.rcParams[\"animation.html\"]=\"jshtml\"\nfrom matplotlib import animation \n\n\ndef animate(i): \n    if i%2 == 0: \n        pnts.set_ydata([4,5,6]) # 4,5,6으로 셋팅\n    else: \n        pnts.set_ydata([5,5,5]) # 5,5,5로 셋팅\n\n\nani = animation.FuncAnimation(fig,animate,frames=10)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n예비학습 끝\n- beta_lst=[-10,-9,-8] 로 이동한다고 하자.\n\nbeta_lst = [-10,-9,-8]\nloss_lst = [(-10/2-1)**2,(-9/2-1)**2,(-8/2-1)**2] \n\n\nfig = plt.figure() \n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nax= fig.add_subplot()\n\n\n_beta = np.linspace(-15,19,100) \n\n\nax.plot(_beta,(_beta/2-1)**2)\n\n\nfig\n\n\n\n\n\npnts, = ax.plot(beta_lst[0],loss_lst[0],'ro')\nfig\n\n\n\n\n\ndef animate(i): \n    pnts.set_xdata(beta_lst[:(i+1)])\n    pnts.set_ydata(loss_lst[:(i+1)])\n\n\nani =animation.FuncAnimation(fig, animate, frames=3)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 최종아웃풋\n\nbeta = tf.Variable(-10.0) \nalpha = 0.01/6\n\n\nbeta_lst=[]\nloss_lst=[]\n\n\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nwith tf.GradientTape(persistent=True) as tape: \n    tape.watch(beta)  # beta에 대해 미분.\n    loss = (beta/2-1)**2\n\n\nbeta.assign_sub(tape.gradient(loss,beta)*alpha) \n\n&lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99&gt;\n\n\n\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nbeta_lst, loss_lst\n\n([-10.0, -9.99], [36.0, 35.94002362785341])\n\n\n\nbeta값과 그에 대응되는 loss값\n\n- for\n\nbeta = tf.Variable(-10.0) \nalpha = 0.01/6\nbeta_lst=[]\nloss_lst=[]\nbeta_lst.append(beta.numpy())\nloss_lst.append((beta.numpy()/2-1)**2)\nfor k in range(100): \n    with tf.GradientTape(persistent=True) as tape: \n        tape.watch(beta) \n        loss = (beta/2-1)**2\n    beta.assign_sub(tape.gradient(loss,beta)*alpha)  # beta update\n    beta_lst.append(beta.numpy()) # update된 beta 추가\n    loss_lst.append((beta.numpy()/2-1)**2) # loss 계산\n\n\nfig = plt.figure() \nax = fig.add_subplot() \nax.plot(_beta,(_beta/2-1)**2) \npnts, = ax.plot(beta_lst[0],loss_lst[0],'or')\n\n\n\n\n\nani = animation.FuncAnimation(fig,animate,frames=100) \nani \n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n너무 느려… 왜 \\(\\alpha\\) 를 바꿔야할지 알듯하다..\n\n\n\n\n\n숙제\n\\(y=(x-1)^2\\)를 최소화 하는 \\(x\\)를 경사하강법을 이용하여 찾아라. 수렴과정을 animation으로 시각화하라.\n- x의 초기값은 -3으로 설정한다. - 적당한 \\(\\alpha\\)를 골라서 100번의 반복안에 수렴하도록 하라."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-zvc1QWi-Li-2GAeW91oicn"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#강의영상",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#강의영상",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-zvc1QWi-Li-2GAeW91oicn"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#imports",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#imports",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "imports",
    "text": "imports\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\ntf.config.experimental.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#softmax-function",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#softmax-function",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "softmax function",
    "text": "softmax function\n\n로지스틱 모형 (1): 활성화함수로 sigmoid 선택\n- 기본버전은 아래와 같다\n\\[y_i \\approx \\text{sigmoid}(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})= \\frac{\\exp(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})}{1+\\exp(b + w_1 x_{1,i} + \\dots + w_{784}x_{784,i})}\\]\n- 벡터버전은 아래와 같다.\n\\[{\\boldsymbol y} \\approx \\text{sigmoid}({\\bf X}{\\bf W} + b) = \\frac{\\exp({\\bf XW} +b)}{1+\\exp({\\bf XW} +b)}\\]\n- 벡터버전에 익숙해지도록 하자. 벡터버전에 사용된 차원 및 연산을 정리하면 아래와 같다.\n\n\\({\\bf X}\\): (n,784) matrix\n\\({\\boldsymbol y}\\): (n,1) matrix\n\\({\\bf W}\\): (784,1) matrix\n\\(b\\): (1,1) matrix\n+, exp 는 브로드캐스팅\n\n\n\n로지스틱 모형 (2): 활성화함수로 softmax 선택\n- \\(y_i=0 \\text{ or } 1\\) 대신에 \\(\\boldsymbol{y}_i=[y_{i1},y_{i2}]= [1,0] \\text { or } [0,1]\\)와 같이 코딩하면 어떠할까? (즉 원핫인코딩을 한다면?)\n- 활성화 함수를 취하기 전의 버전은 아래와 같이 볼 수 있다.\n\\[[{\\boldsymbol y}_1 ~ {\\boldsymbol y}_2] \\propto  [ {\\bf X}{\\bf W}_1  ~ {\\bf X}{\\bf W}_2] + [b_1 ~ b_2]= {\\bf X} [{\\bf W}_1 {\\bf W}_2] + [b_1 ~ b_2]= {\\bf X}{\\bf W} + {\\boldsymbol b}\\]\n여기에서 매트릭스 및 연산의 차원을 정리하면 아래와 같다.\n\n\\({\\bf X}\\): (n,784) matrix\n\\({\\boldsymbol y}_1,{\\boldsymbol y}_2\\): (n,1) matrix\n\\({\\boldsymbol y}:=[{\\boldsymbol y}_1~ {\\boldsymbol y}_2]\\): (n,2) matrix\n\\({\\bf W}_1\\), \\({\\bf W}_2\\): (784,1) matrix\n\\({\\bf W}:=[{\\bf W}_1~ {\\bf W}_2]\\): (784,2) matrix\n\\(b_1,b_2\\): (1,1) matrix\n$:= [b_1 ~b_2] $: (1,2) matrix\n+ 는 브로드캐스팅\n\n- 즉 로지스틱 모형 (1)의 형태를 겹쳐놓은 형태로 해석할 수 있음. 따라서 \\({\\bf X} {\\bf W}_1 + b_1\\)와 \\({\\bf X} {\\bf W}_2 + b_2\\)의 row값이 클수록 \\({\\boldsymbol y}_1\\)와 \\({\\boldsymbol y}_2\\)의 row값이 1이어야 함\n\n\\({\\boldsymbol y}_1 \\propto {\\bf X} {\\bf W}_1 + b_1\\) \\(\\to\\) \\({\\bf X} {\\bf W}_1 + b_1\\)의 row값이 클수록 \\(\\boldsymbol{y}_1\\)의 row 값이 1이라면 모형계수를 잘 추정한것\n\\({\\boldsymbol y}_2 \\propto {\\bf X} {\\bf W}_2 + b_2\\) \\(\\to\\) \\({\\bf X} {\\bf W}_2 + b_2\\)의 row값이 클수록 \\(\\boldsymbol{y}_2\\)의 row 값을 1이라면 모형계수를 잘 추정한것\n\n- (문제) \\({\\bf X}{\\bf W}_1 +b_1\\)의 값이 500, \\({\\bf X}{\\bf W}_2 +b_2\\)의 값이 200 인 row가 있다고 하자. 대응하는 \\(\\boldsymbol{y}_1, \\boldsymbol{y}_2\\)의 row값은 얼마로 적합되어야 하는가?\n\n\\([0,0]\\)\n\\([0,1]\\)\n\\([1,0]\\) &lt;– 이게 답이다!\n\\([1,1]\\)\n\n\nnote: 둘다 0 혹은 둘다 1로 적합할수는 없으니까 (1), (4)는 제외한다. \\({\\bf X}{\\bf W}_1 +b_1\\)의 값이 \\({\\bf X}{\\bf W}_2 +b_2\\)의 값보다 크므로 (3)번이 합리적임\n\n- 목표: 위와 같은 문제의 답을 유도해주는 활성화함수를 설계하자. 즉 합리적인 \\(\\hat{\\boldsymbol{y}}_1,\\hat{\\boldsymbol{y}}_2\\)를 구해주는 활성화 함수를 설계해보자. 이를 위해서는 아래의 사항들이 충족되어야 한다.\n\n\\(\\hat{\\boldsymbol{y}}_1\\), \\(\\hat{\\boldsymbol{y}}_2\\)의 각 원소는 0보다 크고 1보다 작아야 한다. (확률을 의미해야 하니까)\n\\(\\hat{\\boldsymbol{y}}_1+\\hat{\\boldsymbol{y}}_2={\\bf 1}\\) 이어야 한다. (확률의 총합은 1이니까!)\n\\(\\hat{\\boldsymbol{y}}_1\\)와 \\(\\hat{\\boldsymbol{y}}_2\\)를 각각 따로해석하면 로지스틱처럼 되면 좋겠다.\n\n- 아래와 같은 활성화 함수를 도입하면 어떨까?\n\\[\\hat{\\boldsymbol{y}}=[\\hat{\\boldsymbol y}_1 ~ \\hat{\\boldsymbol y}_2] =  \\big[ \\frac{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  ~~ \\frac{\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  \\big]\\]\n- (1),(2)는 만족하는 듯 하다. (3)은 바로 이해되지는 않는다\n\n\\(\\hat{\\boldsymbol{y}}_1\\), \\(\\hat{\\boldsymbol{y}}_2\\)의 각 원소는 0보다 크고 1보다 작아야 한다. –&gt; OK!\n\\(\\hat{\\boldsymbol{y}}_1+\\hat{\\boldsymbol{y}}_2={\\bf 1}\\) 이어야 한다. –&gt; OK!\n\\(\\hat{\\boldsymbol{y}}_1\\)와 \\(\\hat{\\boldsymbol{y}}_2\\)를 각각 따로해석하면 로지스틱처럼 되면 좋겠다. –&gt; ???\n\n- 그런데 조금 따져보면 (3)도 만족된다는 것을 알 수 있다. (sigmoid, softmax Section 참고)\n- 위와 같은 함수를 softmax라고 하자. 즉 아래와 같이 정의하자.\n\\[\n\\hat{\\boldsymbol y} = \\text{softmax}({\\bf X}\\hat{\\bf W} + {\\boldsymbol b})\n= \\big[ \\frac{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  ~~ \\frac{\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}{\\exp({\\bf X}\\hat{\\bf W}_1+\\hat{b}_1)+\\exp({\\bf X}\\hat{\\bf W}_2+\\hat{b}_2)}  \\big]\n\\]\n\n\nsigmoid, softmax\n\nsoftmax는 sigmoid의 확장형\n- 아래의 수식을 관찰하자. \\[\\frac{\\exp(\\beta_0+\\beta_1 x_i)}{1+\\exp(\\beta_0+\\beta_1x_i)}=\\frac{\\exp(\\beta_0+\\beta_1 x_i)}{e^0+\\exp(\\beta_0+\\beta_1x_i)}\\]\n- 1을 \\(e^0\\)로 해석하면 모형2의 해석을 아래와 같이 모형1의 해석으로 적용할수 있다. - 모형2: \\({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1\\) 와 \\({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2\\) 의 크기를 비교하고 확률 결정 - 모형1: \\({\\bf X}\\hat{\\bf W} +\\hat{b}\\) 와 \\(0\\)의 크기를 비교하고 확률 결정 = \\({\\bf X}\\hat{\\bf W} +\\hat{b}\\)의 row값이 양수이면 1로 예측하고 음수이면 0으로 예측\n- 이항분포를 차원이 2인 다항분포로 해석가능한 것처럼 sigmoid는 차원이 2인 softmax로 해석가능하다. 즉 다항분포가 이항분포의 확장형으로 해석가능한 것처럼 softmax도 sigmoid의 확장형으로 해석가능하다.\n\n\n클래스의 수가 2인 경우 softmax vs sigmoid\n- 언뜻 생각하면 클래스가 2인 경우에도 sigmoid 대신 softmax로 활성화함수를 이용해도 될 듯 하다. 즉 \\(y=0 \\text{ or } 1\\)와 같이 정리하지 않고 \\(y=[0,1] \\text{ or } [1,0]\\) 와 같이 정리해도 무방할 듯 하다.\n- 하지만 sigmoid가 좀 더 좋은 선택이다. 즉 \\(y= 0 \\text{ or } 1\\)로 데이터를 정리하는 것이 더 좋은 선택이다. 왜냐하면 sigmoid는 softmax와 비교하여 파라메터의 수가 적지만 표현력은 동등하기 때문이다.\n- 표현력이 동등한 이유? 아래 수식을 관찰하자.\n\\[\\big(\\frac{e^{300}}{e^{300}+e^{500}},\\frac{e^{500}}{e^{300}+e^{500}}\\big) =\\big( \\frac{e^{0}}{e^{0}+e^{200}}, \\frac{e^{200}}{e^{0}+e^{200}}\\big)\\]\n\n\\(\\big(\\frac{e^{300}}{e^{300}+e^{500}},\\frac{e^{500}}{e^{300}+e^{500}}\\big)\\)를 표현하기 위해서 300, 500 이라는 2개의 숫자가 필요한것이 아니고 따지고보면 200이라는 하나의 숫자만 필요하다.\n\\((\\hat{\\boldsymbol{y}}_1,\\hat{\\boldsymbol{y}}_2)\\)의 표현에서도 \\({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1\\) 와 \\({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2\\) 라는 숫자 각각이 필요한 것이 아니고 \\(({\\bf X}\\hat{\\bf W}_1 +\\hat{b}_1)-({\\bf X}\\hat{\\bf W}_2 +\\hat{b}_2)\\)의 값만 알면 된다.\n\n- 클래스의 수가 2개일 경우는 softmax가 sigmoid에 비하여 장점이 없다. 하지만 softmax는 클래스의 수가 3개 이상일 경우로 쉽게 확장할 수 있다는 점에서 매력적인 활성화 함수이다.\n\n\n\n분류할 클래스가 3개 이상일 경우 신경망 모형의 설계\n- y의 모양: [0 1 0 0 0 0 0 0 0 0]\n- 활성화함수의 선택: softmax\n- 손실함수의 선택: cross entropy\n\n\nFashion_MNIST 여러클래스의 분류 (softmax의 실습)\n- 데이터정리\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= x_train.reshape(-1,784)\ny= tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,784)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 시도1: 간단한 신경망\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -&gt; \"node1\"\n    \"x2\" -&gt; \"node1\"\n    \"..\" -&gt; \"node1\"\n\n    \"x784\" -&gt; \"node1\"\n    \"x1\" -&gt; \"node2\"\n    \"x2\" -&gt; \"node2\"\n    \"..\" -&gt; \"node2\"\n    \"x784\" -&gt; \"node2\"\n\n    \"x1\" -&gt; \"...\"\n    \"x2\" -&gt; \"...\"\n    \"..\" -&gt; \"...\"\n    \"x784\" -&gt; \"...\"\n\n    \"x1\" -&gt; \"node30\"\n    \"x2\" -&gt; \"node30\"\n    \"..\" -&gt; \"node30\"\n    \"x784\" -&gt; \"node30\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -&gt; \"y10\"\n    \"node2\" -&gt; \"y10\"\n    \"...\" -&gt; \"y10\"\n    \"node30\" -&gt; \"y10\"\n\n    \"node1\" -&gt; \"y1\"\n    \"node2\" -&gt; \"y1\"\n    \"...\" -&gt; \"y1\"\n    \"node30\" -&gt; \"y1\"\n\n    \"node1\" -&gt; \".\"\n    \"node2\" -&gt; \".\"\n    \"...\" -&gt; \".\"\n    \"node30\" -&gt; \".\"\n\n    label = \"Layer 2: softmax\"\n}\n''')\n\n\n\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 1s 485us/step - loss: 2.2998 - accuracy: 0.4233\nEpoch 2/5\n1875/1875 [==============================] - 1s 475us/step - loss: 1.2198 - accuracy: 0.5444\nEpoch 3/5\n1875/1875 [==============================] - 1s 498us/step - loss: 1.0499 - accuracy: 0.5938\nEpoch 4/5\n1875/1875 [==============================] - 1s 484us/step - loss: 0.9243 - accuracy: 0.6309\nEpoch 5/5\n1875/1875 [==============================] - 1s 484us/step - loss: 0.8560 - accuracy: 0.6506\n\n\n&lt;keras.callbacks.History at 0x7f904d4618b0&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 374us/step - loss: 0.8689 - accuracy: 0.6363\n\n\n[0.8688836693763733, 0.6363000273704529]\n\n\n\nnet.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 30)                23550     \n                                                                 \n dense_1 (Dense)             (None, 10)                310       \n                                                                 \n=================================================================\nTotal params: 23,860\nTrainable params: 23,860\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 시도2: 더 깊은 신경망\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -&gt; \"node1\"\n    \"x2\" -&gt; \"node1\"\n    \"..\" -&gt; \"node1\"\n\n    \"x784\" -&gt; \"node1\"\n    \"x1\" -&gt; \"node2\"\n    \"x2\" -&gt; \"node2\"\n    \"..\" -&gt; \"node2\"\n    \"x784\" -&gt; \"node2\"\n\n    \"x1\" -&gt; \"...\"\n    \"x2\" -&gt; \"...\"\n    \"..\" -&gt; \"...\"\n    \"x784\" -&gt; \"...\"\n\n    \"x1\" -&gt; \"node500\"\n    \"x2\" -&gt; \"node500\"\n    \"..\" -&gt; \"node500\"\n    \"x784\" -&gt; \"node500\"\n\n\n    label = \"Layer 1: relu\"\n}\n\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -&gt; \"node1(2)\"\n    \"node2\" -&gt; \"node1(2)\"\n    \"...\" -&gt; \"node1(2)\"\n    \"node500\" -&gt; \"node1(2)\"\n\n    \"node1\" -&gt; \"node2(2)\"\n    \"node2\" -&gt; \"node2(2)\"\n    \"...\" -&gt; \"node2(2)\"\n    \"node500\" -&gt; \"node2(2)\"\n\n    \"node1\" -&gt; \"....\"\n    \"node2\" -&gt; \"....\"\n    \"...\" -&gt; \"....\"\n    \"node500\" -&gt; \"....\"\n\n    \"node1\" -&gt; \"node500(2)\"\n    \"node2\" -&gt; \"node500(2)\"\n    \"...\" -&gt; \"node500(2)\"\n    \"node500\" -&gt; \"node500(2)\"\n\n\n    label = \"Layer 2: relu\"\n}\n\nsubgraph cluster_4{\n    style=filled;\n    color=lightgrey;\n\n    \"node1(2)\" -&gt; \"y10\"\n    \"node2(2)\" -&gt; \"y10\"\n    \"....\" -&gt; \"y10\"\n    \"node500(2)\" -&gt; \"y10\"\n\n    \"node1(2)\" -&gt; \"y1\"\n    \"node2(2)\" -&gt; \"y1\"\n    \"....\" -&gt; \"y1\"\n    \"node500(2)\" -&gt; \"y1\"\n\n    \"node1(2)\" -&gt; \".\"\n    \"node2(2)\" -&gt; \".\"\n    \"....\" -&gt; \".\"\n    \"node500(2)\" -&gt; \".\"\n\n    label = \"Layer 3: softmax\"\n}\n''')\n\n\n\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.3745 - accuracy: 0.7530\nEpoch 2/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.6109 - accuracy: 0.7965\nEpoch 3/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5170 - accuracy: 0.8190\nEpoch 4/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4650 - accuracy: 0.8356\nEpoch 5/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4269 - accuracy: 0.8481\n\n\n&lt;keras.callbacks.History at 0x7f8fa06980d0&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 714us/step - loss: 0.4323 - accuracy: 0.8456\n\n\n[0.4322887063026428, 0.8456000089645386]\n\n\n\nnet.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 500)               392500    \n                                                                 \n dense_3 (Dense)             (None, 500)               250500    \n                                                                 \n dense_4 (Dense)             (None, 10)                5010      \n                                                                 \n=================================================================\nTotal params: 648,010\nTrainable params: 648,010\nNon-trainable params: 0\n_________________________________________________________________"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#평가지표",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#평가지표",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "평가지표",
    "text": "평가지표\n\n다양한 평가지표들\n- 의문: 왜 다양한 평가지표가 필요한가? (accuray면 끝나는거 아닌가? 더 이상 뭐가 필요해?)\n- 여러가지 평가지표들: https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values - 이걸 다 암기하는건 불가능함. - 몇 개만 뽑아서 암기하고 왜 쓰는지만 생각해보고 넘어가자!\n\n\nconfusion matrix의 이해\n- 표1\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP\nFN\n\n\n안나감(실제)\nFP\nTN\n\n\n\n- 표2 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\n$(y,)= $ (O,O)\n$(y,)= $(O,X)\n\n\n안나감(실제)\n$(y,)= $(X,O)\n$(y,)= $(X,X)\n\n\n\n- 표3 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\#O/X\\)\n\n\n안나감(실제)\nFP, \\(\\#X/O\\)\nTN, \\(\\#X/X\\)\n\n\n\n\n암기법, (1) 두번째 글자를 그대로 쓴다 (2) 첫글자가 T이면 분류를 제대로한것, 첫글자가 F이면 분류를 잘못한것\n\n- 표4 (위키등에 있음)\n\n\n\n\n\n\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\# O/X\\)\nSensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\#O/O}{\\# O/O+ \\#O/X}\\)\n\n\n안나감(실제)\nFP, \\(\\# X/O\\)\nTN, \\(\\# X/X\\)\n\n\n\n\nPrecision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\)\n\nAccuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+\\# X/X}{total}\\)\n\n\n\n\n\n상황극\n- 최규빈은 입사하여 “퇴사자 예측시스템”의 개발에 들어갔다.\n- 자료의 특성상 대부분의 사람이 퇴사하지 않고 회사에 잘 다닌다. 즉 1000명이 있으면 10명정도 퇴사한다.\n\n\nAccuracy\n- 정의: Accuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+ \\#X/X}{total}\\) - 한국말로는 정확도, 정분류율이라고 한다. - 한국말이 헷갈리므로 그냥 영어를 외우는게 좋다. (어차피 Keras에서 옵션도 영어로 넣음)\n- (상확극 시점1) 왜 애큐러시는 불충분한가? - 회사: 퇴사자예측프로그램 개발해 - 최규빈: 귀찮은데 다 안나간다고 하자! -&gt; 99퍼의 accuracy\n\n모델에 사용한 파라메터 = 0. 그런데 애큐러시 = 99! 이거 엄청 좋은 모형이다?\n\n\n\nSensitivity(민감도), Recall(재현율), True Positive Rate(TPR)\n- 정의: Sensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# O/X}\\) - 분모: 실제 O인 관측치 수 - 분자: 실제 O를 O라고 예측한 관측치 수 - 뜻: 실제 O를 O라고 예측한 비율\n- (상황극 시점2) recall을 봐야하는 이유 - 인사팀: 실제 퇴사자를 퇴사자로 예측해야 의미가 있음! 우리는 퇴사할것 같은 10명을 찍어달란 의미였어요! (그래야 면담을 하든 할거아냐!) - 최규빈: 가볍고(=파라메터 적고) 잘 맞추는 모형 만들어 달라면서요?\n\n인사팀: (고민중..) 사실 생각해보니까 이 경우는 애큐러시는 의미가 없네. 실제 나간 사람 중 최규빈이 나간다고 한 사람이 몇인지 카운트 하는게 더 의미가 있겠다. 우리는 앞으로 리컬(혹은 민감도)를 보겠다!\n\n\n예시1: 실제로 퇴사한 10명중 최규빈이 나간다고 찍은 사람이 5명이면 리컬이 50%\n\n\n예시2: 최규빈이 아무도 나가지 않는다고 예측해버린다? 실제 10명중에서 최규빈이 나간다고 적중시킨사람은 0명이므로 이 경우 리컬은 0%\n\n\n결론: 우리가 필요한건 recall이니까 앞으로 recall을 가져와! accuracy는 큰 의미없어. (그래도 명색이 모델인데 accuracy가 90은 되면 좋겠다)\n\n\n\nPrecision\n- 정의: Precision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\) - 분모: O라고 예측한 관측치 - 분자: O라고 예측한 관측치중 진짜 O인 관측치 - 뜻: O라고 예측한 관측치중 진짜 O인 비율\n- (상황극 시점3) recall 만으로 불충분한 이유\n\n최규빈: 에휴.. 귀찮은데 그냥 좀만 수틀리면 다 나갈것 같다고 해야겠다. -&gt; 한 100명 나간다고 했음 -&gt; 실제로 최규빈이 찍은 100명중에 10명이 다 나감!\n\n\n이 경우 애큐러시는 91%, 리컬은 100% (퇴사자 10명을 일단은 다 맞췄으므로).\n\n\n인사팀: (화가 많이 남) 멀쩡한 사람까지 다 퇴사할 것 같다고 하면 어떡해요? 최규빈 연구원이 나간다고 한 100명중에 실제로 10명만 나갔어요.\n인사팀: 마치 총으로 과녁중앙에 맞춰 달라고 했더니 기관총을 가져와서 한번 긁은것이랑 뭐가 달라요? 맞추는게 문제가 아니고 precision이 너무 낮아요.\n최규빈: accuracy 90% 이상, recall은 높을수록 좋다는게 주문 아니었나요?\n인사팀: (고민중..) 앞으로는 recall과 함께 precision도 같이 제출하세요. precision은 당신이 나간다고 한 사람중에 실제 나간사람의 비율을 의미해요. 이 경우는 \\(\\frac{10}{100}\\)이니까 precision이 10%입니다. (속마음: recall 올리겠다고 무작정 너무 많이 예측하지 말란 말이야!)\n\n\n\nF1 score\n- 정의: recall과 precision의 조화평균\n- (상황극 시점4) recall, precision을 모두 고려\n\n최규빈: recall/precision을 같이 내는건 좋은데요, 둘은 trade off의 관계에 있습니다. 물론 둘다 올리는 모형이 있다면 좋지만 그게 쉽지는 않아요. 보통은 precision을 올리려면 recall이 희생되는 면이 있고요, recall을 올리려고 하면 precision이 다소 떨어집니다.\n최규빈: 평가기준이 애매하다는 의미입니다. 모형1,2가 있는데 모형1은 모형2보다 precision이 약간 좋고 대신 recall이 떨어진다면 모형1이 좋은것입니까? 아니면 모형2가 좋은것입니까?\n인사팀: 그렇다면 둘을 평균내서 F1score를 계산해서 제출해주세요.\n\n\n\nSpecificity(특이도), False Positive Rate(FPR)\n- 정의:\n\nSpecificity(특이도)=\\(\\frac{TN}{FP+TN}\\)=\\(\\frac{\\# X/X}{\\# X/O+\\# X/X}\\)\nFalse Positive Rate (FPR) = 1-Specificity(특이도) = \\(\\frac{FP}{FP+TN}\\)=\\(\\frac{\\# X/O}{\\# X/O+\\# X/X}\\)\n\n- 의미: FPR = 오해해서 미안해, recall(=TPR)을 올리려고 보니 어쩔 수 없었어 ㅠㅠ - specificity는 안나간 사람을 안나갔다고 찾아낸 비율인데 별로 안중요하다. - FPR은 recall을 올리기 위해서 “실제로는 회사 잘 다니고 있는 사람 중 최규빈이 나갈것 같다고 찍은 사람들” 의 비율이다.\n\n즉 생사람잡은 비율.. 오해해서 미안한 사람의 비율..\n\n\n\nROC curve\n- 정의: \\(x\\)축=FPR, \\(y\\)축=TPR 을 그린 커브\n- 의미: - 결국 “오해해서 미안해 vs recall”을 그린 곡선이 ROC커브이다. - 생각해보면 오해하는 사람이 많을수록 당연히 recall은 올라간다. 따라서 우상향하는 곡선이다. - 오해한 사람이 매우 적은데 recall이 우수하면 매우 좋은 모형이다. 그래서 초반부터 ROC값이 급격하게 올라가면 좋은 모형이다.\n\n\nFashion MNIST 다양한 평가지표활용\n- data\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= x_train.reshape(-1,784)\ny= tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,784)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 다양한 평가지표를 넣는 방법 (1)\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy','Recall'])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.1689 - accuracy: 0.7489 - recall: 0.7085\nEpoch 2/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6779 - accuracy: 0.7775 - recall: 0.7104\nEpoch 3/5\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6277 - accuracy: 0.7802 - recall: 0.7084\nEpoch 4/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5648 - accuracy: 0.7937 - recall: 0.7370\nEpoch 5/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5308 - accuracy: 0.8032 - recall: 0.7394\n\n\n&lt;keras.callbacks.History at 0x7f8fa054c2b0&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 592us/step - loss: 0.6554 - accuracy: 0.7603 - recall: 0.7150\n\n\n[0.6554250717163086, 0.7602999806404114, 0.7149999737739563]\n\n\n- 다양한 평가지표를 넣는 방법 (2)\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 1.9477 - categorical_accuracy: 0.7461 - recall: 0.7014\nEpoch 2/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.6755 - categorical_accuracy: 0.7792 - recall: 0.7217\nEpoch 3/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5605 - categorical_accuracy: 0.8143 - recall: 0.7659\nEpoch 4/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4762 - categorical_accuracy: 0.8356 - recall: 0.7929\nEpoch 5/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4460 - categorical_accuracy: 0.8422 - recall: 0.8033\n\n\n&lt;keras.callbacks.History at 0x7f904e1603d0&gt;\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 590us/step - loss: 0.4699 - categorical_accuracy: 0.8353 - recall: 0.7885\n\n\n[0.46987518668174744, 0.8353000283241272, 0.7885000109672546]"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#flatten-layer",
    "href": "posts/3_STBDA2022/2022_05_09_(10주차)_5월9일.html#flatten-layer",
    "title": "[STBDA] 10wk. Softmax / 다양한 평가지표",
    "section": "flatten layer",
    "text": "flatten layer\n- 이미지 데이터를 분류하기 좋은 형태로 자료를 재정리하자.\n\nx_train.shape\n\n(60000, 28, 28)\n\n\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nX.shape,XX.shape,y.shape,yy.shape\n\n(TensorShape([60000, 28, 28, 1]),\n TensorShape([10000, 28, 28, 1]),\n (60000, 10),\n (10000, 10))\n\n\n- 일반적인 이미지 분석 모형을 적용하기 용이한 데이터 형태로 정리했다. -&gt; 그런데 모형에 넣고 돌릴려면 다시 차원을 펼쳐야 하지 않을까?\n- 안펼치고 하고싶다.\n\nflttn = tf.keras.layers.Flatten()\n\n\nset(dir(flttn)) & {'__call__'}\n\n{'__call__'}\n\n\n\nX.shape, flttn(X).shape, X.reshape(-1,784).shape\n\n(TensorShape([60000, 28, 28, 1]),\n TensorShape([60000, 784]),\n TensorShape([60000, 784]))\n\n\n\n같은 기능\n다른건? \\(\\to\\) layers (layer로 넣을 수 있는 무기들 중 Flatten이 하나 추가됨!) \\(\\to\\) 네트워크에 넣을 수 있겠다!\n\n- flttn\n\\(X \\to \\text{Dense}(500, relu) \\to \\text{Dense}(500, relu) \\to \\text{Dense}(10,\\text{softmax}):=y\\)\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten()) ## dimension을 맞춰주는 것을 네트워크 안으로 넣은 것.\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(500,activation = 'relu'))\nnet.add(tf.keras.layers.Dense(10,activation = 'softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()])\nnet.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 2.5320 - categorical_accuracy: 0.7491 - recall_1: 0.7126\nEpoch 2/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.6705 - categorical_accuracy: 0.7844 - recall_1: 0.7238\nEpoch 3/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.5829 - categorical_accuracy: 0.8096 - recall_1: 0.7708\nEpoch 4/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4954 - categorical_accuracy: 0.8318 - recall_1: 0.7970\nEpoch 5/5\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4676 - categorical_accuracy: 0.8383 - recall_1: 0.8020\n\n\n&lt;keras.callbacks.History at 0x7f904e050940&gt;\n\n\n\nnet.layers\n\n[&lt;keras.layers.reshaping.flatten.Flatten at 0x7f904df61430&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f904df612e0&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f904deed700&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f904deed970&gt;]\n\n\n\nDense layer는 activation이 합쳐진 layer.\n\n\nprint(X.shape)\nprint(net.layers[0](X).shape)\nprint(net.layers[1](net.layers[0](X)).shape)\nprint(net.layers[2](net.layers[1](net.layers[0](X))).shape)\nprint(net.layers[3](net.layers[2](net.layers[1](net.layers[0](X)))).shape)\n\n(60000, 28, 28, 1)\n(60000, 784)\n(60000, 500)\n(60000, 500)\n(60000, 10)\n\n\n\nnet.layers[1]\n\n&lt;keras.layers.core.dense.Dense at 0x7f904df612e0&gt;\n\n\n- 좀 더 복잡한 네트워크 -&gt; 하지만 한계가 보인다 -&gt; 좀 더 나은 아키텍처는 없을까\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(500,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics='accuracy')\nnet.fit(X,y,epochs=10)\n\nEpoch 1/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 1.1550 - accuracy: 0.7911\nEpoch 2/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.4523 - accuracy: 0.8377\nEpoch 3/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.4129 - accuracy: 0.8532\nEpoch 4/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3926 - accuracy: 0.8610\nEpoch 5/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.3748 - accuracy: 0.8680\nEpoch 6/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3645 - accuracy: 0.8718\nEpoch 7/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3461 - accuracy: 0.8773\nEpoch 8/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3381 - accuracy: 0.8788\nEpoch 9/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3372 - accuracy: 0.8810\nEpoch 10/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.3240 - accuracy: 0.8844\n\n\n&lt;keras.callbacks.History at 0x7f904e1d9d00&gt;\n\n\n\nnet.summary()\n\nModel: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_3 (Flatten)         (None, 784)               0         \n                                                                 \n dense_20 (Dense)            (None, 500)               392500    \n                                                                 \n dense_21 (Dense)            (None, 500)               250500    \n                                                                 \n dense_22 (Dense)            (None, 500)               250500    \n                                                                 \n dense_23 (Dense)            (None, 500)               250500    \n                                                                 \n dense_24 (Dense)            (None, 10)                5010      \n                                                                 \n=================================================================\nTotal params: 1,149,010\nTrainable params: 1,149,010\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nnet.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 752us/step - loss: 0.4136 - accuracy: 0.8566\n\n\n[0.4136269986629486, 0.8565999865531921]\n\n\n- layer중에 우리는 끽해야 Dense정도 쓰고있었음. \\(\\to\\) flatten과 같은 다른 layer도 많음. \\(\\to\\) 이런것도 써보자"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_03_14_(2주차)_3월14일.html",
    "href": "posts/3_STBDA2022/2022_03_14_(2주차)_3월14일.html",
    "title": "[STBDA] 2wk. 텐서플로우 intro1 (tf.constant선언, tnp사용법)",
    "section": "",
    "text": "(2주차) 3월14일\n\n강의노트\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-z8oR8bQZHR0mpy_9OcsWOz\n\n\n\nimport\n\nimport tensorflow as tf\nimport numpy as np\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[]\n\n\n\n\ntf.constant\n\n예비학습: 중첩리스트\n- 리스트\n\nlst = [1,2,4,5,6]\nlst \n\n[1, 2, 4, 5, 6]\n\n\n\nlst[1] # 두번쨰원소 \n\n2\n\n\n\nlst[-1] # 마지막원소 \n\n6\n\n\n- (2,2) matrix 느낌의 list\n\nlst= [[1,2],[3,4]]\nlst\n\n[[1, 2], [3, 4]]\n\n\n위를 아래와 같은 매트릭스로 생각할수 있다.\n1 2 \n3 4 \n\nprint(lst[0][0]) # (1,1) \nprint(lst[0][1]) # (1,2) \nprint(lst[1][0]) # (2,1) \nprint(lst[1][1]) # (2,2) \n\n1\n2\n3\n4\n\n\n- (4,1) matrix 느낌의 list\n\nlst=[[1],[2],[3],[4]] # (4,1) matrix = 길이가 4인 col-vector\nlst\n\n[[1], [2], [3], [4]]\n\n\n\nnp.array(lst), np.array(lst).shape\n\n(array([[1],\n        [2],\n        [3],\n        [4]]),\n (4, 1))\n\n\n- (1,4) matrix 느낌의 list\n\nlst=[[1,2,3,4]] # (1,4) matrix = 길이가 4인 row-vector \nlst\n\n[[1, 2, 3, 4]]\n\n\n\nnp.array(lst), np.array(lst).shape\n\n(array([[1, 2, 3, 4]]), (1, 4))\n\n\n\n\n선언 (변수 선언하는 법?)\n\n텐서플로우를 쓰려면 텐서로 바꿔줘야 한다.\n\n- 스칼라\n\ntf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt;\n\n\n\ntf.constant(3.14)+tf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=6.28&gt;\n\n\n- 벡터\n\ntype(1),type([1,2,3]), type(_vector) # int 혹은 list 타입 --&gt; EagerTensor 타입으로 바꾸라는 뜻.\n\n(int, list, tensorflow.python.framework.ops.EagerTensor)\n\n\n\n_vector=tf.constant([1,2,3])\n\n\n_vector[-1]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt;\n\n\n- 매트릭스\n\n_matrix= tf.constant([[1,0],[0,1]])\n_matrix\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 1]], dtype=int32)&gt;\n\n\n- array\n\ntf.constant([[[0,1,1],[1,2,-1]],[[0,1,2],[1,2,-1]]])\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 0,  1,  1],\n        [ 1,  2, -1]],\n\n       [[ 0,  1,  2],\n        [ 1,  2, -1]]], dtype=int32)&gt;\n\n\n\n\n타입\n\ntype(tf.constant(3.14))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\nEagerTensor 타입이라는 것만 기억!\n\n\n\n인덱싱\n\n_matrix = tf.constant([[1,2],[3,4]])\n_matrix\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n\n_matrix[0][0]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;\n\n\n\n_matrix[0]\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;\n\n\n\n_matrix[0,:]\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;\n\n\n\n_matrix[:,0]\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)&gt;\n\n\n\n\ntf.constant는 불편하다.\n- 불편한점 1. 모든 원소가 같은 dtype을 가지고 있어야함. 2. 원소 수정이 불가능함. 3. 묵시적 형변환이 불가능하다.\n- 원소수정이 불가능함\n\na=tf.constant([1,22,33])\na\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)&gt;\n\n\n\na[0]=11 \n\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment\n\n\n- 묵시적 형변환이 불가능하다\n\n1 + 3.14 # 1(int), 3.14(float) --&gt; 4.14(float)\n\n4.140000000000001\n\n\n\n1을 float으로 암묵적으로 바꿔서 계산함.\n\n\ntf.constant(1)+tf.constant(3.14) ## 에러!\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n\n\n\nint와 float이라 에러나는 것\n\n\ntf.constant(1.0)+tf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.1400003&gt;\n\n\n- 같은 float도 안되는 경우가 있음\n\ntf.constant(1.0,dtype=tf.float64)\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=1.0&gt;\n\n\n\ntf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt;\n\n\n\ntf.constant(1.0,dtype=tf.float64)+tf.constant(3.14)\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2]\n\n\n\n조금만 틀리면 에러남..\n\n\n\ntf.constant \\(\\to\\) 넘파이\n\nnp.array(tf.constant(1)) # 방법1\n\narray(1, dtype=int32)\n\n\n\nnumpy로 연산을 다 해놓고 tensor로 바꾼다..\n\n\na=tf.constant([3.14,-3.14])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na.numpy() # numpy라는 메서드가 있음.\n\narray([ 3.14, -3.14], dtype=float32)\n\n\n\n\n연산\n- 더하기\n\na=tf.constant([1,2])\nb=tf.constant([3,4])\na+b\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt;\n\n\n처음에 int로 선언했으면 나머지도 모두 int로 선언해야해..\n\ntf.add(a,b)\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt;\n\n\n\n결과는 동일\n\n- 곱하기\n\na=tf.constant([[1,2],[3,4]])\nb=tf.constant([[5,6],[7,8]])\na*b # elementwise 하게 곱한하고 한다.\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)&gt;\n\n\n\ntf.multiply(a,b) # 이게 먼저 나왔음.\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)&gt;\n\n\n- 매트릭스의곱\n\na=tf.constant([[1,0],[0,1]]) # (2,2)\nb=tf.constant([[5],[7]]) # (2,1) \na@b\n\n&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)&gt;\n\n\n\ntf.matmul(a,b)\n\n&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)&gt;\n\n\n- 역행렬\n\na=tf.constant([[1,0],[0,2]])\na\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 2]], dtype=int32)&gt;\n\n\n\ntf.linalg.inv(a)\n\nInvalidArgumentError: Value for attr 'T' of int32 is not in the list of allowed values: double, float, half, complex64, complex128\n    ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse]\n\n\n\n1/2을 계산하려면 애초에 float형이어야 했음.\n\n\na=tf.constant([[1.0,0.0],[0.0,2.0]])\ntf.linalg.inv(a)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1. , 0. ],\n       [0. , 0.5]], dtype=float32)&gt;\n\n\n- tf.linalg. + tab을 누르면 좋아보이는 연산들 많음\n\na=tf.constant([[1.0,2.0],[3.0,4.0]])\nprint(a)\ntf.linalg.det(a)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt;\n\n\n\ntf.linalg.trace(a)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;\n\n\n\n\n형태변환\n- 기본: tf.reshape() 를 이용\n\na=tf.constant([1,2,3,4]) #  길이가 4인 vector\na\n\n&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,1)) # column-vec로 변환.\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,1))\n\n&lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\narray([[[1],\n        [2]],\n\n       [[3],\n        [4]]], dtype=int32)&gt;\n\n\n- 다차원\n\na=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12]) # 길이가 12인 vector\na\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,3)) # 2*2*3\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,3)) # 4*3\n\n&lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)&gt;\n\n\n- tf.reshape\n\na=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12])\na\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,-1)) # 원소 어짜피 12개 있으니까 4만 주면 나머지 3은 알아서 맞춰줌.\n\n&lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,-1))\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\nb=tf.reshape(a,(2,2,-1))\nb\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\ntf.reshape(b,-1) # 길이가 12인 vector로 바꿔줌.\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\n\n선언고급\n뭔가를 초기화할 때 필요한 기능을 정리한 것입니다.\n- 다른 자료형 (리스트나 넘파이)로 만들고 바꾸는것도 좋다.\n\nnp.diag([1,2,3,4])\n\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])\n\n\n\ntf.constant(np.diag([1,2,3,4]))\n\n&lt;tf.Tensor: shape=(4, 4), dtype=int64, numpy=\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])&gt;\n\n\n- tf.ones, tf.zeros\n\ntf.zeros([3,3])\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)&gt;\n\n\n\ntf.reshape(tf.constant([0]*9),(3,3)) # tensor로 바꾸고 shape을 바꿔주는 방법도 있다.\n\n&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0]], dtype=int32)&gt;\n\n\n- range(10)\n\na=range(0,12)\ntype(a) # type이 range임!\n\nrange\n\n\n\nlist(a) # 타입을 list로 바꿀 수 있음.\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\n\n\na=range(0,12)\ntf.constant(a)\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32)&gt;\n\n\n\ntf.constant(range(1,20,3)) \n\n&lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([ 1,  4,  7, 10, 13, 16, 19], dtype=int32)&gt;\n\n\n- tf.linspace\n\ntf.linspace(0,1,10) # 0부터시작해서 1까지 총 10개 // 여기서는 1(마지막 숫자)이 포함됨\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])&gt;\n\n\n\n\ntf.concat\n- (2,1) concat (2,1) =&gt; (2,2) - 두번째 축이 바뀌었다. =&gt; axis=1\n\na=tf.constant([[1],[2]])\nb=tf.constant([[3],[4]])\na,b # 2개의 col-vec\n\n(&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=1) # 2col-vec -&gt; matrix\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 3],\n       [2, 4]], dtype=int32)&gt;\n\n\n- (2,1) concat (2,1) =&gt; (4,1) - 첫번째 축이 바뀌었다. =&gt; axis=0\n\na=tf.constant([[1],[2]])\nb=tf.constant([[3],[4]])\na,b\n\n(&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)&gt;\n\n\n- (1,2) concat (1,2) =&gt; (2,2) - 첫번째 // axis=0\n\na=tf.constant([[1,2]])\nb=tf.constant([[3,4]])\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n- (1,2) concat (1,2) =&gt; (1,4) - 두번째 // axis=1\n- (2,3,4,5) concat (2,3,4,5) =&gt; (4,3,4,5) - 첫번째 // axis=0\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(4, 3, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]]],\n\n\n       [[[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,6,4,5) - 두번째 // axis=1\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(2, 6, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]],\n\n        [[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]],\n\n        [[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,8,5) - 세번째 // axis=2\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=2)\n\n&lt;tf.Tensor: shape=(2, 3, 8, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19],\n         [   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39],\n         [ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59],\n         [ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79],\n         [ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99],\n         [ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119],\n         [-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,4,10) - 네번째 // axis=3 # 0,1,2,3 // -4 -3 -2 -1\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4,    0,   -1,   -2,   -3,   -4],\n         [   5,    6,    7,    8,    9,   -5,   -6,   -7,   -8,   -9],\n         [  10,   11,   12,   13,   14,  -10,  -11,  -12,  -13,  -14],\n         [  15,   16,   17,   18,   19,  -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24,  -20,  -21,  -22,  -23,  -24],\n         [  25,   26,   27,   28,   29,  -25,  -26,  -27,  -28,  -29],\n         [  30,   31,   32,   33,   34,  -30,  -31,  -32,  -33,  -34],\n         [  35,   36,   37,   38,   39,  -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44,  -40,  -41,  -42,  -43,  -44],\n         [  45,   46,   47,   48,   49,  -45,  -46,  -47,  -48,  -49],\n         [  50,   51,   52,   53,   54,  -50,  -51,  -52,  -53,  -54],\n         [  55,   56,   57,   58,   59,  -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64,  -60,  -61,  -62,  -63,  -64],\n         [  65,   66,   67,   68,   69,  -65,  -66,  -67,  -68,  -69],\n         [  70,   71,   72,   73,   74,  -70,  -71,  -72,  -73,  -74],\n         [  75,   76,   77,   78,   79,  -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84,  -80,  -81,  -82,  -83,  -84],\n         [  85,   86,   87,   88,   89,  -85,  -86,  -87,  -88,  -89],\n         [  90,   91,   92,   93,   94,  -90,  -91,  -92,  -93,  -94],\n         [  95,   96,   97,   98,   99,  -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104, -100, -101, -102, -103, -104],\n         [ 105,  106,  107,  108,  109, -105, -106, -107, -108, -109],\n         [ 110,  111,  112,  113,  114, -110, -111, -112, -113, -114],\n         [ 115,  116,  117,  118,  119, -115, -116, -117, -118, -119]]]],\n      dtype=int32)&gt;\n\n\n- (4,) concat (4,) =&gt; (8,) - 첫번째축? // axis=0\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1,  2,  3,  4, -1, -2, -3, -4], dtype=int32)&gt;\n\n\n- (4,) concat (4,) =&gt; (4,2) - 두번째축? // axis=1 ==&gt; 이런거없다..\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=1)\n\nInvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat\n\n\n\n에러남! 이럴때는 tf.stack을 쓰면 된다.\n\n\n\ntf.stack\n\nstack은 차원이 늘어남!\n\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.stack([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[ 1,  2,  3,  4],\n       [-1, -2, -3, -4]], dtype=int32)&gt;\n\n\n\ntf.stack([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3],\n       [ 4, -4]], dtype=int32)&gt;\n\n\n\n\n\ntnp\n- tf는 넘파이에 비하여 텐서만들기가 너무힘듬\n\nnp.diag([1,2,3])\n\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n\n\n\nnp.diag([1,2,3]).reshape(-1)\n\narray([1, 0, 0, 0, 2, 0, 0, 0, 3])\n\n\n\n넘파이는 이런식으로 np.diag()도 쓸수 있고 reshape을 메소드로 쓸 수도 있는데…\n\n\ntnp 사용방법 (불만해결방법)\ntensorflow에서 numpy처럼 동작하도록 할 수 있는 모듈이라고 생각 즉, np의 모방버전으로 생각하면 된다.\n\nimport tensorflow.experimental.numpy as tnp\ntnp.experimental_enable_numpy_behavior()\n\n\ntype(tnp.array([1,2,3]))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n이렇게 만들어도 된다.\n\n- int와 float을 더할 수 있음\n\ntnp.array([1,2,3])+tnp.array([1.0,2.0,3.0])\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt;\n\n\n\ntf.constant([1,2,3])+tf.constant([1.0,2.0,3.0]) # 이게 원래 에러났었음!\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt;\n\n\n\ntnp 모듈을 불러오는 순간 tf로 선언하는 모든 것들도 우리가 알고있는 넘파이처럼 동작합니다.\n\n\ntnp.array(1)+tnp.array([1.0,2.0,3.0])\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])&gt;\n\n\n\ntnp.diag([1,2,3])\n\n&lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy=\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])&gt;\n\n\n\na=tnp.diag([1,2,3])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na=tf.constant([1,2,3])\na.reshape(3,1)\n\n&lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3]], dtype=int32)&gt;\n\n\n\n\n선언고급\n\nnp.random.randn(5) # random module\n\narray([1.12411749, 1.43127059, 0.61763568, 0.43586944, 0.33208259])\n\n\n\ntnp.random.randn(5) # 넘파이가 되면 나도 된다.\n\n&lt;tf.Tensor: shape=(5,), dtype=float64, numpy=array([-0.64055227, -1.33606436, -0.71424816,  1.61243245, -2.07980232])&gt;\n\n\n\n\n타입\n\ntype(tnp.random.randn(5))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n\ntf.contant로 만들어도 마치 넘파이인듯 쓰는 기능들\n- 묵시적형변환이 가능\nint랑 float을 계산할 수 있다는 것\n\ntf.constant([1,1])+tf.constant([2.2,3.3])\n\n&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([3.20000005, 4.29999995])&gt;\n\n\n- 메소드를 쓸수 있음.\n사용가능한 메소드가 많아짐..\n\na= tnp.array([[1,2,3,4]])\na.T\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int64, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]])&gt;\n\n\n\n\n그렇지만 np.array는 아님\n- 원소를 할당하는것은 불가능\n\na=tf.constant([1,2,3])\na\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;\n\n\n\na[0]=11\n\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment\n\n\n\n그냥 새로 만들어서 할당해야 함. (그래도 많이 개선된 것@)"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_03_21_(3주차)_3월21일.html",
    "href": "posts/3_STBDA2022/2022_03_21_(3주차)_3월21일.html",
    "title": "[STBDA] 3wk. 텐서플로우 intro2 (tf.GradientTape())",
    "section": "",
    "text": "(3주차) 3월21일\n\n강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-yCZH5zqsORTEkCZ082SCYc\n\n\n\nimports\n\nimport tensorflow as tf\nimport numpy as np\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\n\n지난강의 보충\n- max, min, sum, mean\n\na= tf.constant([1.0,2.0,3.0,4.0])\na\n\n&lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt;\n\n\n\ntf.reduce_mean(a)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.5&gt;\n\n\n\nconcat, stack\n- 예제: (2,3,4,5) stack (2,3,4,5) -&gt; (?,?,?,?,?)\n\na = tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5))\nb = -a \n\n\nTip: 총 5가지 case가 있다. case1 (1,2,3,4,5) stack (1,2,3,4,5) –&gt; (2,2,3,4,5) # axis=0  case2 (2,1,3,4,5) stack (2,1,3,4,5) –&gt; (2,2,3,4,5) # axis=1  case3 (2,3,1,4,5) stack (2,3,1,4,5) –&gt; (2,3,2,4,5) # axis=2  case4 (2,3,4,1,5) stack (2,3,4,1,5) –&gt; (2,3,4,2,5) # axis=3  case5 (2,3,4,5,1) stack (2,3,4,5,1) –&gt; (2,3,4,5,2) # axis=4\n\ncase1 (1,2,3,4,5) stack (1,2,3,4,5) –&gt; (2,2,3,4,5) # axis=0\n\ntf.stack([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]]],\n\n\n\n       [[[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase2 (2,1,3,4,5) stack (2,1,3,4,5) –&gt; (2,2,3,4,5) # axis=1\n\ntf.stack([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase3 (2,3,1,4,5) stack (2,3,1,4,5) –&gt; (2,3,2,4,5) # axis=2\n\ntf.stack([a,b],axis=2)\n\n&lt;tf.Tensor: shape=(2, 3, 2, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase4 (2,3,4,1,5) stack (2,3,4,1,5) –&gt; (2,3,4,2,5) # axis=3\n\ntf.stack([a,b],axis=-2).shape\n\nTensorShape([2, 3, 4, 2, 5])\n\n\n\ntf.stack([a,b],axis=-2)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 2, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   0,   -1,   -2,   -3,   -4]],\n\n         [[   5,    6,    7,    8,    9],\n          [  -5,   -6,   -7,   -8,   -9]],\n\n         [[  10,   11,   12,   13,   14],\n          [ -10,  -11,  -12,  -13,  -14]],\n\n         [[  15,   16,   17,   18,   19],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [ -20,  -21,  -22,  -23,  -24]],\n\n         [[  25,   26,   27,   28,   29],\n          [ -25,  -26,  -27,  -28,  -29]],\n\n         [[  30,   31,   32,   33,   34],\n          [ -30,  -31,  -32,  -33,  -34]],\n\n         [[  35,   36,   37,   38,   39],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [ -40,  -41,  -42,  -43,  -44]],\n\n         [[  45,   46,   47,   48,   49],\n          [ -45,  -46,  -47,  -48,  -49]],\n\n         [[  50,   51,   52,   53,   54],\n          [ -50,  -51,  -52,  -53,  -54]],\n\n         [[  55,   56,   57,   58,   59],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [ -60,  -61,  -62,  -63,  -64]],\n\n         [[  65,   66,   67,   68,   69],\n          [ -65,  -66,  -67,  -68,  -69]],\n\n         [[  70,   71,   72,   73,   74],\n          [ -70,  -71,  -72,  -73,  -74]],\n\n         [[  75,   76,   77,   78,   79],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [ -80,  -81,  -82,  -83,  -84]],\n\n         [[  85,   86,   87,   88,   89],\n          [ -85,  -86,  -87,  -88,  -89]],\n\n         [[  90,   91,   92,   93,   94],\n          [ -90,  -91,  -92,  -93,  -94]],\n\n         [[  95,   96,   97,   98,   99],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [-100, -101, -102, -103, -104]],\n\n         [[ 105,  106,  107,  108,  109],\n          [-105, -106, -107, -108, -109]],\n\n         [[ 110,  111,  112,  113,  114],\n          [-110, -111, -112, -113, -114]],\n\n         [[ 115,  116,  117,  118,  119],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt;\n\n\ncase5 (2,3,4,5,1) stack (2,3,4,5,1) –&gt; (2,3,4,5,2) # axis=4\n\ntf.stack([a,b],axis=-1).shape\n\nTensorShape([2, 3, 4, 5, 2])\n\n\n\ntf.stack([a,b],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 5, 2), dtype=int32, numpy=\narray([[[[[   0,    0],\n          [   1,   -1],\n          [   2,   -2],\n          [   3,   -3],\n          [   4,   -4]],\n\n         [[   5,   -5],\n          [   6,   -6],\n          [   7,   -7],\n          [   8,   -8],\n          [   9,   -9]],\n\n         [[  10,  -10],\n          [  11,  -11],\n          [  12,  -12],\n          [  13,  -13],\n          [  14,  -14]],\n\n         [[  15,  -15],\n          [  16,  -16],\n          [  17,  -17],\n          [  18,  -18],\n          [  19,  -19]]],\n\n\n        [[[  20,  -20],\n          [  21,  -21],\n          [  22,  -22],\n          [  23,  -23],\n          [  24,  -24]],\n\n         [[  25,  -25],\n          [  26,  -26],\n          [  27,  -27],\n          [  28,  -28],\n          [  29,  -29]],\n\n         [[  30,  -30],\n          [  31,  -31],\n          [  32,  -32],\n          [  33,  -33],\n          [  34,  -34]],\n\n         [[  35,  -35],\n          [  36,  -36],\n          [  37,  -37],\n          [  38,  -38],\n          [  39,  -39]]],\n\n\n        [[[  40,  -40],\n          [  41,  -41],\n          [  42,  -42],\n          [  43,  -43],\n          [  44,  -44]],\n\n         [[  45,  -45],\n          [  46,  -46],\n          [  47,  -47],\n          [  48,  -48],\n          [  49,  -49]],\n\n         [[  50,  -50],\n          [  51,  -51],\n          [  52,  -52],\n          [  53,  -53],\n          [  54,  -54]],\n\n         [[  55,  -55],\n          [  56,  -56],\n          [  57,  -57],\n          [  58,  -58],\n          [  59,  -59]]]],\n\n\n\n       [[[[  60,  -60],\n          [  61,  -61],\n          [  62,  -62],\n          [  63,  -63],\n          [  64,  -64]],\n\n         [[  65,  -65],\n          [  66,  -66],\n          [  67,  -67],\n          [  68,  -68],\n          [  69,  -69]],\n\n         [[  70,  -70],\n          [  71,  -71],\n          [  72,  -72],\n          [  73,  -73],\n          [  74,  -74]],\n\n         [[  75,  -75],\n          [  76,  -76],\n          [  77,  -77],\n          [  78,  -78],\n          [  79,  -79]]],\n\n\n        [[[  80,  -80],\n          [  81,  -81],\n          [  82,  -82],\n          [  83,  -83],\n          [  84,  -84]],\n\n         [[  85,  -85],\n          [  86,  -86],\n          [  87,  -87],\n          [  88,  -88],\n          [  89,  -89]],\n\n         [[  90,  -90],\n          [  91,  -91],\n          [  92,  -92],\n          [  93,  -93],\n          [  94,  -94]],\n\n         [[  95,  -95],\n          [  96,  -96],\n          [  97,  -97],\n          [  98,  -98],\n          [  99,  -99]]],\n\n\n        [[[ 100, -100],\n          [ 101, -101],\n          [ 102, -102],\n          [ 103, -103],\n          [ 104, -104]],\n\n         [[ 105, -105],\n          [ 106, -106],\n          [ 107, -107],\n          [ 108, -108],\n          [ 109, -109]],\n\n         [[ 110, -110],\n          [ 111, -111],\n          [ 112, -112],\n          [ 113, -113],\n          [ 114, -114]],\n\n         [[ 115, -115],\n          [ 116, -116],\n          [ 117, -117],\n          [ 118, -118],\n          [ 119, -119]]]]], dtype=int32)&gt;\n\n\n- 예제: (2,3,4), (2,3,4), (2,3,4)\n\na= tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb= -a \nc= 2*a\n\n(예시1) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (6,3,4)\n\ntf.concat([a,b,c],axis=0)\n\n&lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)&gt;\n\n\n(예시2) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,9,4)\n\ntf.concat([a,b,c],axis=1)\n\n&lt;tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11],\n        [  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23],\n        [ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)&gt;\n\n\n(예시3) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,12)\n\ntf.concat([a,b,c],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy=\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3,   0,   2,   4,   6],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7,   8,  10,  12,  14],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11,  16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15,  24,  26,  28,  30],\n        [ 16,  17,  18,  19, -16, -17, -18, -19,  32,  34,  36,  38],\n        [ 20,  21,  22,  23, -20, -21, -22, -23,  40,  42,  44,  46]]],\n      dtype=int32)&gt;\n\n\n(예시4) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (3,2,3,4)\n\n축이 늘어난 경우\n\n\ntf.stack([a,b,c],axis=0)\n\n&lt;tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]]],\n\n\n       [[[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]]],\n\n\n       [[[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)&gt;\n\n\n(예시5) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=1).shape\n\nTensorShape([2, 3, 3, 4])\n\n\n\ntf.stack([a,b,c],axis=1)\n\n&lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)&gt;\n\n\n(예시6) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=2)\n\n&lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  0,  -1,  -2,  -3],\n         [  0,   2,   4,   6]],\n\n        [[  4,   5,   6,   7],\n         [ -4,  -5,  -6,  -7],\n         [  8,  10,  12,  14]],\n\n        [[  8,   9,  10,  11],\n         [ -8,  -9, -10, -11],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [-12, -13, -14, -15],\n         [ 24,  26,  28,  30]],\n\n        [[ 16,  17,  18,  19],\n         [-16, -17, -18, -19],\n         [ 32,  34,  36,  38]],\n\n        [[ 20,  21,  22,  23],\n         [-20, -21, -22, -23],\n         [ 40,  42,  44,  46]]]], dtype=int32)&gt;\n\n\n(예시7) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,4,3)\n\ntf.stack([a,b,c],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy=\narray([[[[  0,   0,   0],\n         [  1,  -1,   2],\n         [  2,  -2,   4],\n         [  3,  -3,   6]],\n\n        [[  4,  -4,   8],\n         [  5,  -5,  10],\n         [  6,  -6,  12],\n         [  7,  -7,  14]],\n\n        [[  8,  -8,  16],\n         [  9,  -9,  18],\n         [ 10, -10,  20],\n         [ 11, -11,  22]]],\n\n\n       [[[ 12, -12,  24],\n         [ 13, -13,  26],\n         [ 14, -14,  28],\n         [ 15, -15,  30]],\n\n        [[ 16, -16,  32],\n         [ 17, -17,  34],\n         [ 18, -18,  36],\n         [ 19, -19,  38]],\n\n        [[ 20, -20,  40],\n         [ 21, -21,  42],\n         [ 22, -22,  44],\n         [ 23, -23,  46]]]], dtype=int32)&gt;\n\n\n- 예제: (2,3,4) (4,3,4) \\(\\to\\) (6,3,4)\n\na=tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4))\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[-24, -25, -26, -27],\n        [-28, -29, -30, -31],\n        [-32, -33, -34, -35]],\n\n       [[-36, -37, -38, -39],\n        [-40, -41, -42, -43],\n        [-44, -45, -46, -47]]], dtype=int32)&gt;\n\n\n\ntf.concat([a,b],axis=1) # dimension이 달라 오류!\n\nInvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n\ntf.concat([a,b],axis=2)\n\nInvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n- (2,2) @ (2,) 의 연산?\nnumpy\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88])\n\narray([ 77, -88])\n\n\n\n차원이 안맞는데 계산이 된다?\n\n\nnp.array([77,-88]) @ np.array([[1,0],[0,1]])\n\narray([ 77, -88])\n\n\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88]).reshape(2,1) # dimension 명시\n\narray([[ 77],\n       [-88]])\n\n\n\nnp.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) # 잘못된 걸 명시해주니까 계산 안됨!\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1)\n\n\n\nnp.array([77,-88]).reshape(1,2) @ np.array([[1,0],[0,1]]) \n\narray([[ 77, -88]])\n\n\n–&gt; 요약: numpy에서 길이가 2인 벡터는 매트릭스를 곱할 때 알아서 계산이 되서 결과가 나옴.\ntensorflow\n\nI = tf.constant([[1.0,0.0],[0.0,1.0]]) \nx = tf.constant([77.0,-88.0]) \n\n\nI @ x \n\nInvalidArgumentError: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul]\n\n\n\nx @ I\n\nInvalidArgumentError: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul]\n\n\n\nI @ tf.reshape(x,(2,1))\n\n&lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[ 77.],\n       [-88.]], dtype=float32)&gt;\n\n\n\ntf.reshape(x,(1,2)) @ I \n\n&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)&gt;\n\n\n\n\n\n\ntf.Variable\n되게 쓸모없어 보이는데 쉽고, 중요합니다.\n\n선언\n- tf.Variable()로 선언\n\ntf.Variable([1,2,3,4])\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n\ntf.Variable([1.0,2.0,3.0,4.0])\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt;\n\n\n- tf.constant() 선언후 변환\n\na_ = tf.Variable(tf.constant([1,2,3,4]))\ntype(a_)\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\ntype이 ResourceVariable\n\n\ntf.Variable(tf.constant([1,2,3,4])) # type이렇게 바로 변환 가능\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n- np 등으로 선언후 변환\n\ntf.Variable(np.array([1,2,3,4]))\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int64, numpy=array([1, 2, 3, 4])&gt;\n\n\n\n\n타입\n\ntype(tf.Variable([1,2,3,4]))\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\n\n인덱싱\n\na=tf.Variable([1,2,3,4])\na\n\n&lt;tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n\ntype(a)\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\na[:2] # 처음 2개의 원소.\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;\n\n\n\ntype(a[:2])\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n연산하는 순간 type이 EagerTensor로 바뀜\n\n\n\n연산가능\n\na=tf.Variable([1,2,3,4])\nb=tf.Variable([-1,-2,-3,-4])\n\n\na+b\n\n&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)&gt;\n\n\ntf.Variable로 열심히 만들어도 연산하는 순간 tf.constant로 바뀜.. (자료형이 깨짐)\n\n\ntf.Variable도 쓰기 불편함\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14]) ## 에러\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n\n\n\n\ntnp의 은총도 일부만 가능\n\nimport tensorflow.experimental.numpy as tnp \ntnp.experimental_enable_numpy_behavior() \n\n- 알아서 형 변환\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14])\n\n&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])&gt;\n\n\n- .reshape 메소드\n\ntf.Variable([1,2,3,4]).reshape(2,2)\n\nAttributeError: 'ResourceVariable' object has no attribute 'reshape'\n\n\n\ntf.constant는 되는데 tf.Variable은 또 안됨…\n\n\n\n대부분의 동작은 tf.constant랑 큰 차이를 모르겠음\n- tf.concat\n\na= tf.Variable([[1,2],[3,4]]) \nb= tf.Variable([[-1,-2],[-3,-4]]) \ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1,  2],\n       [ 3,  4],\n       [-1, -2],\n       [-3, -4]], dtype=int32)&gt;\n\n\n- tf.stack\n\na= tf.Variable([[1,2],[3,4]]) \nb= tf.Variable([[-1,-2],[-3,-4]]) \ntf.stack([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\narray([[[ 1,  2],\n        [ 3,  4]],\n\n       [[-1, -2],\n        [-3, -4]]], dtype=int32)&gt;\n\n\n\n이건 비슷한데?\n\n\n\n변수값변경가능(?)\n\na = 1\nid(a)\n\n7585472\n\n\n\na = 456\nid(a)\n\n140177070372624\n\n\n\n새로만드는 것은 되는데 수정은 안됨. (즉, 재할당밖에 안됨. 수정은 안돼)\n근데 가변형으로 만들어 주는것이 좋은데.. (불변형을 사용하려면 메모리가 커야해..)\n그래서 편집 가능한 변수로 선언하는 것이 의미가 있다.\n보통 딥러닝 학습할 때 Data는 RAM에 올리고 파라미터는 GPU에 올린다. (GPU에 올리면 미분계산(선형연산)이 빨라짐)\n\n\na= tf.Variable([1,2,3,4])\nid(a)\n\n140177068844272\n\n\n\na.assign_add([-1,-2,-3,-4]) # 편집기능.\nid(a)\n\n140177068844272\n\n\n\n주소값이 똑같으니까 편집!\n\n\n\n요약\n- tf.Variable()로 만들어야 하는 뚜렷한 차이는 모르겠음.\n- 애써 tf.Variable()로 만들어도 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐.\n\n\n\n미분\n\n모티브\n- 예제: 컴퓨터를 이용하여 \\(x=2\\)에서 \\(y=3x^2\\)의 접선의 기울기를 구해보자.\n(손풀이)\n\\[\\frac{dy}{dx}=6x\\]\n이므로 \\(x=2\\)를 대입하면 12이다.\n(컴퓨터를 이용한 풀이)\n단계1\n\nx1=2 \ny1= 3*x1**2 \n\n\nx2=2+0.000000001\ny2= 3*x2**2\n\n\n(y2-y1)/(x2-x1)\n\n12.0\n\n\n단계2\n\ndef f(x):\n    return(3*x**2)\n\n\nf(3)\n\n27\n\n\n\ndef d(f,x):\n    return (f(x+0.000000001)-f(x))/0.000000001\n\n\nd(f,2)\n\n12.000000992884452\n\n\n단계3\n\nd(lambda x: 3*x**2 ,2)\n\n12.000000992884452\n\n\n\nd(lambda x: x**2 ,0)\n\n1e-09\n\n\n단계4\n\\[f(x,y)= x^2 +3y\\]\n\ndef f(x,y):\n    return(x**2 +3*y)\n\n\nd(f,(2,3))\n\nTypeError: can only concatenate tuple (not \"float\") to tuple\n\n\n\n\ntf.GradientTape() 사용방법\n- 예제1: \\(x=2\\)에서 \\(y=3x^2\\)의 도함수값을 구하라.\n\nx=tf.Variable(2.0) # 미분 기울기를 구하고 싶은 지점을 tf.Variable로 선언\na=tf.constant(3.0) # 숫자로 선언하고 싶은 것.\n\n\ntf.GradientTape()     # Tape: 뭔가 기록할 수 있는 것.\n\n&lt;tensorflow.python.eager.backprop.GradientTape at 0x7f7d84347bb0&gt;\n\n\n\n실행결과 오브젝트로 나왔는데 ,그것이 0x7f7d4c6cfdf0 이 메모리 주소 안에 있다.\n\n\ndir(mytape) # 여기서 __enter__와 __exit__에 주목!\n\n['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__enter__',\n '__eq__',\n '__exit__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_ensure_recording',\n '_persistent',\n '_pop_tape',\n '_push_tape',\n '_recording',\n '_tape',\n '_tf_api_names',\n '_tf_api_names_v1',\n '_watch_accessed_variables',\n '_watched_variables',\n 'batch_jacobian',\n 'gradient',\n 'jacobian',\n 'reset',\n 'stop_recording',\n 'watch',\n 'watched_variables']\n\n\n\nmytape=tf.GradientTape() \nmytape.__enter__() # 기록 시작 \ny=a*x**2 # y=ax^2 = 3x^2 (기록하고 싶은 것)\nmytape.__exit__(None,None,None) # 기록 끝 \n\n\n그럼 mytape에는 뭔가가 기록되어 있을 것이다.\n위의 코드에서 __enter__()와 __exit__()는 고정이라고 생각\n그 사이에는 수식쓰기\n\n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt;\n\n\n\n미분 결과 12\n\n- 예제2: 조금 다른예제\n\nx=tf.Variable(2.0)\n#a=tf.constant(3.0)\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \na=(x/2)*3 ## a=(3/2)x \ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝 \n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt;\n\n\n\n실행은 되는데 결과가 틀림! 왜 18이지?\n\n\\[a=\\frac{3}{2}x\\] \\[y=ax^2=\\frac{3}{2}x^3\\]\n\\[\\frac{dy}{dx}=\\frac{3}{2} 3x^2\\]\n\n3/2*3*4\n\n18.0\n\n\n- 테이프의 개념 (\\(\\star\\))\n(상황)\n우리가 어려운 미분계산을 컴퓨터에게 부탁하는 상황임. (예를들면 \\(y=3x^2\\)) 컴퓨터에게 부탁을 하기 위해서는 연습장(=테이프)에 \\(y=3x^2\\)이라는 수식을 써서 보여줘야하는데 이때 컴퓨터에게 target이 무엇인지 그리고 무엇으로 미분하고 싶은 것인지를 명시해야함.\n\nmytape = tf.GradientTape(): tf.GradientTape()는 연습장을 만드는 명령어, 만들어진 연습장을 mytape라고 이름을 붙인다.\nmytape.__enter__(): 만들어진 공책을 연다 (=기록할수 있는 상태로 만든다)\na=x/2*3; y=a*x**2: 컴퓨터에게 전달할 수식을 쓴다\nmytape.__exit__(None,None,None): 공책을 닫는다.\nmytape.gradient(y,x): \\(y\\)를 \\(x\\)로 미분하라는 메모를 남기고 컴퓨터에게 전달한다.\n\n- 예제3: 연습장을 언제 열고 닫을지 결정하는건 중요하다.\n\nx=tf.Variable(2.0)\na=(x/2)*3 ## a=(3/2)x\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝 \n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt;\n\n\n- 예제4: with문과 함께 쓰는 tf.GradientTape()\n열고 &gt; 쓰고 &gt; 닫고 &gt; 컴퓨터에 전달 –&gt;&gt; 이 과정을 간략하게 매크로화 시키자.\n\nx=tf.Variable(2.0)\na=(x/2)*3 \n\n\nwith tf.GradientTape() as mytape:\n    ## with문 시작 \n    y=a*x**2 \n    ## with문 끝 \n\n\nmytape.gradient(y,x) # y를 x로 미분하라.\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt;\n\n\n(문법해설)\n아래와 같이 쓴다.\nwith expression as myname:\n    ## with문 시작: myname.__enter__() \n    blabla ~ \n    yadiyadi !! \n    ## with문 끝: myname.__exit__()\n\nexpression 의 실행결과 오브젝트가 생성, 생성된 오브젝트는 myname라고 이름붙임. 이 오브젝트는 .__enter__()와 .__exit__()를 숨겨진 기능으로 포함해야 한다.\nwith문이 시작되면서 myname.__enter__()이 실행된다.\n블라블라와 야디야디가 실행된다.\nwith문이 종료되면서 myname.__exit__()이 실행된다.\n\n- 예제5: 예제2를 with문과 함께 구현\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt;\n\n\n- 예제6: persistent = True\n(관찰1)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상 실행해서 에러를 관측하라\n\nRuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n\n\n(관찰2)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상실행해도 에러가 나지않음 \n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt;\n\n\n- 예제7: watch\n(관찰1)\n\nx=tf.constant(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰2)\n\nx=tf.constant(2.0)\nwith tf.GradientTape(persistent=True) as mytape:\n    mytape.watch(x) # 수동감시\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰3)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제 \n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰4)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제\n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰5)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True) as mytape: \n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n- 예제9: 카페예제로 돌아오자.\n- 예제10: 카페예제의 매트릭스 버전\n- 예제11: 위의 예제에서 이론적인 \\(\\boldsymbol{\\beta}\\)의 최적값을 찾아보고 (즉 \\(\\hat{\\boldsymbol{\\beta}}\\)을 찾고) 그곳에서 loss의 미분을 구하라. 구한결과가 \\(\\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}\\) 임을 확인하라."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "",
    "text": "이번주 선형대수 // 다음주 확률 // optimization"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#notations",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#notations",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Notations",
    "text": "Notations\n- Scalar\nSingle number. (real number or natural number)\n- Vector\n숫자들이 순서대로 배열된 1차원 배열.\n- Matrix\n2차원 배열. (벡터를 쌓아놓은 것)\n\\[{\\bf A} = \\begin{bmatrix}A_{1,1} & A_{1,2} \\\\ A_{2,1} & A_{2,2} \\end{bmatrix}\\]\n- Tensor\n3차원 이상의 배열."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#transpose-multiplication",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#transpose-multiplication",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Transpose & Multiplication",
    "text": "Transpose & Multiplication\n- 1. Transpose\n\\({\\bf A}_{i,j}^\\top = {\\bf A}_{j,i}^\\top\\)\n- 2. Addition\n\n\\({\\bf A} + {\\bf B} = {\\bf C}\\) where \\(C_{i,j} = A_{i,j} + B_{i,j}\\)\n\\(a\\cdot {\\bf A} + c = {\\bf G}\\) where \\(G_{i,j} = a\\cdot A_{i,j}+c\\)\n\n\\(\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}=\\begin{bmatrix} a_{11} + b_1 & a_{12} + b_2 \\\\ a_{21} + b_1 & a_{22} + b_2 \\end{bmatrix}\\)\n\ninner product는 차원을 줄이는 것이라면 outer product는 차원을 늘리는 것.\n\n- 3. Multiplication\nMultiplication\nmatrices \\({\\bf A} \\in \\mathbb{R}^{a_1\\times b}\\) and \\({\\bf B} \\in \\mathbb{R}^{b\\times a_2}\\)의 곱은 아래와 같다.\n\\[{\\bf C} = {\\bf A}{\\bf B}\\]\nwhere \\(C_{i,j} = \\sum_{l}A_{i,l}B_{l,j} (i,j) = \\{1,\\dots,a_1\\}\\times \\{1,\\dots, a_2\\}\\).\nHadamard product\nHadamard product (element-wise product) of \\(A \\in \\mathbb{R}^{a\\times b}\\) and \\(B^{a\\times b}\\)\n\\[{\\bf C} = {\\bf A} \\odot {\\bf B}\\]\nwhere \\(C_{i,j} = A_{i,j}B_{i,j}.\\)\n아다마르 곱(또는 요소별 곱셈, Hadamard product)은 다양한 분야에서 사용되며 주로 두 개의 행렬 또는 벡터 간의 요소별 연산을 나타냅니다. 아다마르 곱은 다음과 같은 상황에서 쓰입니다:\n\n신호 처리: 아다마르 곱은 디지털 신호 처리에서 자주 사용됩니다. 예를 들어, 두 시계열 데이터를 아다마르 곱하여 두 신호 간의 상관 관계를 계산하거나 신호를 필터링하는 데 사용될 수 있습니다.\n이미지 처리: 컴퓨터 비전 및 이미지 처리에서 두 이미지나 이미지와 마스크(필터) 사이의 요소별 곱셈은 특정 이미지 처리 작업에 사용됩니다. 예를 들어, 이미지를 선명하게 만들거나 특정 부분을 강조하는 데 유용합니다.\n뉴럴 네트워크: 인공 신경망에서 아다마르 곱은 활성화 함수와 가중치 간의 요소별 연산에 사용됩니다. 이를 통해 네트워크의 비선형성을 증가시키고 특정 기능을 강조할 수 있습니다.\n행렬 연산: 다른 행렬 연산과 결합하여 특정 형태의 계산을 수행할 때 아다마르 곱이 사용될 수 있습니다. 예를 들어, 고유값 분해(Eigendecomposition)와 같은 행렬 분해 기술에서도 사용됩니다.\n요소별 연산: 두 개의 행렬 또는 벡터 사이의 각 요소를 독립적으로 처리하고 싶을 때 아다마르 곱을 사용합니다. 이렇게 하면 각 요소 간의 관계를 보존하면서 연산을 수행할 수 있습니다.\n\n아다마르 곱은 행렬 곱셈과는 다르며, 두 개의 행렬 또는 벡터의 크기가 동일해야 합니다. 따라서 요소별 연산을 수행하려면 같은 크기의 입력이 필요합니다.\n4. Properties of multiplication\n5. Linear equation\n\\({\\bf A}{\\bf x} = {\\bf b}\\) where \\({\\bf A} \\in \\mathbb{R}^{m\\times n}, {\\bf b} \\in \\mathbb{R}^{m\\times 1}\\)"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#identity-and-inverse-matrix",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#identity-and-inverse-matrix",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Identity and Inverse Matrix",
    "text": "Identity and Inverse Matrix\nIdentity matrix: all diagonal terms are one, others are zero such that \\({\\bf I}_n \\in \\mathbb{R}^{n\\times n}\\)\n- 해가 엄청나게 많은 상황\n- 해를 못구하는 상황"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#norms-and-special-kinds-of-matrices-and-vectors",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#norms-and-special-kinds-of-matrices-and-vectors",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Norms and Special Kinds of Matrices and Vectors",
    "text": "Norms and Special Kinds of Matrices and Vectors\n\n1. Norm\n- Definition of \\(L^p\\) norm\n\\[||{\\bf x}||_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}\\]\nNorm function \\(f\\) 는 다음을 만족한다.\n\n\\(f(x) = 0 \\Rightarrow {\\bf x} = 0\\)\n\\(f(x+y) \\leq f(x) + f(y)\\) (triangle inequality)\n\\(\\forall \\alpha \\in \\mathbb{R}, f(\\alpha {\\bf x}) = |\\alpha|f(x)\\)\n\n- \\(L^1\\) and \\(L^0\\) norm are considered to address the above.\n\\[||{\\bf x}||_1 = \\sum_i |x_i|\\]\n\\[||{\\bf x}||_0 = \\sum_i \\mathbb{I} (x_i \\neq 0)\\]\nhigh dimension에서는 \\(L^0\\)나 \\(L^1\\)을 많이 쓴다.\n- \\(L^2\\) norm은 \\(0\\)과 \\(0\\)이 아닌 기계학습 구별에서 너무 큰 값을 제공한다.\n- Other norms such as max norm and Frobenius norm\n\\[||{\\bf x}||_{\\infty} = \\max|x_i|\\]\n\\[||{\\bf A}||_F = \\sqrt{\\sum_{i,j}{\\bf A}^2_{i,j}}\\]\n- Definition of an angle between two vectors.\n\\(\\cos \\theta = \\frac{x^\\top y}{||x|| ||y||}\\)\n\\(w_1(1,0,0) + w_1(0,1,0) + w_3(0,0,1) = (w_1, w_2, w_3)\\)"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#linear-dependence-and-span",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#linear-dependence-and-span",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Linear Dependence and Span",
    "text": "Linear Dependence and Span"
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#decompositions",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#decompositions",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Decompositions",
    "text": "Decompositions\n\n1. Eigendecomposition\n\\[{\\bf A} = {\\bf V}\\text{diag}({\\lambda}){\\bf V}^{-1}\\]\n\n데이터의 correaltion이 강할때 오른쪽 그림과 같다.\n\n\n2. Singular value decomposition\nDecomposition for a non-square matrix \\({\\bf A} \\in \\mathbb{R}^{n\\times m}\\)\n\\[{\\bf A} = {\\bf U}{\\bf D}{\\bf V}^\\top\\]\nwhere \\({\\bf U} \\in \\mathbb{R}^{n\\times n}, {\\bf D} \\in \\mathbb{R}^{n\\times m}\\), and \\({\\bf V}\\in \\mathbb{R}^{m\\times m}\\)."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#the-moore-penrose-pseudoinverse-trace-and-determinant",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#the-moore-penrose-pseudoinverse-trace-and-determinant",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "The Moore-Penrose Pseudoinverse, Trace, and Determinant",
    "text": "The Moore-Penrose Pseudoinverse, Trace, and Determinant\n\nlinear equation의 해가 너무 많아서 못푸는 문제 같은 경우는 대안이 있지 않을까?\n\n변환시킨 경우 \\(n\\)차원 중 어떤 한 축이 0이면 \\(n-1\\) 차원. 그럼 볼륨이 0이된다."
  },
  {
    "objectID": "posts/4_DL2023/2023-09-11-DL-2wk.html#example-pca",
    "href": "posts/4_DL2023/2023-09-11-DL-2wk.html#example-pca",
    "title": "[DL] 2wk. Applied Math and Machine Learning Basics",
    "section": "Example: PCA",
    "text": "Example: PCA\n통계: 분산이 가장 큰 축을 찾는다.\ninput data는 엄청 큰데 작은 차원으로 줄어들면서 다시 커지는 과정을 PCA가 하고 있다. 단, 제약이 있음.\n인코딩 디코딩 개념 알아두자.\n\n\\(f(x)\\): 인코딩 (관측을 하는 큰 차원에서 관측하지 못하는 작은 차원으로)\n\\(g(f(x))\\): 디코딩 (관측하지 못하는 작은 차원에서 관측을 하는 큰차원으로)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html",
    "title": "05wk-2",
    "section": "",
    "text": "훌륭한 시각화, mpg 데이터 소개, plotnine(p9)을 이용한 고차원 산점도"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#애드워드-터프티",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#애드워드-터프티",
    "title": "05wk-2",
    "section": "애드워드 터프티",
    "text": "애드워드 터프티\n- 데이터 시각화계의 거장\n- 터프티의 이론중 백미: 엄격한 미니멀리즘\n\n최소한의 잉크로 많은 정보를 전달할 수 있다면 그것이 바로 좋은 그래프이다.\n작은 지면 내에서 잉크를 최대한 적게 써서 짧은 시간 안에 많은 영감을 주어야 한다.\n\n- 데이터-잉크비: 데이터를 표현하는데 들아가는 잉크의 양 / 그래픽을 인쇄하는데 들어가는 잉크의 총량\n- 차트정크 (나이젤홈즈의 그래프)\n\n\n“Lurking behind chartjunk is contempt both for information and for the audience. Chartjunk promoters imagine that numbers and details are boring, dull, and tedious, requiring ornament to enliven. Cosmetic decoration, which frequently distorts the data, will never salvage an underlying lack of content. If the numbers are boring, then you’ve got the wrong numbers (…) Worse is contempt for our audience, designing as if readers were obtuse and uncaring. In fact, consumers of graphics are often more intelligent about the information at hand than those who fabricate the data decoration (…) The operating moral premise of information design should be that our readers are alert and caring; they may be busy, eager to get on with it, but they are not stupid.”\n\n\n차트정크 = 대중을 멸시 + 데이터에 대한 모독\n차트정크 옹호가는 숫자와 데이터가 지루하여 활기가 필요하다고 생각하는 모양이다..\n\n- 별로인 그래프 (왼쪽) / 우수한 그래프 오른쪽\n\n- 별로인 그래프 (왼쪽) / 우수한 그래프 오른쪽\n\n- 별로인 그래프 (왼쪽) / 우수한 그래프 오른쪽\n\n- 제 생각: 글쎄…"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#찰스미나드의-도표",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#찰스미나드의-도표",
    "title": "05wk-2",
    "section": "찰스미나드의 도표",
    "text": "찰스미나드의 도표\n\n인류역사상 가장 훌륭한 시각화\n\n\n- 터프티의 평\n\n지금까지 그려진 최고의 통계 그래픽일지도 모른다.\n여기에서는 군대의 크기, 2차원 평면상의 위치, 군대의 이동방향, 모스코바에서 퇴각하는 동안의 여러날짜, 온도 \\(\\to\\) 6차원의 변수\n백만번에 한번 이런 그림을 그릴수는 있겠지만 이러한 멋진 그래픽을 만드는 방법에 대한 원칙은 없다. \\(\\to\\) 미니멀리즘..\n\n- 왜 우수한 그래프일까?\n\n자료를 파악하는 기법은 최근까지도 산점도, 막대그래프, 라인플랏에 의존\n이러한 플랏의 단점은 고차원의 자료를 분석하기 어렵다는 것임\n미나드는 여러그램을 그리는 방법 대신에 한 그림에서 패널을 늘리는 방법을 선택함."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#미나드처럼-그리는게-왜-어려운가",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#미나드처럼-그리는게-왜-어려운가",
    "title": "05wk-2",
    "section": "미나드처럼 그리는게 왜 어려운가?",
    "text": "미나드처럼 그리는게 왜 어려운가?\n- 몸무게, 키, 성별, 국적\n\ndf1=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/male1.csv')\ndf2=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/male2.csv')  \ndf3=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/female.csv') \ndf4=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/foreign.csv')\n\n- 미나드의 접근방법\n\n_df = pd.concat([pd.concat([df1,df2],axis=1).assign(g='m'),df3.assign(g='f')])\ndf = pd.concat([_df.assign(g2='korea'),df4.assign(g2='foreign')]).reset_index(drop=True)\ndf\n\n\n\n\n\n\n\n\nw\nh\ng\ng2\n\n\n\n\n0\n72.788217\n183.486773\nm\nkorea\n\n\n1\n66.606430\n173.599877\nm\nkorea\n\n\n2\n69.806324\n173.237903\nm\nkorea\n\n\n3\n67.449439\n173.223805\nm\nkorea\n\n\n4\n70.463183\n174.931946\nm\nkorea\n\n\n...\n...\n...\n...\n...\n\n\n1525\n78.154632\n188.324350\nm\nforeign\n\n\n1526\n74.754308\n183.017979\nf\nforeign\n\n\n1527\n91.196208\n190.100456\nm\nforeign\n\n\n1528\n87.770394\n187.987255\nm\nforeign\n\n\n1529\n88.021995\n193.456798\nm\nforeign\n\n\n\n\n1530 rows × 4 columns\n\n\n\n\nsns.scatterplot(data=df,x='w',y='h',hue='g',style='g2')\n\n&lt;AxesSubplot:xlabel='w', ylabel='h'&gt;\n\n\n\n\n\n- 어려운점: (1) 센스가 없어서 hue/style을 이용하여 그룹을 구분할 생각을 못함 (2) long df (=tidy data) 형태로 데이터를 정리할 생각을 못함 (3) long df 형태로 데이터를 변형하는 코드를 모름\n\n\n기획력부족 -&gt; 훌륭한 시각화를 많이 볼 것\n\n\n데이터프레임에 대한 이해부족 -&gt; tidydata에 대한 개념\n\n\n프로그래밍 능력 -&gt; 코딩공부열심히 (pandas를 엄청 잘해야함)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#방법1-rpy2-코랩-아닌경우-실습금지",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#방법1-rpy2-코랩-아닌경우-실습금지",
    "title": "05wk-2",
    "section": "방법1: rpy2 (코랩 아닌경우 실습금지)",
    "text": "방법1: rpy2 (코랩 아닌경우 실습금지)\n\nimport rpy2\n%load_ext rpy2.ipython\n\n\n%%R \n### 여기는 R처럼 쓸 수 있다. \na &lt;- c(1,2,3) \na+1\n\n[1] 2 3 4\n\n\n\na\n\nNameError: name 'a' is not defined\n\n\n\n%%R \nlibrary(tidyverse)\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# … with 224 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nmpg\n\nNameError: name 'mpg' is not defined\n\n\n\n%R -o mpg # R에 있는 자료가 파이썬으로 넘어옴\n\n\nmpg\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n1\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n2\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n4\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n5\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n234\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#방법2-저장된-csv파일을-통하여-데이터를-확보",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#방법2-저장된-csv파일을-통하여-데이터를-확보",
    "title": "05wk-2",
    "section": "방법2: 저장된 csv파일을 통하여 데이터를 확보",
    "text": "방법2: 저장된 csv파일을 통하여 데이터를 확보\n\nmpg.to_csv(\"mpg.csv\",index=False)\n\n\npd.read_csv(\"mpg.csv\")\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#방법3-github등에-공개된-csv를-읽어오기",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#방법3-github등에-공개된-csv를-읽어오기",
    "title": "05wk-2",
    "section": "방법3: github등에 공개된 csv를 읽어오기",
    "text": "방법3: github등에 공개된 csv를 읽어오기\n\npd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/mpg.csv')\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nvolkswagen\npassat\n2.0\n2008\n4\nauto(s6)\nf\n19\n28\np\nmidsize\n\n\n230\nvolkswagen\npassat\n2.0\n2008\n4\nmanual(m6)\nf\n21\n29\np\nmidsize\n\n\n231\nvolkswagen\npassat\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\nmidsize\n\n\n232\nvolkswagen\npassat\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\nmidsize\n\n\n233\nvolkswagen\npassat\n3.6\n2008\n6\nauto(s6)\nf\n17\n26\np\nmidsize\n\n\n\n\n234 rows × 11 columns\n\n\n\n- 깃허브 저장소에 아예 데이터만 따로 모아서 관리하는 것도 좋은 방법입니다."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#data-설명",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#data-설명",
    "title": "05wk-2",
    "section": "data 설명",
    "text": "data 설명\n- displ: 자동차의 엔진크기\n- hwy: 연료의 효율, 동일한 연료로 얼마나 멀리 가느냐?\n- 자세한 설명은 R에서 ?mpg를 이용해 스스로 찾아볼 것"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#python에서-plotnine을-이용한-산점도",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#python에서-plotnine을-이용한-산점도",
    "title": "05wk-2",
    "section": "python에서: plotnine을 이용한 산점도",
    "text": "python에서: plotnine을 이용한 산점도\n\nggplot(data=mpg) + geom_point(mapping=aes(x='displ',y='hwy')) ## plotnine\n\n\n\n\n\n산점도 해석: 엔진크기가 클수록 효율이 낮음.\n\n- 빠르게 그리기: data=와 mapping=은 생략가능함\n\nggplot(mpg) + geom_point(aes(x='displ',y='hwy')) ## plotnine"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#r에서-ggplot2를-이용한-산점도",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#r에서-ggplot2를-이용한-산점도",
    "title": "05wk-2",
    "section": "R에서: ggplot2를 이용한 산점도",
    "text": "R에서: ggplot2를 이용한 산점도\n- R에서도 거의 똑같은 문법으로 그릴 수 있음 (데이터프레임 혹은 티블에 저장된 column 이름을 사용할때 따옴표만 제거하면 된다!)\n\n%%R -w 800\nggplot(mpg) + geom_point(aes(x=displ,y=hwy)) ## plotnine"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#python에서-객체지향적인-느낌으로-산점도-그리기",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#python에서-객체지향적인-느낌으로-산점도-그리기",
    "title": "05wk-2",
    "section": "python에서: 객체지향적인 느낌으로 산점도 그리기",
    "text": "python에서: 객체지향적인 느낌으로 산점도 그리기\nstep1: 도화지를 준비한다.\n\nfig = ggplot(data=mpg)\nfig\n\n\n\n\nstep2 변수와 에스테틱사이의 맵핑을 설정한다.\n\na1= aes(x='displ',y='hwy')\na1\n\n{'x': 'displ', 'y': 'hwy'}\n\n\nstep3 점들의 집합을 만든다. 즉 포인트 지옴을 만든다.\n\npoint1=geom_point(mapping=a1)\n\n\ngeom_point(): 점들을 그려! 어떻게?\na1에서 설정된 표를 보고\n\nstep4 도화지와 지옴을 합친다.\n\nfig+point1"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경",
    "title": "05wk-2",
    "section": "산점도 + 점크기변경",
    "text": "산점도 + 점크기변경\n\nggplot(data=mpg) + geom_point(mapping = aes(x='displ',y='hwy',size='class'))\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-투명도변경",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-투명도변경",
    "title": "05wk-2",
    "section": "산점도 + 투명도변경",
    "text": "산점도 + 투명도변경\n\nggplot(data=mpg) + geom_point(mapping = aes(x='displ',y='hwy',alpha='class'))\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_alpha.py:70: PlotnineWarning: Using alpha for a discrete variable is not advised."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-투명도점크기를-동시에-적용",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-투명도점크기를-동시에-적용",
    "title": "05wk-2",
    "section": "산점도 + 투명도/점크기를 동시에 적용",
    "text": "산점도 + 투명도/점크기를 동시에 적용\n\nggplot(data=mpg) + geom_point(mapping = aes(x='displ',y='hwy',alpha='class',size='class'))\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_alpha.py:70: PlotnineWarning: Using alpha for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-형태",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-형태",
    "title": "05wk-2",
    "section": "산점도 + 형태",
    "text": "산점도 + 형태\n\nggplot(data=mpg) + geom_point(mapping = aes(x='displ',y='hwy',shape='class'))"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-색깔",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-색깔",
    "title": "05wk-2",
    "section": "산점도 + 색깔",
    "text": "산점도 + 색깔\n\nggplot(data=mpg) + geom_point(mapping = aes(x='displ',y='hwy',color='class'))"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#객체지향적-느낌으로",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#객체지향적-느낌으로",
    "title": "05wk-2",
    "section": "객체지향적 느낌으로?",
    "text": "객체지향적 느낌으로?\n\na2 = aes(x='displ', y='hwy', color='class') \n\n\na1,a2\n\n({'x': 'displ', 'y': 'hwy'}, {'x': 'displ', 'y': 'hwy', 'color': 'class'})\n\n\n\npoint2=geom_point(a2)\n\n\nfig+point2"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-색깔-적합선",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-색깔-적합선",
    "title": "05wk-2",
    "section": "산점도 + 색깔 + 적합선",
    "text": "산점도 + 색깔 + 적합선\n- 일단 색깔이 없는 포인트 지옴부터 연습\n\nfig+point1\n\n\n\n\n\nline1 = geom_smooth(a1)\n\n\nfig+point1+line1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- point1(색깔없는 포인트 지옴)을 point2(색깔있는 포인트 지옴)으로 언제든지 바꿔치기 가능!\n\nfig+point2+line1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 명령어로 한번에 그리기\n\nggplot(data=mpg) + \\\ngeom_point(mapping=aes(x='displ',y='hwy',color='class')) + \\\ngeom_smooth(mapping=aes(x='displ',y='hwy'))\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 공통적인 맵핑규칙은 ggplot()쪽으로 빼기도 한다. (figure를 선언하는 곳에서 공통으로 선언함)\n\nggplot(data=mpg,mapping=aes(x='displ',y='hwy')) + \\\ngeom_point(mapping=aes(color='class')) + \\\ngeom_smooth()\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- R에서는 confidence interval도 geom_smooth()를 이용하여 확인할 수 있다.\n\n%%R -w 800\nggplot(data=mpg,mapping=aes(x=displ,y=hwy)) + geom_point(mapping=aes(color=class)) + geom_smooth()\n\nR[write to console]: `geom_smooth()` using method = 'loess' and formula 'y ~ x'"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔",
    "title": "05wk-2",
    "section": "산점도 + 점크기변경 + 색깔",
    "text": "산점도 + 점크기변경 + 색깔\n- drv (전륜, 후륜, 4륜 구동)에 따라서 데이터를 시각화 하고 싶다.\n\nggplot(data=mpg, mapping=aes(x='displ',y='hwy')) + geom_point(mapping=aes(size='class',color='drv'),alpha=0.3)\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n모든 \\(x\\)에 대하여 붉은색 점들이 대부분 초록색과 보라색 점들에 비하여 아래쪽에 있음 \\(\\to\\) 4륜구동방식이 연비가 좋지 않음"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-객체지향버전",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-객체지향버전",
    "title": "05wk-2",
    "section": "산점도 + 점크기변경 + 색깔 (객체지향버전)",
    "text": "산점도 + 점크기변경 + 색깔 (객체지향버전)\n- 맵핑규칙\n\na1,a2\n\n({'x': 'displ', 'y': 'hwy'}, {'x': 'displ', 'y': 'hwy', 'color': 'class'})\n\n\n\na3 = a2.copy() \n\n\na3['color'] = 'drv'\na3['size'] = 'class'\na3\n\n{'x': 'displ', 'y': 'hwy', 'color': 'drv', 'size': 'class'}\n\n\n\n아래와 같이 선언해도 괜찮음\n\na3= aes(x='displ',y='hwy',color='drv',size='class')\n\npoint3=geom_point(a3)\n\n\nfig+point3\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n그림의 전체적인 투명도를 조절하면 좋겠음\n\n\npoint3=geom_point(a3,alpha=0.2)\nfig+point3\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-선추가",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-선추가",
    "title": "05wk-2",
    "section": "산점도 + 점크기변경 + 색깔 + 선추가",
    "text": "산점도 + 점크기변경 + 색깔 + 선추가\n\nfig+point3+line1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-drv별로-선추가",
    "href": "posts/2_DV2022/2022-10-06-5wk-2.html#산점도-점크기변경-색깔-drv별로-선추가",
    "title": "05wk-2",
    "section": "산점도 + 점크기변경 + 색깔 + drv별로 선추가",
    "text": "산점도 + 점크기변경 + 색깔 + drv별로 선추가\n- 맵핑규칙\n\na1,a2,a3\n\n({'x': 'displ', 'y': 'hwy'},\n {'x': 'displ', 'y': 'hwy', 'color': 'class'},\n {'x': 'displ', 'y': 'hwy', 'color': 'drv', 'size': 'class'})\n\n\n\na4 = a2.copy() \na4['color']='drv'\na4\n\n{'x': 'displ', 'y': 'hwy', 'color': 'drv'}\n\n\n\nline2 = geom_smooth(a4)\n\n\nfig + point3 +line2\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 선의 색깔을 동일하게 하고 선의 타입을 변경하여 drv를 표시하고 싶다면?\n\na1,a2,a3,a4\n\n({'x': 'displ', 'y': 'hwy'},\n {'x': 'displ', 'y': 'hwy', 'color': 'class'},\n {'x': 'displ', 'y': 'hwy', 'color': 'drv', 'size': 'class'},\n {'x': 'displ', 'y': 'hwy', 'color': 'drv'})\n\n\n\na5=a1.copy()\na5['linetype']='drv' \na5\n\n{'x': 'displ', 'y': 'hwy', 'linetype': 'drv'}\n\n\n\nline3 = geom_smooth(a5,size=0.5,color='gray')\n\n\nfig+point3+line3\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 전체적인 추세선도 추가하고 싶다면?\n\nfig+point3+line3+line1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 그려보니까 역시 drv별로 그려지는 추세선은 색깔별로 구분하는게 좋겠음.\n\nline2 = geom_smooth(a4,size=0.5,linetype='dashed')\nfig+point3+line2+line1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/scales/scale_size.py:50: PlotnineWarning: Using size for a discrete variable is not advised.\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n- 고차원을 변수를 표현할 수 있는 무기는 다양하다.\n\n산점도(포인트지옴): 점의크기, 점의형태, 점의색깔, 점의투명도\n라인플랏(스무스지옴,라인지옴): 선의형태, 선의색깔, 선의굵기"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html",
    "title": "05wk-1",
    "section": "",
    "text": "seaborn(2)–scatterplot, mpl미세먼지팁(2)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#plt-복습",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#plt-복습",
    "title": "05wk-1",
    "section": "plt 복습",
    "text": "plt 복습\n\nplt.plot(x1,y1,'o')\nplt.plot(x2,y2,'o')"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-array",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-array",
    "title": "05wk-1",
    "section": "sns: array",
    "text": "sns: array\n\nsns.scatterplot(data=None,x=x1,y=y1)\nsns.scatterplot(data=None,x=x2,y=y2)\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-wide-df",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-wide-df",
    "title": "05wk-1",
    "section": "sns: wide df",
    "text": "sns: wide df\n\nsns.scatterplot(data=pd.DataFrame({'x':x1,'y':y1}),x='x',y='y')\nsns.scatterplot(data=pd.DataFrame({'x':x2,'y':y2}),x='x',y='y')\n#sns.scatterplot(data=None,x=x2,y=y2)\n\n&lt;AxesSubplot:xlabel='x', ylabel='y'&gt;\n\n\n\n\n\n\n억지로 그리긴 했는데 이 경우는 wide하게 만든 df는 별로 경쟁력이 없음"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-long-df",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#sns-long-df",
    "title": "05wk-1",
    "section": "sns: long df",
    "text": "sns: long df\n\nx= np.concatenate([x1,x2])\ny= np.concatenate([y1,y2])\ncat = ['x1']*len(x1) + ['x2']*len(x2)\ndf2 = pd.DataFrame({'x':x,'y':y,'cat':cat})\ndf2\n\n\n\n\n\n\n\n\nx\ny\ncat\n\n\n\n\n0\n2.023919\n-0.400176\nx1\n\n\n1\n1.229622\n-1.763752\nx1\n\n\n2\n-0.413211\n2.293004\nx1\n\n\n3\n-1.343073\n0.404232\nx1\n\n\n4\n1.062845\n0.030775\nx1\n\n\n...\n...\n...\n...\n\n\n1995\n2.226805\n3.683857\nx2\n\n\n1996\n2.768263\n2.678292\nx2\n\n\n1997\n2.525295\n2.815478\nx2\n\n\n1998\n1.750193\n2.289812\nx2\n\n\n1999\n1.153290\n2.095922\nx2\n\n\n\n\n2000 rows × 3 columns\n\n\n\n\nsns.scatterplot(data=df2,x='x',y='y',hue='cat') \n\n&lt;AxesSubplot:xlabel='x', ylabel='y'&gt;"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#예제1",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#예제1",
    "title": "05wk-1",
    "section": "예제1",
    "text": "예제1\n\nfig,ax = plt.subplots(1,3,figsize=(12,4))\nax[0].plot([1,2,4,3],'--o')\nsns.scatterplot(x=x1,y=y1,ax=ax[1])\nsns.scatterplot(x=x1,y=y1,ax=ax[2])\nsns.scatterplot(x=x2,y=y2,ax=ax[2])\nax[2].plot([1,2,4,3],'-r',lw=5)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#예제2",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#예제2",
    "title": "05wk-1",
    "section": "예제2",
    "text": "예제2\n\nimport cv2\n\n\n!wget https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg \nimg = cv2.imread('Unequalized_Hawkes_Bay_NZ.jpg',0)\n!rm Unequalized_Hawkes_Bay_NZ.jpg \n\n--2022-10-05 16:33:56--  https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\nResolving upload.wikimedia.org (upload.wikimedia.org)... 103.102.166.240, 2001:df2:e500:ed1a::2:b\nConnecting to upload.wikimedia.org (upload.wikimedia.org)|103.102.166.240|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 110895 (108K) [image/jpeg]\nSaving to: ‘Unequalized_Hawkes_Bay_NZ.jpg’\n\nUnequalized_Hawkes_ 100%[===================&gt;] 108.30K   548KB/s    in 0.2s    \n\n2022-10-05 16:33:57 (548 KB/s) - ‘Unequalized_Hawkes_Bay_NZ.jpg’ saved [110895/110895]\n\n\n\n\nimg2 = cv2.equalizeHist(img)\n\n\nimg.reshape(-1)\n\narray([127, 145, 149, ..., 146, 145, 144], dtype=uint8)\n\n\n\nfig,ax = plt.subplots(2,2,figsize=(10,5))\nax[0,0].imshow(img,vmin=0,vmax=255,cmap='gray')\nsns.histplot(img.reshape(-1),ax=ax[0,1],bins=15,lw=0,kde=True,color='C1')\nax[0,1].set_xlim(0,255)\nax[1,0].imshow(img2,vmin=0,vmax=255,cmap='gray')\nsns.histplot(img2.reshape(-1),ax=ax[1,1],bins=15,lw=0,kde=True,color='C1')\n\n&lt;AxesSubplot:ylabel='Count'&gt;\n\n\n\n\n\n- seaborn: figure-level vs axes-level 의 개념\nref: https://seaborn.pydata.org/tutorial/function_overview.html#figure-level-vs-axes-level-functions"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#축-간격조정",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#축-간격조정",
    "title": "05wk-1",
    "section": "축 간격조정",
    "text": "축 간격조정\n\nimport matplotlib as mpl\n\n\nfig, ax = plt.subplots()\nax.plot([(xi/30)**2 for xi in range(30)],'--o')\nax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(3)) # 큰 눈금간격을 3으로\nax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(1)) # 작은 눈금간격을 1로"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#축-삭제",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#축-삭제",
    "title": "05wk-1",
    "section": "축 삭제",
    "text": "축 삭제\n\nfig, ax = plt.subplots()\nax.plot([(xi/30)**2 for xi in range(30)],'--o')\nax.xaxis.set_major_locator(mpl.ticker.NullLocator()) # x축 눈금삭제\nax.yaxis.set_major_locator(mpl.ticker.NullLocator()) # y축 눈금삭제"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#축-범위조정",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#축-범위조정",
    "title": "05wk-1",
    "section": "축 범위조정",
    "text": "축 범위조정\n\nfig, ax = plt.subplots()\nax.plot([(xi/30)**2 for xi in range(30)],'--o')\nax.set_ylim(-1,2) \nax.set_xlim(-5,35)\n#plt.ylim(-1,2)\n#plt.xlim(-5,35)\n\n(-5.0, 35.0)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-03-5wk-1.html#gcf-gca",
    "href": "posts/2_DV2022/2022-10-03-5wk-1.html#gcf-gca",
    "title": "05wk-1",
    "section": "gcf, gca",
    "text": "gcf, gca\n- gcf\n\nplt.plot([1,2,3,2])\nfig = plt.gcf()\n\n\n\n\n\nfig.suptitle('suptitle')\n\nText(0.5, 0.98, 'suptitle')\n\n\n\nfig\n\n\n\n\n- gca\n\nfig\n\n\n\n\n\nax = fig.gca()\n\n\nax.set_title('title') \nfig"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-19-7wk-2.html",
    "href": "posts/2_DV2022/2022-10-19-7wk-2.html",
    "title": "07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)",
    "section": "",
    "text": "아이스크림을 많이 먹으면 걸리는 병(2)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-19-7wk-2.html#자료생성-좀-더-그럴듯한-자료-만들기",
    "href": "posts/2_DV2022/2022-10-19-7wk-2.html#자료생성-좀-더-그럴듯한-자료-만들기",
    "title": "07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)",
    "section": "자료생성: 좀 더 그럴듯한 자료 (만들기)",
    "text": "자료생성: 좀 더 그럴듯한 자료 (만들기)\n- 지난 시간의 toy example은 데이터가 너무 작아서 억지스러움 \\(\\to\\) 기상자료개방포털, 회원가입해야 자료받을 수 있음.\n\n_df=pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv')\n_df\n\n\n\n\n\n\n\n\n지점번호\n지점명\n일시\n평균기온(℃)\n최고기온(℃)\n최고기온시각\n최저기온(℃)\n\n\n\n\n0\n146\n전주\n2020-01-01\n-0.5\n4.3\n15:09\n-6.4\n\n\n1\n146\n전주\n2020-01-02\n1.4\n6.5\n14:12\n-3.0\n\n\n2\n146\n전주\n2020-01-03\n2.6\n7.6\n13:32\n-0.5\n\n\n3\n146\n전주\n2020-01-04\n2.0\n7.7\n13:51\n-2.6\n\n\n4\n146\n전주\n2020-01-05\n2.5\n8.6\n14:05\n-3.2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n651\n146\n전주\n2021-10-13\n19.9\n25.5\n14:29\n15.6\n\n\n652\n146\n전주\n2021-10-14\n20.4\n25.5\n13:36\n17.0\n\n\n653\n146\n전주\n2021-10-15\n18.3\n22.0\n13:47\n15.7\n\n\n654\n146\n전주\n2021-10-16\n12.8\n17.4\n0:01\n6.5\n\n\n655\n146\n전주\n2021-10-17\n6.7\n12.4\n15:18\n2.2\n\n\n\n\n656 rows × 7 columns\n\n\n\n- 평균기온만 선택\n\npd.Series(_df.columns)\n\n0       지점번호\n1        지점명\n2         일시\n3    평균기온(℃)\n4    최고기온(℃)\n5     최고기온시각\n6    최저기온(℃)\ndtype: object\n\n\n\ntemp = np.array(_df.iloc[:,3])\ntemp[:5]\n\narray([-0.5,  1.4,  2.6,  2. ,  2.5])\n\n\n\n# 숨은진짜상황1: 온도 \\(\\to\\) 아이스크림 판매량\n- 아래와 같은 관계가 있다고 하자.\n\\[\\text{아이스크림 판매량} = 20 + 2 \\times \\text{온도} + \\epsilon\\]\n\nnp.random.seed(1)\neps = np.random.normal(size=len(temp), scale=10) \nicecream = 20 + 2*temp + eps\n\n\nplt.plot(temp,icecream,'o',alpha=0.3)\nplt.xlabel(\"temp\",size=15)\nplt.ylabel(\"icecream\",size=15)\n\nText(0, 0.5, 'icecream')\n\n\n\n\n\n\n\n# 숨은진짜상황1: 온도 \\(\\to\\) 아이스크림 판매량\n- 아래와 같은 관계가 있다고 하자.\n\\[\\text{소아마비 반응수치} = 30 + 0.5 \\times \\text{온도} + \\epsilon\\]\n\nnp.random.seed(2) \neps=np.random.normal(size=len(temp),scale=1)\ndisease= 30 + 0.5 * temp + eps\n\n\nplt.plot(temp,disease,'o',alpha=0.3)\nplt.xlabel(\"temp\",size=15)\nplt.ylabel(\"disease\",size=15)\n\nText(0, 0.5, 'disease')\n\n\n\n\n\n\n\n# 우리가 관측한 상황 (온도는 은닉되어있음)\n\nplt.plot(icecream,disease,'o',alpha=0.3)\nplt.xlabel(\"icecream\",size=15)\nplt.ylabel(\"disease\",size=15)\n\nText(0, 0.5, 'disease')\n\n\n\n\n\n\nnp.corrcoef(icecream,disease)\n\narray([[1.        , 0.86298975],\n       [0.86298975, 1.        ]])\n\n\n\n0.86정도.."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-19-7wk-2.html#직관-여름만-뽑아서-plot-해보자.",
    "href": "posts/2_DV2022/2022-10-19-7wk-2.html#직관-여름만-뽑아서-plot-해보자.",
    "title": "07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)",
    "section": "직관: 여름만 뽑아서 plot 해보자.",
    "text": "직관: 여름만 뽑아서 plot 해보자.\n- temp&gt;25 (여름으로 간주) 인 관측치만 플랏\n\nplt.plot(icecream[temp&gt;25],disease[temp&gt;25], 'o', color='C1') ## 평균기온이 25도가 넘어가면 여름이라 생각 \n\n\n\n\n- 전체적인 산점도\n\nfig , ((ax1,ax2), (ax3,ax4)) = plt.subplots(2,2,figsize=(8,6)) \nax1.plot(temp,icecream,'o',alpha=0.2); ax1.set_xlabel('temp'); ax1.set_ylabel('icecream'); ax1.set_title(\"hidden1\")\nax2.plot(temp,disease,'o',alpha=0.2); ax2.set_xlabel('temp'); ax2.set_ylabel('disease'); ax2.set_title(\"hidden2\")\nax3.plot(icecream,disease,'o',alpha=0.2); ax3.set_xlabel('icecream'); ax3.set_ylabel('disease'); ax3.set_title(\"observed\")\nax4.plot(icecream,disease,'o',alpha=0.2); ax4.set_xlabel('icecream'); ax4.set_ylabel('disease'); ax4.set_title(\"observed\")\nax4.plot(icecream[temp&gt;25],disease[temp&gt;25],'o',label='temp&gt;25')\nax4.legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-19-7wk-2.html#ggplot-온도구간을-세분화-하여-시각화",
    "href": "posts/2_DV2022/2022-10-19-7wk-2.html#ggplot-온도구간을-세분화-하여-시각화",
    "title": "07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)",
    "section": "ggplot: 온도구간을 세분화 하여 시각화",
    "text": "ggplot: 온도구간을 세분화 하여 시각화\n- 목표: 모든 온도구간에 대하여 각각 색을 다르게 하여 그려보자.\n\n사실 지금 변수는 온도, 아이스크림판매량, 소아마비\n온도가 유사한 지역을 색으로 묶으면 3차원 플랏이 가능함\n\n\n# df로 자료정리\n- 일단 데이터 프레임을 정리하자.\n\ndf = pd.DataFrame({'temp':temp,'icecream':icecream,'disease':disease})\ndf\n\n\n\n\n\n\n\n\ntemp\nicecream\ndisease\n\n\n\n\n0\n-0.5\n35.243454\n29.333242\n\n\n1\n1.4\n16.682436\n30.643733\n\n\n2\n2.6\n19.918282\n29.163804\n\n\n3\n2.0\n13.270314\n32.640271\n\n\n4\n2.5\n33.654076\n29.456564\n\n\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.633906\n\n\n652\n20.4\n76.554679\n38.920443\n\n\n653\n18.3\n68.666079\n39.882650\n\n\n654\n12.8\n42.771364\n36.613159\n\n\n655\n6.7\n30.736731\n34.902513\n\n\n\n\n656 rows × 3 columns\n\n\n\n\n\n# 구간세분화\n- 온도를 카테고리화 하자 \\(\\to\\) 적당한 구긴을 설정하기 위해서 히스토그램을 그려보자.\n\ndf.temp.hist() # ? 이거 14주차쯤 배우는데 미리 스포합니다.. 엄청 편해요 \n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nplt.hist(df.temp) # 원래는 이걸 배웠죠\n\n(array([  3.,   9.,  29.,  60.,  92.,  86.,  65.,  93., 139.,  80.]),\n array([-12.4 ,  -8.16,  -3.92,   0.32,   4.56,   8.8 ,  13.04,  17.28,\n         21.52,  25.76,  30.  ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n- 구간은 5정도로 하면 적당할 것 같다.\n\ndef cut(x): # 이거보다 더 좋은 방법이 있을 것 같긴 한데요..\n    if x&lt;0: \n        y='Temp: &lt;0'\n    elif x&lt;5: \n        y='Temp: 0~5'\n    elif x&lt;10: \n        y='Temp: 5~10'\n    elif x&lt;15: \n        y='Temp: 10~15'\n    elif x&lt;20:\n        y='Temp: 15~20'\n    elif x&lt;25: \n        y='Temp: 20~25'\n    else: \n        y='Temp: &gt;30'\n    return y \n\n\ndf.assign(temp2 = list(map(cut,df.temp)))\n\n\n\n\n\n\n\n\ntemp\nicecream\ndisease\ntemp2\n\n\n\n\n0\n-0.5\n35.243454\n29.333242\nTemp: &lt;0\n\n\n1\n1.4\n16.682436\n30.643733\nTemp: 0~5\n\n\n2\n2.6\n19.918282\n29.163804\nTemp: 0~5\n\n\n3\n2.0\n13.270314\n32.640271\nTemp: 0~5\n\n\n4\n2.5\n33.654076\n29.456564\nTemp: 0~5\n\n\n...\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.633906\nTemp: 15~20\n\n\n652\n20.4\n76.554679\n38.920443\nTemp: 20~25\n\n\n653\n18.3\n68.666079\n39.882650\nTemp: 15~20\n\n\n654\n12.8\n42.771364\n36.613159\nTemp: 10~15\n\n\n655\n6.7\n30.736731\n34.902513\nTemp: 5~10\n\n\n\n\n656 rows × 4 columns\n\n\n\n\n\n# ggplot\n- 온도를 색으로 구분하면\n\nfig = ggplot(data=df.assign(temp2 = list(map(cut,df.temp))))\np1 = geom_point(aes(x='icecream',y='disease',colour='temp2'),alpha=0.5)\nfig + p1\n\n\n\n\n- 추세선을 추가하면\n\nl1 = geom_smooth(aes(x='icecream',y='disease',colour='temp2'))\n\n\nfig+p1+l1\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n\n각 온도별로 추세선은 거의 기울기가 0이다. \\(\\to\\) 온도가 비슷한 구간별로 묶어서 보니까 상관관계가 없다는 거!\n아이스크림 판매량과 소아마비의 corr은 유의미해보이지만, 온도를 통제하였을 경우 아이스크림 판매량과 소아마비의 partial corr은 유의미해보이지 않음.\n\n\n\n# 해석\n- 해피앤딩: 온도를 통제하니까 아이스크림과 질병은 관련이 없어보인다. \\(\\to\\) 아이스크림을 먹으면 소아마비를 유발한다는 이상한 결론이 나올뻔 했지만 우리는 온도라는 흑막을 잘 찾았고 결과적으로 “온도-&gt;아이스크림판매량,소아마비” 이라는 합리적인 진리를 얻을 수 있었다.\n\n온도와 같은 변수를 은닉변수라고 한다.\n\n- 또 다른 흑막? 고려할 흑막이 온도뿐이라는 보장이 어디있지? 사실 흑막2, 흑막3이 있어서 그런 흑막들을 고려하다보니까 아이스크림과 소아마비사이의 상관관계가 다시 보이면 어떡하지?\n\n이러한 이유 때문에 상관계수로 인과성을 유추하는건 사실상 불가능.\n그런데 이론적으로는 “세상의 모든 은닉변수를 통제하였을 경우에도 corr(X,Y)의 값이 1에 가깝다면 그때는 인과성이 있다고 봐도 무방함, (물론 이 경우에도 무엇이 원인인지는 통계적으로 따지는것이 불가)” 이라고 주장할 수 있다. 즉 모든 흑막을 제거한다면 “상관성=인과성”이다.\n\n- 실험계획법, 인과추론: 세상의 모든 흑막을 제거하는건 상식적으로 불가능\n\n피셔의주장(실험계획법): 그런데 실험계획을 잘하면 흑막을 제거한 효과가 있음 (무작위로 사람뽑아서 담배를 피우게 한다든가)\n인과추론: 실험계획이 사실상 불가능한 경우가 있음 \\(\\to\\) 모인 데이터에서 최대한 흑막2,3,4,.. 등이 비슷한 그룹끼리 “매칭”을 시킨다!"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-19-7wk-2.html#그냥-궁금해서-진짜-만약에-아이스크림과-소아마비가-관련있는-경우라면",
    "href": "posts/2_DV2022/2022-10-19-7wk-2.html#그냥-궁금해서-진짜-만약에-아이스크림과-소아마비가-관련있는-경우라면",
    "title": "07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)",
    "section": "그냥 궁금해서: 진짜 만약에 아이스크림과 소아마비가 관련있는 경우라면?",
    "text": "그냥 궁금해서: 진짜 만약에 아이스크림과 소아마비가 관련있는 경우라면?\n- 온도는 아이스크림 판매에 여전히 영향을 주지만\n\\[\\text{아이스크림 판매량} = 20 + 2 \\times \\text{온도} + \\epsilon\\]\n\nnp.random.seed(1)\neps=np.random.normal(size=len(temp), scale=10) \nicecream = 20 + 2 * temp + eps \n\n- 수영장이 원인이 아니라 진짜 아이스크림을 먹고 소아마비에 걸린상황이라면?\n\\[\\text{소아마비 반응수치} = 30 + 0 \\times \\text{온도} + 0.15 \\times \\text{아이스크림 판매량} + \\epsilon\\]\n\nnp.random.seed(2) \neps = np.random.normal(size=len(temp),scale=2)\ndisease= 30+ 0*temp + 0.15*icecream + eps\n\n\ndf2=pd.DataFrame({'temp':temp,'icecream':icecream,'disease':disease})\ndf2.assign(temp2=list(map(cut,df2.temp)))\n\n\n\n\n\n\n\n\ntemp\nicecream\ndisease\ntemp2\n\n\n\n\n0\n-0.5\n35.243454\n34.453002\nTemp: &lt;0\n\n\n1\n1.4\n16.682436\n32.389832\nTemp: 0~5\n\n\n2\n2.6\n19.918282\n28.715350\nTemp: 0~5\n\n\n3\n2.0\n13.270314\n35.271089\nTemp: 0~5\n\n\n4\n2.5\n33.654076\n31.461240\nTemp: 0~5\n\n\n...\n...\n...\n...\n...\n\n\n651\n19.9\n68.839992\n39.693811\nTemp: 15~20\n\n\n652\n20.4\n76.554679\n38.924088\nTemp: 20~25\n\n\n653\n18.3\n68.666079\n41.765212\nTemp: 15~20\n\n\n654\n12.8\n42.771364\n36.842022\nTemp: 10~15\n\n\n655\n6.7\n30.736731\n37.715537\nTemp: 5~10\n\n\n\n\n656 rows × 4 columns\n\n\n\n\nggplot(data=df2.assign(temp2=list(map(cut,df2.temp))))+\\\ngeom_point(aes(x='icecream',y='disease',colour='temp2'),alpha=0.2)+\\\ngeom_smooth(aes(x='icecream',y='disease',colour='temp2'))\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/plotnine/stats/smoothers.py:311: PlotnineWarning: Confidence intervals are not yet implementedfor lowess smoothings.\n\n\n\n\n\n\n이번엔 partial corr도 유의미하게 나옴\n\n- 단순 corr을 봐서는 “온도-&gt;아이스크림,소아마비” 인지, “온도-&gt;아이스크림-&gt;소아마비” 인지 알기 어렵다.\n\ndf.corr()\n\n\n\n\n\n\n\n\ntemp\nicecream\ndisease\n\n\n\n\ntemp\n1.000000\n0.884366\n0.975609\n\n\nicecream\n0.884366\n1.000000\n0.862990\n\n\ndisease\n0.975609\n0.862990\n1.000000\n\n\n\n\n\n\n\n\ndf2.corr()\n\n\n\n\n\n\n\n\ntemp\nicecream\ndisease\n\n\n\n\ntemp\n1.000000\n0.884366\n0.725505\n\n\nicecream\n0.884366\n1.000000\n0.830539\n\n\ndisease\n0.725505\n0.830539\n1.000000"
  },
  {
    "objectID": "2_dv2022.html",
    "href": "2_dv2022.html",
    "title": "DV2022",
    "section": "",
    "text": "This page is organized based on the contents of the Data Visualization (2022-2) and lecture notes of Professor Guebin Choi of Jeonbuk National University."
  },
  {
    "objectID": "2_dv2022.html#contents",
    "href": "2_dv2022.html#contents",
    "title": "DV2022",
    "section": "Contents",
    "text": "Contents\n1. 시각화 차트 소개\n\nboxplot, histogram, lineplot, scatterplot\n\n2. 파이썬 데이터 시각화 패키지 사용법\n\nmatplotlib, seaborn, plotnine/ggplot2\n\n3. 데이터 시각화와 통계적 해석\n\n히스토그램 이퀄라이제이션\n표본상관계수, 앤스콤의 플랏, 무상관, 무상관과 독립\n\n4. Data Wrangling\n\nlambda, map\npandas: indexing\n\n5. 인포그래픽과 데이터시각화\n\n나이젤홈즈와 애드워드터프티, 찰스미나드의 도표"
  },
  {
    "objectID": "4_dl2023.html",
    "href": "4_dl2023.html",
    "title": "DL2023",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 11, 2023\n\n\n[DL] 2wk. Applied Math and Machine Learning Basics\n\n\nJiyunLim \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "3_stbda2022.html",
    "href": "3_stbda2022.html",
    "title": "STBDA2022",
    "section": "",
    "text": "This page is organized based on the contents of the Big Data Analysis Special Lecture (2022-1) and lecture notes of Professor Guebin Choi of Jeonbuk National University.\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 11, 2023\n\n\n[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들\n\n\nJiyunLim \n\n\n\n\nMay 30, 2023\n\n\n[STBDA] 11wk. MaxPool2D, Conv2D\n\n\nJiyunLim \n\n\n\n\nMay 28, 2023\n\n\n[STBDA] 10wk. Softmax / 다양한 평가지표\n\n\nJiyunLim \n\n\n\n\nMay 26, 2023\n\n\n[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법\n\n\nJiyunLim \n\n\n\n\nMay 24, 2023\n\n\n[STBDA] 9wk. Likelihood function\n\n\nJiyunLim \n\n\n\n\nMay 22, 2023\n\n\n[STBDA] 중간고사\n\n\nJiyunLim \n\n\n\n\nMay 20, 2023\n\n\n[STBDA] 7wk. Piece-wise LR / Logistic Regression\n\n\nJiyunLim \n\n\n\n\nMay 18, 2023\n\n\n[STBDA] 6wk. 회귀모형 적합 with keras\n\n\nJiyunLim \n\n\n\n\nMay 16, 2023\n\n\n[STBDA] 5wk. optimizer를 이용한 최적화\n\n\nJiyunLim \n\n\n\n\nMay 14, 2023\n\n\n[STBDA] 4wk. 미분 / 경사하강법\n\n\nJiyunLim \n\n\n\n\nMay 12, 2023\n\n\n[STBDA] 3wk. 텐서플로우 intro2 (tf.GradientTape())\n\n\nJiyunLim \n\n\n\n\nMay 10, 2023\n\n\n[STBDA] 2wk. 텐서플로우 intro1 (tf.constant선언, tnp사용법)\n\n\nJiyunLim \n\n\n\n\nMay 8, 2023\n\n\n[STBDA] 1wk. 강의소개 및 단순선형회귀\n\n\nJiyunLim \n\n\n\n\nJan 1, 2023\n\n\nJupyter\n\n\nJiyunLim \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog ~~~"
  },
  {
    "objectID": "posts/2_DV2022/2022-11-09-10wk-2.html",
    "href": "posts/2_DV2022/2022-11-09-10wk-2.html",
    "title": "10wk-2 심슨의 역설",
    "section": "",
    "text": "심슨의 역설을 bar plot으로 시각화하는 방법과 왜 생기게 되는지에 대해 알아보자."
  },
  {
    "objectID": "posts/2_DV2022/2022-11-09-10wk-2.html#버클리대학교의-입학데이터",
    "href": "posts/2_DV2022/2022-11-09-10wk-2.html#버클리대학교의-입학데이터",
    "title": "10wk-2 심슨의 역설",
    "section": "버클리대학교의 입학데이터",
    "text": "버클리대학교의 입학데이터\n\nhttps://github.com/pinkocto/Quarto-Blog/blob/main/posts/DV/ds.pdf\n\n- 주장: 버클리대학에 gender bias가 존재한다.\n\n1973년 가을학기의 입학통계에 따르면 지원하는 남성이 여성보다 훨씬 많이 합격했고, 그 차이가 너무 커서 우연의 일치라 보기 어렵다.\n\n\ndf=pd.read_csv(\"https://raw.githubusercontent.com/guebin/DV2022/master/posts/Simpson.csv\",index_col=0,header=[0,1])\\\n.stack().stack().reset_index()\\\n.rename({'level_0':'department','level_1':'result','level_2':'gender',0:'count'},axis=1)\ndf\n\n\n\n\n\n\n\n\ndepartment\nresult\ngender\ncount\n\n\n\n\n0\nA\nfail\nfemale\n19\n\n\n1\nA\nfail\nmale\n314\n\n\n2\nA\npass\nfemale\n89\n\n\n3\nA\npass\nmale\n511\n\n\n4\nB\nfail\nfemale\n7\n\n\n5\nB\nfail\nmale\n208\n\n\n6\nB\npass\nfemale\n18\n\n\n7\nB\npass\nmale\n352\n\n\n8\nC\nfail\nfemale\n391\n\n\n9\nC\nfail\nmale\n204\n\n\n10\nC\npass\nfemale\n202\n\n\n11\nC\npass\nmale\n121\n\n\n12\nD\nfail\nfemale\n244\n\n\n13\nD\nfail\nmale\n279\n\n\n14\nD\npass\nfemale\n131\n\n\n15\nD\npass\nmale\n138\n\n\n16\nE\nfail\nfemale\n299\n\n\n17\nE\nfail\nmale\n137\n\n\n18\nE\npass\nfemale\n94\n\n\n19\nE\npass\nmale\n54\n\n\n20\nF\nfail\nfemale\n103\n\n\n21\nF\nfail\nmale\n149\n\n\n22\nF\npass\nfemale\n238\n\n\n23\nF\npass\nmale\n224"
  },
  {
    "objectID": "posts/2_DV2022/2022-11-09-10wk-2.html#시각화1-전체합격률",
    "href": "posts/2_DV2022/2022-11-09-10wk-2.html#시각화1-전체합격률",
    "title": "10wk-2 심슨의 역설",
    "section": "시각화1: 전체합격률",
    "text": "시각화1: 전체합격률\n- df1\n\ndf.groupby(['gender','result']).agg({'count':np.sum}).reset_index()\n\n\n\n\n\n\n\n\ngender\nresult\ncount\n\n\n\n\n0\nfemale\nfail\n1063\n\n\n1\nfemale\npass\n772\n\n\n2\nmale\nfail\n1291\n\n\n3\nmale\npass\n1400\n\n\n\n\n\n\n\n- df2\n\n# df.query('gender ==\"female\"')\n\n\ndf.groupby('gender').agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1)\n\n\n\n\n\n\n\n\ngender\ncount2\n\n\n\n\n0\nfemale\n1835\n\n\n1\nmale\n2691\n\n\n\n\n\n\n\n- merge: 두개의 데이터프레임을 합친다\n\ndf.groupby(['gender','result']).agg({'count':np.sum}).reset_index()\\\n.merge(df.groupby('gender').agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1))\n\n\n\n\n\n\n\n\ngender\nresult\ncount\ncount2\n\n\n\n\n0\nfemale\nfail\n1063\n1835\n\n\n1\nfemale\npass\n772\n1835\n\n\n2\nmale\nfail\n1291\n2691\n\n\n3\nmale\npass\n1400\n2691\n\n\n\n\n\n\n\n- 비율계산\n\ndf.groupby(['gender','result']).agg({'count':np.sum}).reset_index()\\\n.merge(df.groupby('gender').agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1))\\\n.eval('rate = count/count2')\n\n\n\n\n\n\n\n\ngender\nresult\ncount\ncount2\nrate\n\n\n\n\n0\nfemale\nfail\n1063\n1835\n0.579292\n\n\n1\nfemale\npass\n772\n1835\n0.420708\n\n\n2\nmale\nfail\n1291\n2691\n0.479747\n\n\n3\nmale\npass\n1400\n2691\n0.520253\n\n\n\n\n\n\n\n- 시각화\n\ndata1= df.groupby(['gender','result']).agg({'count':np.sum}).reset_index()\\\n.merge(df.groupby('gender').agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1))\\\n.eval('rate = count/count2')\nggplot(data1.query('result==\"pass\"'))+geom_col(aes(x='gender',fill='gender',y='rate')) # 합격률만 시각화.\n\n\n\n\n- 결론: 남자의 합격률이 더 높다. \\(\\to\\) 성차별이 있어보인다(?)"
  },
  {
    "objectID": "posts/2_DV2022/2022-11-09-10wk-2.html#시각화2-학과별-합격률",
    "href": "posts/2_DV2022/2022-11-09-10wk-2.html#시각화2-학과별-합격률",
    "title": "10wk-2 심슨의 역설",
    "section": "시각화2: 학과별 합격률",
    "text": "시각화2: 학과별 합격률\n- df2\n\ndf.groupby(['department','gender']).agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1)\n\n\n\n\n\n\n\n\ndepartment\ngender\ncount2\n\n\n\n\n0\nA\nfemale\n108\n\n\n1\nA\nmale\n825\n\n\n2\nB\nfemale\n25\n\n\n3\nB\nmale\n560\n\n\n4\nC\nfemale\n593\n\n\n5\nC\nmale\n325\n\n\n6\nD\nfemale\n375\n\n\n7\nD\nmale\n417\n\n\n8\nE\nfemale\n393\n\n\n9\nE\nmale\n191\n\n\n10\nF\nfemale\n341\n\n\n11\nF\nmale\n373\n\n\n\n\n\n\n\n- merge\n\ndf.merge(df.groupby(['department','gender']).agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1))\\\n.eval('rate = count/count2')\n\n\n\n\n\n\n\n\ndepartment\nresult\ngender\ncount\ncount2\nrate\n\n\n\n\n0\nA\nfail\nfemale\n19\n108\n0.175926\n\n\n1\nA\npass\nfemale\n89\n108\n0.824074\n\n\n2\nA\nfail\nmale\n314\n825\n0.380606\n\n\n3\nA\npass\nmale\n511\n825\n0.619394\n\n\n4\nB\nfail\nfemale\n7\n25\n0.280000\n\n\n5\nB\npass\nfemale\n18\n25\n0.720000\n\n\n6\nB\nfail\nmale\n208\n560\n0.371429\n\n\n7\nB\npass\nmale\n352\n560\n0.628571\n\n\n8\nC\nfail\nfemale\n391\n593\n0.659359\n\n\n9\nC\npass\nfemale\n202\n593\n0.340641\n\n\n10\nC\nfail\nmale\n204\n325\n0.627692\n\n\n11\nC\npass\nmale\n121\n325\n0.372308\n\n\n12\nD\nfail\nfemale\n244\n375\n0.650667\n\n\n13\nD\npass\nfemale\n131\n375\n0.349333\n\n\n14\nD\nfail\nmale\n279\n417\n0.669065\n\n\n15\nD\npass\nmale\n138\n417\n0.330935\n\n\n16\nE\nfail\nfemale\n299\n393\n0.760814\n\n\n17\nE\npass\nfemale\n94\n393\n0.239186\n\n\n18\nE\nfail\nmale\n137\n191\n0.717277\n\n\n19\nE\npass\nmale\n54\n191\n0.282723\n\n\n20\nF\nfail\nfemale\n103\n341\n0.302053\n\n\n21\nF\npass\nfemale\n238\n341\n0.697947\n\n\n22\nF\nfail\nmale\n149\n373\n0.399464\n\n\n23\nF\npass\nmale\n224\n373\n0.600536\n\n\n\n\n\n\n\n- 시각화\n\ndata2=df.merge(df.groupby(['department','gender']).agg({'count':np.sum}).reset_index().rename({'count':'count2'},axis=1))\\\n.eval('rate = count/count2')\nggplot(data2.query('result==\"pass\"'))+geom_col(aes(x='gender',fill='gender',y='rate'))\\\n+facet_wrap('department')\n\n\n\n\n\n학과별로 살펴보니 오히려 A,B,F,D의 경우 여성의 합격률이 높다.\n\n- 교재에서 설명한 이유: 여성이 합격률이 낮은 학과에만 많이 지원하였기 때문\n\nggplot(data2.query('result==\"pass\"'))+geom_col(aes(x='department',y='count2',fill='gender'),position='dodge')\n\n\n\n\n\n살펴보니 합격률이 높은 A,B학과의 경우 상대적으로 남성이 많이 지원하였음. 합격률이 낮은 C,D학과는 상대적으로 여성이 많이 지원함. D,F의 지원수는 비슷"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "",
    "text": "판다스– 인덱싱(2), 판다스–새로운열의할당(1), 아이스크림을 많이 먹으면 걸리는 병(1)"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#데이터",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#데이터",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "데이터",
    "text": "데이터\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/movie.csv')\ndf\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nnum_critic_for_reviews\nduration\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\ngross\ngenres\n...\nnum_user_for_reviews\nlanguage\ncountry\ncontent_rating\nbudget\ntitle_year\nactor_2_facebook_likes\nimdb_score\naspect_ratio\nmovie_facebook_likes\n\n\n\n\n0\nColor\nJames Cameron\n723.0\n178.0\n0.0\n855.0\nJoel David Moore\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\n...\n3054.0\nEnglish\nUSA\nPG-13\n237000000.0\n2009.0\n936.0\n7.9\n1.78\n33000\n\n\n1\nColor\nGore Verbinski\n302.0\n169.0\n563.0\n1000.0\nOrlando Bloom\n40000.0\n309404152.0\nAction|Adventure|Fantasy\n...\n1238.0\nEnglish\nUSA\nPG-13\n300000000.0\n2007.0\n5000.0\n7.1\n2.35\n0\n\n\n2\nColor\nSam Mendes\n602.0\n148.0\n0.0\n161.0\nRory Kinnear\n11000.0\n200074175.0\nAction|Adventure|Thriller\n...\n994.0\nEnglish\nUK\nPG-13\n245000000.0\n2015.0\n393.0\n6.8\n2.35\n85000\n\n\n3\nColor\nChristopher Nolan\n813.0\n164.0\n22000.0\n23000.0\nChristian Bale\n27000.0\n448130642.0\nAction|Thriller\n...\n2701.0\nEnglish\nUSA\nPG-13\n250000000.0\n2012.0\n23000.0\n8.5\n2.35\n164000\n\n\n4\nNaN\nDoug Walker\nNaN\nNaN\n131.0\nNaN\nRob Walker\n131.0\nNaN\nDocumentary\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n7.1\nNaN\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\nColor\nScott Smith\n1.0\n87.0\n2.0\n318.0\nDaphne Zuniga\n637.0\nNaN\nComedy|Drama\n...\n6.0\nEnglish\nCanada\nNaN\nNaN\n2013.0\n470.0\n7.7\nNaN\n84\n\n\n4912\nColor\nNaN\n43.0\n43.0\nNaN\n319.0\nValorie Curry\n841.0\nNaN\nCrime|Drama|Mystery|Thriller\n...\n359.0\nEnglish\nUSA\nTV-14\nNaN\nNaN\n593.0\n7.5\n16.00\n32000\n\n\n4913\nColor\nBenjamin Roberds\n13.0\n76.0\n0.0\n0.0\nMaxwell Moody\n0.0\nNaN\nDrama|Horror|Thriller\n...\n3.0\nEnglish\nUSA\nNaN\n1400.0\n2013.0\n0.0\n6.3\nNaN\n16\n\n\n4914\nColor\nDaniel Hsia\n14.0\n100.0\n0.0\n489.0\nDaniel Henney\n946.0\n10443.0\nComedy|Drama|Romance\n...\n9.0\nEnglish\nUSA\nPG-13\nNaN\n2012.0\n719.0\n6.3\n2.35\n660\n\n\n4915\nColor\nJon Gunn\n43.0\n90.0\n16.0\n16.0\nBrian Herzlinger\n86.0\n85222.0\nDocumentary\n...\n84.0\nEnglish\nUSA\nPG\n1100.0\n2004.0\n23.0\n6.6\n1.85\n456\n\n\n\n\n4916 rows × 28 columns\n\n\n\n- 열의 이름을 출력하여 보자.\n\ndf.columns\n\nIndex(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n      dtype='object')\n\n\n\ndf.keys()\n\nIndex(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n      dtype='object')"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#기본인덱싱-df-인덱싱공부-1단계-내용",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#기본인덱싱-df-인덱싱공부-1단계-내용",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "기본인덱싱 (df 인덱싱공부 1단계 내용)",
    "text": "기본인덱싱 (df 인덱싱공부 1단계 내용)\n- color ~ num_voted_user 를 뽑고 + aspect_ratio 도 추가적으로 뽑고싶다. -&gt; loc으로는 못하겠어요..\n\ndf.loc[:,['color':'num_voted_users','aspect_ratio']] # 이건 안됨.\n\nSyntaxError: invalid syntax (&lt;ipython-input-7-0b4cb2e2977c&gt;, line 1)\n\n\n\ndf.loc[:,'color':'num_voted_users'].head(2) # 이건 잘 됨.\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nnum_critic_for_reviews\nduration\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\ngross\ngenres\nactor_1_name\nmovie_title\nnum_voted_users\n\n\n\n\n0\nColor\nJames Cameron\n723.0\n178.0\n0.0\n855.0\nJoel David Moore\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\nCCH Pounder\nAvatar\n886204\n\n\n1\nColor\nGore Verbinski\n302.0\n169.0\n563.0\n1000.0\nOrlando Bloom\n40000.0\n309404152.0\nAction|Adventure|Fantasy\nJohnny Depp\nPirates of the Caribbean: At World's End\n471220\n\n\n\n\n\n\n\n- (팁) 복잡한 조건은 iloc으로 쓰는게 편할때가 있다. \\(\\to\\) 그런데 df.columns 변수들이 몇번인지 알아보기 힘듬 \\(\\to\\) 아래와 같이 하면 열의 이름을 인덱스와 함께 출력할 수 있음\n\npd.Series(df.columns) ## 제일 편함.\n\n0                         color\n1                 director_name\n2        num_critic_for_reviews\n3                      duration\n4       director_facebook_likes\n5        actor_3_facebook_likes\n6                  actor_2_name\n7        actor_1_facebook_likes\n8                         gross\n9                        genres\n10                 actor_1_name\n11                  movie_title\n12              num_voted_users\n13    cast_total_facebook_likes\n14                 actor_3_name\n15         facenumber_in_poster\n16                plot_keywords\n17              movie_imdb_link\n18         num_user_for_reviews\n19                     language\n20                      country\n21               content_rating\n22                       budget\n23                   title_year\n24       actor_2_facebook_likes\n25                   imdb_score\n26                 aspect_ratio\n27         movie_facebook_likes\ndtype: object\n\n\n\nlist(range(13))+[26]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 26]\n\n\n\ndf.iloc[:,list(range(13))+[26]].head(2)\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nnum_critic_for_reviews\nduration\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\ngross\ngenres\nactor_1_name\nmovie_title\nnum_voted_users\naspect_ratio\n\n\n\n\n0\nColor\nJames Cameron\n723.0\n178.0\n0.0\n855.0\nJoel David Moore\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\nCCH Pounder\nAvatar\n886204\n1.78\n\n\n1\nColor\nGore Verbinski\n302.0\n169.0\n563.0\n1000.0\nOrlando Bloom\n40000.0\n309404152.0\nAction|Adventure|Fantasy\nJohnny Depp\nPirates of the Caribbean: At World's End\n471220\n2.35"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#actor라는-단어가-포함된-column-선택",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#actor라는-단어가-포함된-column-선택",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "actor라는 단어가 포함된 column 선택",
    "text": "actor라는 단어가 포함된 column 선택\n- 다시 열의 이름들을 확인\n\ndf.columns\n\nIndex(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n      dtype='object')\n\n\n\n- 방법1\n\n'actor' in 'actor_1_facebook_likes'\n\nTrue\n\n\n\n# list(map(lambda x: 'actor' in x, df.columns))\n\n\ndf.columns[list(map(lambda x: 'actor' in x, df.columns))]\n\nIndex(['actor_3_facebook_likes', 'actor_2_name', 'actor_1_facebook_likes',\n       'actor_1_name', 'actor_3_name', 'actor_2_facebook_likes'],\n      dtype='object')\n\n\n\ndf.iloc[:,list(map(lambda x : 'actor' in x, df.columns) )]\n\n\n\n\n\n\n\n\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\nactor_1_name\nactor_3_name\nactor_2_facebook_likes\n\n\n\n\n0\n855.0\nJoel David Moore\n1000.0\nCCH Pounder\nWes Studi\n936.0\n\n\n1\n1000.0\nOrlando Bloom\n40000.0\nJohnny Depp\nJack Davenport\n5000.0\n\n\n2\n161.0\nRory Kinnear\n11000.0\nChristoph Waltz\nStephanie Sigman\n393.0\n\n\n3\n23000.0\nChristian Bale\n27000.0\nTom Hardy\nJoseph Gordon-Levitt\n23000.0\n\n\n4\nNaN\nRob Walker\n131.0\nDoug Walker\nNaN\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n318.0\nDaphne Zuniga\n637.0\nEric Mabius\nCrystal Lowe\n470.0\n\n\n4912\n319.0\nValorie Curry\n841.0\nNatalie Zea\nSam Underwood\n593.0\n\n\n4913\n0.0\nMaxwell Moody\n0.0\nEva Boehnke\nDavid Chandler\n0.0\n\n\n4914\n489.0\nDaniel Henney\n946.0\nAlan Ruck\nEliza Coupe\n719.0\n\n\n4915\n16.0\nBrian Herzlinger\n86.0\nJohn August\nJon Gunn\n23.0\n\n\n\n\n4916 rows × 6 columns\n\n\n\n\n\n- 방법2\n\ndf.loc[:,list(map(lambda x : 'actor' in x, df.columns) )]\n\n\n\n\n\n\n\n\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\nactor_1_name\nactor_3_name\nactor_2_facebook_likes\n\n\n\n\n0\n855.0\nJoel David Moore\n1000.0\nCCH Pounder\nWes Studi\n936.0\n\n\n1\n1000.0\nOrlando Bloom\n40000.0\nJohnny Depp\nJack Davenport\n5000.0\n\n\n2\n161.0\nRory Kinnear\n11000.0\nChristoph Waltz\nStephanie Sigman\n393.0\n\n\n3\n23000.0\nChristian Bale\n27000.0\nTom Hardy\nJoseph Gordon-Levitt\n23000.0\n\n\n4\nNaN\nRob Walker\n131.0\nDoug Walker\nNaN\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n318.0\nDaphne Zuniga\n637.0\nEric Mabius\nCrystal Lowe\n470.0\n\n\n4912\n319.0\nValorie Curry\n841.0\nNatalie Zea\nSam Underwood\n593.0\n\n\n4913\n0.0\nMaxwell Moody\n0.0\nEva Boehnke\nDavid Chandler\n0.0\n\n\n4914\n489.0\nDaniel Henney\n946.0\nAlan Ruck\nEliza Coupe\n719.0\n\n\n4915\n16.0\nBrian Herzlinger\n86.0\nJohn August\nJon Gunn\n23.0\n\n\n\n\n4916 rows × 6 columns\n\n\n\n\n\n- 방법3\n\ndf.iloc[:,map(lambda x : 'actor' in x, df.columns)]\n\n\n\n\n\n\n\n\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\nactor_1_name\nactor_3_name\nactor_2_facebook_likes\n\n\n\n\n0\n855.0\nJoel David Moore\n1000.0\nCCH Pounder\nWes Studi\n936.0\n\n\n1\n1000.0\nOrlando Bloom\n40000.0\nJohnny Depp\nJack Davenport\n5000.0\n\n\n2\n161.0\nRory Kinnear\n11000.0\nChristoph Waltz\nStephanie Sigman\n393.0\n\n\n3\n23000.0\nChristian Bale\n27000.0\nTom Hardy\nJoseph Gordon-Levitt\n23000.0\n\n\n4\nNaN\nRob Walker\n131.0\nDoug Walker\nNaN\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n318.0\nDaphne Zuniga\n637.0\nEric Mabius\nCrystal Lowe\n470.0\n\n\n4912\n319.0\nValorie Curry\n841.0\nNatalie Zea\nSam Underwood\n593.0\n\n\n4913\n0.0\nMaxwell Moody\n0.0\nEva Boehnke\nDavid Chandler\n0.0\n\n\n4914\n489.0\nDaniel Henney\n946.0\nAlan Ruck\nEliza Coupe\n719.0\n\n\n4915\n16.0\nBrian Herzlinger\n86.0\nJohn August\nJon Gunn\n23.0\n\n\n\n\n4916 rows × 6 columns\n\n\n\n\n\n- 방법4\n\ndf.loc[:,map(lambda x : 'actor' in x, df.columns)]\n\n\n\n\n\n\n\n\nactor_3_facebook_likes\nactor_2_name\nactor_1_facebook_likes\nactor_1_name\nactor_3_name\nactor_2_facebook_likes\n\n\n\n\n0\n855.0\nJoel David Moore\n1000.0\nCCH Pounder\nWes Studi\n936.0\n\n\n1\n1000.0\nOrlando Bloom\n40000.0\nJohnny Depp\nJack Davenport\n5000.0\n\n\n2\n161.0\nRory Kinnear\n11000.0\nChristoph Waltz\nStephanie Sigman\n393.0\n\n\n3\n23000.0\nChristian Bale\n27000.0\nTom Hardy\nJoseph Gordon-Levitt\n23000.0\n\n\n4\nNaN\nRob Walker\n131.0\nDoug Walker\nNaN\n12.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n318.0\nDaphne Zuniga\n637.0\nEric Mabius\nCrystal Lowe\n470.0\n\n\n4912\n319.0\nValorie Curry\n841.0\nNatalie Zea\nSam Underwood\n593.0\n\n\n4913\n0.0\nMaxwell Moody\n0.0\nEva Boehnke\nDavid Chandler\n0.0\n\n\n4914\n489.0\nDaniel Henney\n946.0\nAlan Ruck\nEliza Coupe\n719.0\n\n\n4915\n16.0\nBrian Herzlinger\n86.0\nJohn August\nJon Gunn\n23.0\n\n\n\n\n4916 rows × 6 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#s로-끝나는-column-선택",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#s로-끝나는-column-선택",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "s로 끝나는 column 선택",
    "text": "s로 끝나는 column 선택\n\n## 참고 (iterable object -&gt; for문 안에 넣어서 돌릴 수 있다.)\n_map = df.loc[:,map(lambda x: x[-1] == 's', df.columns)]\nset(dir(_map)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\n_gen = iter(_map)\n\n\n_gen.__next__()\n\n'num_critic_for_reviews'\n\n\n\n_gen.__next__()\n\n'director_facebook_likes'\n\n\n- 방법1\n\ndf.iloc[:,map(lambda x: 's' == x[-1],df.columns )]\n\n\n\n\n\n\n\n\nnum_critic_for_reviews\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_1_facebook_likes\ngross\ngenres\nnum_voted_users\ncast_total_facebook_likes\nplot_keywords\nnum_user_for_reviews\nactor_2_facebook_likes\nmovie_facebook_likes\n\n\n\n\n0\n723.0\n0.0\n855.0\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\n886204\n4834\navatar|future|marine|native|paraplegic\n3054.0\n936.0\n33000\n\n\n1\n302.0\n563.0\n1000.0\n40000.0\n309404152.0\nAction|Adventure|Fantasy\n471220\n48350\ngoddess|marriage ceremony|marriage proposal|pi...\n1238.0\n5000.0\n0\n\n\n2\n602.0\n0.0\n161.0\n11000.0\n200074175.0\nAction|Adventure|Thriller\n275868\n11700\nbomb|espionage|sequel|spy|terrorist\n994.0\n393.0\n85000\n\n\n3\n813.0\n22000.0\n23000.0\n27000.0\n448130642.0\nAction|Thriller\n1144337\n106759\ndeception|imprisonment|lawlessness|police offi...\n2701.0\n23000.0\n164000\n\n\n4\nNaN\n131.0\nNaN\n131.0\nNaN\nDocumentary\n8\n143\nNaN\nNaN\n12.0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n1.0\n2.0\n318.0\n637.0\nNaN\nComedy|Drama\n629\n2283\nfraud|postal worker|prison|theft|trial\n6.0\n470.0\n84\n\n\n4912\n43.0\nNaN\n319.0\n841.0\nNaN\nCrime|Drama|Mystery|Thriller\n73839\n1753\ncult|fbi|hideout|prison escape|serial killer\n359.0\n593.0\n32000\n\n\n4913\n13.0\n0.0\n0.0\n0.0\nNaN\nDrama|Horror|Thriller\n38\n0\nNaN\n3.0\n0.0\n16\n\n\n4914\n14.0\n0.0\n489.0\n946.0\n10443.0\nComedy|Drama|Romance\n1255\n2386\nNaN\n9.0\n719.0\n660\n\n\n4915\n43.0\n16.0\n16.0\n86.0\n85222.0\nDocumentary\n4285\n163\nactress name in title|crush|date|four word tit...\n84.0\n23.0\n456\n\n\n\n\n4916 rows × 12 columns\n\n\n\n- 방법2\n\ndf.loc[:,map(lambda x: 's' == x[-1],df.columns )]\n\n\n\n\n\n\n\n\nnum_critic_for_reviews\ndirector_facebook_likes\nactor_3_facebook_likes\nactor_1_facebook_likes\ngross\ngenres\nnum_voted_users\ncast_total_facebook_likes\nplot_keywords\nnum_user_for_reviews\nactor_2_facebook_likes\nmovie_facebook_likes\n\n\n\n\n0\n723.0\n0.0\n855.0\n1000.0\n760505847.0\nAction|Adventure|Fantasy|Sci-Fi\n886204\n4834\navatar|future|marine|native|paraplegic\n3054.0\n936.0\n33000\n\n\n1\n302.0\n563.0\n1000.0\n40000.0\n309404152.0\nAction|Adventure|Fantasy\n471220\n48350\ngoddess|marriage ceremony|marriage proposal|pi...\n1238.0\n5000.0\n0\n\n\n2\n602.0\n0.0\n161.0\n11000.0\n200074175.0\nAction|Adventure|Thriller\n275868\n11700\nbomb|espionage|sequel|spy|terrorist\n994.0\n393.0\n85000\n\n\n3\n813.0\n22000.0\n23000.0\n27000.0\n448130642.0\nAction|Thriller\n1144337\n106759\ndeception|imprisonment|lawlessness|police offi...\n2701.0\n23000.0\n164000\n\n\n4\nNaN\n131.0\nNaN\n131.0\nNaN\nDocumentary\n8\n143\nNaN\nNaN\n12.0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\n1.0\n2.0\n318.0\n637.0\nNaN\nComedy|Drama\n629\n2283\nfraud|postal worker|prison|theft|trial\n6.0\n470.0\n84\n\n\n4912\n43.0\nNaN\n319.0\n841.0\nNaN\nCrime|Drama|Mystery|Thriller\n73839\n1753\ncult|fbi|hideout|prison escape|serial killer\n359.0\n593.0\n32000\n\n\n4913\n13.0\n0.0\n0.0\n0.0\nNaN\nDrama|Horror|Thriller\n38\n0\nNaN\n3.0\n0.0\n16\n\n\n4914\n14.0\n0.0\n489.0\n946.0\n10443.0\nComedy|Drama|Romance\n1255\n2386\nNaN\n9.0\n719.0\n660\n\n\n4915\n43.0\n16.0\n16.0\n86.0\n85222.0\nDocumentary\n4285\n163\nactress name in title|crush|date|four word tit...\n84.0\n23.0\n456\n\n\n\n\n4916 rows × 12 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#c-혹은-d로-시작하는-column-선택",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#c-혹은-d로-시작하는-column-선택",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "c 혹은 d로 시작하는 column 선택",
    "text": "c 혹은 d로 시작하는 column 선택\n\ndf.columns\n\nIndex(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n      dtype='object')\n\n\n\n_str = 'color'\n(_str[0] == 'c') or (_str[0] =='d')\n\nTrue\n\n\n\nlist(map(lambda x: (x[0] == 'c') or (x[0] =='d'), df.columns)) # list안해도 이미 iterable object.\n\n[True,\n True,\n False,\n True,\n True,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n True,\n False,\n False,\n False,\n False,\n False,\n False,\n True,\n True,\n False,\n False,\n False,\n False,\n False,\n False]\n\n\n- 방법1\n\ndf.iloc[:,map(lambda x: 'c' == x[0] or 'd' == x[0] ,df.columns )]\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nduration\ndirector_facebook_likes\ncast_total_facebook_likes\ncountry\ncontent_rating\n\n\n\n\n0\nColor\nJames Cameron\n178.0\n0.0\n4834\nUSA\nPG-13\n\n\n1\nColor\nGore Verbinski\n169.0\n563.0\n48350\nUSA\nPG-13\n\n\n2\nColor\nSam Mendes\n148.0\n0.0\n11700\nUK\nPG-13\n\n\n3\nColor\nChristopher Nolan\n164.0\n22000.0\n106759\nUSA\nPG-13\n\n\n4\nNaN\nDoug Walker\nNaN\n131.0\n143\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\nColor\nScott Smith\n87.0\n2.0\n2283\nCanada\nNaN\n\n\n4912\nColor\nNaN\n43.0\nNaN\n1753\nUSA\nTV-14\n\n\n4913\nColor\nBenjamin Roberds\n76.0\n0.0\n0\nUSA\nNaN\n\n\n4914\nColor\nDaniel Hsia\n100.0\n0.0\n2386\nUSA\nPG-13\n\n\n4915\nColor\nJon Gunn\n90.0\n16.0\n163\nUSA\nPG\n\n\n\n\n4916 rows × 7 columns\n\n\n\n- 방법2\n\ndf.loc[:,map(lambda x: 'c' == x[0] or 'd' == x[0] ,df.columns )]\n\n\n\n\n\n\n\n\ncolor\ndirector_name\nduration\ndirector_facebook_likes\ncast_total_facebook_likes\ncountry\ncontent_rating\n\n\n\n\n0\nColor\nJames Cameron\n178.0\n0.0\n4834\nUSA\nPG-13\n\n\n1\nColor\nGore Verbinski\n169.0\n563.0\n48350\nUSA\nPG-13\n\n\n2\nColor\nSam Mendes\n148.0\n0.0\n11700\nUK\nPG-13\n\n\n3\nColor\nChristopher Nolan\n164.0\n22000.0\n106759\nUSA\nPG-13\n\n\n4\nNaN\nDoug Walker\nNaN\n131.0\n143\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4911\nColor\nScott Smith\n87.0\n2.0\n2283\nCanada\nNaN\n\n\n4912\nColor\nNaN\n43.0\nNaN\n1753\nUSA\nTV-14\n\n\n4913\nColor\nBenjamin Roberds\n76.0\n0.0\n0\nUSA\nNaN\n\n\n4914\nColor\nDaniel Hsia\n100.0\n0.0\n2386\nUSA\nPG-13\n\n\n4915\nColor\nJon Gunn\n90.0\n16.0\n163\nUSA\nPG\n\n\n\n\n4916 rows × 7 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#방법1-concat",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#방법1-concat",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "방법1: concat",
    "text": "방법1: concat\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\n_df = pd.DataFrame({'c':[3,4,5]}) \n_df\n\n\n\n\n\n\n\n\nc\n\n\n\n\n0\n3\n\n\n1\n4\n\n\n2\n5\n\n\n\n\n\n\n\n\npd.concat([df,_df],axis=1)\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n3\n4\n5"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#방법2-4가지-컨셉에-따른-할당",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#방법2-4가지-컨셉에-따른-할당",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "방법2: 4가지 컨셉에 따른 할당",
    "text": "방법2: 4가지 컨셉에 따른 할당\n\n# 컨셉1: 불가능\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.c = pd.Series([1,2,3]) \ndf\n\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\n\n# 컨셉2: 가능\n(예시1) – 사실상 이렇게 해야함.\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf['c']=[3,4,5]\ndf\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n3\n4\n5\n\n\n\n\n\n\n\n(예시2) - 굳이 이렇게까지 할필요는 없음.\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf[['c','d']]=np.array([[3,4,5],[4,5,6]]).T # 굳이.. \ndf\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n(예시3)\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf['c'],df['d']=[3,4,5],[4,5,6]\ndf\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n\n\n# 컨셉3: 불가능\n(예시1)\n\name({'a':[1,2,3],'b':[2,3,4]})\ndfdf = pd.DataFr\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.iloc[:,2] = [3,4,5] \ndf\n\nIndexError: iloc cannot enlarge its target object\n\n\n\n\n# 컨셉4: 가능\n(예시1)\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.loc[:,'c'] = [3,4,5] \ndf\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n3\n4\n5\n\n\n\n\n\n\n\n(예시2) – 굳이 쓸 필요가 없다.\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.loc[:,['c','d']] = np.array([[3,4,5],[4,5,6]]).T # 이거 솔직히 되는지 몰랐어요.. \ndf\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n(예시3)\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.loc[:,'c'],df.loc[:,'d'] = [3,4,5],[4,5,6] \ndf\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#방법3-.assign으로-할당-star-제-최애",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#방법3-.assign으로-할당-star-제-최애",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "방법3: .assign으로 할당 (\\(\\star\\)) – 제 최애",
    "text": "방법3: .assign으로 할당 (\\(\\star\\)) – 제 최애\n\n확장성이 있고 다양한 상황에 사용하기 좋음.\n\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.assign(c=[3,4,5]) \n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n3\n4\n5\n\n\n\n\n\n\n\n\ndf.assign(c=[3,4,5],d=[4,5,6]) \n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n\ndf.assign(c=[3,4,5]).assign(d=[4,5,6]) # 1-&gt;2, 2-&gt;3 으로 가는 과정이 메모리 공간안에 모두 저장되어 있다.\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\n오오오오 원래 df가 살아있음."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#방법4-.eval을-이용한-할당",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#방법4-.eval을-이용한-할당",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "방법4: .eval을 이용한 할당",
    "text": "방법4: .eval을 이용한 할당\n\ndf = pd.DataFrame({'a':[1,2,3],'b':[2,3,4]})\ndf\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n\n\n\n\n\n\ndf.eval('c=[3,4,5]')\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1\n2\n3\n\n\n1\n2\n3\n4\n\n\n2\n3\n4\n5\n\n\n\n\n\n\n\n\ndf.eval('c=[3,4,5]').eval('d=[4,5,6]')\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n2\n3\n4\n5\n\n\n2\n3\n4\n5\n6\n\n\n\n\n\n\n\n\n이 방법은 좀 꺼려하는데 아래의 예제를 통해 이유를 알아보자."
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#연습해보기",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#연습해보기",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "연습해보기",
    "text": "연습해보기\n\n# 데이터프레임 생성\n\ndf=pd.DataFrame({'x':np.random.randn(1000),'y':np.random.randn(1000)})\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n-0.528686\n-0.822504\n\n\n1\n-0.570925\n0.177597\n\n\n2\n-2.095003\n0.334422\n\n\n3\n-0.382900\n0.573522\n\n\n4\n-0.971033\n-1.840163\n\n\n...\n...\n...\n\n\n995\n0.172025\n-0.770867\n\n\n996\n-0.086068\n-0.087574\n\n\n997\n0.691668\n0.850134\n\n\n998\n-0.359600\n0.913740\n\n\n999\n0.568702\n-0.808420\n\n\n\n\n1000 rows × 2 columns\n\n\n\n\n\n# 새로운열 r을 생성하고 \\(r=\\sqrt{x^2 + y^2}\\)를 계산\n- 방법1: 브로드캐스팅\n\ndf.assign(r=np.sqrt(df.x**2 + df.y**2))\n\n\n\n\n\n\n\n\nx\ny\nr\n\n\n\n\n0\n1.085469\n-1.427839\n1.793590\n\n\n1\n-1.473272\n-1.527442\n2.122171\n\n\n2\n-1.007274\n-1.312202\n1.654229\n\n\n3\n1.220634\n-0.474995\n1.309796\n\n\n4\n-0.101496\n1.636326\n1.639470\n\n\n...\n...\n...\n...\n\n\n995\n-0.668557\n-0.435391\n0.797831\n\n\n996\n0.455894\n0.796826\n0.918026\n\n\n997\n-1.004412\n1.843344\n2.099229\n\n\n998\n-2.115145\n-1.971965\n2.891796\n\n\n999\n0.861141\n-0.193742\n0.882667\n\n\n\n\n1000 rows × 3 columns\n\n\n\n- 방법2: lambda + map을 이용한 개별원소 계산\n\ndf.assign(r=list(map(lambda x,y: np.sqrt(x**2+y**2), df.x,df.y)))\n\n\n\n\n\n\n\n\nx\ny\nr\n\n\n\n\n0\n1.085469\n-1.427839\n1.793590\n\n\n1\n-1.473272\n-1.527442\n2.122171\n\n\n2\n-1.007274\n-1.312202\n1.654229\n\n\n3\n1.220634\n-0.474995\n1.309796\n\n\n4\n-0.101496\n1.636326\n1.639470\n\n\n...\n...\n...\n...\n\n\n995\n-0.668557\n-0.435391\n0.797831\n\n\n996\n0.455894\n0.796826\n0.918026\n\n\n997\n-1.004412\n1.843344\n2.099229\n\n\n998\n-2.115145\n-1.971965\n2.891796\n\n\n999\n0.861141\n-0.193742\n0.882667\n\n\n\n\n1000 rows × 3 columns\n\n\n\n- 방법3: eval\n\ndf.eval('r=sqrt(x**2+y**2)')\n\n\n\n\n\n\n\n\nx\ny\nr\n\n\n\n\n0\n-0.528686\n-0.822504\n0.977764\n\n\n1\n-0.570925\n0.177597\n0.597910\n\n\n2\n-2.095003\n0.334422\n2.121527\n\n\n3\n-0.382900\n0.573522\n0.689594\n\n\n4\n-0.971033\n-1.840163\n2.080650\n\n\n...\n...\n...\n...\n\n\n995\n0.172025\n-0.770867\n0.789828\n\n\n996\n-0.086068\n-0.087574\n0.122788\n\n\n997\n0.691668\n0.850134\n1.095962\n\n\n998\n-0.359600\n0.913740\n0.981954\n\n\n999\n0.568702\n-0.808420\n0.988415\n\n\n\n\n1000 rows × 3 columns"
  },
  {
    "objectID": "posts/2_DV2022/2022-10-17-7wk-1.html#toy-exam",
    "href": "posts/2_DV2022/2022-10-17-7wk-1.html#toy-exam",
    "title": "07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)",
    "section": "Toy exam",
    "text": "Toy exam\n- 교재의 예제상황은 예를들면 아래와 같다.\n(숨은진짜상황1)\n\\[\\text{아이스크림 판매량} = 20 + 2 \\times \\text{온도} + \\epsilon\\]\n\nnp.random.seed(1) \ntemp= np.array([-10.2, -5.2, 0.1, 10.1, 12.2, 14.7, \n                25.4, 26.8, 28.9, 35.1, 32.2, 34.6])\neps= np.random.normal(size=12,scale=5)\nicecream= 20 + temp * 2 + eps\n\n\nplt.plot(temp,icecream,'.')\n\n\n\n\n\n온도와 아이스크림 판매량의 산점도\n\n(숨은진짜상황2)\n\\[\\text{소아마비 반응수치} = 30 + 0.5 \\times \\text{온도} + \\epsilon\\] - 좌변은 소아마비임을 나타내는 어떠한 반응수치라고 생각하자.\n\nnp.random.seed(2) \neps = np.random.normal(size=12,scale=5) \ndisease = 30+ temp* 0.5 + eps\n\n\nplt.plot(temp,disease,'.')\n\n\n\n\n\n온도와 소아마비의 산점도\n\n(우리가 데이터로부터 관측한 상황)\n- 아이스크림과 질병의 산점도를 그려보자.\n\nplt.plot(icecream,disease,'.')\n\n\n\n\n\n양의 상관관계에 있다.\n\n- 아이스크림 중 어떠한 물질이 소아마비를 일으키는것이 분명하므로 (인과성이 분명해보이니까) 아래와 같은 모형을 세우자. &lt;– 여기서부터 틀렸음\n\\[{\\tt disease}_i =\\beta_0 +\\beta_1 {\\tt icecream}_i +\\epsilon_i,\\quad \\textbf{for} ~~ i=1,2,\\dots, 12\\]\n- 적절한 \\(\\beta_0\\)와 \\(\\beta_1\\)을 추정하면 우리는 아이스크림과 소아마비의 관계를 알 수 있다. &lt;– 틀린주장\n\n틀린 모형\n도데체 우리가 뭘 잘못했는가?\n\n- 두 변수 사이에 상관관계가 있어도 실제 원인은 다른 변수에 숨겨져 있는 경우가 많다.\n(ex1)\n\n온도 \\(\\to\\) 익사\n온도 \\(\\to\\) 아이스크림\n아이스크림과 익사자도 양의 상관관계에 있을것이다.\n아이스크림을 먹이면 물에 빠져 죽는다 \\(\\to\\) 틀린주장\n사실 기온이 숨겨진 원인이다. 기온이 증가하면 아이스크림 판매량도 증가하고 폭염때문에 익사사고율도 높아지는 구조이다.\n\n(ex2)\n\n인구수 \\(\\to\\) 교회\n인구수 \\(\\to\\) 범죄건수\n지역별 교회와 범죄건수를 살펴보면 상관관계가 높게 나올것임\n교회를 지으면 범죄건수도 증가한다? \\(\\to\\) 틀린주장\n사실 인구가 숨겨진 요인임\n\n- ex2, ex1에 대하여 바른 분석을 하려면?\n\nex2: 인구가 비슷한 도시끼리 묶어서 비교해보면 교회와 범죄의 건수는 양의 상관관계에 있지 않을것임\nex1: 온도가 비슷한 그룹끼리 묶어보자.\n\n- 올바른 분석: 온도가 비슷한 그룹끼리 묶어서 그려보자. \\(\\to\\) 상관계수가 줄어들 것이다.\n\nplt.plot(icecream[:6],disease[:6],'.')\n\n\n\n\n\nplt.plot(icecream[6:],disease[6:],'.')\n\n\n\n\n\n진짜로 선형관계가 약해졌다.."
  },
  {
    "objectID": "posts/8_ts/2023-09-10-chap10.html",
    "href": "posts/8_ts/2023-09-10-chap10.html",
    "title": "학습 파이프라인",
    "section": "",
    "text": "%matplotlib inline\n\nwidth = 6\nheight = 3\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = [width, height]\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n## utilities\nimport os\n\n## deep learning module\nimport mxnet as mx\n\n## data processing\nimport numpy as np\nimport pandas as pd\n\n\nelec = pd.read_csv('electricity.txt', header=None)\n\n\nelec.columns = ['V' + str(i) for i in range(1, elec.shape[1]+1)]\n\n\nelec.head()\n\n\n\n\n\n\n\n\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\n...\nV312\nV313\nV314\nV315\nV316\nV317\nV318\nV319\nV320\nV321\n\n\n\n\n0\n14.0\n69.0\n234.0\n415.0\n215.0\n1056.0\n29.0\n840.0\n226.0\n265.0\n...\n676.0\n372.0\n80100.0\n4719.0\n5002.0\n48.0\n38.0\n1558.0\n182.0\n2162.0\n\n\n1\n18.0\n92.0\n312.0\n556.0\n292.0\n1363.0\n29.0\n1102.0\n271.0\n340.0\n...\n805.0\n452.0\n95200.0\n4643.0\n6617.0\n65.0\n47.0\n2177.0\n253.0\n2835.0\n\n\n2\n21.0\n96.0\n312.0\n560.0\n272.0\n1240.0\n29.0\n1025.0\n270.0\n300.0\n...\n817.0\n430.0\n96600.0\n4285.0\n6571.0\n64.0\n43.0\n2193.0\n218.0\n2764.0\n\n\n3\n20.0\n92.0\n312.0\n443.0\n213.0\n845.0\n24.0\n833.0\n179.0\n211.0\n...\n801.0\n291.0\n94500.0\n4222.0\n6365.0\n65.0\n39.0\n1315.0\n195.0\n2735.0\n\n\n4\n22.0\n91.0\n312.0\n346.0\n190.0\n647.0\n16.0\n733.0\n186.0\n179.0\n...\n807.0\n279.0\n91300.0\n4116.0\n6298.0\n75.0\n40.0\n1378.0\n191.0\n2721.0\n\n\n\n\n5 rows × 321 columns\n\n\n\n\nplt.plot(elec.V1[:200], '--r', label='V1')\nplt.plot(elec.V2[:200],'--b', label='V2')\nplt.plot(elec.V3[:200], '--g', label='V3')\nplt.legend(loc=1)\nplt.show()\n\n\n\n\n\nDATA_SEGMENTS = {'tr':0.6, 'va':0.2, 'tst':0.2}\nTHRESHOLD_EPOCHS = 5\nCOR_THRESHOLD = 0.0005\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--win', type=int, default=24*7)\nparser.add_argument('--h', type=int, default=3)\n\nparser.add_argument('--model', type=str, default='rnn_model')\n\nparser.add_argument('--sz-filt', type=int, default=8)\nparser.add_argument('--n-filt', type=int, default=10)\nparser.add_argument('--rnn-units', type=int, default=10)\n\nparser.add_argument('--batch-n', type=int, default=1024)\nparser.add_argument('--lr', type=float, default=0.0001)\nparser.add_argument('--drop', type=float, default=0.2)\nparser.add_argument('--n-epochs', type=int, default=30)\n\nparser.add_argument('--data-dir', type=str, default='../data')\nparser.add_argument('--save-dir', type=str, default=None)\n\n_StoreAction(option_strings=['--save-dir'], dest='save_dir', nargs=None, const=None, default=None, type=&lt;class 'str'&gt;, choices=None, help=None, metavar=None)\n\n\n\ndef prepare_iters(data_dir, win, h, model, batch_n):\n    X, Y = prepared_data(data_dir, win, h, model)\n    \n    n_tr = int(Y.shape[0] * DATA_SEGMENTS['tr'])\n    n_va = int(Y.shape[0] * DATA_SEGMENTS['va'])\n    \n    X_tr, X_valid, X_test = X[:n_tr], X[n_tr:n_tr+n_va], X[n_tr+n_va:]\n    Y_tr, Y_valid, Y_test = Y[:n_tr], Y[n_tr:n_tr+n_va], Y[n_tr+n_va:]\n    \n    iter_tr = mx.io.NDArrayIter(data=X_tr, label=Y_tr, batch_size=batch_n)\n    iter_val = mx.io.NDArrayIter(data=X_valid,\n                                 label=Y_valid,\n                                 batch_size = batch_n)\n    iter_test = mx.io.NDArrayIter(data=X_test,\n                                  label=Y_test,\n                                  batch_size=batch_n)\n    \n    return (iter_tr, iter_val, iter_test)\n\n\ndef prepared_data(data_dir, win, h, model_name):\n    df = pd.read_csv(os.path.join(dat_dir, 'electricity.diff.txt'), sep=',',header=0)\n    x = df.as_matrix()\n\n\nx = elec.iloc[:,:].values\nnormalized_x = (x-np.mean(x, axis=0)) / (np.std(x, axis=0))\n\n\n\n\n(26301, 321)"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_03_07_(1주차)_3월7일.html",
    "href": "posts/3_STBDA2022/2022_03_07_(1주차)_3월7일.html",
    "title": "[STBDA] 1wk. 강의소개 및 단순선형회귀",
    "section": "",
    "text": "(1주차) 3월7일\n\n강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-yKGpQh49tnRrA-o2Odea8r\n\n\n\n강의보충자료\n- https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp1.pdf\n- https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp2.pdf\n\n\n로드맵\n- 오늘수업할내용: 단순선형회귀\n- 단순선형회귀를 배우는 이유?\n\n우리가 배우고싶은것: 심층신경망(DNN) \\(\\to\\) 합성곱신경망(CNN) \\(\\to\\) 적대적생성신경망(GAN)\n심층신경망을 바로 이해하기 어려움\n다음의 과정으로 이해해야함: (선형대수학 \\(\\to\\)) 회귀분석 \\(\\to\\) 로지스틱회귀분석 \\(\\to\\) 심층신경망\n\n\n\n선형회귀\n- 상황극 - 나는 동네에 커피점을 하나 차렸음. - 장사를 하다보니까 날이 더울수록 아이스아메리카노의 판매량이 증가한다는 사실을 깨달았다. - 일기예보는 미리 나와있으니까 그 정보를 잘 이용하면 ‘온도 -&gt; 아이스아메리카노 판매량 예측’ 이 가능할것 같다. (내가 앞으로 얼마나 벌지 예측가능)\n- 가짜자료 생성\n\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \n\n온도 \\({\\bf x}\\)가 아래와 같다고 하자.\n\nx=tf.constant([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) # 기온 \nx\n\n&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4],\n      dtype=float32)&gt;\n\n\n아이스아메리카노의 판매량 \\({\\bf y}\\)이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자)\n\\[{\\bf y} \\approx 10.2 +2.2 {\\bf x}\\]\n\n여기에서 10.2, 2.2 의 숫자는 제가 임의로 정한것임\n식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림\n물결의의미: 현실반영. 세상은 꼭 수식대로 정확하게 이루어지지 않음.\n\n\ntf.random.set_seed(43052)\nepsilon=tf.random.normal([10])\ny=10.2 + 2.2*x + epsilon\ny\n\n&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([55.418365, 58.194283, 61.230827, 62.312557, 63.107002, 63.69569 ,\n       67.247055, 71.4365  , 73.1013  , 77.84988 ], dtype=float32)&gt;\n\n\n- 우리는 아래와 같은 자료를 모았다고 생각하자.\n\ntf.transpose(tf.concat([[x],[y]],0))\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[20.1     , 55.418365],\n       [22.2     , 58.194283],\n       [22.7     , 61.230827],\n       [23.3     , 62.312557],\n       [24.4     , 63.107002],\n       [25.1     , 63.69569 ],\n       [26.2     , 67.247055],\n       [27.3     , 71.4365  ],\n       [28.4     , 73.1013  ],\n       [30.4     , 77.84988 ]], dtype=float32)&gt;\n\n\n- 그려보자.\n\nplt.plot(x,y,'.') # 파란점, 관측한 데이터 \nplt.plot(x,10.2 + 2.2*x, '--')  # 주황색점선, 세상의 법칙 \n\n\n\n\n- 우리의 목표: 파란색점 \\(\\to\\) 주황색점선을 추론 // 데이터를 바탕으로 세상의 법칙을 추론\n- 아이디어: 데이터를 보니까 \\(x\\)와 \\(y\\)가 선형의 관계에 있는듯 보인다. 즉 모든 \\(i=1,2,\\dots, 10\\)에 대하여 아래를 만족하는 적당한 a,b (혹은 \\(\\beta_0,\\beta_1\\)) 가 존재할것 같다. - \\(y_{i} \\approx ax_{i}+b\\) - \\(y_{i} \\approx \\beta_1 x_{i}+\\beta_0\\)\n- 어림짐작으로 \\(a,b\\)를 알아내보자.\n데이터를 살펴보자.\n\ntf.transpose(tf.concat([[x],[y]],0))\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[20.1     , 55.418365],\n       [22.2     , 58.194283],\n       [22.7     , 61.230827],\n       [23.3     , 62.312557],\n       [24.4     , 63.107002],\n       [25.1     , 63.69569 ],\n       [26.2     , 67.247055],\n       [27.3     , 71.4365  ],\n       [28.4     , 73.1013  ],\n       [30.4     , 77.84988 ]], dtype=float32)&gt;\n\n\n적당히 왼쪽*2+15 = 오른쪽의 관계가 성립하는것 같다.\n따라서 \\(a=2, b=15\\) 혹은 \\(\\beta_0=15, \\beta_1=2\\) 로 추론할 수 있겠다.\n- 누군가가 \\((\\beta_0,\\beta_1)=(14,2)\\) 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까)\n- 새로운 주장으로 인해서 \\((\\beta_0,\\beta_1)=(15,2)\\) 로 볼 수도 있고 \\((\\beta_0,\\beta_1)=(14,2)\\) 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? - 후보1: \\((\\beta_0,\\beta_1)=(15,2)\\) - 후보2: \\((\\beta_0,\\beta_1)=(14,2)\\)\n- 가능한 \\(y_i \\approx \\beta_0 + \\beta_1 x_i\\) 이 되도록 만드는 \\((\\beta_0,\\beta_1)\\) 이 좋을 것이다. \\(\\to\\) 후보 1,2를 비교해보자.\n(관찰에 의한 비교)\n후보1에 대해서 \\(i=1,2\\)를 넣고 관찰하여 보자.\n\n20.1 * 2 + 15 , 55.418365 # i=1 \n\n(55.2, 55.418365)\n\n\n\n22.2 * 2 + 15 , 58.194283 # i=2\n\n(59.4, 58.194283)\n\n\n후보2에 대하여 \\(i=1,2\\)를 넣고 관찰하여 보자.\n\n20.1 * 2 + 14 , 55.418365 # i=1 \n\n(54.2, 55.418365)\n\n\n\n22.2 * 2 + 14 , 58.194283 # i=2\n\n(58.4, 58.194283)\n\n\n\\(i=1\\)인 경우에는 후보1이 더 잘맞는것 같은데 \\(i=2\\)인 경우는 후보2가 더 잘맞는것 같다.\n(좀 더 체계적인 비교)\n\\(i=1,2,3, \\dots, 10\\) 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자.\n후보 1,2에 대하여 \\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)를 계산하여 비교해보자.\n\nsum1=0 \nfor i in range(10):\n    sum1=sum1+(y[i]-15-2*x[i])**2 \n\n\nsum2=0 \nfor i in range(10):\n    sum2=sum2+(y[i]-14-2*x[i])**2 \n\n\nsum1,sum2\n\n(&lt;tf.Tensor: shape=(), dtype=float32, numpy=14.734169&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=31.521088&gt;)\n\n\n후보1이 더 \\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)의 값이 작다.\n후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다.\n- 그런데 이 알고리즘은 현실적으로 구현이 불가능하다. (무한번 계산하기도 힘들고, 언제 멈출지도 애매함)\n- 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 \\(\\beta_0,\\beta_1\\)을 찾으면 된다.\n\\(\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2\\)\n그런데 결국 \\(\\beta_0, \\beta_1\\)에 대한 이차식인데 이 식을 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하기 위해서는 아래를 연립하여 풀면된다.\n\\(\\begin{cases} \\frac{\\partial}{\\partial \\beta_0}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0 \\\\ \\frac{\\partial}{\\partial \\beta_1}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0 \\end{cases}\\)\n- 풀어보자.\n\\(\\begin{cases} \\sum_{i=1}^{10} -2(y_i -\\beta_0 -\\beta_1 x_i)=0 \\\\ \\sum_{i=1}^{10} -2x_i(y_i -\\beta_0 -\\beta_1 x_i)=0 \\end{cases}\\)\n정리하면\n\\[\\hat{\\beta}_0= \\bar{y}-\\hat{\\beta}_1 \\bar{x}\\]\n\\[\\hat{\\beta}_1= \\frac{S_{xy}}{S_{xx}}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\]\n- 따라서 최적의 추정치 \\((\\hat{\\beta}_0,\\hat{\\beta}_1)\\)를 이용한 추세선을 아래와 같이 계산할 수 있음.\n\nSxx= sum((x-sum(x)/10)**2)\nSxx\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=87.848976&gt;\n\n\n\nSxy=  sum((x-sum(x)/10)*(y-sum(y)/10))\nSxy\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=194.64737&gt;\n\n\n\nbeta1_estimated = Sxy/Sxx \nbeta1_estimated\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157044&gt;\n\n\n\nbeta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 \nbeta0_estimated\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.944572&gt;\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,beta0_estimated + beta1_estimated * x, '--') # 주황색선: 세상의 법칙을 추정한선 \nplt.plot(x,10.2 + 2.2* x, '--') # 초록색선: ture, 세상의법칙 \n\n\n\n\n\nNote: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다.\n\n- 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다.\n\n공식이 좀 복잡함..\n\\(x\\)가 여러개일 경우 확장이 어려움\n\n- 단점을 극복하기 위해서 우리가 지금까지 했던논의를 매트릭스로 바꾸어서 다시 써보자.\n- 모형의 매트릭스화\n우리의 모형은 아래와 같다.\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10\\)\n풀어서 쓰면\n\\(\\begin{cases} y_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ y_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \\dots \\\\ y_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \\end{cases}\\)\n아래와 같이 쓸 수 있다.\n$\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\dots \\\\\ny_{10}\n\\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\dots & \\dots \\\\\n1 & x_{10}\n\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\]\n\n\\[\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\dots \\\\\n\\epsilon_{10}\n\\end{bmatrix}\\]\n$\n\n벡터와 매트릭스 형태로 정리하면\n\\({\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}\\)\n- 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\)\n이것을 벡터표현으로 하면 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\)\n풀어보면\n\\(loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n- 미분하는 과정의 매트릭스화\nloss를 최소화하는 \\({\\boldsymbol \\beta}\\)를 구해야하므로 loss를 \\({\\boldsymbol \\beta}\\)로 미분한식을 0이라고 놓고 풀면 된다.\n\\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n$= 0 - {}^- {}^ + 2{}^ $\n따라서 \\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0\\)을 풀면 아래와 같다.\n$= ({}){-1}{}^ $\n- 공식도 매트릭스로 표현하면: $= ({}){-1}{}^ $ &lt;– 외우세요\n- 적용을 해보자.\n(X를 만드는 방법1)\n\nX=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # \nX\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)&gt;\n\n\n(X를 만드는 방법2)\n\nfrom tensorflow.python.ops.numpy_ops import np_config\nnp_config.enable_numpy_behavior()\n\n\nX=tf.concat([[[1.0]*10],[x]],0).T\nX\n\n&lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)&gt;\n\n\n\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.944702, 2.215706], dtype=float32)&gt;\n\n\n- 잘 구해진다.\n- 그런데..\n\nbeta0_estimated,beta1_estimated\n\n(&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.944572&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157044&gt;)\n\n\n값이 좀 다르다..?\n- 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다.\n\nimport tensorflow.experimental.numpy as tnp \n\n\nx=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) \ny=10.2 + 2.2*x + epsilon \n\n\nbeta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2)\nbeta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 \n\n\nbeta0_estimated, beta1_estimated\n\n(&lt;tf.Tensor: shape=(), dtype=float64, numpy=9.944573294798559&gt;,\n &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.2157046054834106&gt;)\n\n\n\nX=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457329, 2.21570461])&gt;\n\n\n\n\n앞으로 할것\n- 선형대수학의 미분이론..\n- 실습 (tensorflow에서 매트릭스를 자유롭게 다루기!)"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-xOfpHJG0LrtYt4TUVgqUNy"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#강의영상",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#강의영상",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-xOfpHJG0LrtYt4TUVgqUNy"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#imports",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#imports",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "imports",
    "text": "imports\n\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#cnn",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#cnn",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "CNN",
    "text": "CNN\n\nCONV의 역할\n- 데이터생성 (그냥 흑백대비 데이터)\n\n_X1 = tnp.ones([50,25])*10\n_X1\n\n&lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       ...,\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.]])&gt;\n\n\n\n_X2 = tnp.zeros([50,25])*10\n_X2\n\n&lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])&gt;\n\n\n\ntf.concat([_X1,_X2],axis=1)\n\n&lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       ...,\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.]])&gt;\n\n\n\nplt.imshow(tf.concat([_X1,_X2], axis=1),cmap='gray')\n\n&lt;matplotlib.image.AxesImage at 0x7f37fdcce3d0&gt;\n\n\n\n\n\n\n값이 크면 흰색, 값이 작으면 검정색으로 되어있음.\n\n여기다 적당한 noise를 섞어보자.\n\n_noise_vec = tnp.random.randn(50*50) # 2500개의 vector\n_noise_vec\n\n&lt;tf.Tensor: shape=(2500,), dtype=float64, numpy=\narray([-0.06902639,  0.76575606,  0.62313143, ..., -0.15415241,\n       -0.52788634,  0.77923277])&gt;\n\n\n\n_noise = tnp.random.randn(50*50).reshape(50,50) # matrix 형태로\n_noise\n\n&lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[ 0.64448515,  0.48672712, -0.21792212, ..., -0.4029161 ,\n        -0.76942793,  0.42752944],\n       [-0.85736695,  1.27257844,  0.86595728, ...,  0.17527877,\n         1.74959789, -0.8465042 ],\n       [-2.37767743,  1.12817978,  0.80667681, ..., -1.69588932,\n         0.66389614,  0.04199325],\n       ...,\n       [ 0.35348866,  0.8854033 ,  0.57155344, ...,  1.47500872,\n        -0.56131948,  0.44347445],\n       [ 1.58838754, -1.37524759,  1.12635227, ..., -0.6870017 ,\n         0.63987008,  0.55168672],\n       [ 0.27925012,  0.04426039,  0.19833725, ...,  0.00770918,\n        -2.02424407, -0.04405339]])&gt;\n\n\n\n# image data 생성\nXXX = tf.concat([_X1,_X2],axis=1) + _noise\n\n\n# shape을 맞춰줘야함. \nXXX=XXX.reshape(1,50,50,1)\n\n\nplt.imshow(XXX.reshape(50,50),cmap='gray')\n\n&lt;matplotlib.image.AxesImage at 0x7f37fdc40490&gt;\n\n\n\n\n\n- conv layer 생성\n\nconv = tf.keras.layers.Conv2D(2,(2,2)) # (2,2) kernel\n\n\nconv.weights # 처음에는 가중치가 없음\n\n[]\n\n\n\nconv(XXX).shape # 2x2 kernel을 만들었으니까 이미지가 하나씩 날라가서 49x49의 이미지 생성\n\nTensorShape([1, 49, 49, 2])\n\n\n\nconv(XXX) # 가중치를 만들기 위해서 XXX를 conv에 한번 통과시킴\nconv.weights # 이제 가중치가 생김\n\n[&lt;tf.Variable 'conv2d/kernel:0' shape=(2, 2, 1, 2) dtype=float32, numpy=\n array([[[[-0.10361075, -0.03655446]],\n \n         [[-0.25615066, -0.6408293 ]]],\n \n \n        [[[-0.19069207, -0.6668661 ]],\n \n         [[-0.18648207, -0.534903  ]]]], dtype=float32)&gt;,\n &lt;tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;]\n\n\n\n이제 가중치가 생김.\n\n- 가중치의 값을 확인해보자.\n\nconv.weights[0] # kernel에 해당하는것\n\n&lt;tf.Variable 'conv2d/kernel:0' shape=(2, 2, 1, 2) dtype=float32, numpy=\narray([[[[-0.10361075, -0.03655446]],\n\n        [[-0.25615066, -0.6408293 ]]],\n\n\n       [[[-0.19069207, -0.6668661 ]],\n\n        [[-0.18648207, -0.534903  ]]]], dtype=float32)&gt;\n\n\n\nshape=(2, 2, 1, 2) : 커널사이즈 2x2 // XXX의 채널 1 // Conv(XXX)의 출력채널 2\n\n## 참고\nconv = tf.keras.layers.Conv2D(2,(2,2))\n\n# 여기서 unit을 2개로 받았으니까 2개의 출력 채널이 만들어진다.\n\nconv.weights[1] # bias에 해당하는것\n\n&lt;tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;\n\n\n- 필터값을 원하는 것으로 변경해보자.\n\nw0 = [[0.25,0.25],[0.25,0.25]] # 잡티를 제거하는 효과를 준다.\nw1 = [[-1.0,1.0],[-1.0,1.0]] # 경계를 찾기 좋아보이는 필터이다. (엣지검출)\n\n\nnp.array(w0), np.array(w1) # (2,2,1,2)\n\n(array([[0.25, 0.25],\n        [0.25, 0.25]]),\n array([[-1.,  1.],\n        [-1.,  1.]]))\n\n\n\nw=np.concatenate([np.array(w0).reshape(2,2,1,1),np.array(w1).reshape(2,2,1,1)],axis=-1)\nw\n\narray([[[[ 0.25, -1.  ]],\n\n        [[ 0.25,  1.  ]]],\n\n\n       [[[ 0.25, -1.  ]],\n\n        [[ 0.25,  1.  ]]]])\n\n\n\nb= np.array([0.0,0.0])\nb\n\narray([0., 0.])\n\n\n\nconv.set_weights([w,b])\nconv.get_weights()\n\n[array([[[[ 0.25, -1.  ]],\n \n         [[ 0.25,  1.  ]]],\n \n \n        [[[ 0.25, -1.  ]],\n \n         [[ 0.25,  1.  ]]]], dtype=float32),\n array([0., 0.], dtype=float32)]\n\n\n\n첫번째는 평균을 구하는 필터,\n두번째는 엣지를 검출하는 필터\n\n- 필터를 넣은 결과를 확인\n\nXXX0=conv(XXX)[...,0] # 채널0\nXXX0\n\n&lt;tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy=\narray([[[10.386606  , 10.601835  , 10.11681   , ..., -0.26418784,\n          0.18813315,  0.1402988 ],\n        [ 9.791428  , 11.018348  , 10.635733  , ..., -0.15474442,\n          0.22322088,  0.4022458 ],\n        [ 9.269302  , 10.1986265 , 10.747881  , ..., -0.42707908,\n         -0.01979728,  0.31640166],\n        ...,\n        [10.478983  , 10.708517  ,  9.69488   , ...,  0.23036546,\n         -0.513018  , -0.63264334],\n        [10.363008  , 10.302015  , 10.524318  , ...,  0.52903736,\n          0.21663941,  0.26842797],\n        [10.134163  ,  9.9984255 , 10.623085  , ...,  0.01135473,\n         -0.5159166 , -0.21918514]]], dtype=float32)&gt;\n\n\n\nXXX1=conv(XXX)[...,1] # 채널1\nXXX1\n\n&lt;tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy=\narray([[[ 1.9721851 , -1.111269  , -0.8288326 , ...,  0.60147667,\n          1.2078073 , -1.3991446 ],\n        [ 5.6358013 , -0.7281227 , -0.80233765, ..., -2.4222436 ,\n          3.9341047 , -3.2180052 ],\n        [ 2.7431278 ,  0.9741707 ,  1.222847  , ..., -0.033876  ,\n          1.6630032 , -0.31820738],\n        ...,\n        [ 3.2966785 , -2.378542  , -1.6760101 , ..., -0.11025763,\n         -2.863276  ,  2.3847747 ],\n        [-2.4317207 ,  2.1877518 , -1.298542  , ..., -0.5401355 ,\n         -0.7094564 ,  0.91661054],\n        [-3.1986256 ,  2.6556778 , -0.15703964, ..., -1.4040039 ,\n         -0.7050814 ,  1.8920072 ]]], dtype=float32)&gt;\n\n\n- 각 채널을 시각화\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)\n\n\n\n\n\nax1.imshow(XXX.reshape(50,50),cmap='gray') # original image\n\n&lt;matplotlib.image.AxesImage at 0x7f37fd4f2070&gt;\n\n\n\nax3.imshow(XXX0.reshape(49,49),cmap='gray') # 채널0을 통과시킨 이미지\n\n&lt;matplotlib.image.AxesImage at 0x7f37fd4c2700&gt;\n\n\n\nax4.imshow(XXX1.reshape(49,49),cmap='gray') # 채널1을 통과시킨 이미지\n\n&lt;matplotlib.image.AxesImage at 0x7f37fd3511c0&gt;\n\n\n\nfig\n\n\n\n\n\n2사분면: 원래이미지\n3사분면: 원래이미지 -&gt; 평균을 의미하는 conv적용\n4사분면: 원래이미지 -&gt; 엣지를 검출하는 conv적용\n\n- conv(XXX)의 각 채널에 한번더 conv를 통과시켜보자\n\nchannel0 : 평균필터\nchannel1 : 엣지필터\n\n\nconv(XXX0.reshape(1,49,49,1))[...,0] ### XXX0 -&gt; 평균필터 &lt;=&gt; XXX -&gt; 평균필터 -&gt; 평균필터\nconv(XXX0.reshape(1,49,49,1))[...,1] ### XXX0 -&gt; 엣지필터 &lt;=&gt; XXX -&gt; 평균필터 -&gt; 엣지필터\nconv(XXX1.reshape(1,49,49,1))[...,0] ### XXX1 -&gt; 평균필터 &lt;=&gt; XXX -&gt; 엣지필터 -&gt; 평균필터\nconv(XXX1.reshape(1,49,49,1))[...,1] ### XXX1 -&gt; 엣지필터 &lt;=&gt; XXX -&gt; 엣지필터 -&gt; 엣지필터\n\n&lt;tf.Tensor: shape=(1, 48, 48), dtype=float32, numpy=\narray([[[-9.447378  ,  0.20822144,  0.19534683, ..., -0.7362902 ,\n          6.962679  , -9.759062  ],\n        [-8.132881  ,  0.17446136, -3.0925026 , ..., -2.7580416 ,\n          8.053227  , -9.13332   ],\n        [-0.93447495, -2.5899773 , -1.0724258 , ...,  5.2640886 ,\n          0.94379497, -3.4876597 ],\n        ...,\n        [-9.087433  ,  1.0329819 ,  6.3635483 , ...,  0.6270219 ,\n         -6.605945  ,  9.485929  ],\n        [-1.055748  , -2.783762  ,  2.398777  , ...,  2.1154294 ,\n         -2.9223394 ,  6.874118  ],\n        [10.473776  , -6.299011  , -0.73449326, ...,  1.5050641 ,\n          0.5296016 ,  4.2231555 ]]], dtype=float32)&gt;\n\n\n\nfig,ax =plt.subplots(3,4)\n\n\n\n\n\nax[0][0].imshow(XXX.reshape(50,50),cmap='gray') # 원래이미지\nax[0][0].set_title('original image')\n\nText(0.5, 1.0, 'original image')\n\n\n\nfig\n\n\n\n\n\nax[1][0].imshow(XXX0.reshape(49,49),cmap='gray') # 원래이미지 -&gt; 평균필터\nax[1][2].imshow(XXX1.reshape(49,49),cmap='gray') # 원래이미지 -&gt; 엣지필터\n\n&lt;matplotlib.image.AxesImage at 0x7f37fd58ebe0&gt;\n\n\n\nax[1][0].set_title('Average filter')\nax[1][2].set_title('Edge filter')\n\nText(0.5, 1.0, 'Edge filter')\n\n\n\nax[2][0].imshow(conv(XXX0.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지(평균필터를 1번 통과한 이미지) -&gt; 평균필터\nax[2][1].imshow(conv(XXX0.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지(평균필터를 1번 통과한 이미지) -&gt; 엣지필터\nax[2][2].imshow(conv(XXX1.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지(엣지필터를 1번 통과한 이미지) -&gt; 평균필터\nax[2][3].imshow(conv(XXX1.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지(엣지필터를 1번 통과한 이미지) -&gt; 엣지필터\n\n&lt;matplotlib.image.AxesImage at 0x7f37fced5eb0&gt;\n\n\n\nax[2][0].set_title('Average + Average')\nax[2][1].set_title('Average + Edge')\nax[2][2].set_title('Edge + Average')\nax[2][3].set_title('Edge + Edge')\n\nText(0.5, 1.0, 'Edge + Edge')\n\n\n\nfig.set_figheight(8)\nfig.set_figwidth(16)\nfig.tight_layout()\nfig\n\n\n\n\n- 요약 - conv의 weight에 따라서 엣지를 검출하는 필터가 만들어지기도 하고 스무딩의 역할을 하는 필터가 만들어지기도 한다. 그리고 우리는 의미를 알 수 없지만 어떠한 역할을 하는 필터가 만들어질 것이다. - 이것들을 조합하다보면 우연히 이미지를 분류하기에 유리한 특징을 뽑아내는 weight가 맞춰질 수도 있겠다. - 채널수를 많이 만들고 다양한 웨이트조합을 실험하다보면 보다 복잡한 이미지의 특징을 추출할 수도 있을 것이다? - 컨볼루션 레이어의 역할 = 이미지의 특징을 추출하는 역할\n\n\n(참고) 스트라이드, 패딩\n- 참고: 스트라이드, 패딩 - 스트라이드: 윈도우가 1칸씩 이동하는 것이 아니라 2~3칸씩 이동함 - 패딩: 이미지의 가장자리에 정당한 값을 넣어서 (예를들어 0) 컨볼루션을 수행. 따라서 컨볼루션 연산 이후에도 이미지의 크기가 줄어들지 않도록 방지한다.\n\n\nMAXPOOL\n- 기본적역할: 이미지의 크기를 줄이는 것 - 이미지의의 크기를 줄여야하는 이유? 어차피 최종적으로 10차원으로 줄어야하므로 - 이미지의 크기를 줄이면서도 동시에 아주 크리티컬한 특징은 손실없이 유지하고 싶다~\n- 점점 작은 이미지가 되면서 중요한 특징들은 살아남지만 그렇지 않으면 죽는다. (캐리커쳐 느낌)\n- 평균이 아니라 max를 쓴 이유는? 그냥 평균보다 나을것이라고 생각했음.. - 그런데 사실은 꼭 그렇지만은 않아서 최근에는 꼭 맥스풀링을 고집하진 않는 추세 (평균풀링도 많이씀)\n\n\nCNN 아키텍처의 표현방법\n- 아래와 같이 아키텍처의 다이어그램형태로 표현하고 굳이 노드별로 이미지를 그리진 않음\n\n\n\n위키에서 긁어온 이미지\n\n\n- 물론 아래와 같이 그리는 경우도 있음\n\n\n\nDiscusstion about CNN\n- 격자형태로 배열된 자료를 처리하는데 특화된 신경망이다. - 시계열 (1차원격자), 이미지 (2차원격자)\n- 실제응용에서 엄청난 성공을 거두었다.\n- 이름의 유래는 컨볼루션이라는 수학적 연산을 사용했기 때문 - 컨볼루션은 조금 특별한 선형변환이다.\n- 신경과학의 원리가 심층학습에 영향을 미친 사례이다.\n\n\nCNN의 모티브\n- 희소성 + 매개변수의 공유 - 다소 철학적인 모티브임 - 희소성: 이미지를 분석하여 특징을 뽑아낼때 부분부분의 특징만 뽑으면 된다는 의미1 - 매개변수의 공유: 한 채널에는 하나의 역할을 하는 커널을 설계하면 된다는 의미 (스무딩이든 엣징이든). 즉 어떤지역은 스무딩, 어떤지역은 엣징을 할 필요가 없이 한채널에서는 엣징만, 다른채널에서는 스무딩만 수행한뒤 여러채널을 조합해서 이해하면 된다.\n- 매개변수 공유효과로 인해서 파라메터가 확 줄어든다.\n(예시) (1,6,6,1) -&gt; (1,5,5,2) - MLP방식이면 (36,50) 의 차원을 가진 매트릭스가 필요함 =&gt; 1800개의 매개변수 필요 - CNN은 8개의 매개변수 필요\n\n## MLP\n36*50\n\n1800\n\n\n\n## CNN\n(2*2) * 2 # (2x2) kernel이 2개\n\n8\n\n\n\n매개변수가 적으면 GPU에 올릴때 좋음.\n\n\n\nCNN 신경망의 기본구조\n- 기본유닛 - conv2 - activation3 - pooling4 - conv5 - conv6 - activation7 - pooling8"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#모형의-성능을-올리기-위한-노력들",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#모형의-성능을-올리기-위한-노력들",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "모형의 성능을 올리기 위한 노력들",
    "text": "모형의 성능을 올리기 위한 노력들\n\ndropout\n- 아래의 예제를 복습하자.\n\nnp.random.seed(43052)\nx = np.linspace(0,1,100).reshape(100,1)\ny = np.random.normal(loc=0,scale=0.01,size=(100,1))\nplt.plot(x,y)\n\n\n\n\n\n이 예제는 랜덤으로 만든 데이터이기 때문에 fitting을 하면 직선이 나와야 한다.\n그게 아니라면 오퍼피팅.\n\nDense layer를 \\(2048\\)로 받아서 오버피팅이 일어나기 좋은 환경을 일부러 만들고 있다.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() # 네트워크 생성\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1)) # activation이 identity로 받아지는 것.\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x,y,epochs=5000,verbose=0,batch_size=100) # 적합.\n\n&lt;keras.callbacks.History at 0x7f37e6134820&gt;\n\n\n\nplt.plot(x,y)\nplt.plot(x,net(x),'--')\n\n\n\n\n\n얘는 랜덤인데9 위와 같이 데이터를 따라가는 피팅 결과가 나오면 오버피팅이라고 할 수 있다.\n이러한 추세가 있는게 맞을수도 있지 않느냐? 라고 생각할 수 있는데 그것을 아님을 보이기 위해 train/test로 나누어서 생각해보자.\n\n- train/test로 나누어서 생각해보자.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n&lt;keras.callbacks.History at 0x7f37e5f1eee0&gt;\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\n\n\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--', label='train')\nplt.plot(x[80:],net(x[80:]),'--', label='test')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f37fd08b640&gt;\n\n\n\n\n\n\ntrain에서 추세를 따라가는게 좋은게 아니다 \\(\\to\\) 그냥 직선으로 핏하는거 이외에는 다 오버핏이다.\n\n(생각) 우리가 노드를 \\(2048\\)개를 만들었었는데 학습을 해보니 과적합이 일어났다. 즉, 노드들이 학습을 너무 열심히 했다. 학습을 좀 더 대충했으면 이렇게 세밀하게는 안따라갔을 텐데..\n- 매 에폭마다 적당히 80%의 노드들을 빼고 학습하자 \\(\\to\\) 너무 잘 학습되는 문제는 생기지 않을 것이다 (과적합이 방지될것이다?)\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dropout(0.8)) ## Dropout층 추가\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n&lt;keras.callbacks.History at 0x7f37fd5a9520&gt;\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\nplt.plot(x[80:],net(x[80:]),'--')\n\n\n\n\n\n오버핏이 확실히 줄어들었다. (완전히 없어진 것은 아니지만)\n\n\n\n\n\n\n\nNote\n\n\n\n\\(80\\%\\) 노드를 빼고 학습하면 특징을 잘 학습하지 못하는거 아니냐? 라고 생각할 수 있지만 그렇게 중요한 특징이면 노드들을 랜덤으로 빼도 결국 학습을 해낼 것이라는 믿음이 있는 것이다. 증명이 있는건 아니지만 그렇게 믿음. 그러한 직관이 있다.\n\n\n- 드랍아웃에 대한 summary - 직관: 특정노드를 랜덤으로 off시키면 학습이 방해되어 오히려 과적합이 방지되는 효과가 있다 (그렇지만 진짜 중요한 특징이라면 랜덤으로 off 되더라도 어느정도는 학습될 듯) - note: 드랍아웃을 쓰면 오버핏이 줄어드는건 맞지만 완전히 없어지는건 아니다. - note: 오버핏을 줄이는 유일한 방법이 드랍아웃만 있는것도 아니며, 드랍아웃이 오버핏을 줄이는 가장 효과적인 방법도 아니다 (최근에는 dropout보다 batch nomalization을 사용하는 추세임)\n\n\ntrain / val / test\n만약 train으로 에폭 5000정도로 열심히 학습했다고 가정해보자. 그런데 테스트를 해봤더니 오버피팅이 심한 엉뚱한 모형이 나왔다면 너무 아깝다. (비효율적)\ntrain, validation을 비교해보고, validation loss가 줄어들지 않고 오히려 overfitting이 되면서 커진다면 학습의 에폭을 줄여봐야겠다 내지는 노드를 줄여봐야 겠다 이런식으로 조정을 할 수 있다.\n- data\nFashion MNIST 데이터셋은 위 그림과 같이 운동화, 셔츠, 샌들과 같은 작은 이미지들의 모음이며, 기본 MNIST 데이터셋과 같이 열 가지로 분류될 수 있는 28×28 픽셀의 이미지 70,000개로 이루어져 있습니다.\n\n\n\nFashion MNIST 이미지 데이터셋.\n\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= x_train.reshape(-1,28,28,1)/255 ## 입력이 0~255 -&gt; 0~1로 표준화 시키는 효과 + float으로 자료형이 바뀜\ny = tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,28,28,1)/255\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nX.shape, y.shape, XX.shape, yy.shape\n\n((60000, 28, 28, 1), (60000, 10), (10000, 28, 28, 1), (10000, 10))\n\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten()) # DNN 쓸거니까 flatten()!\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#collapse_output\ncb1 = tf.keras.callbacks.TensorBoard()\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb1,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 0s 1ms/step - loss: 0.7011 - accuracy: 0.7695 - val_loss: 0.4931 - val_accuracy: 0.8338\nEpoch 2/200\n240/240 [==============================] - 0s 818us/step - loss: 0.4582 - accuracy: 0.8434 - val_loss: 0.4590 - val_accuracy: 0.8414\nEpoch 3/200\n240/240 [==============================] - 0s 829us/step - loss: 0.4142 - accuracy: 0.8560 - val_loss: 0.4216 - val_accuracy: 0.8543\nEpoch 4/200\n240/240 [==============================] - 0s 824us/step - loss: 0.3920 - accuracy: 0.8625 - val_loss: 0.3953 - val_accuracy: 0.8624\nEpoch 5/200\n240/240 [==============================] - 0s 822us/step - loss: 0.3718 - accuracy: 0.8690 - val_loss: 0.3842 - val_accuracy: 0.8654\nEpoch 6/200\n240/240 [==============================] - 0s 827us/step - loss: 0.3567 - accuracy: 0.8750 - val_loss: 0.3825 - val_accuracy: 0.8662\nEpoch 7/200\n240/240 [==============================] - 0s 823us/step - loss: 0.3477 - accuracy: 0.8770 - val_loss: 0.3805 - val_accuracy: 0.8684\nEpoch 8/200\n240/240 [==============================] - 0s 837us/step - loss: 0.3377 - accuracy: 0.8789 - val_loss: 0.3564 - val_accuracy: 0.8741\nEpoch 9/200\n240/240 [==============================] - 0s 828us/step - loss: 0.3288 - accuracy: 0.8830 - val_loss: 0.3481 - val_accuracy: 0.8758\nEpoch 10/200\n240/240 [==============================] - 0s 826us/step - loss: 0.3204 - accuracy: 0.8852 - val_loss: 0.3541 - val_accuracy: 0.8771\nEpoch 11/200\n240/240 [==============================] - 0s 832us/step - loss: 0.3136 - accuracy: 0.8880 - val_loss: 0.3541 - val_accuracy: 0.8749\nEpoch 12/200\n240/240 [==============================] - 0s 827us/step - loss: 0.3071 - accuracy: 0.8887 - val_loss: 0.3508 - val_accuracy: 0.8748\nEpoch 13/200\n240/240 [==============================] - 0s 827us/step - loss: 0.3020 - accuracy: 0.8914 - val_loss: 0.3450 - val_accuracy: 0.8796\nEpoch 14/200\n240/240 [==============================] - 0s 826us/step - loss: 0.2968 - accuracy: 0.8934 - val_loss: 0.3507 - val_accuracy: 0.8752\nEpoch 15/200\n240/240 [==============================] - 0s 824us/step - loss: 0.2929 - accuracy: 0.8946 - val_loss: 0.3494 - val_accuracy: 0.8772\nEpoch 16/200\n240/240 [==============================] - 0s 823us/step - loss: 0.2909 - accuracy: 0.8948 - val_loss: 0.3875 - val_accuracy: 0.8594\nEpoch 17/200\n240/240 [==============================] - 0s 819us/step - loss: 0.2853 - accuracy: 0.8969 - val_loss: 0.3411 - val_accuracy: 0.8792\nEpoch 18/200\n240/240 [==============================] - 0s 822us/step - loss: 0.2796 - accuracy: 0.8979 - val_loss: 0.3503 - val_accuracy: 0.8773\nEpoch 19/200\n240/240 [==============================] - 0s 818us/step - loss: 0.2745 - accuracy: 0.9007 - val_loss: 0.3381 - val_accuracy: 0.8800\nEpoch 20/200\n240/240 [==============================] - 0s 820us/step - loss: 0.2709 - accuracy: 0.9023 - val_loss: 0.3402 - val_accuracy: 0.8807\nEpoch 21/200\n240/240 [==============================] - 0s 821us/step - loss: 0.2690 - accuracy: 0.9026 - val_loss: 0.3368 - val_accuracy: 0.8805\nEpoch 22/200\n240/240 [==============================] - 0s 835us/step - loss: 0.2635 - accuracy: 0.9047 - val_loss: 0.3346 - val_accuracy: 0.8816\nEpoch 23/200\n240/240 [==============================] - 0s 827us/step - loss: 0.2608 - accuracy: 0.9053 - val_loss: 0.3386 - val_accuracy: 0.8812\nEpoch 24/200\n240/240 [==============================] - 0s 815us/step - loss: 0.2585 - accuracy: 0.9072 - val_loss: 0.3383 - val_accuracy: 0.8808\nEpoch 25/200\n240/240 [==============================] - 0s 816us/step - loss: 0.2525 - accuracy: 0.9075 - val_loss: 0.3341 - val_accuracy: 0.8809\nEpoch 26/200\n240/240 [==============================] - 0s 822us/step - loss: 0.2519 - accuracy: 0.9095 - val_loss: 0.3393 - val_accuracy: 0.8810\nEpoch 27/200\n240/240 [==============================] - 0s 815us/step - loss: 0.2499 - accuracy: 0.9101 - val_loss: 0.3312 - val_accuracy: 0.8859\nEpoch 28/200\n240/240 [==============================] - 0s 832us/step - loss: 0.2437 - accuracy: 0.9118 - val_loss: 0.3343 - val_accuracy: 0.8844\nEpoch 29/200\n240/240 [==============================] - 0s 842us/step - loss: 0.2446 - accuracy: 0.9108 - val_loss: 0.3575 - val_accuracy: 0.8759\nEpoch 30/200\n240/240 [==============================] - 0s 821us/step - loss: 0.2402 - accuracy: 0.9124 - val_loss: 0.3381 - val_accuracy: 0.8802\nEpoch 31/200\n240/240 [==============================] - 0s 824us/step - loss: 0.2363 - accuracy: 0.9141 - val_loss: 0.3459 - val_accuracy: 0.8787\nEpoch 32/200\n240/240 [==============================] - 0s 820us/step - loss: 0.2365 - accuracy: 0.9144 - val_loss: 0.3396 - val_accuracy: 0.8804\nEpoch 33/200\n240/240 [==============================] - 0s 827us/step - loss: 0.2328 - accuracy: 0.9159 - val_loss: 0.3367 - val_accuracy: 0.8837\nEpoch 34/200\n240/240 [==============================] - 0s 830us/step - loss: 0.2310 - accuracy: 0.9161 - val_loss: 0.3516 - val_accuracy: 0.8808\nEpoch 35/200\n240/240 [==============================] - 0s 820us/step - loss: 0.2293 - accuracy: 0.9173 - val_loss: 0.3541 - val_accuracy: 0.8766\nEpoch 36/200\n240/240 [==============================] - 0s 826us/step - loss: 0.2268 - accuracy: 0.9183 - val_loss: 0.3503 - val_accuracy: 0.8802\nEpoch 37/200\n240/240 [==============================] - 0s 820us/step - loss: 0.2262 - accuracy: 0.9179 - val_loss: 0.3543 - val_accuracy: 0.8797\nEpoch 38/200\n240/240 [==============================] - 0s 827us/step - loss: 0.2266 - accuracy: 0.9175 - val_loss: 0.3432 - val_accuracy: 0.8825\nEpoch 39/200\n240/240 [==============================] - 0s 817us/step - loss: 0.2190 - accuracy: 0.9208 - val_loss: 0.3364 - val_accuracy: 0.8847\nEpoch 40/200\n240/240 [==============================] - 0s 809us/step - loss: 0.2156 - accuracy: 0.9232 - val_loss: 0.3400 - val_accuracy: 0.8838\nEpoch 41/200\n240/240 [==============================] - 0s 805us/step - loss: 0.2138 - accuracy: 0.9231 - val_loss: 0.3530 - val_accuracy: 0.8776\nEpoch 42/200\n240/240 [==============================] - 0s 816us/step - loss: 0.2154 - accuracy: 0.9224 - val_loss: 0.3461 - val_accuracy: 0.8833\nEpoch 43/200\n240/240 [==============================] - 0s 836us/step - loss: 0.2105 - accuracy: 0.9242 - val_loss: 0.3517 - val_accuracy: 0.8806\nEpoch 44/200\n240/240 [==============================] - 0s 822us/step - loss: 0.2104 - accuracy: 0.9237 - val_loss: 0.3439 - val_accuracy: 0.8852\nEpoch 45/200\n240/240 [==============================] - 0s 832us/step - loss: 0.2064 - accuracy: 0.9250 - val_loss: 0.3457 - val_accuracy: 0.8840\nEpoch 46/200\n240/240 [==============================] - 0s 817us/step - loss: 0.2053 - accuracy: 0.9265 - val_loss: 0.3698 - val_accuracy: 0.8756\nEpoch 47/200\n240/240 [==============================] - 0s 824us/step - loss: 0.2085 - accuracy: 0.9249 - val_loss: 0.3476 - val_accuracy: 0.8824\nEpoch 48/200\n240/240 [==============================] - 0s 837us/step - loss: 0.1996 - accuracy: 0.9290 - val_loss: 0.3581 - val_accuracy: 0.8812\nEpoch 49/200\n240/240 [==============================] - 0s 823us/step - loss: 0.2002 - accuracy: 0.9274 - val_loss: 0.3568 - val_accuracy: 0.8802\nEpoch 50/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1991 - accuracy: 0.9289 - val_loss: 0.3607 - val_accuracy: 0.8798\nEpoch 51/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1987 - accuracy: 0.9278 - val_loss: 0.3527 - val_accuracy: 0.8832\nEpoch 52/200\n240/240 [==============================] - 0s 828us/step - loss: 0.1962 - accuracy: 0.9295 - val_loss: 0.3781 - val_accuracy: 0.8738\nEpoch 53/200\n240/240 [==============================] - 0s 832us/step - loss: 0.1952 - accuracy: 0.9300 - val_loss: 0.3603 - val_accuracy: 0.8807\nEpoch 54/200\n240/240 [==============================] - 0s 824us/step - loss: 0.1956 - accuracy: 0.9295 - val_loss: 0.3941 - val_accuracy: 0.8712\nEpoch 55/200\n240/240 [==============================] - 0s 837us/step - loss: 0.1929 - accuracy: 0.9301 - val_loss: 0.3656 - val_accuracy: 0.8806\nEpoch 56/200\n240/240 [==============================] - 0s 842us/step - loss: 0.1887 - accuracy: 0.9321 - val_loss: 0.3676 - val_accuracy: 0.8788\nEpoch 57/200\n240/240 [==============================] - 0s 827us/step - loss: 0.1892 - accuracy: 0.9321 - val_loss: 0.3638 - val_accuracy: 0.8806\nEpoch 58/200\n240/240 [==============================] - 0s 839us/step - loss: 0.1866 - accuracy: 0.9329 - val_loss: 0.3626 - val_accuracy: 0.8808\nEpoch 59/200\n240/240 [==============================] - 0s 829us/step - loss: 0.1839 - accuracy: 0.9336 - val_loss: 0.3672 - val_accuracy: 0.8797\nEpoch 60/200\n240/240 [==============================] - 0s 842us/step - loss: 0.1823 - accuracy: 0.9347 - val_loss: 0.3803 - val_accuracy: 0.8773\nEpoch 61/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1822 - accuracy: 0.9350 - val_loss: 0.3680 - val_accuracy: 0.8801\nEpoch 62/200\n240/240 [==============================] - 0s 822us/step - loss: 0.1808 - accuracy: 0.9351 - val_loss: 0.3691 - val_accuracy: 0.8815\nEpoch 63/200\n240/240 [==============================] - 0s 835us/step - loss: 0.1815 - accuracy: 0.9343 - val_loss: 0.3730 - val_accuracy: 0.8792\nEpoch 64/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1802 - accuracy: 0.9352 - val_loss: 0.3845 - val_accuracy: 0.8775\nEpoch 65/200\n240/240 [==============================] - 0s 819us/step - loss: 0.1752 - accuracy: 0.9372 - val_loss: 0.3918 - val_accuracy: 0.8770\nEpoch 66/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1748 - accuracy: 0.9375 - val_loss: 0.3770 - val_accuracy: 0.8818\nEpoch 67/200\n240/240 [==============================] - 0s 835us/step - loss: 0.1742 - accuracy: 0.9369 - val_loss: 0.3845 - val_accuracy: 0.8783\nEpoch 68/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1732 - accuracy: 0.9386 - val_loss: 0.3857 - val_accuracy: 0.8798\nEpoch 69/200\n240/240 [==============================] - 0s 832us/step - loss: 0.1738 - accuracy: 0.9373 - val_loss: 0.3766 - val_accuracy: 0.8813\nEpoch 70/200\n240/240 [==============================] - 0s 825us/step - loss: 0.1710 - accuracy: 0.9390 - val_loss: 0.3822 - val_accuracy: 0.8797\nEpoch 71/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1690 - accuracy: 0.9393 - val_loss: 0.3843 - val_accuracy: 0.8784\nEpoch 72/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1683 - accuracy: 0.9383 - val_loss: 0.3878 - val_accuracy: 0.8803\nEpoch 73/200\n240/240 [==============================] - 0s 835us/step - loss: 0.1644 - accuracy: 0.9414 - val_loss: 0.3926 - val_accuracy: 0.8792\nEpoch 74/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1649 - accuracy: 0.9420 - val_loss: 0.3928 - val_accuracy: 0.8773\nEpoch 75/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1657 - accuracy: 0.9408 - val_loss: 0.3850 - val_accuracy: 0.8816\nEpoch 76/200\n240/240 [==============================] - 0s 825us/step - loss: 0.1631 - accuracy: 0.9411 - val_loss: 0.3984 - val_accuracy: 0.8806\nEpoch 77/200\n240/240 [==============================] - 0s 829us/step - loss: 0.1638 - accuracy: 0.9415 - val_loss: 0.3905 - val_accuracy: 0.8804\nEpoch 78/200\n240/240 [==============================] - 0s 833us/step - loss: 0.1644 - accuracy: 0.9413 - val_loss: 0.4109 - val_accuracy: 0.8789\nEpoch 79/200\n240/240 [==============================] - 0s 836us/step - loss: 0.1616 - accuracy: 0.9420 - val_loss: 0.3997 - val_accuracy: 0.8764\nEpoch 80/200\n240/240 [==============================] - 0s 821us/step - loss: 0.1601 - accuracy: 0.9434 - val_loss: 0.4165 - val_accuracy: 0.8758\nEpoch 81/200\n240/240 [==============================] - 0s 817us/step - loss: 0.1573 - accuracy: 0.9438 - val_loss: 0.4006 - val_accuracy: 0.8807\nEpoch 82/200\n240/240 [==============================] - 0s 815us/step - loss: 0.1552 - accuracy: 0.9444 - val_loss: 0.4066 - val_accuracy: 0.8792\nEpoch 83/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1545 - accuracy: 0.9437 - val_loss: 0.4042 - val_accuracy: 0.8792\nEpoch 84/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1538 - accuracy: 0.9444 - val_loss: 0.4126 - val_accuracy: 0.8753\nEpoch 85/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1563 - accuracy: 0.9445 - val_loss: 0.4049 - val_accuracy: 0.8799\nEpoch 86/200\n240/240 [==============================] - 0s 811us/step - loss: 0.1579 - accuracy: 0.9440 - val_loss: 0.4054 - val_accuracy: 0.8770\nEpoch 87/200\n240/240 [==============================] - 0s 810us/step - loss: 0.1511 - accuracy: 0.9464 - val_loss: 0.4145 - val_accuracy: 0.8789\nEpoch 88/200\n240/240 [==============================] - 0s 839us/step - loss: 0.1512 - accuracy: 0.9465 - val_loss: 0.4105 - val_accuracy: 0.8780\nEpoch 89/200\n240/240 [==============================] - 0s 834us/step - loss: 0.1509 - accuracy: 0.9458 - val_loss: 0.4125 - val_accuracy: 0.8784\nEpoch 90/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1515 - accuracy: 0.9461 - val_loss: 0.4185 - val_accuracy: 0.8784\nEpoch 91/200\n240/240 [==============================] - 0s 813us/step - loss: 0.1464 - accuracy: 0.9485 - val_loss: 0.4116 - val_accuracy: 0.8797\nEpoch 92/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1492 - accuracy: 0.9474 - val_loss: 0.4180 - val_accuracy: 0.8784\nEpoch 93/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1501 - accuracy: 0.9460 - val_loss: 0.4196 - val_accuracy: 0.8797\nEpoch 94/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1460 - accuracy: 0.9485 - val_loss: 0.4242 - val_accuracy: 0.8789\nEpoch 95/200\n240/240 [==============================] - 0s 825us/step - loss: 0.1465 - accuracy: 0.9473 - val_loss: 0.4143 - val_accuracy: 0.8794\nEpoch 96/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1436 - accuracy: 0.9490 - val_loss: 0.4276 - val_accuracy: 0.8789\nEpoch 97/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1434 - accuracy: 0.9500 - val_loss: 0.4361 - val_accuracy: 0.8788\nEpoch 98/200\n240/240 [==============================] - 0s 843us/step - loss: 0.1404 - accuracy: 0.9507 - val_loss: 0.4261 - val_accuracy: 0.8783\nEpoch 99/200\n240/240 [==============================] - 0s 816us/step - loss: 0.1391 - accuracy: 0.9507 - val_loss: 0.4392 - val_accuracy: 0.8788\nEpoch 100/200\n240/240 [==============================] - 0s 822us/step - loss: 0.1437 - accuracy: 0.9487 - val_loss: 0.4266 - val_accuracy: 0.8776\nEpoch 101/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1378 - accuracy: 0.9506 - val_loss: 0.4445 - val_accuracy: 0.8767\nEpoch 102/200\n240/240 [==============================] - 0s 813us/step - loss: 0.1385 - accuracy: 0.9506 - val_loss: 0.4478 - val_accuracy: 0.8733\nEpoch 103/200\n240/240 [==============================] - 0s 824us/step - loss: 0.1372 - accuracy: 0.9520 - val_loss: 0.4424 - val_accuracy: 0.8767\nEpoch 104/200\n240/240 [==============================] - 0s 816us/step - loss: 0.1404 - accuracy: 0.9495 - val_loss: 0.4454 - val_accuracy: 0.8783\nEpoch 105/200\n240/240 [==============================] - 0s 827us/step - loss: 0.1396 - accuracy: 0.9499 - val_loss: 0.4475 - val_accuracy: 0.8739\nEpoch 106/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1340 - accuracy: 0.9526 - val_loss: 0.4597 - val_accuracy: 0.8762\nEpoch 107/200\n240/240 [==============================] - 0s 813us/step - loss: 0.1363 - accuracy: 0.9512 - val_loss: 0.4534 - val_accuracy: 0.8743\nEpoch 108/200\n240/240 [==============================] - 0s 844us/step - loss: 0.1341 - accuracy: 0.9525 - val_loss: 0.4524 - val_accuracy: 0.8772\nEpoch 109/200\n240/240 [==============================] - 0s 819us/step - loss: 0.1314 - accuracy: 0.9535 - val_loss: 0.4499 - val_accuracy: 0.8781\nEpoch 110/200\n240/240 [==============================] - 0s 821us/step - loss: 0.1329 - accuracy: 0.9530 - val_loss: 0.4555 - val_accuracy: 0.8772\nEpoch 111/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1320 - accuracy: 0.9532 - val_loss: 0.4566 - val_accuracy: 0.8757\nEpoch 112/200\n240/240 [==============================] - 0s 831us/step - loss: 0.1324 - accuracy: 0.9532 - val_loss: 0.4464 - val_accuracy: 0.8786\nEpoch 113/200\n240/240 [==============================] - 0s 831us/step - loss: 0.1300 - accuracy: 0.9534 - val_loss: 0.4571 - val_accuracy: 0.8765\nEpoch 114/200\n240/240 [==============================] - 0s 843us/step - loss: 0.1294 - accuracy: 0.9543 - val_loss: 0.4609 - val_accuracy: 0.8759\nEpoch 115/200\n240/240 [==============================] - 0s 829us/step - loss: 0.1294 - accuracy: 0.9546 - val_loss: 0.4627 - val_accuracy: 0.8769\nEpoch 116/200\n240/240 [==============================] - 0s 825us/step - loss: 0.1268 - accuracy: 0.9548 - val_loss: 0.4595 - val_accuracy: 0.8785\nEpoch 117/200\n240/240 [==============================] - 0s 813us/step - loss: 0.1279 - accuracy: 0.9546 - val_loss: 0.4736 - val_accuracy: 0.8745\nEpoch 118/200\n240/240 [==============================] - 0s 831us/step - loss: 0.1303 - accuracy: 0.9532 - val_loss: 0.4683 - val_accuracy: 0.8770\nEpoch 119/200\n240/240 [==============================] - 0s 831us/step - loss: 0.1257 - accuracy: 0.9550 - val_loss: 0.4896 - val_accuracy: 0.8696\nEpoch 120/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1275 - accuracy: 0.9539 - val_loss: 0.4760 - val_accuracy: 0.8758\nEpoch 121/200\n240/240 [==============================] - 0s 831us/step - loss: 0.1242 - accuracy: 0.9558 - val_loss: 0.4716 - val_accuracy: 0.8758\nEpoch 122/200\n240/240 [==============================] - 0s 818us/step - loss: 0.1227 - accuracy: 0.9570 - val_loss: 0.4944 - val_accuracy: 0.8706\nEpoch 123/200\n240/240 [==============================] - 0s 816us/step - loss: 0.1225 - accuracy: 0.9565 - val_loss: 0.4770 - val_accuracy: 0.8755\nEpoch 124/200\n240/240 [==============================] - 0s 809us/step - loss: 0.1215 - accuracy: 0.9564 - val_loss: 0.4969 - val_accuracy: 0.8732\nEpoch 125/200\n240/240 [==============================] - 0s 815us/step - loss: 0.1246 - accuracy: 0.9554 - val_loss: 0.4846 - val_accuracy: 0.8766\nEpoch 126/200\n240/240 [==============================] - 0s 821us/step - loss: 0.1210 - accuracy: 0.9578 - val_loss: 0.5119 - val_accuracy: 0.8697\nEpoch 127/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1213 - accuracy: 0.9571 - val_loss: 0.4964 - val_accuracy: 0.8737\nEpoch 128/200\n240/240 [==============================] - 0s 838us/step - loss: 0.1176 - accuracy: 0.9586 - val_loss: 0.4787 - val_accuracy: 0.8767\nEpoch 129/200\n240/240 [==============================] - 0s 824us/step - loss: 0.1200 - accuracy: 0.9581 - val_loss: 0.4976 - val_accuracy: 0.8733\nEpoch 130/200\n240/240 [==============================] - 0s 817us/step - loss: 0.1177 - accuracy: 0.9587 - val_loss: 0.4856 - val_accuracy: 0.8759\nEpoch 131/200\n240/240 [==============================] - 0s 837us/step - loss: 0.1165 - accuracy: 0.9590 - val_loss: 0.4881 - val_accuracy: 0.8742\nEpoch 132/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1198 - accuracy: 0.9581 - val_loss: 0.4968 - val_accuracy: 0.8737\nEpoch 133/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1182 - accuracy: 0.9581 - val_loss: 0.5100 - val_accuracy: 0.8739\nEpoch 134/200\n240/240 [==============================] - 0s 822us/step - loss: 0.1161 - accuracy: 0.9595 - val_loss: 0.5058 - val_accuracy: 0.8727\nEpoch 135/200\n240/240 [==============================] - 0s 828us/step - loss: 0.1198 - accuracy: 0.9578 - val_loss: 0.5181 - val_accuracy: 0.8730\nEpoch 136/200\n240/240 [==============================] - 0s 818us/step - loss: 0.1157 - accuracy: 0.9587 - val_loss: 0.5176 - val_accuracy: 0.8744\nEpoch 137/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1145 - accuracy: 0.9592 - val_loss: 0.5112 - val_accuracy: 0.8773\nEpoch 138/200\n240/240 [==============================] - 0s 830us/step - loss: 0.1121 - accuracy: 0.9614 - val_loss: 0.5236 - val_accuracy: 0.8728\nEpoch 139/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1120 - accuracy: 0.9601 - val_loss: 0.5358 - val_accuracy: 0.8698\nEpoch 140/200\n240/240 [==============================] - 0s 829us/step - loss: 0.1116 - accuracy: 0.9603 - val_loss: 0.5276 - val_accuracy: 0.8717\nEpoch 141/200\n240/240 [==============================] - 0s 827us/step - loss: 0.1111 - accuracy: 0.9612 - val_loss: 0.5178 - val_accuracy: 0.8721\nEpoch 142/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1124 - accuracy: 0.9604 - val_loss: 0.5285 - val_accuracy: 0.8738\nEpoch 143/200\n240/240 [==============================] - 0s 825us/step - loss: 0.1102 - accuracy: 0.9607 - val_loss: 0.5256 - val_accuracy: 0.8734\nEpoch 144/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1095 - accuracy: 0.9608 - val_loss: 0.5328 - val_accuracy: 0.8727\nEpoch 145/200\n240/240 [==============================] - 0s 828us/step - loss: 0.1115 - accuracy: 0.9615 - val_loss: 0.5214 - val_accuracy: 0.8739\nEpoch 146/200\n240/240 [==============================] - 0s 835us/step - loss: 0.1134 - accuracy: 0.9591 - val_loss: 0.5244 - val_accuracy: 0.8730\nEpoch 147/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1082 - accuracy: 0.9619 - val_loss: 0.5270 - val_accuracy: 0.8724\nEpoch 148/200\n240/240 [==============================] - 0s 839us/step - loss: 0.1061 - accuracy: 0.9629 - val_loss: 0.5361 - val_accuracy: 0.8723\nEpoch 149/200\n240/240 [==============================] - 0s 820us/step - loss: 0.1076 - accuracy: 0.9621 - val_loss: 0.5285 - val_accuracy: 0.8749\nEpoch 150/200\n240/240 [==============================] - 0s 813us/step - loss: 0.1068 - accuracy: 0.9626 - val_loss: 0.5590 - val_accuracy: 0.8675\nEpoch 151/200\n240/240 [==============================] - 0s 812us/step - loss: 0.1065 - accuracy: 0.9623 - val_loss: 0.5348 - val_accuracy: 0.8748\nEpoch 152/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1056 - accuracy: 0.9636 - val_loss: 0.5363 - val_accuracy: 0.8725\nEpoch 153/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1077 - accuracy: 0.9621 - val_loss: 0.5535 - val_accuracy: 0.8683\nEpoch 154/200\n240/240 [==============================] - 0s 814us/step - loss: 0.1030 - accuracy: 0.9642 - val_loss: 0.5398 - val_accuracy: 0.8726\nEpoch 155/200\n240/240 [==============================] - 0s 836us/step - loss: 0.1076 - accuracy: 0.9618 - val_loss: 0.5322 - val_accuracy: 0.8757\nEpoch 156/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1065 - accuracy: 0.9630 - val_loss: 0.5485 - val_accuracy: 0.8730\nEpoch 157/200\n240/240 [==============================] - 0s 814us/step - loss: 0.1054 - accuracy: 0.9634 - val_loss: 0.5554 - val_accuracy: 0.8749\nEpoch 158/200\n240/240 [==============================] - 0s 826us/step - loss: 0.1015 - accuracy: 0.9645 - val_loss: 0.5482 - val_accuracy: 0.8736\nEpoch 159/200\n240/240 [==============================] - 0s 814us/step - loss: 0.1021 - accuracy: 0.9641 - val_loss: 0.5769 - val_accuracy: 0.8652\nEpoch 160/200\n240/240 [==============================] - 0s 824us/step - loss: 0.1002 - accuracy: 0.9651 - val_loss: 0.5563 - val_accuracy: 0.8738\nEpoch 161/200\n240/240 [==============================] - 0s 827us/step - loss: 0.1059 - accuracy: 0.9627 - val_loss: 0.5700 - val_accuracy: 0.8712\nEpoch 162/200\n240/240 [==============================] - 0s 823us/step - loss: 0.1009 - accuracy: 0.9647 - val_loss: 0.5543 - val_accuracy: 0.8725\nEpoch 163/200\n240/240 [==============================] - 0s 835us/step - loss: 0.0978 - accuracy: 0.9666 - val_loss: 0.5470 - val_accuracy: 0.8733\nEpoch 164/200\n240/240 [==============================] - 0s 818us/step - loss: 0.1007 - accuracy: 0.9648 - val_loss: 0.5643 - val_accuracy: 0.8731\nEpoch 165/200\n240/240 [==============================] - 0s 832us/step - loss: 0.1005 - accuracy: 0.9653 - val_loss: 0.5695 - val_accuracy: 0.8735\nEpoch 166/200\n240/240 [==============================] - 0s 826us/step - loss: 0.0998 - accuracy: 0.9650 - val_loss: 0.5811 - val_accuracy: 0.8703\nEpoch 167/200\n240/240 [==============================] - 0s 818us/step - loss: 0.0980 - accuracy: 0.9659 - val_loss: 0.5800 - val_accuracy: 0.8723\nEpoch 168/200\n240/240 [==============================] - 0s 851us/step - loss: 0.0967 - accuracy: 0.9665 - val_loss: 0.5753 - val_accuracy: 0.8741\nEpoch 169/200\n240/240 [==============================] - 0s 824us/step - loss: 0.0948 - accuracy: 0.9667 - val_loss: 0.5742 - val_accuracy: 0.8732\nEpoch 170/200\n240/240 [==============================] - 0s 815us/step - loss: 0.0957 - accuracy: 0.9674 - val_loss: 0.5791 - val_accuracy: 0.8729\nEpoch 171/200\n240/240 [==============================] - 0s 828us/step - loss: 0.0965 - accuracy: 0.9661 - val_loss: 0.5887 - val_accuracy: 0.8707\nEpoch 172/200\n240/240 [==============================] - 0s 831us/step - loss: 0.0956 - accuracy: 0.9674 - val_loss: 0.5799 - val_accuracy: 0.8736\nEpoch 173/200\n240/240 [==============================] - 0s 840us/step - loss: 0.0958 - accuracy: 0.9669 - val_loss: 0.5815 - val_accuracy: 0.8742\nEpoch 174/200\n240/240 [==============================] - 0s 825us/step - loss: 0.0953 - accuracy: 0.9668 - val_loss: 0.5971 - val_accuracy: 0.8684\nEpoch 175/200\n240/240 [==============================] - 0s 829us/step - loss: 0.0952 - accuracy: 0.9662 - val_loss: 0.6041 - val_accuracy: 0.8700\nEpoch 176/200\n240/240 [==============================] - 0s 827us/step - loss: 0.0900 - accuracy: 0.9691 - val_loss: 0.5917 - val_accuracy: 0.8719\nEpoch 177/200\n240/240 [==============================] - 0s 815us/step - loss: 0.0961 - accuracy: 0.9661 - val_loss: 0.5820 - val_accuracy: 0.8724\nEpoch 178/200\n240/240 [==============================] - 0s 823us/step - loss: 0.0928 - accuracy: 0.9685 - val_loss: 0.6020 - val_accuracy: 0.8708\nEpoch 179/200\n240/240 [==============================] - 0s 819us/step - loss: 0.0913 - accuracy: 0.9683 - val_loss: 0.6174 - val_accuracy: 0.8692\nEpoch 180/200\n240/240 [==============================] - 0s 824us/step - loss: 0.0953 - accuracy: 0.9662 - val_loss: 0.5866 - val_accuracy: 0.8705\nEpoch 181/200\n240/240 [==============================] - 0s 816us/step - loss: 0.0928 - accuracy: 0.9679 - val_loss: 0.6090 - val_accuracy: 0.8708\nEpoch 182/200\n240/240 [==============================] - 0s 825us/step - loss: 0.0935 - accuracy: 0.9673 - val_loss: 0.5961 - val_accuracy: 0.8719\nEpoch 183/200\n240/240 [==============================] - 0s 859us/step - loss: 0.0901 - accuracy: 0.9692 - val_loss: 0.6071 - val_accuracy: 0.8687\nEpoch 184/200\n240/240 [==============================] - 0s 818us/step - loss: 0.0890 - accuracy: 0.9694 - val_loss: 0.6067 - val_accuracy: 0.8707\nEpoch 185/200\n240/240 [==============================] - 0s 817us/step - loss: 0.0896 - accuracy: 0.9700 - val_loss: 0.6109 - val_accuracy: 0.8716\nEpoch 186/200\n240/240 [==============================] - 0s 815us/step - loss: 0.0913 - accuracy: 0.9677 - val_loss: 0.6126 - val_accuracy: 0.8727\nEpoch 187/200\n240/240 [==============================] - 0s 824us/step - loss: 0.0943 - accuracy: 0.9665 - val_loss: 0.6081 - val_accuracy: 0.8708\nEpoch 188/200\n240/240 [==============================] - 0s 843us/step - loss: 0.0858 - accuracy: 0.9699 - val_loss: 0.6124 - val_accuracy: 0.8736\nEpoch 189/200\n240/240 [==============================] - 0s 825us/step - loss: 0.0900 - accuracy: 0.9693 - val_loss: 0.6128 - val_accuracy: 0.8703\nEpoch 190/200\n240/240 [==============================] - 0s 833us/step - loss: 0.0860 - accuracy: 0.9699 - val_loss: 0.6175 - val_accuracy: 0.8726\nEpoch 191/200\n240/240 [==============================] - 0s 823us/step - loss: 0.0866 - accuracy: 0.9704 - val_loss: 0.6292 - val_accuracy: 0.8693\nEpoch 192/200\n240/240 [==============================] - 0s 831us/step - loss: 0.0866 - accuracy: 0.9709 - val_loss: 0.6300 - val_accuracy: 0.8700\nEpoch 193/200\n240/240 [==============================] - 0s 832us/step - loss: 0.0849 - accuracy: 0.9705 - val_loss: 0.6230 - val_accuracy: 0.8700\nEpoch 194/200\n240/240 [==============================] - 0s 819us/step - loss: 0.0894 - accuracy: 0.9686 - val_loss: 0.6218 - val_accuracy: 0.8737\nEpoch 195/200\n240/240 [==============================] - 0s 828us/step - loss: 0.0864 - accuracy: 0.9707 - val_loss: 0.6399 - val_accuracy: 0.8713\nEpoch 196/200\n240/240 [==============================] - 0s 815us/step - loss: 0.0855 - accuracy: 0.9704 - val_loss: 0.6343 - val_accuracy: 0.8691\nEpoch 197/200\n240/240 [==============================] - 0s 825us/step - loss: 0.0852 - accuracy: 0.9702 - val_loss: 0.6686 - val_accuracy: 0.8656\nEpoch 198/200\n240/240 [==============================] - 0s 824us/step - loss: 0.0852 - accuracy: 0.9712 - val_loss: 0.6403 - val_accuracy: 0.8706\nEpoch 199/200\n240/240 [==============================] - 0s 826us/step - loss: 0.0833 - accuracy: 0.9719 - val_loss: 0.6442 - val_accuracy: 0.8698\nEpoch 200/200\n240/240 [==============================] - 0s 814us/step - loss: 0.0849 - accuracy: 0.9702 - val_loss: 0.6350 - val_accuracy: 0.8728\n\n\n&lt;keras.callbacks.History at 0x7f37e5e348b0&gt;\n\n\n\n40에폭쯤에서 멈췄었어야 할 것 같은데?\n그래서 나온 개념이 early stopping!\n\n- 텐서보드 여는 방법1\n\n%load_ext tensorboard\n# 주피터노트북 (혹은 주피터랩)에서 텐서보드를 임베딩하여 넣을 수 있도록 도와주는 매직펑션\n\n\n#\n!rm -rf logs # lags 파일 삭제\n# !kill 313799\n!kill 507234 \n\n\n#\n# %tensorboard --logdir logs --host 0.0.0.0\n# %tensorboard --logdir logs # &lt;-- 실습에서는 이렇게 하면됩니다.\n\n\n\ntrain accuray는 점점 증가하는데 validation accuracy는 오히려 지지부진해 지다 점점 떨어진다. \\(\\to\\) 오버피팅의 징조\ntraining loss는 계속 떨어지고 있지만 validation loss는 감소하다 어느 시점이 지나면 오히려 점점 증가한다. \\(\\to\\) 오버피팅의 징조\n\n\n\n\n\n\n\n(참고사항) 파이썬 3.10의 경우 아래의 수정이 필요\n\n\n\n?/python3.10/site-packages/tensorboard/_vendor/html5lib/_trie/_base.py 을 열고\nfrom collections import Mapping ### 수정전\nfrom collections.abc import Mapping ### 수정후\n와 같이 수정한다.\n\n왜냐하면 파이썬 3.10부터 from collections import Mapping 가 동작하지 않고 from collections.abc import Mapping 가 동작하도록 문법이 바뀜\n\n\n\n- 텐서보드를 실행하는 방법2\n\n#\n# !tensorboard --logdir logs --host 0.0.0.0\n# !tensorboard --logdir logs # &lt;-- 실습에서는 이렇게 하면됩니다.\n\n\n\n조기종료\n- 텐서보드를 살펴보니 특정에폭 이후에는 오히려 과적합이 진행되는 듯 하다 (학습할수록 손해인듯 하다) \\(\\to\\) 그 특정에폭까지만 학습해보자\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 40s 164ms/step - loss: 0.5284 - accuracy: 0.8175 - val_loss: 0.4271 - val_accuracy: 0.8418\nEpoch 2/200\n240/240 [==============================] - 40s 165ms/step - loss: 0.3579 - accuracy: 0.8672 - val_loss: 0.3539 - val_accuracy: 0.8719\nEpoch 3/200\n240/240 [==============================] - 40s 165ms/step - loss: 0.3207 - accuracy: 0.8802 - val_loss: 0.3578 - val_accuracy: 0.8716\n\n\n&lt;keras.callbacks.History at 0x7f37e5586550&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 40s 165ms/step - loss: 0.2977 - accuracy: 0.8891 - val_loss: 0.3353 - val_accuracy: 0.8752\nEpoch 2/200\n240/240 [==============================] - 40s 165ms/step - loss: 0.2783 - accuracy: 0.8941 - val_loss: 0.3460 - val_accuracy: 0.8802\n\n\n&lt;keras.callbacks.History at 0x7f37e4c0ca90&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2627 - accuracy: 0.9010 - val_loss: 0.3251 - val_accuracy: 0.8799\nEpoch 2/200\n240/240 [==============================] - 39s 165ms/step - loss: 0.2463 - accuracy: 0.9068 - val_loss: 0.3460 - val_accuracy: 0.8823\n\n\n&lt;keras.callbacks.History at 0x7f37e44f8790&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2339 - accuracy: 0.9104 - val_loss: 0.3190 - val_accuracy: 0.8873\nEpoch 2/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2207 - accuracy: 0.9157 - val_loss: 0.3449 - val_accuracy: 0.8837\n\n\n&lt;keras.callbacks.History at 0x7f37e55660a0&gt;\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2111 - accuracy: 0.9188 - val_loss: 0.3703 - val_accuracy: 0.8785\nEpoch 2/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2030 - accuracy: 0.9225 - val_loss: 0.3199 - val_accuracy: 0.8922\nEpoch 3/200\n240/240 [==============================] - 39s 164ms/step - loss: 0.2002 - accuracy: 0.9236 - val_loss: 0.3231 - val_accuracy: 0.8881\n\n\n&lt;keras.callbacks.History at 0x7f37e5521a30&gt;\n\n\n- 몇 번 좀 참았다가 멈추면 좋겠다.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=5) # 좀더 참다가 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1)\n\nEpoch 1/200\n240/240 [==============================] - 40s 167ms/step - loss: 0.5611 - accuracy: 0.8114 - val_loss: 0.4034 - val_accuracy: 0.8518\nEpoch 2/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.3582 - accuracy: 0.8669 - val_loss: 0.3627 - val_accuracy: 0.8688\nEpoch 3/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.3252 - accuracy: 0.8784 - val_loss: 0.3576 - val_accuracy: 0.8723\nEpoch 4/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2966 - accuracy: 0.8887 - val_loss: 0.3566 - val_accuracy: 0.8767\nEpoch 5/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2789 - accuracy: 0.8959 - val_loss: 0.3215 - val_accuracy: 0.8852\nEpoch 6/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2621 - accuracy: 0.9010 - val_loss: 0.3186 - val_accuracy: 0.8844\nEpoch 7/200\n240/240 [==============================] - 40s 167ms/step - loss: 0.2466 - accuracy: 0.9085 - val_loss: 0.3297 - val_accuracy: 0.8852\nEpoch 8/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2350 - accuracy: 0.9107 - val_loss: 0.3187 - val_accuracy: 0.8905\nEpoch 9/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2266 - accuracy: 0.9143 - val_loss: 0.3132 - val_accuracy: 0.8921\nEpoch 10/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2114 - accuracy: 0.9196 - val_loss: 0.3212 - val_accuracy: 0.8938\nEpoch 11/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.2059 - accuracy: 0.9206 - val_loss: 0.3292 - val_accuracy: 0.8896\nEpoch 12/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.1969 - accuracy: 0.9247 - val_loss: 0.3230 - val_accuracy: 0.8959\nEpoch 13/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.1868 - accuracy: 0.9274 - val_loss: 0.3289 - val_accuracy: 0.8929\nEpoch 14/200\n240/240 [==============================] - 40s 166ms/step - loss: 0.1807 - accuracy: 0.9306 - val_loss: 0.3586 - val_accuracy: 0.8892\n\n\n&lt;keras.callbacks.History at 0x7f37e44f8d60&gt;\n\n\n- 텐서보드로 그려보자?\n\n#\n# %tensorboard --logdir logs --host 0.0.0.0\n# 아무것도 안나온다 -&gt; 왜? cb1을 써야 텐서보드가 나옴\n\n- 조기종료와 텐서보드를 같이 쓰려면?\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\ncb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=7) # 좀더 참다가 멈추어라\nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2]) # callbacks에 cb1,cb2 리스트로 같이 전달.\n\nEpoch 1/200\n240/240 [==============================] - 1s 1ms/step - loss: 0.7273 - accuracy: 0.7593 - val_loss: 0.5007 - val_accuracy: 0.8295\nEpoch 2/200\n240/240 [==============================] - 0s 825us/step - loss: 0.4678 - accuracy: 0.8409 - val_loss: 0.4602 - val_accuracy: 0.8397\nEpoch 3/200\n240/240 [==============================] - 0s 823us/step - loss: 0.4223 - accuracy: 0.8521 - val_loss: 0.4262 - val_accuracy: 0.8525\nEpoch 4/200\n240/240 [==============================] - 0s 832us/step - loss: 0.3984 - accuracy: 0.8594 - val_loss: 0.4001 - val_accuracy: 0.8612\nEpoch 5/200\n240/240 [==============================] - 0s 825us/step - loss: 0.3773 - accuracy: 0.8675 - val_loss: 0.3906 - val_accuracy: 0.8614\nEpoch 6/200\n240/240 [==============================] - 0s 838us/step - loss: 0.3638 - accuracy: 0.8713 - val_loss: 0.3897 - val_accuracy: 0.8604\nEpoch 7/200\n240/240 [==============================] - 0s 833us/step - loss: 0.3516 - accuracy: 0.8749 - val_loss: 0.3808 - val_accuracy: 0.8672\nEpoch 8/200\n240/240 [==============================] - 0s 822us/step - loss: 0.3425 - accuracy: 0.8773 - val_loss: 0.3640 - val_accuracy: 0.8721\nEpoch 9/200\n240/240 [==============================] - 0s 836us/step - loss: 0.3347 - accuracy: 0.8811 - val_loss: 0.3567 - val_accuracy: 0.8739\nEpoch 10/200\n240/240 [==============================] - 0s 836us/step - loss: 0.3249 - accuracy: 0.8834 - val_loss: 0.3585 - val_accuracy: 0.8737\nEpoch 11/200\n240/240 [==============================] - 0s 825us/step - loss: 0.3175 - accuracy: 0.8868 - val_loss: 0.3582 - val_accuracy: 0.8728\nEpoch 12/200\n240/240 [==============================] - 0s 827us/step - loss: 0.3104 - accuracy: 0.8884 - val_loss: 0.3531 - val_accuracy: 0.8734\nEpoch 13/200\n240/240 [==============================] - 0s 819us/step - loss: 0.3047 - accuracy: 0.8909 - val_loss: 0.3513 - val_accuracy: 0.8758\nEpoch 14/200\n240/240 [==============================] - 0s 838us/step - loss: 0.3000 - accuracy: 0.8915 - val_loss: 0.3624 - val_accuracy: 0.8714\nEpoch 15/200\n240/240 [==============================] - 0s 828us/step - loss: 0.2962 - accuracy: 0.8937 - val_loss: 0.3492 - val_accuracy: 0.8781\nEpoch 16/200\n240/240 [==============================] - 0s 828us/step - loss: 0.2948 - accuracy: 0.8934 - val_loss: 0.3793 - val_accuracy: 0.8650\nEpoch 17/200\n240/240 [==============================] - 0s 837us/step - loss: 0.2875 - accuracy: 0.8951 - val_loss: 0.3459 - val_accuracy: 0.8783\nEpoch 18/200\n240/240 [==============================] - 0s 829us/step - loss: 0.2834 - accuracy: 0.8963 - val_loss: 0.3452 - val_accuracy: 0.8775\nEpoch 19/200\n240/240 [==============================] - 0s 843us/step - loss: 0.2788 - accuracy: 0.8977 - val_loss: 0.3482 - val_accuracy: 0.8769\nEpoch 20/200\n240/240 [==============================] - 0s 837us/step - loss: 0.2737 - accuracy: 0.8997 - val_loss: 0.3446 - val_accuracy: 0.8759\nEpoch 21/200\n240/240 [==============================] - 0s 814us/step - loss: 0.2724 - accuracy: 0.9018 - val_loss: 0.3377 - val_accuracy: 0.8797\nEpoch 22/200\n240/240 [==============================] - 0s 839us/step - loss: 0.2666 - accuracy: 0.9033 - val_loss: 0.3370 - val_accuracy: 0.8832\nEpoch 23/200\n240/240 [==============================] - 0s 827us/step - loss: 0.2638 - accuracy: 0.9039 - val_loss: 0.3398 - val_accuracy: 0.8789\nEpoch 24/200\n240/240 [==============================] - 0s 833us/step - loss: 0.2603 - accuracy: 0.9067 - val_loss: 0.3458 - val_accuracy: 0.8812\nEpoch 25/200\n240/240 [==============================] - 0s 831us/step - loss: 0.2549 - accuracy: 0.9072 - val_loss: 0.3407 - val_accuracy: 0.8779\nEpoch 26/200\n240/240 [==============================] - 0s 821us/step - loss: 0.2546 - accuracy: 0.9067 - val_loss: 0.3381 - val_accuracy: 0.8848\nEpoch 27/200\n240/240 [==============================] - 0s 823us/step - loss: 0.2519 - accuracy: 0.9093 - val_loss: 0.3401 - val_accuracy: 0.8806\nEpoch 28/200\n240/240 [==============================] - 0s 818us/step - loss: 0.2461 - accuracy: 0.9112 - val_loss: 0.3469 - val_accuracy: 0.8812\nEpoch 29/200\n240/240 [==============================] - 0s 840us/step - loss: 0.2468 - accuracy: 0.9096 - val_loss: 0.3478 - val_accuracy: 0.8807\n\n\n&lt;keras.callbacks.History at 0x7f37e5426c40&gt;\n\n\n\n#\n# 조기종료가 구현된 그림이 출력\n# %tensorboard --logdir logs --host 0.0.0.0\n\n\n\n\nEarlyStopping을 적용한 결과 텐서보드로 실행한 결과\n\n\n\n# !kill 508556\n# !rm -rf logs\n\n\n\n하이퍼파라메터 선택\n- 하이퍼파라메터 설정\n\nfrom tensorboard.plugins.hparams import api as hp\n\n\na=net.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 368us/step - loss: 0.3726 - accuracy: 0.8690\n\n\n\n!rm -rf logs\nfor u in [50,5000]:\n    for d in [0.0,0.5]:\n        for o in ['adam','sgd']:\n            logdir = 'logs/hpguebin_{}_{}_{}'.format(u,d,o)\n            with tf.summary.create_file_writer(logdir).as_default():\n                net = tf.keras.Sequential()\n                net.add(tf.keras.layers.Flatten())\n                net.add(tf.keras.layers.Dense(u,activation='relu'))\n                net.add(tf.keras.layers.Dropout(d))\n                net.add(tf.keras.layers.Dense(10,activation='softmax'))\n                net.compile(optimizer=o,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])\n                cb3 = hp.KerasCallback(logdir, {'유닛수':u, '드랍아웃비율':d, '옵티마이저':o})\n                net.fit(X,y,epochs=3,callbacks=cb3)\n                _rslt=net.evaluate(XX,yy)  # test accuracy, test recall\n                _mymetric=_rslt[1]*0.8 + _rslt[2]*0.2\n                tf.summary.scalar('애큐러시와리컬의가중평균(테스트셋)', _mymetric, step=1)\n\nEpoch 1/3\n1875/1875 [==============================] - 1s 523us/step - loss: 0.5223 - accuracy: 0.8170 - recall: 0.7559\nEpoch 2/3\n1875/1875 [==============================] - 1s 513us/step - loss: 0.3940 - accuracy: 0.8602 - recall: 0.8296\nEpoch 3/3\n1875/1875 [==============================] - 1s 517us/step - loss: 0.3578 - accuracy: 0.8711 - recall: 0.8455\n313/313 [==============================] - 0s 390us/step - loss: 0.3902 - accuracy: 0.8600 - recall: 0.8347\nEpoch 1/3\n1875/1875 [==============================] - 1s 483us/step - loss: 0.7881 - accuracy: 0.7447 - recall: 0.5665\nEpoch 2/3\n1875/1875 [==============================] - 1s 478us/step - loss: 0.5283 - accuracy: 0.8214 - recall: 0.7521\nEpoch 3/3\n1875/1875 [==============================] - 1s 465us/step - loss: 0.4769 - accuracy: 0.8361 - recall: 0.7843\n313/313 [==============================] - 0s 401us/step - loss: 0.4903 - accuracy: 0.8302 - recall: 0.7817\nEpoch 1/3\n1875/1875 [==============================] - 1s 527us/step - loss: 0.7370 - accuracy: 0.7401 - recall: 0.6217\nEpoch 2/3\n1875/1875 [==============================] - 1s 522us/step - loss: 0.5597 - accuracy: 0.7993 - recall: 0.7249\nEpoch 3/3\n1875/1875 [==============================] - 1s 520us/step - loss: 0.5226 - accuracy: 0.8110 - recall: 0.7480\n313/313 [==============================] - 0s 393us/step - loss: 0.4303 - accuracy: 0.8447 - recall: 0.7962\nEpoch 1/3\n1875/1875 [==============================] - 1s 488us/step - loss: 1.0124 - accuracy: 0.6446 - recall: 0.4268\nEpoch 2/3\n1875/1875 [==============================] - 1s 488us/step - loss: 0.7222 - accuracy: 0.7500 - recall: 0.6124\nEpoch 3/3\n1875/1875 [==============================] - 1s 488us/step - loss: 0.6513 - accuracy: 0.7767 - recall: 0.6656\n313/313 [==============================] - 0s 382us/step - loss: 0.5226 - accuracy: 0.8165 - recall: 0.7442\nEpoch 1/3\n1875/1875 [==============================] - 28s 15ms/step - loss: 0.4779 - accuracy: 0.8285 - recall: 0.7886\nEpoch 2/3\n1875/1875 [==============================] - 28s 15ms/step - loss: 0.3614 - accuracy: 0.8677 - recall: 0.8425\nEpoch 3/3\n1875/1875 [==============================] - 28s 15ms/step - loss: 0.3205 - accuracy: 0.8819 - recall: 0.8609\n313/313 [==============================] - 0s 994us/step - loss: 0.3640 - accuracy: 0.8684 - recall: 0.8475\nEpoch 1/3\n1875/1875 [==============================] - 7s 3ms/step - loss: 0.6689 - accuracy: 0.7886 - recall: 0.6448\nEpoch 2/3\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.4828 - accuracy: 0.8371 - recall: 0.7770\nEpoch 3/3\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.4422 - accuracy: 0.8490 - recall: 0.8011\n313/313 [==============================] - 0s 967us/step - loss: 0.4644 - accuracy: 0.8375 - recall: 0.7946\nEpoch 1/3\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.5713 - accuracy: 0.7975 - recall: 0.7544\nEpoch 2/3\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.4424 - accuracy: 0.8395 - recall: 0.8069\nEpoch 3/3\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.4066 - accuracy: 0.8521 - recall: 0.8217\n313/313 [==============================] - 0s 1ms/step - loss: 0.3829 - accuracy: 0.8602 - recall: 0.8310\nEpoch 1/3\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.6914 - accuracy: 0.7763 - recall: 0.6367\nEpoch 2/3\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.5047 - accuracy: 0.8304 - recall: 0.7671\nEpoch 3/3\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.4622 - accuracy: 0.8413 - recall: 0.7901\n313/313 [==============================] - 0s 985us/step - loss: 0.4639 - accuracy: 0.8398 - recall: 0.7945\n\n\n\n#\n%tensorboard --logdir logs --host 0.0.0.0"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#숙제",
    "href": "posts/3_STBDA2022/2022_05_23_(12주차)_5월23일.html#숙제",
    "title": "[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들",
    "section": "숙제",
    "text": "숙제\n- 아래의 네트워크에서 옵티마이저를 adam, sgd를 선택하여 각각 적합시켜보고 testset의 loss를 성능비교를 하라. epoch은 5정도로 설정하라.\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer=???,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-wvV9xuYHvx0Gn7KDGNJbwj"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#강의영상",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#강의영상",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "",
    "text": "youtube: https://youtube.com/playlist?list=PLQqh36zP38-wvV9xuYHvx0Gn7KDGNJbwj"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#import",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#import",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "import",
    "text": "import\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntf.config.experimental.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')\n\n\nbasis가 orthogonal하냐?\noverfitting 이슈는 변수가 많다고해서 무조건 발생하는 것은 아님! 변수가 많이 있어도 orthogonal하게 잘 넣으면 심지어 무한대의 basis를 갖고있어도 overfitting이슈가 발생하지 않는다. 이렇게 맞추는 것을 semi-parametric modeling 이라고 한다!\n\nex. 직선의 basis: 절편과 기울기"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#중간고사-관련-잡담",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#중간고사-관련-잡담",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "중간고사 관련 잡담",
    "text": "중간고사 관련 잡담\n\n중간고사 3번문제\n- 특이한모형: 오버핏이 일어날 수 없는 모형이다. - 유의미한 coef: 상수항(bias), \\(\\cos(t)\\)의 계수, \\(\\cos(2t)\\)의 계수, \\(\\cos(5t)\\)의 계수. - 유의미하지 않은 coef: \\(\\cos(3t)\\)의 계수, \\(\\cos(4t)\\)의 계수 - 유의미하지 않은 계수는 \\(n%\\)이 커질수록 0으로 추정된다 = \\(\\cos(3t)\\)와 \\(\\cos(5t)\\)는 사용자가 임의로 제외하지 않아도 결국 모형에서 알아서 제거된다 = overfit이 일어나지 않는다. 모형이 알아서 유의미한 변수만 뽑아서 fit하는 느낌\n- 3번문제는 overfit이 일어나지 않는다. 이러한 신기한 일이 일어나는 이유는 모든 설명변수가 직교하기 때문임. - 이런 모형의 장점: overfit이 일어날 위험이 없으므로 train/test로 나누어 학습할 이유가 없다. (샘플만 버리는 꼴, test에 빼둔 observation까지 모아서 학습해 \\(\\beta\\)를 좀 더 정확히 추론하는게 차라리 더 이득) - 이러한 모형에서 할일: 추정된 계수들이 0인지 아닌지만 test하면 된다. (이것을 유의성검정이라고 한다)\n- 직교기저의 예시 - 빨강과 파랑을 255,255만큼 섞으면 보라색이 된다. - 빨강과 파랑과 노랑을 각각 255,255,255만큼 섞으면 검은색이 된다. - 임의의 어떠한 색도 빨강,파랑,노랑의 조합으로 표현가능하다. 즉 \\(\\text{color}= \\text{red}*\\beta_1 + \\text{blue}*\\beta_2 + \\text{yellow}*\\beta_3\\) 이다. - (빨,파,노)는 색을 표현하는 basis이다. (적절한 \\(\\beta_1,\\beta_2,\\beta_3\\)을 구하기만 하면 임의의 색도 표현가능) - (빨,보,노)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요할때 보라색-빨간색을 하면되니까) - (빨,보,검)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요하면 보라색-빨간색을 하면되고, 노란색이 필요하면 검정색-보라색을 하면 되니까) - (빨,파,노)는 직교기저이다.\n- 3번에서 알아둘 것: (1) 직교기저의 개념 (추후 재설명) (2) 임의의 색을 표현하려면 3개의 basis가 필요함\n\n\n중간고사 1-(3)번 문제\n- 그림을 그려보자.\n\n_x= tf.constant(np.arange(1,10001)/10000)\n_y= tnp.random.randn(10000) + (0.5 + 2*_x)\nplt.plot(_x,_y,'.',alpha=0.1)\n\n\n\n\n- 저것 꼭 10000개 다 모아서 loss계산해야할까?\n\nplt.plot(_x,_y,'.',alpha=0.1)\nplt.plot(_x[::10],_y[::10],'.') # 10씩 jump해서 점이 찍힘.\n\n\n\n\n- 대충 이정도만 모아서 해도 비슷하지 않을까? \\(\\to\\) 해보자!\n\n주황색만가지고 기울기, 절편 추론을 해보자."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#경사하강법과-확률적경사하강법",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#경사하강법과-확률적경사하강법",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "경사하강법과 확률적경사하강법",
    "text": "경사하강법과 확률적경사하강법\n원래 확률적경사하강법이 딥러닝을 하려고 만든것이 아니다.(만들어진 의도와 사용이 다름) 그런데 거기에 맞게 진화를 한 것. 그래서 되게 헷갈린다…\n\nver1: 모든 샘플을 사용하여 slope계산 (gradient descent)\n- 단순회귀분석에서 샘플 10개 관측: \\((x_1,y_1),\\dots,(x_{10},y_{10})\\).\n(epoch1) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\nfor문 이 3번 돌아감.\n\n\nver2: 하나의 샘플만 사용하여 slope계산\n(epoch1) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\nfor문이 30번 돌아감.\n\n\nver3: \\(m(\\leq n)\\)개의 샘플만 사용하여 slope계산 (mini-batch)\n\\(m=3\\)이라고 하자.\n(epoch1) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n1,2,3번 observation들만 뽑아서 loss를 계산해서 그것만가지고 update \\(\\to\\) 4,5,6먼만 가지고 loss계산해서 update \\(\\to\\) 7,8,9를가지고 loss구하고 업데이트 \\(\\to\\) 남은 하나가지고 loss구하고 업데이트… // 한 에폭 끝!\n(epoch2) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\n\n용어의 정리\n\n옛날 (좀 더 엄밀)\n- ver1: gradient descent, batch gradient descent\n- ver2: stochastic gradient descent\n- ver3: mini-batch gradient descent, mini-batch stochastic gradient descent\n\n\n요즘\n- ver1: gradient descent\n- ver2: stochastic gradient descent with batch size = 1\n- ver3: stochastic gradient descent - https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고.\n\n지금은 mini-batch가 포함된 방법을 stochastic gradient descent라고 부른다. 왜냐하면 1,2는 사장된 방법. 버전 3만 쓴다.(유명한 사람들이 학회에서 그렇게 부르기 시작했음.)\nnote: 이렇게 많이 쓰는 이유? ver1,2는 사실상 없는 방법이므로\n\n\n\nver1,2,3 이외에 좀 더 지저분한 것들이 있다.\n- ver2,3에서 샘플을 셔플할 수도 있다.\n- ver3에서 일부 샘플이 학습에 참여 안하는 버전도 있다.\n- 개인적 생각: 크게3개정도만 알면 괜찮고 나머지는 그렇게 유의미하지 않아보인다.\n\n\nDiscussion\n- 핵심개념\n\n메모리사용량1: ver1 &gt; ver3 &gt; ver2\n계산속도: ver1 &gt; ver3 &gt; ver2\nlocal-min에 갇힘: ver1 &gt; ver3 &gt; ver2\n\n로컬미니멈에 갇힌가는 건 로컬미니멈을 잘 찾는다라고 생각 (정신이 제대로 박힌애)\n대충대충 학습하면 로컬미니멈에서 딱 멈춰야하는데 대충대충 계산해서 기울기가 딱 \\(0\\)이 안나오는 것. (운 좋게 탈출하는 경우도 있음.)\n- 본질: GPU 메모리가 한정되어 있어서 ver1을 쓰지는 못한다. GPU 메모리를 가장 적게쓰는것은 ver2인데 이것은 너무 불안정하다.\n- 틀리진 않지만 어색한 블로그 정리 내용들\n\n경사하강법은 종종 국소최소점에 갇히는 문제가 있다. 이를 해결하기 위해서 등장한 방법이 확률적 경사하강법이다. –&gt; 영 틀린말은 아니지만 그걸 의도하고 만든건 아님 (이건 side effect)\n경사하강법은 계산시간이 오래걸린다. 계산을 빠르게 하기 위해서 등장한 방법이 확률적 경사하강법이다. –&gt; 1회 업데이트는 빠르게 계산함. 하지만 그것이 최적의 \\(\\beta\\)를 빠르게 얻을 수 있다는 의미는 아님\n\n원래 경사하강법은 local minimum에 빠지기 쉬운 알고리즘! 그런데 그나마 둘 중에 비교를 하자면 확률적으로 하면 로컬 미니멈에 빠졌다가 어쩌다 운좋아서 튀어 나가는 경우가 있다.\n동일한 컴퓨터 자원으로 수렴을 더 빨리 시킬 수 있냐? 그건 아님 (그건 모름).\n그럼 왜 쓰냐???\n메모리 사용량만 보면 됩니다!"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#fashion_mnist-모듈",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#fashion_mnist-모듈",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "fashion_mnist 모듈",
    "text": "fashion_mnist 모듈\n\ntf.keras.datasets.fashion_mnist.load_data()\n- tf.keras.datasets.fashion_mnist.load_data 의 리턴값 조사\n\ntf.keras.datasets.fashion_mnist.load_data??\n\n\nSignature: tf.keras.datasets.fashion_mnist.load_data()\nSource:   \n@keras_export(\"keras.datasets.fashion_mnist.load_data\")\ndef load_data():\n    \"\"\"Loads the Fashion-MNIST dataset.\n    This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories,\n    along with a test set of 10,000 images. This dataset can be used as\n    a drop-in replacement for MNIST.\n    The classes are:\n    | Label | Description |\n    |:-----:|-------------|\n    |   0   | T-shirt/top |\n    |   1   | Trouser     |\n    |   2   | Pullover    |\n    |   3   | Dress       |\n    |   4   | Coat        |\n    |   5   | Sandal      |\n    |   6   | Shirt       |\n    |   7   | Sneaker     |\n    |   8   | Bag         |\n    |   9   | Ankle boot  |\n    Returns:\n      Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n    **x_train**: uint8 NumPy array of grayscale image data with shapes\n      `(60000, 28, 28)`, containing the training data.\n    **y_train**: uint8 NumPy array of labels (integers in range 0-9)\n      with shape `(60000,)` for the training data.\n    **x_test**: uint8 NumPy array of grayscale image data with shapes\n      (10000, 28, 28), containing the test data.\n    **y_test**: uint8 NumPy array of labels (integers in range 0-9)\n      with shape `(10000,)` for the test data.\n    Example:\n    ```python\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n    assert x_train.shape == (60000, 28, 28)\n    assert x_test.shape == (10000, 28, 28)\n    assert y_train.shape == (60000,)\n    assert y_test.shape == (10000,)\n    ```\n    License:\n      The copyright for Fashion-MNIST is held by Zalando SE.\n      Fashion-MNIST is licensed under the [MIT license](\n      https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE).\n    \"\"\"\n    dirname = os.path.join(\"datasets\", \"fashion-mnist\")\n    base = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n    files = [\n        \"train-labels-idx1-ubyte.gz\",\n        \"train-images-idx3-ubyte.gz\",\n        \"t10k-labels-idx1-ubyte.gz\",\n        \"t10k-images-idx3-ubyte.gz\",\n    ]\n    paths = []\n    for fname in files:\n        paths.append(get_file(fname, origin=base + fname, cache_subdir=dirname))\n    with gzip.open(paths[0], \"rb\") as lbpath:\n        y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n    with gzip.open(paths[1], \"rb\") as imgpath:\n        x_train = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(\n            len(y_train), 28, 28\n        )\n    with gzip.open(paths[2], \"rb\") as lbpath:\n        y_test = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n    with gzip.open(paths[3], \"rb\") as imgpath:\n        x_test = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(\n            len(y_test), 28, 28\n        )\n    return (x_train, y_train), (x_test, y_test)\nFile:      ~/anaconda3/envs/torch/lib/python3.8/site-packages/keras/datasets/fashion_mnist.py\nType:      function\n\n\n\n\n\n데이터생성 및 탐색\n- tf.keras.datasets.fashion_mnist.load_data()를 이용한 데이터 생성\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 2s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\n\n\n- 차원확인\n\nx_train.shape, y_train.shape, x_test.shape,y_test.shape\n\n((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))\n\n\n\n60000은 obs숫자인듯\n(28,28)은 28픽셀,28픽셀을 의미하는듯\ntrain/test는 6:1로 나눈것 같음\n\n- 첫번째 obs\n\nplt.imshow(x_train[0])\n\n&lt;matplotlib.image.AxesImage at 0x7febf458eeb0&gt;\n\n\n\n\n\n\ny_train[0]\n\n9\n\n\n\n첫번쨰 obs에 대응하는 라벨\n\n- 첫번째 obs와 동일한 라벨을 가지는 그림을 찾아보자.\n\nnp.where(y_train==9)\n\n(array([    0,    11,    15, ..., 59932, 59970, 59978]),)\n\n\n\ny_train[11]\n\n9\n\n\n\nplt.imshow(x_train[11])\n\n&lt;matplotlib.image.AxesImage at 0x7f65219f9e80&gt;\n\n\n\n\n\n\n\n데이터구조\n- \\({\\bf X}\\): (n,28,28)\n- \\({\\bf y}\\): (n,) , \\(y=0,1,2,3,\\dots,9\\)"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#예제1",
    "href": "posts/3_STBDA2022/2022_05_02_(9주차)_5월2일(2).html#예제1",
    "title": "[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법",
    "section": "예제1",
    "text": "예제1\n\n데이터 정리\n- y=0,1에 대응하는 이미지만 정리하자. (우리가 배운건 로지스틱이니까)\n\ny= y_train[(y_train==0) | (y_train==1)].reshape(-1,1)\nX= x_train[(y_train==0) | (y_train==1)].reshape(-1,784)\nyy= y_test[(y_test==0) | (y_test==1)].reshape(-1,1)\nXX= x_test[(y_test==0) | (y_test==1)].reshape(-1,784)\n\n\nX.shape, y.shape, XX.shape, yy.shape\n\n((12000, 784), (12000, 1), (2000, 784), (2000, 1))\n\n\n\n\n풀이1: 은닉층을 포함한 신경망 // epochs=100\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -&gt; \"node1\"\n    \"x2\" -&gt; \"node1\"\n    \"..\" -&gt; \"node1\"\n\n    \"x784\" -&gt; \"node1\"\n    \"x1\" -&gt; \"node2\"\n    \"x2\" -&gt; \"node2\"\n    \"..\" -&gt; \"node2\"\n    \"x784\" -&gt; \"node2\"\n\n    \"x1\" -&gt; \"...\"\n    \"x2\" -&gt; \"...\"\n    \"..\" -&gt; \"...\"\n    \"x784\" -&gt; \"...\"\n\n    \"x1\" -&gt; \"node30\"\n    \"x2\" -&gt; \"node30\"\n    \"..\" -&gt; \"node30\"\n    \"x784\" -&gt; \"node30\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -&gt; \"y\"\n    \"node2\" -&gt; \"y\"\n    \"...\" -&gt; \"y\"\n    \"node30\" -&gt; \"y\"\n    label = \"Layer 2: sigmoid\"\n}\n''')\n\n\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='sgd',loss=tf.losses.binary_crossentropy)\nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 122ms/step - loss: 220.9145\nEpoch 2/100\n1/1 [==============================] - 0s 9ms/step - loss: 6800.3174\nEpoch 3/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7045\nEpoch 4/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7012\nEpoch 5/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7004\nEpoch 6/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6997\nEpoch 7/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6991\nEpoch 8/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6985\nEpoch 9/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6979\nEpoch 10/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6976\nEpoch 11/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6973\nEpoch 12/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6970\nEpoch 13/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6968\nEpoch 14/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6966\nEpoch 15/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6964\nEpoch 16/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6963\nEpoch 17/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6961\nEpoch 18/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6959\nEpoch 19/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6958\nEpoch 20/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6956\nEpoch 21/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6955\nEpoch 22/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6953\nEpoch 23/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6952\nEpoch 24/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6951\nEpoch 25/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6949\nEpoch 26/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6948\nEpoch 27/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6947\nEpoch 28/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6946\nEpoch 29/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6945\nEpoch 30/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6944\nEpoch 31/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6943\nEpoch 32/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6942\nEpoch 33/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6942\nEpoch 34/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6941\nEpoch 35/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6940\nEpoch 36/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6940\nEpoch 37/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6939\nEpoch 38/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6939\nEpoch 39/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6938\nEpoch 40/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6937\nEpoch 41/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6937\nEpoch 42/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6936\nEpoch 43/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6936\nEpoch 44/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6935\nEpoch 45/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6935\nEpoch 46/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6934\nEpoch 47/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6934\nEpoch 48/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6934\nEpoch 49/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 50/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 51/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 52/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 53/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 54/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 55/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 56/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 57/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 58/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 59/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 60/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 61/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 62/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6933\nEpoch 63/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 64/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 65/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 66/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 67/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6933\nEpoch 68/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 69/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 70/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 71/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 72/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 73/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 74/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 75/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 76/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 77/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 78/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 79/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 80/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 81/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 82/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 83/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 84/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.6932\nEpoch 85/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 86/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6932\nEpoch 87/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 88/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 89/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 90/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 91/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 92/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 93/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 94/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 95/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 96/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 97/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 98/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 99/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\nEpoch 100/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6932\n\n\n&lt;keras.callbacks.History at 0x7f640c5e9c40&gt;\n\n\n\nnp.mean((net(X)&gt;0.5) == y)\n\n0.5000833333333333\n\n\n\nnp.mean((net(XX)&gt;0.5) == yy)\n\n0.5\n\n\n\n\n풀이2: 옵티마이저 개선\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy)\nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 138ms/step - loss: 220.9145\nEpoch 2/100\n1/1 [==============================] - 0s 10ms/step - loss: 88.9490\nEpoch 3/100\n1/1 [==============================] - 0s 10ms/step - loss: 7.5895\nEpoch 4/100\n1/1 [==============================] - 0s 9ms/step - loss: 33.7521\nEpoch 5/100\n1/1 [==============================] - 0s 9ms/step - loss: 40.2290\nEpoch 6/100\n1/1 [==============================] - 0s 9ms/step - loss: 28.9675\nEpoch 7/100\n1/1 [==============================] - 0s 9ms/step - loss: 16.5128\nEpoch 8/100\n1/1 [==============================] - 0s 10ms/step - loss: 9.4911\nEpoch 9/100\n1/1 [==============================] - 0s 10ms/step - loss: 6.2027\nEpoch 10/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.2417\nEpoch 11/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.5172\nEpoch 12/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.5900\nEpoch 13/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.8605\nEpoch 14/100\n1/1 [==============================] - 0s 9ms/step - loss: 8.5884\nEpoch 15/100\n1/1 [==============================] - 0s 9ms/step - loss: 8.3991\nEpoch 16/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.4675\nEpoch 17/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.2581\nEpoch 18/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.1274\nEpoch 19/100\n1/1 [==============================] - 0s 9ms/step - loss: 4.2382\nEpoch 20/100\n1/1 [==============================] - 0s 9ms/step - loss: 3.6033\nEpoch 21/100\n1/1 [==============================] - 0s 9ms/step - loss: 3.1860\nEpoch 22/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.9233\nEpoch 23/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.7560\nEpoch 24/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.6421\nEpoch 25/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.5490\nEpoch 26/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.4612\nEpoch 27/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.3617\nEpoch 28/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.2378\nEpoch 29/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.0874\nEpoch 30/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.9117\nEpoch 31/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.7239\nEpoch 32/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.5409\nEpoch 33/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.3663\nEpoch 34/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.2210\nEpoch 35/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.1035\nEpoch 36/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0208\nEpoch 37/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9766\nEpoch 38/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9628\nEpoch 39/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.9717\nEpoch 40/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9883\nEpoch 41/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0039\nEpoch 42/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0156\nEpoch 43/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0181\nEpoch 44/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0067\nEpoch 45/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9809\nEpoch 46/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9443\nEpoch 47/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9019\nEpoch 48/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.8571\nEpoch 49/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.8146\nEpoch 50/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7768\nEpoch 51/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7489\nEpoch 52/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7294\nEpoch 53/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7186\nEpoch 54/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7125\nEpoch 55/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7080\nEpoch 56/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7044\nEpoch 57/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7002\nEpoch 58/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6949\nEpoch 59/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6884\nEpoch 60/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6806\nEpoch 61/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6715\nEpoch 62/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6615\nEpoch 63/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6510\nEpoch 64/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6404\nEpoch 65/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6302\nEpoch 66/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6209\nEpoch 67/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6127\nEpoch 68/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6060\nEpoch 69/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6007\nEpoch 70/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5963\nEpoch 71/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5924\nEpoch 72/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5888\nEpoch 73/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5853\nEpoch 74/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5816\nEpoch 75/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5778\nEpoch 76/100\n1/1 [==============================] - 0s 10ms/step - loss: 0.5736\nEpoch 77/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5691\nEpoch 78/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5644\nEpoch 79/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5595\nEpoch 80/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5547\nEpoch 81/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5501\nEpoch 82/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5457\nEpoch 83/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5417\nEpoch 84/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5379\nEpoch 85/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5344\nEpoch 86/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5310\nEpoch 87/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5276\nEpoch 88/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5242\nEpoch 89/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5208\nEpoch 90/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5174\nEpoch 91/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5140\nEpoch 92/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5108\nEpoch 93/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5075\nEpoch 94/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5044\nEpoch 95/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5014\nEpoch 96/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4986\nEpoch 97/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4960\nEpoch 98/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.4935\nEpoch 99/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4909\nEpoch 100/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4885\n\n\n&lt;keras.callbacks.History at 0x7f640c513e50&gt;\n\n\n\nnp.mean((net(X)&gt;0.5) == y)\n\n0.98125\n\n\n\nnp.mean((net(XX)&gt;0.5) == yy)\n\n0.977\n\n\n\n\n풀이3: 컴파일시 metrics=[‘accuracy’] 추가\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=100,batch_size=12000)\n\nEpoch 1/100\n1/1 [==============================] - 0s 150ms/step - loss: 220.9145 - accuracy: 0.5000\nEpoch 2/100\n1/1 [==============================] - 0s 9ms/step - loss: 88.9490 - accuracy: 0.5073\nEpoch 3/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.5895 - accuracy: 0.8208\nEpoch 4/100\n1/1 [==============================] - 0s 10ms/step - loss: 33.7521 - accuracy: 0.5972\nEpoch 5/100\n1/1 [==============================] - 0s 9ms/step - loss: 40.2290 - accuracy: 0.5723\nEpoch 6/100\n1/1 [==============================] - 0s 9ms/step - loss: 28.9675 - accuracy: 0.6442\nEpoch 7/100\n1/1 [==============================] - 0s 11ms/step - loss: 16.5128 - accuracy: 0.8061\nEpoch 8/100\n1/1 [==============================] - 0s 9ms/step - loss: 9.4911 - accuracy: 0.8947\nEpoch 9/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.2027 - accuracy: 0.9355\nEpoch 10/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.2417 - accuracy: 0.9404\nEpoch 11/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.5172 - accuracy: 0.9270\nEpoch 12/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.5900 - accuracy: 0.9021\nEpoch 13/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.8605 - accuracy: 0.8788\nEpoch 14/100\n1/1 [==============================] - 0s 9ms/step - loss: 8.5884 - accuracy: 0.8647\nEpoch 15/100\n1/1 [==============================] - 0s 9ms/step - loss: 8.3991 - accuracy: 0.8664\nEpoch 16/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.4675 - accuracy: 0.8793\nEpoch 17/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.2581 - accuracy: 0.8982\nEpoch 18/100\n1/1 [==============================] - 0s 9ms/step - loss: 5.1274 - accuracy: 0.9156\nEpoch 19/100\n1/1 [==============================] - 0s 9ms/step - loss: 4.2382 - accuracy: 0.9302\nEpoch 20/100\n1/1 [==============================] - 0s 9ms/step - loss: 3.6033 - accuracy: 0.9426\nEpoch 21/100\n1/1 [==============================] - 0s 9ms/step - loss: 3.1860 - accuracy: 0.9509\nEpoch 22/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.9233 - accuracy: 0.9551\nEpoch 23/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.7560 - accuracy: 0.9574\nEpoch 24/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.6421 - accuracy: 0.9594\nEpoch 25/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.5490 - accuracy: 0.9599\nEpoch 26/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.4612 - accuracy: 0.9603\nEpoch 27/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.3617 - accuracy: 0.9608\nEpoch 28/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.2378 - accuracy: 0.9612\nEpoch 29/100\n1/1 [==============================] - 0s 9ms/step - loss: 2.0874 - accuracy: 0.9619\nEpoch 30/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.9117 - accuracy: 0.9630\nEpoch 31/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.7239 - accuracy: 0.9641\nEpoch 32/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.5409 - accuracy: 0.9657\nEpoch 33/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.3663 - accuracy: 0.9670\nEpoch 34/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.2210 - accuracy: 0.9685\nEpoch 35/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.1035 - accuracy: 0.9688\nEpoch 36/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0208 - accuracy: 0.9696\nEpoch 37/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9766 - accuracy: 0.9705\nEpoch 38/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9628 - accuracy: 0.9708\nEpoch 39/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9717 - accuracy: 0.9715\nEpoch 40/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9883 - accuracy: 0.9706\nEpoch 41/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0039 - accuracy: 0.9699\nEpoch 42/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0156 - accuracy: 0.9685\nEpoch 43/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0181 - accuracy: 0.9681\nEpoch 44/100\n1/1 [==============================] - 0s 9ms/step - loss: 1.0067 - accuracy: 0.9686\nEpoch 45/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9809 - accuracy: 0.9693\nEpoch 46/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9443 - accuracy: 0.9703\nEpoch 47/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.9019 - accuracy: 0.9711\nEpoch 48/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.8571 - accuracy: 0.9722\nEpoch 49/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.8146 - accuracy: 0.9737\nEpoch 50/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7768 - accuracy: 0.9743\nEpoch 51/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7489 - accuracy: 0.9753\nEpoch 52/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7294 - accuracy: 0.9759\nEpoch 53/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7186 - accuracy: 0.9767\nEpoch 54/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7125 - accuracy: 0.9774\nEpoch 55/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7080 - accuracy: 0.9776\nEpoch 56/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7044 - accuracy: 0.9777\nEpoch 57/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.7002 - accuracy: 0.9776\nEpoch 58/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6949 - accuracy: 0.9778\nEpoch 59/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6884 - accuracy: 0.9779\nEpoch 60/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6806 - accuracy: 0.9784\nEpoch 61/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6715 - accuracy: 0.9786\nEpoch 62/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6615 - accuracy: 0.9786\nEpoch 63/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6510 - accuracy: 0.9784\nEpoch 64/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6404 - accuracy: 0.9786\nEpoch 65/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6302 - accuracy: 0.9787\nEpoch 66/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6209 - accuracy: 0.9791\nEpoch 67/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.6127 - accuracy: 0.9787\nEpoch 68/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6060 - accuracy: 0.9791\nEpoch 69/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.6007 - accuracy: 0.9792\nEpoch 70/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5963 - accuracy: 0.9795\nEpoch 71/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5924 - accuracy: 0.9793\nEpoch 72/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5888 - accuracy: 0.9791\nEpoch 73/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5853 - accuracy: 0.9790\nEpoch 74/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5816 - accuracy: 0.9793\nEpoch 75/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5778 - accuracy: 0.9794\nEpoch 76/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5736 - accuracy: 0.9795\nEpoch 77/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5691 - accuracy: 0.9794\nEpoch 78/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5644 - accuracy: 0.9794\nEpoch 79/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5595 - accuracy: 0.9796\nEpoch 80/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5547 - accuracy: 0.9796\nEpoch 81/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5501 - accuracy: 0.9798\nEpoch 82/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5457 - accuracy: 0.9800\nEpoch 83/100\n1/1 [==============================] - 0s 9ms/step - loss: 0.5417 - accuracy: 0.9800\nEpoch 84/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5379 - accuracy: 0.9804\nEpoch 85/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5344 - accuracy: 0.9807\nEpoch 86/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5310 - accuracy: 0.9807\nEpoch 87/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5276 - accuracy: 0.9807\nEpoch 88/100\n1/1 [==============================] - 0s 7ms/step - loss: 0.5242 - accuracy: 0.9808\nEpoch 89/100\n1/1 [==============================] - 0s 7ms/step - loss: 0.5208 - accuracy: 0.9808\nEpoch 90/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5174 - accuracy: 0.9811\nEpoch 91/100\n1/1 [==============================] - 0s 7ms/step - loss: 0.5140 - accuracy: 0.9812\nEpoch 92/100\n1/1 [==============================] - 0s 7ms/step - loss: 0.5108 - accuracy: 0.9812\nEpoch 93/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5075 - accuracy: 0.9813\nEpoch 94/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5044 - accuracy: 0.9814\nEpoch 95/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.5014 - accuracy: 0.9816\nEpoch 96/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4986 - accuracy: 0.9815\nEpoch 97/100\n1/1 [==============================] - 0s 7ms/step - loss: 0.4960 - accuracy: 0.9815\nEpoch 98/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4935 - accuracy: 0.9812\nEpoch 99/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4909 - accuracy: 0.9812\nEpoch 100/100\n1/1 [==============================] - 0s 8ms/step - loss: 0.4885 - accuracy: 0.9812\n\n\n&lt;keras.callbacks.History at 0x7f640c46bc70&gt;\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 0s 349us/step - loss: 0.4860 - accuracy: 0.9812\n\n\n[0.48598653078079224, 0.981249988079071]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 617us/step - loss: 0.4294 - accuracy: 0.9770\n\n\n[0.42936256527900696, 0.9769999980926514]\n\n\n\n\n풀이4: 확률적경사하강법 이용 // epochs=10\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=10,batch_size=120)\n\nEpoch 1/10\n100/100 [==============================] - 0s 827us/step - loss: 5.6484 - accuracy: 0.9418\nEpoch 2/10\n100/100 [==============================] - 0s 747us/step - loss: 0.5078 - accuracy: 0.9793\nEpoch 3/10\n100/100 [==============================] - 0s 734us/step - loss: 0.3784 - accuracy: 0.9818\nEpoch 4/10\n100/100 [==============================] - 0s 765us/step - loss: 0.3390 - accuracy: 0.9828\nEpoch 5/10\n100/100 [==============================] - 0s 735us/step - loss: 0.2474 - accuracy: 0.9857\nEpoch 6/10\n100/100 [==============================] - 0s 717us/step - loss: 0.2116 - accuracy: 0.9870\nEpoch 7/10\n100/100 [==============================] - 0s 734us/step - loss: 0.1724 - accuracy: 0.9889\nEpoch 8/10\n100/100 [==============================] - 0s 784us/step - loss: 0.1711 - accuracy: 0.9880\nEpoch 9/10\n100/100 [==============================] - 0s 795us/step - loss: 0.1491 - accuracy: 0.9894\nEpoch 10/10\n100/100 [==============================] - 0s 723us/step - loss: 0.1550 - accuracy: 0.9896\n\n\n&lt;keras.callbacks.History at 0x7f640c2d9fa0&gt;\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 0s 339us/step - loss: 0.1124 - accuracy: 0.9923\n\n\n[0.11242959648370743, 0.9922500252723694]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 566us/step - loss: 0.2988 - accuracy: 0.9845\n\n\n[0.29883989691734314, 0.984499990940094]"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_01_(9주차)_5월2일(1).html",
    "href": "posts/3_STBDA2022/2022_05_01_(9주차)_5월2일(1).html",
    "title": "[STBDA] 9wk. Likelihood function",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-wrJ6CivKKBuJUN7ukm5OHr\n\n\n\nimports\n\nimport numpy as np\nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt \n\n\n\n우도함수와 최대우도추정량\n(예제)\n\\(X_i \\overset{iid}{\\sim} Ber(p)\\)에서 얻은 샘플이 아래와 같다고 하자.\n\nx=[0,1,0,1] \nx\n\n[0, 1, 0, 1]\n\n\n\\(p\\)는 얼마라고 볼 수 있는가? –&gt; 0.5\n왜?? \\(p\\)가 0.5라고 주장할 수 있는 이론적 근거, 혹은 논리체계가 무엇인가?\n- suppose: \\(p=0.1\\) 이라고 하자.\n그렇다면 \\((x_1,x_2,x_3,x_4)=(0,1,0,1)\\)와 같은 샘플이 얻어질 확률이 아래와 같다.\n\n0.9 * 0.1 * 0.9 * 0.1\n\n0.008100000000000001\n\n\n- suppose: \\(p=0.2\\) 이라고 하자.\n그렇다면 \\((x_1,x_2,x_3,x_4)=(0,1,0,1)\\)와 같은 샘플이 얻어질 확률이 아래와 같다.\n\n0.8 * 0.2 * 0.8 * 0.2\n\n0.025600000000000008\n\n\n- 질문1: \\(p=0.1\\)인것 같냐? 아니면 \\(p=0.2\\)인것 같냐? -&gt; \\(p=0.2\\)\n\n왜?? \\(p=0.2\\)일 확률이 더 크다! \\(\\to\\) \\(p=0.2\\)일 가능도가 더 크다!\n\n\n(여기서 잠깐 중요한것) 확률이라는 말을 함부로 쓸 수 없다.\n- 0.0256은 “\\(p=0.2\\)일 경우 샘플 (0,1,0,1)이 얻어질 확률”이지 “\\(p=0.2\\)일 확률”은 아니다.\n“\\(p=0.2\\)인 확률” 이라는 개념이 성립하려면 아래코드에서 sum([(1-p)*p*(1-p)*p for p in _plist])이 1보다는 작아야 한다. (그런데 1보다 크다)\n\n_plist = np.linspace(0.499,0.501,1000) \nsum([(1-p)*p*(1-p)*p for p in _plist])\n\n62.49983299986714\n\n\n\n확률은 다 더하면 \\(1\\)이어야 하는데 가뿐히 넘어버림..\n\n- 확률이라는 말을 쓸 수 없지만 확률의 느낌은 있음 -&gt; 가능도라는 말을 쓰자.\n\n0.0256 \\(=\\) \\(p\\)가 0.2일 경우 샘플 (0,1,0,1)이 얻어질 확률 \\(=\\) \\(p\\)가 0.2일 가능도\n\n\n- 다시 질문1로 돌아가자!\n\n질문1: \\(p=0.1\\)인 것 같냐? 아니면 \\(p=0.2\\)인 것 같냐? -&gt; 답 \\(p=0.2\\) -&gt; 왜? \\(p=0.2\\)인 가능도가 더 크니까!\n질문2: \\(p=0.2\\)인 것 같냐? 아니면 \\(p=0.3\\)인 것 같냐? -&gt; 답 \\(p=0.3\\) -&gt; 왜? \\(p=0.3\\)인 가능도가 더 크니까!\n질문3: …\n\n- 궁극의 질문: \\(p\\)가 뭐일 것 같아?\n\n\\(p\\)가 입력으로 들어가면 가능도가 계산되는 함수를 만들자.\n그 함수를 최대화하는 \\(p\\)를 찾자.\n그 \\(p\\)가 궁극의 질문에 대한 대답이 된다.\n\n- 잠깐 용어정리\n\n가능도함수 \\(=\\) 우도함수 \\(=\\) likelihood function \\(:=\\) \\(L(p)\\)\n\\(p\\)의 maximum likelihood estimator \\(=\\) p의 MLE \\(:=\\) \\(\\hat{p}^{mle}\\) \\(=\\) \\(\\text{argmax}_p L(p)\\) \\(=\\) \\(\\hat{p}\\)\n\n(예제의 풀이)\n- 이 예제의 경우 가능도함수를 정의하자.\n\n\\(L(p)\\): \\(p\\)의 가능도함수 = \\(p\\)가 모수일때 샘플 (0,1,0,1)이 얻어질 확률 = \\(p\\)가 모수일때 \\(x_1\\)이 0일 확률 \\(\\times \\dots \\times\\) \\(p\\)가 모수일때 \\(x_4\\)가 1일 확률\n\\(L(p)=\\prod_{i=1}^{4} f(x_i;p)= \\prod_{i=1}^{4}p^{x_i}(1-p)^{1-x_i}\\)\n\n\nnote: 참고로 이 과정을 일반화 하면 \\(X_1,\\dots,X_n \\overset{iid}{\\sim} Ber(p)\\) 일때 \\(p\\)의 likelihood function은 \\(\\prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}\\) 라고 볼 수 있다.\n\n\nnote: 더 일반화: \\(x_1,\\dots,x_n\\)이 pdf가 \\(f(x)\\)인 분포에서 뽑힌 서로 독립인 샘플일때 likelihood function은 \\(\\prod_{i=1}^{n}f(x_i)\\)라고 볼 수 있다.\n\n- 이 예제의 경우 \\(p\\)의 최대우도추정량을 구하면\n\\[\\hat{p}^{mle} = \\text{argmax}_p L(p) = \\text{argmax}_p  \\big\\{ p^2(1-p)^2 \\big\\}= \\frac{1}{2}\\]\n\n\n중간고사 1번\n(1) \\(N(\\mu,\\sigma)\\)에서 얻은 샘플이 아래와 같다고 할때 \\(\\mu,\\sigma\\)의 MLE를 구하여라.\n&lt;tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])&gt;\n(2) \\(Ber(p)\\)에서 얻은 샘플이 아래와 같다고 할 때 \\(p\\)의 MLE를 구하여라.\n&lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])&gt;\n(3) \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\), \\(\\epsilon_i \\overset{iid}{\\sim} N(0,1)\\) 일때 \\((\\beta_0,\\beta_1)\\)의 MLE를 구하여라. (회귀모형)\n(풀이) 가능도함수\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]\n를 최대화하는 \\(\\beta_0,\\beta_1\\)을 구하면된다. 그런데 이것은 아래를 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하는 것과 같다.\n\\[-\\log L(\\beta_0,\\beta_1) = \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\]\n위의 식은 SSE와 같다. 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하면 된다.\n중간고사 1-(3)의 다른 풀이\nstep1: 생성\n\nx= tf.constant(np.arange(1,10001)/10000)\ny= tnp.random.randn(10000) + (0.5 + 2*x) \n\nstep2: minimize MSEloss (원래는 maximize log-likelihood)\n\nmaximize likelihood였던 문제를 minimize MSEloss로 바꾸어도 되는근거? 주어진 함수(=가능도함수)를 최대화하는 \\(\\beta_0,\\beta_1\\)은 MSE를 최소화하는 \\(\\beta_0,\\beta_1\\)과 동일하므로\n\n\nbeta0= tf.Variable(1.0)\nbeta1= tf.Variable(1.0) \nfor i in range(2000):\n    with tf.GradientTape() as tape: \n        #minus_log_likelihood = tf.reduce_sum((y-beta0-beta1*x)**2)\n        loss =  tf.reduce_sum((y-beta0-beta1*x)**2)\n    slope1, slope2 = tape.gradient(loss,[beta0,beta1]) \n    beta0.assign_sub(slope1* 0.1/10000) # N=10000 \n    beta1.assign_sub(slope2* 0.1/10000) \n\n\nbeta0,beta1\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.52694947&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.956429&gt;)\n\n\n\n\\(\\beta_0 = 0.5, \\beta_1 = 2\\) 가 true값임.\n\n- 문제를 풀면서 생각해보니 손실함수는 -로그가능도함수로 선택하면 될 것 같다?\n\n손실함수를 선택하는 기준이 -로그가능도함수만 존재하는 것은 아니나 대부분 그러하긴함\n\n(4) 출제하지 못한 중간고사 문제\n아래의 모형을 생각하자. - \\(Y_i \\overset{iid}{\\sim} Ber(\\pi_i)\\) - \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}=\\frac{\\exp(-1+5x_i)}{1+\\exp(-1+5x_i)}\\)\n아래는 위의 모형에서 얻은 샘플이다.\n\nx = tnp.linspace(-1,1,2000) # sample\npi = tnp.exp(-1+5*x) / (1+tnp.exp(-1+5*x)) # sample이 나올 확률\ny = np.random.binomial(1,pi)\ny = tf.constant(y)\n\n함수 \\(L(w_0,w_1)\\)을 최대화하는 \\((w_0,w_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\((w_0,w_1)\\)의 초기값은 모두 0.1로 설정할 것)\n\\[L(w_0,w_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(x_i)={\\pi_i}^{y_i}(1-\\pi_i)^{1-y_i},\\quad \\pi_i=\\text{sigmoid}(w_0+w_1x_i)\\]\n(풀이1)\n\nw0hat = tf.Variable(1.0) \nw1hat = tf.Variable(1.0) \n\n\nfor i in range(1000): \n    with tf.GradientTape() as tape: \n        pihat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n        pdf = pihat**y * (1-pihat)**(1-y) \n        logL = tf.reduce_mean(tnp.log(pdf)) \n    slope1,slope2 = tape.gradient(logL,[w0hat,w1hat])\n    w0hat.assign_add(slope1*0.1) \n    w1hat.assign_add(slope2*0.1) \n\n\nw0hat,w1hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.88308984&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.144893&gt;)\n\n\n\ntrue값인 \\(-1\\)과 \\(5\\)에 근접한 값이 나와 적합이 잘 되었다.\n\n(해석) - 로지스틱에서 가능도함수와 BCEloss의 관계\n\\(L(w_0,w_1)\\)를 최대화하는 \\(w_0,w_1\\)은 아래를 최소화하는 \\(w_0,w_1\\)와 같다.\n\\[-\\log L(w_0,w_1) = - \\sum_{i=1}^{n}\\big(y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i)\\big)\\]\n이것은 최적의 \\(w_0,w_1\\)을 \\(\\hat{w}_0,\\hat{w}_1\\)이라고 하면 \\(\\hat{\\pi}_i=\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\hat{y}_i\\)이 되고 따라서 위의 식은 \\(n\\times\\)BCEloss의 형태임을 쉽게 알 수 있다.\n결국 로지스틱 모형에서 \\((w_0,w_1)\\)의 MLE를 구하기 위해서는 BCEloss를 최소화하는 \\((w_0,w_1)\\)을 구하면 된다!\n결국 BCE Loss는 -로그가능도함수를 \\(n\\)으로 나눈 것과 같다.\nlikelihood라는 개념이없다면 회귀모형일 경우에는 MSELoss를 쓰고, 로지스틱일 경우에는 BCELoss를 쓴다고 외워야하는 문제였지만, Likelihood라는 개념이 있다면 로지스틱이든 회귀모형이든 손실함수는 -loglikelihood로 잡는군! 이렇게 넘어갈 수 있다.\n(풀이2)\n\nw0hat = tf.Variable(1.0) \nw1hat = tf.Variable(1.0) \n\n\nfor i in range(1000): \n    with tf.GradientTape() as tape: \n        yhat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n        loss = tf.losses.binary_crossentropy(y,yhat)\n    slope1,slope2 = tape.gradient(loss,[w0hat,w1hat])\n    w0hat.assign_sub(slope1*0.1) \n    w1hat.assign_sub(slope2*0.1) \n\n\nw0hat,w1hat\n\n(&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.8830899&gt;,\n &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.144893&gt;)\n\n\n\n\n손실함수의 설계 (선택)\n- 회귀분석이든 로지스틱이든 손실함수는 minus_log_likelihood 로 선택한다. - 그런데 (오차항이 정규분포인) 회귀분석 일때는 minus_log_likelihood 가 MSEloss가 되고 - 로지스틱일때는 minus_log_likelihood 가 BCEloss가 된다\n- minus_log_likelihood가 손실함수를 선택하는 유일한 기준은 아니다. &lt;— 참고만하세요, 이 수업에서는 안중요합니다. - 오차항이 대칭이고 서로독립이며 등분산 가정을 만족하는 어떠한 분포에서의 회귀모형이 있다고 하자. 이 회귀모형에서 \\(\\hat{\\beta}\\)은 여전히 MSEloss를 최소화하는 \\(\\beta\\)를 구함으로써 얻을 수 있다. - 이 경우 MSEloss를 쓰는 이론적근거? \\(\\hat{\\beta}\\)이 BLUE가 되기 때문임 (가우스-마코프정리)"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_18_(7주차)_4월18일.html",
    "href": "posts/3_STBDA2022/2022_04_18_(7주차)_4월18일.html",
    "title": "[STBDA] 7wk. Piece-wise LR / Logistic Regression",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-ws3T1xD-bBU46dtduUlwmP\n\n\n\nimports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\n\npiece-wise linear regression\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x&gt;0 \\end{cases}\\)\n\nnp.random.seed(43052)\nN=100\nx = np.linspace(-1,1,N)\nlamb = lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3 \ny= np.array(list(map(lamb,x)))\ny\n\narray([-0.88497385, -0.65454563, -0.61676249, -0.84702584, -0.84785569,\n       -0.79220455, -1.3777105 , -1.27341781, -1.41643729, -1.26404671,\n       -0.79590224, -0.78824395, -0.86064773, -0.52468679, -1.18247354,\n       -0.29327295, -0.69373049, -0.90561768, -1.07554911, -0.7225404 ,\n       -0.69867774, -0.34811037,  0.11188474, -1.05046296, -0.03840085,\n       -0.38356861, -0.24299798, -0.58403161, -0.20344022, -0.13872303,\n       -0.529586  , -0.27814478, -0.10852781, -0.38294596,  0.02669763,\n       -0.23042603, -0.77720364, -0.34287396, -0.04512022, -0.30180793,\n       -0.26711438, -0.51880349, -0.53939672, -0.32052379, -0.32080763,\n        0.28917092,  0.18175206, -0.48988124, -0.08084459,  0.37706178,\n        0.14478908,  0.07621827, -0.071864  ,  0.05143365,  0.33932009,\n       -0.35071776,  0.87742867,  0.51370399,  0.34863976,  0.55855514,\n        1.14196717,  0.86421076,  0.72957843,  0.57342304,  1.54803332,\n        0.98840018,  1.11129366,  1.42410801,  1.44322465,  1.25926455,\n        1.12940772,  1.46516829,  1.16365096,  1.45560853,  1.9530553 ,\n        2.45940445,  1.52921129,  1.8606463 ,  1.86406718,  1.5866523 ,\n        1.49033473,  2.35242686,  2.12246412,  2.41951931,  2.43615052,\n        1.96024441,  2.65843789,  2.46854394,  2.76381882,  2.78547462,\n        2.56568465,  3.15212157,  3.11482949,  3.17901774,  3.31268904,\n        3.60977818,  3.40949166,  3.30306495,  3.74590922,  3.85610433])\n\n\n\nplt.plot(x,y,'.')\n\n\n\n\n\n풀이1: 단순회귀모형\n\nx= x.reshape(N,1)\ny= y.reshape(N,1) \n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n&lt;keras.callbacks.History at 0x7f88c01c7820&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)&gt;]\n\n\n\nyhat = x * 2.2616348 + 0.6069048\nyhat = net.predict(x)\n\n4/4 [==============================] - 0s 470us/step\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,yhat,'--')\n\n\n\n\n- 실패: 이 모형은 epoch을 10억번 돌려도 실패할 모형임 - 왜? 아키텍처 설계자체가 틀렸음 - 꺽인부분을 표현하기에는 아키텍처의 표현력이 너무 부족하다 -&gt; under fit의 문제\n\n\n풀이2: 비선형 활성화 함수의 도입\n- 여기에서 비선형 활성화 함수는 relu\n- 네트워크를 아래와 같이 수정하자.\n(수정전) hat은 생략\n\n#collapse\ngv('''\n\"x\" -&gt; \"x*w,    bias=True\"[label=\"*w\"] ;\n\"x*w,    bias=True\" -&gt; \"y\"[label=\"indentity\"] ''')\n\n\n\n\n(수정후) hat은 생략\n\n#collapse\ngv('''\n\"x\" -&gt; \"x*w,    bias=True\"[label=\"*w\"] ;\n\"x*w,    bias=True\" -&gt; \"y\"[label=\"relu\"] ''')\n\n\n\n\n\n마지막에 \\(f(x)=x\\) 라는 함수대신에 relu 를 취하는 것으로 구조를 약간 변경\n활성화함수(acitivation function)를 indentity에서 relu로 변경\n\n- relu함수란?\n\n_x = np.linspace(-1,1,100)\ntf.nn.relu(_x)\n\n&lt;tf.Tensor: shape=(100,), dtype=float64, numpy=\narray([0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.01010101, 0.03030303, 0.05050505, 0.07070707, 0.09090909,\n       0.11111111, 0.13131313, 0.15151515, 0.17171717, 0.19191919,\n       0.21212121, 0.23232323, 0.25252525, 0.27272727, 0.29292929,\n       0.31313131, 0.33333333, 0.35353535, 0.37373737, 0.39393939,\n       0.41414141, 0.43434343, 0.45454545, 0.47474747, 0.49494949,\n       0.51515152, 0.53535354, 0.55555556, 0.57575758, 0.5959596 ,\n       0.61616162, 0.63636364, 0.65656566, 0.67676768, 0.6969697 ,\n       0.71717172, 0.73737374, 0.75757576, 0.77777778, 0.7979798 ,\n       0.81818182, 0.83838384, 0.85858586, 0.87878788, 0.8989899 ,\n       0.91919192, 0.93939394, 0.95959596, 0.97979798, 1.        ])&gt;\n\n\n\nplt.plot(_x,_x)\nplt.plot(_x,tf.nn.relu(_x))\n\n\n\n\n\n파란색을 주황색으로 바꿔주는 것이 렐루함수임\n\\(f(x)=\\max(0,x)=\\begin{cases} 0 & x\\leq 0 \\\\ x & x&gt;0 \\end{cases}\\)\n\n- 아키텍처: \\(\\hat{y}_i=relu(\\hat{w}_0+\\hat{w}_1x_i)\\), \\(relu(x)=\\max(0,x)\\)\n- 풀이시작\n1단계\n\nnet2 = tf.keras.Sequential() \n\n2단계\n\ntf.random.set_seed(43053)\nl1 = tf.keras.layers.Dense(1, input_shape=(1,)) \na1 = tf.keras.layers.Activation(tf.nn.relu) \n\n\nnet2.add(l1)\n\n\nnet2.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7f888c109820&gt;]\n\n\n\nnet2.add(a1)\n\n\nnet2.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7f888c109820&gt;,\n &lt;keras.layers.core.activation.Activation at 0x7f888c109250&gt;]\n\n\n\nl1.get_weights()\n\n[array([[0.3830086]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nnet2.get_weights()\n\n[array([[0.3830086]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nl1에 있는 weight가 net2에 그대로 들어가게 된다.\n왜 시드고정이 안되는거지??\n\n(네트워크 상황 확인)\n\nu1= l1(x)\n#u1= x@l1.weights[0] + l1.weights[1]\n\n\nv1= a1(u1)\n#v1= tf.nn.relu(u1) \n\n\nplt.plot(x,x)\nplt.plot(x,u1,'--r')\nplt.plot(x,v1,'--b')\n\n\n\n\n3단계\n\nnet2.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\n\n4단계\n\nnet2.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f888c0b6d90&gt;\n\n\n- result\n\n# 다 똑같은 코드.\nyhat = tf.nn.relu(x@l1.weights[0] + l1.weights[1]) \nyhat = net2.predict(x)\nyhat = net2(x)\nyhat = a1(l1(x))\nyhat = net2.layers[1](net2.layers[0](x))\n\n4/4 [==============================] - 0s 438us/step\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,yhat,'--')\n\n\n\n\n- discussion - 이것 역시 수백억번 에폭을 반복해도 이 이상 적합이 힘들다 \\(\\to\\) 모형의 표현력이 떨어진다. - 해결책: 주황색점선이 2개 있다면 어떨까?\n\n\n풀이3: 노드수추가 + 레이어추가\n목표: 2개의 주황색 점선을 만들자.\n1단계\n\nnet3 = tf.keras.Sequential()\n\n2단계\n\n# tf.random.set_seed(43053)\ntf.keras.utils.set_random_seed(43053)\nl1 = tf.keras.layers.Dense(2,input_shape=(1,)) # 출력을 2로 하면 직선이 2개 만들어짐.\na1 = tf.keras.layers.Activation(tf.nn.relu)\n\n\nnet3.add(l1)\nnet3.add(a1) \n\n\nnet3.layers # 2개가 들어가있음!\n\n[&lt;keras.layers.core.dense.Dense at 0x7f8840271d60&gt;,\n &lt;keras.layers.core.activation.Activation at 0x7f888c268c40&gt;]\n\n\n(네트워크 상황 확인)\n\nl1(x).shape\n# l1(x) : (100,1) -&gt; (100,2) \n\nTensorShape([100, 2])\n\n\n\n출력차원이 2라고 했으니까!\n\n\nplt.plot(x,x) # 입력 (파란)\nplt.plot(x,l1(x),'--') # 출력 (주황, 초록)\n\n\n\n\n\nplt.plot(x,x)\nplt.plot(x,a1(l1(x)),'--')\n\n\n\n\n- 이 상태에서는 yhat이 안나온다. 왜? - 차원이 안맞음. a1(l1(x))의 차원은 (N,2)인데 최종적인 yhat의 차원은 (N,1)이어야 함. (선이 하나여야 하잖아..) - 차원이 어찌저찌 맞다고 쳐도 relu를 통과하면 항상 yhat&gt;0 임. 따라서 음수값을 가지는 y는 0으로 밖에 맞출 수 없음.\n- 해결책: a1(l1(x))에 연속으로(Sequential하게!) 또 다른 레이어를 설계! (N,2) -&gt; (N,1) 이 되도록! - yhat= bias + weight1 * a1(l1(x))[0] + weight2 * a1(l1(x))[1]\n- 즉 a1(l1(x)) 를 새로운 입력으로 해석하고 출력을 만들어주는 선형모형을 다시태우면 된다. - 입력차원: 2 - 출력차원: 1\n\nnet3.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7f8840271d60&gt;,\n &lt;keras.layers.core.activation.Activation at 0x7f888c268c40&gt;]\n\n\n\n# tf.random.set_seed(43053) \ntf.keras.utils.set_random_seed(43053)\nl2 = tf.keras.layers.Dense(1, input_shape=(2,)) # 출력차원은 1, 입력차원은 2\n\n\nnet3.add(l2) \n\n\nnet3.layers\n\n[&lt;keras.layers.core.dense.Dense at 0x7f8840271d60&gt;,\n &lt;keras.layers.core.activation.Activation at 0x7f888c268c40&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f88401fdaf0&gt;]\n\n\n\nnet3.summary()\n\nModel: \"sequential_32\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_39 (Dense)            (None, 2)                 4         \n                                                                 \n activation_37 (Activation)  (None, 2)                 0         \n                                                                 \n dense_40 (Dense)            (None, 1)                 3         \n                                                                 \n=================================================================\nTotal params: 7\nTrainable params: 7\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nDense layer \\(\\to\\) activation \\(\\to\\) Dense layer\n\n- 추정해야할 파라메터수가 4,0,3으로 나온다.\n- 수식표현: \\(X \\to X@W^{(1)}+b^{(1)} \\to relu(X@W^{(1)}+b^{(1)}) \\to relu(X@W^{(1)}+b^{(1)})@W^{(2)}+b^{(2)}=yhat\\)\n\n\\(X\\): (N,1)\n\\(W^{(1)}\\): (1,2) ==&gt; 파라메터 2개 추정\n\\(b^{(1)}\\): (2,) ==&gt; 파라메터 2개가 추가 // 여기까지 추정할 파라메터는 4개\n\\(W^{(2)}\\): (2,1) ==&gt; 파라메터 2개 추정\n\\(b^{(2)}\\): (1,) ==&gt; 파라메터 1개가 추가 // 따라서 3개\n\n- 참고: 추정할 파라메터수가 많다 = 복잡한 모형이다. - 초거대AI: 추정할 파라메터수가 엄청 많은..\n\nnet3.weights\n\n[&lt;tf.Variable 'dense_39/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[ 0.8359591, -0.8971499]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_39/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 0.8359591],\n        [-0.8971499]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]\n\n\n\nl1.weights\n\n[&lt;tf.Variable 'dense_39/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[ 0.8359591, -0.8971499]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_39/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;]\n\n\n\nl2.weights\n\n[&lt;tf.Variable 'dense_40/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 0.8359591],\n        [-0.8971499]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]\n\n\n- 좀 더 간단한 수식표현: \\(X \\to (u_1 \\to v_1) \\to (u_2 \\to v_2) = yhat\\) - \\(u_1= X@W^{(1)}+b^{(1)}\\) - \\(v_1= relu(u_1)\\) - \\(u_2= v_1@W^{(2)}+b^{(2)}\\) - \\(v_2= indentity(u_2):=yhat\\)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -&gt; \"u1[:,0]\"[label=\"*W1[0,0]\"]\n    \"X\" -&gt; \"u1[:,1]\"[label=\"*W1[0,1]\"]\n    \"u1[:,0]\" -&gt; \"v1[:,0]\"[label=\"relu\"]\n    \"u1[:,1]\" -&gt; \"v1[:,1]\"[label=\"relu\"]\n    label = \"Layer 1\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"v1[:,0]\" -&gt; \"yhat\"[label=\"*W2[0,0]\"]\n    \"v1[:,1]\" -&gt; \"yhat\"[label=\"*W2[1,0]\"]\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\n\n하나는 주황색선, 하나는 초록색선이 만들어진다.\n너무 복잡한데..?\n\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -&gt; \"node1\"\n    \"X\" -&gt; \"node2\"\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -&gt; \"yhat\"\n    \"node2\" -&gt; \"yhat\"\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\n\n좀 더 간단한 형태의 아키텍쳐\n\n3단계\n\nnet3.compile(loss='mse',optimizer=tf.optimizers.SGD(0.1))\n\n4단계\n\nnet3.fit(x,y,epochs=1000,verbose=0, batch_size=N) \n\n&lt;keras.callbacks.History at 0x7f88400fba30&gt;\n\n\n- 결과확인\n\nnet3.weights\n\n[&lt;tf.Variable 'dense_39/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[ 1.6574922, -0.8332077]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_39/bias:0' shape=(2,) dtype=float32, numpy=array([-0.0729818,  0.8324026], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 1.6503255],\n        [-1.1728343]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/bias:0' shape=(1,) dtype=float32, numpy=array([0.95030355], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'.') \nplt.plot(x,net3(x),'--')\n\n\n\n\n\n잘 맞춤!\n\n- 분석\n\nplt.plot(x,y,'.') \nplt.plot(x,l1(x),'--')\n\n\n\n\n\nplt.plot(x,y,'.') \nplt.plot(x,a1(l1(x)),'--')\n\n\n\n\n\nplt.plot(x,y,'.') \nplt.plot(x,l2(a1(l1(x))),'--')\n\n\n\n\n- 마지막 2개의 그림을 분석\n\nl2.weights\n\n[&lt;tf.Variable 'dense_40/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[ 1.6503255],\n        [-1.1728343]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_40/bias:0' shape=(1,) dtype=float32, numpy=array([0.95030355], dtype=float32)&gt;]\n\n\n\nfig, (ax1,ax2,ax3) = plt.subplots(1,3) \nfig.set_figwidth(12) \nax1.plot(x,y,'.')\nax1.plot(x,a1(l1(x))[:,0],'--r')\nax1.plot(x,a1(l1(x))[:,1],'--b')\nax2.plot(x,y,'.')\nax2.plot(x,a1(l1(x))[:,0]*1.6328746,'--r')\nax2.plot(x,a1(l1(x))[:,1]*(-1.2001747)+1.0253307,'--b')\nax3.plot(x,y,'.')\nax3.plot(x,a1(l1(x))[:,0]*1.6328746+a1(l1(x))[:,1]*(-1.2001747)+1.0253307,'--')\n\n\n\n\n\n\n풀이3의 실패\n\n# tf.random.set_seed(43054) \ntf.keras.utils.set_random_seed(43052)\n## 1단계\nnet3 = tf.keras.Sequential() \n## 2단계\nnet3.add(tf.keras.layers.Dense(2))\nnet3.add(tf.keras.layers.Activation('relu')) \nnet3.add(tf.keras.layers.Dense(1)) # 출력 1\n## 3단계 \nnet3.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\n## 4단계 \nnet3.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f8821a15c70&gt;\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net3(x),'--')\n\n\n\n\n- 엥? 에폭이 부족한가?\n\nnet3.fit(x,y,epochs=10000,verbose=0,batch_size=N)\nplt.plot(x,y,'.')\nplt.plot(x,net3(x),'--')\n\n\n\n\n\n똑같은데…? 결국 에폭문제가 아니였음.\n\n- 실패분석\n\nl1,a1,l2 = net3.layers\n\n\nl2.weights\n\n[&lt;tf.Variable 'dense_46/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[0.5306579],\n        [1.7407396]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_46/bias:0' shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;]\n\n\n\nfig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) \nfig.set_figwidth(16) \nax1.plot(x,y,'.')\nax1.plot(x,l1(x)[:,0],'--r')\nax1.plot(x,l1(x)[:,1],'--b')\nax2.plot(x,y,'.')\nax2.plot(x,a1(l1(x))[:,0],'--r')\nax2.plot(x,a1(l1(x))[:,1],'--b')\nax3.plot(x,y,'.')\nax3.plot(x,a1(l1(x))[:,0]*0.5306579,'--r')\nax3.plot(x,a1(l1(x))[:,1]*(1.7407396)+(-0.60076195),'--b')\nax4.plot(x,y,'.')\nax4.plot(x,a1(l1(x))[:,0]*0.5306579+a1(l1(x))[:,1]*(1.7407396)+(-0.60076195),'--')\n\n\n\n\n\n보니까 빨간색선이 하는 역할을 없음\n그런데 생각해보니까 이 상황에서는 빨간색선이 할수 있는 일이 별로 없음\n왜? 지금은 나름 파란색선에 의해서 최적화가 된 상태임 \\(\\to\\) 빨간선이 뭔가 하려고하면 최적화된 상태가 깨질 수 있음 (loss 증가)\n즉 이 상황 자체가 나름 최적화된 상태이다. 이러한 현상을 “global minimum을 찾지 못하고 local minimum에 빠졌다” 라고 표현한다.\n\n확인:\n\nnet3.weights\n\n[&lt;tf.Variable 'dense_45/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[-0.3914748,  1.9987504]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_45/bias:0' shape=(2,) dtype=float32, numpy=array([-0.392621,  0.34811 ], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_46/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[0.5306579],\n        [1.7407396]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_46/bias:0' shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;]\n\n\n\nW1= tf.Variable(tnp.array([[-0.03077251,  1.8713338 ]]))\nb1= tf.Variable(tnp.array([-0.04834982,  0.3259186 ]))\nW2= tf.Variable(tnp.array([[0.65121335],[1.8592643 ]]))\nb2= tf.Variable(tnp.array([-0.60076195])) \n\n\nwith tf.GradientTape() as tape: \n    u = tf.constant(x) @ W1 + b1 \n    v = tf.nn.relu(u) \n    yhat = v@W2 + b2 \n    loss = tf.losses.mse(y,yhat) \n\n\ntape.gradient(loss,[W1,b1,W2,b2])\n\n[&lt;tf.Tensor: shape=(1, 2), dtype=float64, numpy=array([[ 0.00000000e+00, -4.77330119e-05]])&gt;,\n &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([0.0000000e+00, 3.1478608e-06])&gt;,\n &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\n array([[ 0.00000000e+00],\n        [-4.74910706e-05]])&gt;,\n &lt;tf.Tensor: shape=(1,), dtype=float64, numpy=array([-2.43031263e-05])&gt;]\n\n\n예상대로 계수값이 거의 다 0이다.\n\n\n풀이4: 노드수를 더 추가한다면?\n- 노드수를 더 추가해보면 어떻게 될까? (주황색 점선이 더 여러개 있다면?)\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -&gt; \"node1\"\n    \"X\" -&gt; \"node2\"\n    \"X\" -&gt; \"...\"\n    \"X\" -&gt; \"node512\"\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -&gt; \"yhat\"\n    \"node2\" -&gt; \"yhat\"\n    \"...\" -&gt; \"yhat\"\n    \"node512\" -&gt; \"yhat\"\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\n\ntf.random.set_seed(43056)\nnet4= tf.keras.Sequential()\nnet4.add(tf.keras.layers.Dense(512,activation='relu')) # 이렇게 해도됩니다. \nnet4.add(tf.keras.layers.Dense(1))         \nnet4.compile(loss='mse',optimizer=tf.optimizers.SGD(0.1)) \nnet4.fit(x,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7f8821327160&gt;\n\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net4(x),'--')\n\n\n\n\n\n잘된다..\n한두개의 노드가 역할을 못해도 다른노드들이 잘 보완해주는듯!\n\n- 노드수가 많으면 무조건 좋다? -&gt; 대부분 나쁘지 않음. 그런데 종종 맞추지 말아야할것도 맞춤.. (overfit)\n\nnp.random.seed(43052)\nN=100 \n_x = np.linspace(0,1,N).reshape(N,1) \n_y = np.random.normal(loc=0,scale=0.001,size=(N,1))\nplt.plot(_x,_y)\n\n\n\n\n\ntf.random.set_seed(43052) \nnet4 = tf.keras.Sequential()\nnet4.add(tf.keras.layers.Dense(512,activation='relu'))\nnet4.add(tf.keras.layers.Dense(1))\nnet4.compile(loss='mse',optimizer=tf.optimizers.SGD(0.5))\nnet4.fit(_x,_y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f88210403a0&gt;\n\n\n\nplt.plot(_x,_y)\nplt.plot(_x,net4(_x),'--')\n\n\n\n\n\n맞추지 말아야 할 것까지 맞춘다..\n가장 좋은 fit은 직선으로 맞추는것.\n이 예제는 추후 다시 공부할 예정\n\n\n\n\nLogistic regression\n\nmotive\n- 현실에서 이런 경우가 많음 - \\(x\\)가 커질수록 (혹은 작아질수록) 성공확률이 올라간다.\nex) 전자제품에 열을 많이 가할수록 불량률일 증가한다.\nex) 성적이 좋을수록 합격률이 증가한다.\n- 이러한 모형은 아래와 같이 설계할 수 있음 &lt;– 외우세요!! - \\(y_i \\sim Ber(\\pi_i)\\), where \\(\\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\)\n\n\\(\\hat{y}_i =\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\frac{1}{1+exp(-\\hat{w}_0-\\hat{w}_1x_i)}\\)\n\\(loss=-\\frac{1}{n}\\sum_{i=1}^{n}\\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\n\n- 위와 같은 손실함수를 BCEloss라고 부른다. (BCE는 Binary Cross Entropy의 약자)\n\n\n예제\n\nN = 2000\n\n\nx = tnp.linspace(-1,1,N).reshape(N,1)\nw0 = -1 \nw1 = 5 \nu = w0 + x*w1 \n#v = tf.constant(np.exp(u)/(1+np.exp(u))) # v=πi \nv = tf.nn.sigmoid(u) \ny = tf.constant(np.random.binomial(1,v),dtype=tf.float64) \n\n\nplt.plot(x,y,'.',alpha=0.02)\nplt.plot(x,v,'--r')\n\n\n\n\n- 이 아키텍처(yhat을 얻어내는 과정)를 다어어그램으로 나타내면 아래와 같다.\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x\" -&gt; \"x*w, bias=True\"[label=\"*w\"]\n    \"x*w, bias=True\" -&gt; \"yhat\"[label=\"sigmoid\"]\n    label = \"Layer 1\"\n}\n''')\n\n\n\n\n- 또는 간단하게 아래와 같이 쓸 수 있다.\n\n#collapse\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    x\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    x -&gt; \"node1=yhat\"\n    label = \"Layer 1: sigmoid\"\n}\n''')\n\n\n\n\n- 케라스를 이용하여 적합을 해보면\n\n\\(loss=-\\frac{1}{n}\\sum_{i=1}^{n}\\big(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i)\\big)\\)\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nbceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat))\nnet.compile(loss=bceloss_fn, optimizer=tf.optimizers.SGD(0.1))\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7f8820868070&gt;\n\n\n\nnet.weights\n\n[&lt;tf.Variable 'dense_55/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[4.4220047]], dtype=float32)&gt;,\n &lt;tf.Variable 'dense_55/bias:0' shape=(1,) dtype=float32, numpy=array([-0.84613276], dtype=float32)&gt;]\n\n\n\nplt.plot(x,y,'.',alpha=0.1) # 관측데이터\nplt.plot(x,v,'--r') # 추정하고 싶은 확률\nplt.plot(x,net(x),'--b')  # 네트워크에 의한 추정 결과.\n\n\n\n\n\n거의 비슷하게 잘 추정되었다.\n\n\n\nMSE loss?\n- mse loss를 쓰면 왜 안되는지?\nMSE loss를 써서 다시 돌려보자.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nmseloss_fn = lambda y,yhat: tf.reduce_mean((y-yhat)**2)\nnet.compile(loss=mseloss_fn, optimizer=tf.optimizers.SGD(0.1))\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) \n\n&lt;keras.callbacks.History at 0x7f88201c2580&gt;\n\n\n\nplt.plot(x,y,'.',alpha=0.1)\nplt.plot(x,v,'--r')\nplt.plot(x,net(x),'--b')\n\n\n\n\n\n일단 BCE loss와 비교해보니까 동일 초기값, 동일 epochs에서 적합이 별로임\n\n\n\nMSE loss vs BCE loss\n- MSEloss, BCEloss의 시각화\n\nw0, w1 = np.meshgrid(np.arange(-10,3,0.2), np.arange(-1,10,0.2), indexing='ij')\nw0, w1 = w0.reshape(-1), w1.reshape(-1)\n\ndef loss_fn1(w0,w1):\n    u = w0+w1*x \n    yhat = np.exp(u)/(np.exp(u)+1)\n    return mseloss_fn(y,yhat) \n\ndef loss_fn2(w0,w1):\n    u = w0+w1*x \n    yhat = np.exp(u)/(np.exp(u)+1)\n    return bceloss_fn(y,yhat) \n\nloss1 = list(map(loss_fn1,w0,w1))\nloss2 = list(map(loss_fn2,w0,w1))\n\n\nfig = plt.figure()\nfig.set_figwidth(9)\nfig.set_figheight(9)\nax1=fig.add_subplot(1,2,1,projection='3d')\nax2=fig.add_subplot(1,2,2,projection='3d')\nax1.elev=15\nax2.elev=15\nax1.azim=75\nax2.azim=75\nax1.scatter(w0,w1,loss1,s=0.1)\nax2.scatter(w0,w1,loss2,s=0.1) \n\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f8803ecb910&gt;\n\n\n\n\n\n\n왼쪽곡면(MSEloss)보다 오른쪽곡면(BCEloss)이 좀더 예쁘게 생김 -&gt; 오른쪽 곡면에서 더 학습이 잘될것 같음\n\n\n\n학습과정 시각화예시1\n- 파라메터학습과정 시각화 // 옵티마이저: SGD, 초기값: (w0,w1) = (-3.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n&lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])&gt;\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_mse.compile(optimizer=tf.optimizers.SGD(0.1),loss=mseloss_fn) \nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 75ms/step - loss: 0.2461\n\n\n&lt;keras.callbacks.History at 0x7f8803dd3940&gt;\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_bce.compile(optimizer=tf.optimizers.SGD(0.1),loss=bceloss_fn) \nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 85ms/step - loss: 1.1333\n\n\n&lt;keras.callbacks.History at 0x7f882080f0d0&gt;\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[0.40601483],\n         [0.3624403 ]], dtype=float32)],\n [array([[ 1.363407  ],\n         [-0.33031225]], dtype=float32)])\n\n\n\n둘이 weight이 다르니까 강제로 맞춰놓자.\n\n\nnet_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-3.],\n         [-1.]], dtype=float32)],\n [array([[-3.],\n         [-1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29): \n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) \n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) \n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"SGD, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n왼쪽 MSE, 오른쪽 BCE\nMSE의 경우 너무 천천히 수렴을 한다.\n동일 학습조건에서 오른쪽이 학습이 더 잘된다.\n\n\n\n학습과정 시각화예시2\n이번에는 똑같은 초깃값에 다른 옵티마이저(Adam)를 써보자.\n- 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-3.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n&lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])&gt;\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) \nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 92ms/step - loss: 0.2902\n\n\n&lt;keras.callbacks.History at 0x7f88211b1430&gt;\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) \nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 98ms/step - loss: 0.8690\n\n\n&lt;keras.callbacks.History at 0x7f88402718b0&gt;\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[1.1395919],\n         [1.3250527]], dtype=float32)],\n [array([[-0.77553874],\n         [-0.59953135]], dtype=float32)])\n\n\n\nnet_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-3.],\n         [-1.]], dtype=float32)],\n [array([[-3.],\n         [-1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29): \n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) \n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) \n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"Adam, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nAdam을 쓴게 확실히 학습을 효율적으로 빨리함!\n\n동일한 loss function에 동일한 초깃값이라 해도 Adam을 쓰면 학습이 더 빠르다.\n그럼 MSE+Adam 조합으로 하면 적합이 잘 되겠구나? 라고 생각할 수 있는데 그건 아님.\n\n\n\n학습과정 시각화예시3\n이번에는 Adam을 쓸건데 다른 초깃값에서 시작해보자.\n- 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-10.0,-1.0)\n\n데이터정리\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\nX\n\n&lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy=\narray([[ 1.       , -1.       ],\n       [ 1.       , -0.9989995],\n       [ 1.       , -0.997999 ],\n       ...,\n       [ 1.       ,  0.997999 ],\n       [ 1.       ,  0.9989995],\n       [ 1.       ,  1.       ]])&gt;\n\n\n\n1ter돌려봄\n\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) \nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 93ms/step - loss: 0.1645\n\n\n&lt;keras.callbacks.History at 0x7f87f3edb880&gt;\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) \nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 101ms/step - loss: 0.5978\n\n\n&lt;keras.callbacks.History at 0x7f87f1ce8400&gt;\n\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-0.9448344],\n         [ 1.4777311]], dtype=float32)],\n [array([[0.47022673],\n         [1.2034082 ]], dtype=float32)])\n\n\n\nnet_mse.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-10.],\n         [ -1.]], dtype=float32)],\n [array([[-10.],\n         [ -1.]], dtype=float32)])\n\n\n\n학습과정기록: 15에폭마다 기록\n\n\nWhat_mse = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29): \n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) \n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) \n\n\n시각화\n\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"Adam, Winit=(-10,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n아무리 아담이라고 해도 이건 힘듬\n\n학습을 잘 되게 하기 위해서 옵티마이저를 개선하는 것은 되게 좋은 방법이지만 그것보다 근본적으로 loss function을 좀 더 예쁘게 만드려는 노력이 훨씬 더 좋은 생각..\n\n\ndiscussion / Summary\n- discussion - mse_loss는 경우에 따라서 엄청 수렴속도가 느릴수도 있음. - 근본적인 문제점: mse_loss일 경우 loss function의 곡면이 예쁘지 않음. (전문용어로 convex가 아니라고 말함) - 좋은 옵티마지어를 이용하면 mse_loss일 경우에도 수렴속도를 올릴 수 있음 (학습과정 시각화예시2). 그렇지만 이는 근본적인 해결책은 아님. (학습과정 시각화예시3)\n- 요약: 왜 logistic regression에서 mse loss를 쓰면 안되는가? - mse loss를 사용하면 손실함수가 convex하지 않으니까! - 그리고 bce loss를 사용하면 손실함수가 convex하니까!"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_05_16_(11주차)_5월16일.html",
    "href": "posts/3_STBDA2022/2022_05_16_(11주차)_5월16일.html",
    "title": "[STBDA] 11wk. MaxPool2D, Conv2D",
    "section": "",
    "text": "강의영상\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-wlEuWT75L4hqGNEwoPpjWw\n\n\n\nimports\n\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n- 첫시도\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(500,activation='relu')) # flatten(): x의 shape을 네트워크 모양에 따라 바꾸지 않아도 된다.\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(500,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\nnet1.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.1752 - accuracy: 0.7889\nEpoch 2/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4585 - accuracy: 0.8347\nEpoch 3/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.4124 - accuracy: 0.8537\nEpoch 4/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.3948 - accuracy: 0.8602\nEpoch 5/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.3813 - accuracy: 0.8649\n\n\n&lt;keras.callbacks.History at 0x7f7fc8338f40&gt;\n\n\n\nnet1.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 735us/step - loss: 0.4072 - accuracy: 0.8473\n\n\n[0.40723177790641785, 0.8472999930381775]\n\n\n\nnet1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense (Dense)               (None, 500)               392500    \n                                                                 \n dense_1 (Dense)             (None, 500)               250500    \n                                                                 \n dense_2 (Dense)             (None, 500)               250500    \n                                                                 \n dense_3 (Dense)             (None, 500)               250500    \n                                                                 \n dense_4 (Dense)             (None, 10)                5010      \n                                                                 \n=================================================================\nTotal params: 1,149,010\nTrainable params: 1,149,010\nNon-trainable params: 0\n_________________________________________________________________\n\n\n- 두번째 시도 : 네트워크를 다르게 설계\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Conv2D(30,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.MaxPool2D())\nnet2.add(tf.keras.layers.Conv2D(30,(2,2),activation='relu'))\nnet2.add(tf.keras.layers.MaxPool2D())\nnet2.add(tf.keras.layers.Flatten())\n#net2.add(tf.keras.layers.Dense(500,activation='relu'))\nnet2.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet2.compile(optimizer='adam', loss=tf.losses.categorical_crossentropy,metrics='accuracy')\nnet2.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.8491 - accuracy: 0.7960\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4071 - accuracy: 0.8539\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3550 - accuracy: 0.8719\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3221 - accuracy: 0.8831\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3096 - accuracy: 0.8877\n\n\n&lt;keras.callbacks.History at 0x7f7f847d1430&gt;\n\n\n\nnet2.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 845us/step - loss: 0.3697 - accuracy: 0.8627\n\n\n[0.36973485350608826, 0.8626999855041504]\n\n\n\nnet2.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 27, 27, 30)        150       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 30)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 12, 12, 30)        3630      \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 6, 6, 30)         0         \n 2D)                                                             \n                                                                 \n flatten_1 (Flatten)         (None, 1080)              0         \n                                                                 \n dense_5 (Dense)             (None, 10)                10810     \n                                                                 \n=================================================================\nTotal params: 14,590\nTrainable params: 14,590\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n14590/ 1149010\n\n0.012697887746842934\n\n\n파라미터를 네트워크1에 비해 1.2%정도 밖에 안쓰고 있는데 성능은 더 좋다.\n\nnet2.layers\n\n[&lt;keras.layers.convolutional.conv2d.Conv2D at 0x7f7f847bea90&gt;,\n &lt;keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f7fc00ef160&gt;,\n &lt;keras.layers.convolutional.conv2d.Conv2D at 0x7f7fc00e7fd0&gt;,\n &lt;keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f7fc007b280&gt;,\n &lt;keras.layers.reshaping.flatten.Flatten at 0x7f7fc008f2e0&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7f7fc0075cd0&gt;]\n\n\n\n\\(x\\)라는 데이터가 들어가서 6장의 레이어를 순차적으로 통과하는 구조이다.\n\n\nc1, m1, c2, m2, flttn, dns = net2.layers\n\n\nprint(X.shape) # 입력이미지 = 2D\nprint(c1(X).shape) #2D\nprint(m1(c1(X)).shape)  #2D\nprint(c2(m1(c1(X))).shape) #2D\nprint(m2(c2(m1(c1(X)))).shape) #2D\nprint(flttn(m2(c2(m1(c1(X))))).shape)# 1D\nprint(dns(flttn(m2(c2(m1(c1(X)))))).shape)# 1D\n\n(60000, 28, 28, 1)\n(60000, 27, 27, 30)\n(60000, 13, 13, 30)\n(60000, 12, 12, 30)\n(60000, 6, 6, 30)\n(60000, 1080)\n(60000, 10)\n\n\n\n\nMaxPool2D\n\n테스트1\n- 레이어생성\n\nm=tf.keras.layers.MaxPool2D()\n\n- 입력데이터\n\nXXX = tnp.arange(1*4*4*1).reshape(1,4,4,1)\nXXX.reshape(1,4,4)\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]]])&gt;\n\n\n- 입력데이터가 레이어를 통과한 모습\n\nm(XXX).reshape(1,2,2)\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]]])&gt;\n\n\n- MaxPool2D layer의 역할: (2,2)윈도우를 만들고 (2,2)윈도우에서 max를 뽑아 값을 기록, 윈도우를 움직이면서 반복\n\n\n테스트2\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n&lt;tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])&gt;\n\n\n\nm(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 7,  9, 11],\n        [19, 21, 23],\n        [31, 33, 35]]])&gt;\n\n\n\n\n테스트3\n\nm=tf.keras.layers.MaxPool2D(pool_size=(3, 3))\n\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n&lt;tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])&gt;\n\n\n\nm(XXX).reshape(1,2,2)\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[14, 17],\n        [32, 35]]])&gt;\n\n\n\n\n테스트4\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n\n\nXXX = tnp.arange(1*5*5*1).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24]]])&gt;\n\n\n\nm(XXX).reshape(1,2,2)\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 6,  8],\n        [16, 18]]])&gt;\n\n\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2, 2),padding=\"same\")\n\n\nXXX = tnp.arange(1*5*5*1).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24]]])&gt;\n\n\n\n차원이 안맞는 부분은 버린다.. (이게 Default)\n\n\nm(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 6,  8,  9],\n        [16, 18, 19],\n        [21, 23, 24]]])&gt;\n\n\n\n\n테스트5\n\nXXX = tnp.arange(2*4*4*1).reshape(2,4,4,1)\nXXX.reshape(2,4,4)\n\n&lt;tf.Tensor: shape=(2, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]],\n\n       [[16, 17, 18, 19],\n        [20, 21, 22, 23],\n        [24, 25, 26, 27],\n        [28, 29, 30, 31]]])&gt;\n\n\n\nm(XXX).reshape(2,2,2)\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]],\n\n       [[21, 23],\n        [29, 31]]])&gt;\n\n\n\n\n테스트6\n\nXXX = tnp.arange(1*4*4*3).reshape(1,4,4,3)\n\n\nXXX[...,0]\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  3,  6,  9],\n        [12, 15, 18, 21],\n        [24, 27, 30, 33],\n        [36, 39, 42, 45]]])&gt;\n\n\n\nm(XXX)[...,0]\n\n&lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[15, 21],\n        [39, 45]]])&gt;\n\n\n\n\n\nConv2D\n\n테스트1\n- 레이어생성\n\ncnv = tf.keras.layers.Conv2D(1,(2,2))\n\n- XXX생성\n\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1) # data type이 float형이어야 에러가 안남! 4-dim\nXXX.reshape(1,4,4)\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])&gt;\n\n\n\ncnv(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 0.9716801,  2.230203 ,  3.4887257],\n        [ 6.0057716,  7.2642946,  8.522818 ],\n        [11.039864 , 12.298387 , 13.55691  ]]], dtype=float32)&gt;\n\n\n\nXXX에서 cnv(XXX)로 가는 맵핑을 찾는건 쉽지 않아보인다.\n심지어 랜덤으로 결정되는 부분도 있어보임\n\n- 코드정리 + 시드통일\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(2,2))\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1)\n\n- conv의 입출력\n\nprint(XXX.reshape(1,4,4))\nprint(cnv(XXX).reshape(1,3,3))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.]\n  [ 4.  5.  6.  7.]\n  [ 8.  9. 10. 11.]\n  [12. 13. 14. 15.]]], shape=(1, 4, 4), dtype=float64)\ntf.Tensor(\n[[[ 0.36673212 -0.5838206  -1.5343733 ]\n  [-3.4354792  -4.3860326  -5.336584  ]\n  [-7.23769    -8.188243   -9.138796  ]]], shape=(1, 3, 3), dtype=float32)\n\n\n\ntype(cnv.weights) # 길이가 2인 list\n\nlist\n\n\n- conv연산 추론\n\ntf.reshape(cnv.weights[0],(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.7749905 , -0.50674176],\n       [ 0.7824232 , -0.45124376]], dtype=float32)&gt;\n\n\n\n## XXX\ntf.Tensor(\n[[[ 0.  1.  2.  3.]\n  [ 4.  5.  6.  7.]\n  [ 8.  9. 10. 11.]\n  [12. 13. 14. 15.]]]\n\n## cnv(XXX)\n[[[ 0.36673212 -0.5838206  -1.5343733 ]\n  [-3.4354792  -4.3860326  -5.336584  ]\n  [-7.23769    -8.188243   -9.138796  ]]]\n## cnv.weights\narray([[-0.7749905 , -0.50674176],\n       [ 0.7824232 , -0.45124376]], dtype=float32)&gt;\n\n# my version\nprint(0*-0.7749905 + 1*-0.50674176 + 4*0.7824232 + 5*-0.45124376 + 0) # bias=0\nprint(1*-0.7749905 + 2*-0.50674176 + 5*0.7824232 + 6*-0.45124376  + 0) # bias=0\nprint(2*-0.7749905 + 3*-0.50674176 + 6*0.7824232 + 7*-0.45124376 + 0) # bias=0\n\n0.3667322399999997\n-0.5838205799999998\n-1.5343734000000002\n\n\n\n# element wise하게 곱한다.\n# ver. 교수님\n0 * -0.13014299 + 1 * -0.23927206 + 4 * -0.20175874 + 5 * -0.6158894 + 0\n\n-4.1257540200000005\n\n\n\nweights를 위에 처럼 다 쓰긴 너무 귀찮아..\n\n- 내가 정의한 weights를 대입하여 conv 연산 확인\n\ncnv.get_weights()[0].shape\n\n(2, 2, 1, 1)\n\n\n\nw = np.array([1/4,1/4,1/4,1/4],dtype=np.float32).reshape(2, 2, 1, 1)\nb = np.array([3],dtype=np.float32)\n\n\ncnv.set_weights([w,b])\n\n\ncnv.weights\n\n[&lt;tf.Variable 'conv2d_9/kernel:0' shape=(2, 2, 1, 1) dtype=float32, numpy=\n array([[[[0.25]],\n \n         [[0.25]]],\n \n \n        [[[0.25]],\n \n         [[0.25]]]], dtype=float32)&gt;,\n &lt;tf.Variable 'conv2d_9/bias:0' shape=(1,) dtype=float32, numpy=array([3.], dtype=float32)&gt;]\n\n\n\nbias=3\n\n\nXXX.reshape(1,4,4)\n\n&lt;tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])&gt;\n\n\n\ncnv(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 5.5,  6.5,  7.5],\n        [ 9.5, 10.5, 11.5],\n        [13.5, 14.5, 15.5]]], dtype=float32)&gt;\n\n\n\nnp.mean([0,1,4,5])+3, np.mean([1,2,5,6])+3, np.mean([2,3,6,7])+3\n\n(5.5, 6.5, 7.5)\n\n\n\n\ntf.keras.layers.Conv2D(1,kernel_size=(2,2)) 요약\n- 요약\n\nsize=(2,2)인 윈도우를 만듬.\nXXX에 윈도우를 통과시켜서 (2,2)크기의 sub XXX 를 얻음. sub XXX의 각 원소에 conv2d.weights[0]의 각 원소를 element-wise하게 곱한다.\n(2)의 결과를 모두 더한다. 그리고 그 결과에 다시 conv2d.weights[1]을 수행\n윈도우를 이동시키면서 반복!\n\n\n\n테스트2\n- 레이어와 XXX생성\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(3,3)) # kernel size: 3x3\nXXX = tnp.arange(1*5*5*1,dtype=tf.float64).reshape(1,5,5,1)\n\n\nXXX.reshape(1,5,5) ## 입력: XXX\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.],\n        [10., 11., 12., 13., 14.],\n        [15., 16., 17., 18., 19.],\n        [20., 21., 22., 23., 24.]]])&gt;\n\n\n\ncnv(XXX)\n\n&lt;tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=\narray([[[[-2.1662652 ],\n         [-1.4241629 ],\n         [-0.68206024]],\n\n        [[ 1.5442457 ],\n         [ 2.2863474 ],\n         [ 3.0284495 ]],\n\n        [[ 5.254756  ],\n         [ 5.996857  ],\n         [ 6.73896   ]]]], dtype=float32)&gt;\n\n\n\ncnv.get_weights()\n\n[array([[[[ 0.25344223]],\n \n         [[ 0.4941373 ]],\n \n         [[ 0.21964025]]],\n \n \n        [[[ 0.5367935 ]],\n \n         [[-0.38218382]],\n \n         [[-0.1195904 ]]],\n \n \n        [[[-0.52197355]],\n \n         [[ 0.5757177 ]],\n \n         [[-0.3138811 ]]]], dtype=float32),\n array([0.], dtype=float32)]\n\n\n\n하나는 kernel에 대한 weight, 다른 하나는 bias에 대한 weight(=0)\n\n\ntf.reshape(cnv.weights[0],(3,3)) ## 커널의 가중치\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 0.25344223,  0.4941373 ,  0.21964025],\n       [ 0.5367935 , -0.38218382, -0.1195904 ],\n       [-0.52197355,  0.5757177 , -0.3138811 ]], dtype=float32)&gt;\n\n\n\ncnv(XXX).reshape(1,3,3) ## 출력: conv(XXX)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[-2.1662652 , -1.4241629 , -0.68206024],\n        [ 1.5442457 ,  2.2863474 ,  3.0284495 ],\n        [ 5.254756  ,  5.996857  ,  6.73896   ]]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(XXX.reshape(1,5,5)[0,:3,:3] * tf.reshape(cnv.weights[0],(3,3)))\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=-2.1662647128105164&gt;\n\n\n\ntf.reduce_sum(XXX.reshape(1,5,5)[0,:3,1:4]  * tf.reshape(cnv.weights[0],(3,3)))\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=-1.4241626560688019&gt;\n\n\n\ntf.reduce_sum(XXX.reshape(1,5,5)[0,:3,2:5]  * tf.reshape(cnv.weights[0],(3,3)))\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=-0.6820605993270874&gt;\n\n\n\n\n테스트3\n\n\nXXX = tf.constant([[3,3,2,1,0],[0,0,1,3,1],[3,1,2,2,3],[2,0,0,2,2],[2,0,0,0,1]],dtype=tf.float64).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n&lt;tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[3., 3., 2., 1., 0.],\n        [0., 0., 1., 3., 1.],\n        [3., 1., 2., 2., 3.],\n        [2., 0., 0., 2., 2.],\n        [2., 0., 0., 0., 1.]]])&gt;\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\ndtype은 항상 float\nshape은 항상 4D로 나와야 한다.\n\n\n\n\ncnv = tf.keras.layers.Conv2D(1,(3,3)) # 3x3 window를 갖는 커널\n\n\ncnv.weights\n\n[]\n\n\n\n일단 cnv(XXX) 이걸 돌려줘야 weight이 생긴다.\n\n\ncnv(XXX).reshape(1,3,3) \n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 0.5037645 , -1.312168  ,  0.6543627 ],\n        [ 2.4390626 ,  1.264232  , -0.26046228],\n        [ 2.5233214 ,  0.00339425,  1.0914648 ]]], dtype=float32)&gt;\n\n\n\nlayer에 통과하는 순간 weight이 정의된다.\n\n\ncnv.weights[0]\n\n&lt;tf.Variable 'conv2d_18/kernel:0' shape=(3, 3, 1, 1) dtype=float32, numpy=\narray([[[[-0.01447701]],\n\n        [[-0.02373308]],\n\n        [[ 0.40413225]]],\n\n\n       [[[ 0.47097933]],\n\n        [[ 0.52465725]],\n\n        [[-0.37146354]]],\n\n\n       [[[ 0.42013115]],\n\n        [[-0.37700126]],\n\n        [[-0.35089916]]]], dtype=float32)&gt;\n\n\n\ncnv.weights[1] # bias\n\n&lt;tf.Variable 'conv2d_18/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;\n\n\n\nbias는 \\(0\\)이니까 무시..\n\n\n_w = tf.constant([[0,1,2],[2,2,0],[0,1,2]],dtype=tf.float64).reshape(3,3,1,1)\n_b = tf.constant([0],dtype=tf.float64)\n\n\ncnv.set_weights([_w,_b])\n\n\ncnv(XXX).reshape(1,3,3)\n\n&lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[12., 12., 17.],\n        [10., 17., 19.],\n        [ 9.,  6., 14.]]], dtype=float32)&gt;\n\n\n\n\n테스트4: X의 channel, Conv channel 늘려보기.\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(2,2))\nXXX = tnp.arange(2*5*5*1,dtype=tf.float64).reshape(2,5,5,1) # 무조건 4D, float형\n\n\nprint(XXX.reshape(2,5,5))\ncnv(XXX) # weights를 초기화 시키기 위해서 레이어를 1회 통과\ncnv.set_weights([w,b])\nprint(cnv(XXX).reshape(2,4,4))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.  4.]\n  [ 5.  6.  7.  8.  9.]\n  [10. 11. 12. 13. 14.]\n  [15. 16. 17. 18. 19.]\n  [20. 21. 22. 23. 24.]]\n\n [[25. 26. 27. 28. 29.]\n  [30. 31. 32. 33. 34.]\n  [35. 36. 37. 38. 39.]\n  [40. 41. 42. 43. 44.]\n  [45. 46. 47. 48. 49.]]], shape=(2, 5, 5), dtype=float64)\ntf.Tensor(\n[[[ 6.  7.  8.  9.]\n  [11. 12. 13. 14.]\n  [16. 17. 18. 19.]\n  [21. 22. 23. 24.]]\n\n [[31. 32. 33. 34.]\n  [36. 37. 38. 39.]\n  [41. 42. 43. 44.]\n  [46. 47. 48. 49.]]], shape=(2, 4, 4), dtype=float32)\n\n\n## 첫번째 observation\n[[[ 0.  1.  2.  3.  4.]\n  [ 5.  6.  7.  8.  9.]\n  [10. 11. 12. 13. 14.]\n  [15. 16. 17. 18. 19.]\n  [20. 21. 22. 23. 24.]]\n\n ## 두번째 observation\n [[25. 26. 27. 28. 29.]\n  [30. 31. 32. 33. 34.]\n  [35. 36. 37. 38. 39.]\n  [40. 41. 42. 43. 44.]\n  [45. 46. 47. 48. 49.]]]\n\nprint(cnv.weights[0]) # weights\nprint(cnv.weights[1]) # bias\n\n&lt;tf.Variable 'conv2d_24/kernel:0' shape=(2, 2, 1, 1) dtype=float32, numpy=\narray([[[[0.25]],\n\n        [[0.25]]],\n\n\n       [[[0.25]],\n\n        [[0.25]]]], dtype=float32)&gt;\n&lt;tf.Variable 'conv2d_24/bias:0' shape=(1,) dtype=float32, numpy=array([3.], dtype=float32)&gt;\n\n\n\nnp.mean([0,1,5,6])+3,np.mean([25,26,30,31])+3,\n\n(6.0, 31.0)\n\n\n\n첫번째 obs에 컨볼루션 연산, 두번째 obs도 반복..\n\n\n\n테스트5 : obs 그대로(고정), 출력 채널이 바뀔 때\n- 여기부터 어려워짐..\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2),activation='relu')\nXXX = tnp.arange(1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[0. 1.]\n  [2. 3.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\ncnv(XXX)\n\n&lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=array([[[[0.       , 1.4783237, 0.       , 1.3432117]]]], dtype=float32)&gt;\n\n\n\ncnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수\n\n&lt;tf.Variable 'conv2d_27/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.14884505,  0.1156525 , -0.4904389 , -0.0581924 ]],\n\n        [[-0.05579314,  0.28279382, -0.27906388, -0.15229541]]],\n\n\n       [[[-0.02228564,  0.08160239,  0.44105333,  0.49807286]],\n\n        [[-0.51174355,  0.34410834, -0.38598925,  0.16645378]]]],\n      dtype=float32)&gt;\n\n\n\n1개의 채널에서 4개의 채널로 뻥튀기 됨.\n\n\ncnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.14884505, -0.05579314],\n       [-0.02228564, -0.51174355]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=-1.635595053434372&gt;\n\n\n- 계산결과를 확인하기 쉽게 하기 위한 약간의 트릭\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2))\nXXX = tnp.array([1]*1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[1. 1.]\n  [1. 1.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\n이렇게 XXX를 설정하면 cnv(XXX)의 결과는 단지 cnv의 weight들의 sum이 된다.\n\n\ncnv(XXX)\n\n&lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=array([[[[1.1908543, 0.       , 0.       , 0.       ]]]], dtype=float32)&gt;\n\n\n\ncnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수\n\n&lt;tf.Variable 'conv2d_24/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]],\n\n        [[-0.36398047,  0.07347518, -0.08780673,  0.46633136]]],\n\n\n       [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]],\n\n        [[ 0.33916563, -0.08248386,  0.11705655, -0.49948823]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.08230966, -0.36398047],\n       [ 0.19759327,  0.33916563]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0])\n#tf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.090468764&gt;\n\n\n\n\n테스트6\n- 결과확인을 쉽게하기 위해서 XXX를 1로 통일\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,(2,2))\nXXX = tnp.array([1]*1*2*2*3,dtype=tf.float64).reshape(1,2,2,3)\n\n\ncnv(XXX)\n\n&lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 0.3297621, -0.4498347, -1.0487393, -1.580095 ]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0] ## (2,2)는 커널의 사이즈 // 3은 XXX의채널 // 4는 cnv(XXX)의 채널\n\n&lt;tf.Variable 'conv2d_33/kernel:0' shape=(2, 2, 3, 4) dtype=float32, numpy=\narray([[[[-0.06956434, -0.12789628, -0.10784459, -0.32920673],\n         [-0.30761963,  0.06209785, -0.07421023,  0.3941219 ],\n         [ 0.16699678, -0.38913035, -0.13020593, -0.29443866]],\n\n        [[ 0.28664726, -0.0697116 ,  0.09893084, -0.4221446 ],\n         [-0.23161241, -0.16410837, -0.36420006,  0.12424195],\n         [-0.14245945,  0.36286396, -0.10751781,  0.1733647 ]]],\n\n\n       [[[ 0.02764335,  0.15547717, -0.42024496, -0.31893867],\n         [ 0.22414821,  0.3619454 , -0.00282967, -0.3503708 ],\n         [ 0.4610079 , -0.17417148,  0.00401336, -0.29777044]],\n\n        [[-0.1620284 , -0.42066965, -0.01578814, -0.4240524 ],\n         [ 0.37925082,  0.24236053,  0.3949356 , -0.20996472],\n         [-0.30264795, -0.28889188, -0.3237777 ,  0.37506342]]]],\n      dtype=float32)&gt;\n\n\n\ncnv.weights[0][...,0] ## cnv(XXX)의 첫번째 채널결과를 얻기 위해서 사용하는 w\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\narray([[[-0.06956434, -0.30761963,  0.16699678],\n        [ 0.28664726, -0.23161241, -0.14245945]],\n\n       [[ 0.02764335,  0.22414821,  0.4610079 ],\n        [-0.1620284 ,  0.37925082, -0.30264795]]], dtype=float32)&gt;\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0]) ### cnv(XXX)의 첫번째 채널의 결과\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.32976213&gt;\n\n\n\nprint(tf.reduce_sum(cnv.weights[0][...,0]))\nprint(tf.reduce_sum(cnv.weights[0][...,1]))\nprint(tf.reduce_sum(cnv.weights[0][...,2]))\nprint(tf.reduce_sum(cnv.weights[0][...,3])) ### cnv(XXX)의 결과\n\ntf.Tensor(0.32976213, shape=(), dtype=float32)\ntf.Tensor(-0.44983464, shape=(), dtype=float32)\ntf.Tensor(-1.0487392, shape=(), dtype=float32)\ntf.Tensor(-1.5800952, shape=(), dtype=float32)\n\n\n\nw_red = cnv.weights[0][...,0][...,0]\nw_green = cnv.weights[0][...,0][...,1]\nw_blue = cnv.weights[0][...,0][...,2]\n\n\ntf.reduce_sum(XXX[...,0] * w_red + XXX[...,1] * w_green + XXX[...,2] * w_blue) ## cnv(XXX)의 첫채널 출력결과\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.32976213097572327&gt;\n\n\n\n\n\nhw\n아래와 같은 흑백이미지가 있다고 하자.\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\n위의 이미지에 아래와 같은 weight를 가진 필터를 적용하여 convolution한 결과를 계산하라. (bias는 0으로 가정한다)\n-1 1\n-1 1"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "import numpy as np\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt\n\n\n\n(1) 아래는 \\(X_i \\overset{iid}{\\sim} N(3,2^2)\\) 를 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tnp.random.randn(10000)*2+3\nx\n\n&lt;tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])&gt;\n\n\n함수 \\(L(\\mu,\\sigma)\\)을 최대화하는 \\((\\mu,\\sigma)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\mu\\)의 초기값은 2로 \\(\\sigma\\)의 초기값은 3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2}\\]\n(2) 아래는 \\(X_i \\overset{iid}{\\sim} Ber(0.8)\\)을 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tf.constant(np.random.binomial(1,0.8,(10000,)))\nx\n\n&lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([0, 0, 1, ..., 1, 1, 1])&gt;\n\n\n함수 \\(L(p)\\)을 최대화하는 \\(p\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(p\\)의 초기값은 0.3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=p^{x_i}(1-p)^{1-x_i}\\]\n(3) 아래의 모형에 따라서 \\(\\{Y_i\\}_{i=1}^{10000}\\)를 생성하는 코드를 작성하라. - \\(Y_i \\overset{iid}{\\sim} N(\\mu_i,1)\\) - \\(\\mu_i = \\beta_0 + \\beta_1 x_i = 0.5 + 2 x_i\\) , where \\(x_i = \\frac{i}{10000}\\)\n함수 \\(L(\\beta_0,\\beta_1)\\)을 최대화하는 \\((\\beta_0,\\beta_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\beta_0,\\beta_1\\)의 초기값은 모두 1로 설정할 것)\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]\n\n\n\n아래와 같은 선형모형을 고려하자.\n\\[y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i.\\]\n이때 오차항은 정규분포로 가정한다. 즉 \\(\\epsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)\\)라고 가정한다.\n관측데이터가 아래와 같을때 아래의 물음에 답하라.\n\nx= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\ny= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 ,\n              63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286])\n# X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4],\n#               [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]])\n\n(1) MSE loss를 최소화 하는 \\(\\beta_0,\\beta_1\\)의 해석해를 구하라.\n(2) 경사하강법과 MSE loss의 도함수를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것.\n(3) tf.keras.optimizers의 apply_gradients()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n(4) tf.keras.optimizers의 minimize()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n\n\n\n(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i, \\quad i=0,1,\\dots, 999\\]\n여기에서 \\(t_i=\\frac{2\\pi i}{1000}\\) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.array(range(1000))* np.pi/1000\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.2)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n(2) 아래와 같은 모형을 고려하자.\n\\[y_i \\sim Ber(\\pi_i), ~ \\text{where} ~ \\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\]\n위의 모형에서 관측한 데이터는 아래와 같다.\n\ntf.random.set_seed(43052)\nx = tnp.linspace(-1,1,2000)\ny = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64)\nplt.plot(x,y,'.',alpha=0.05)\n\n\n\n\ntf.keras를 이용하여 \\(w_0,w_1\\)을 추정하라. (참고: \\(w_0, w_1\\)에 대한 참값은 -1과 5이다.)\n\n\n\n아래의 모형을 고려하자.\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x&gt;0 \\end{cases}\\)\n아래는 위의 모형에서 생성한 샘플이다.\n\n## data\nnp.random.seed(43052)\nN=100\nx= np.linspace(-1,1,N).reshape(N,1)\ny= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1)\n\n(1) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n$ = _0+_1x $\n\n\ntf.random.set_seed(43054)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n&lt;keras.callbacks.History at 0x7f6b142800d0&gt;\n\n\n케라스에 의해 추정된 \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\)을 구하라.\n(2) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n\\(\\boldsymbol{u}= x\\boldsymbol{W}^{(1)}+\\boldsymbol{b}^{(1)}\\)\n\\(\\boldsymbol{v}= \\text{relu}(u)\\)\n\\(y= \\boldsymbol{v}\\boldsymbol{W}^{(2)}+b^{(2)}\\)\n\n\ntf.random.set_seed(43056)\n## 1단계\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2))\nnet.add(tf.keras.layers.Activation('relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f6af6c39f30&gt;\n\n\n\\({\\boldsymbol u}\\)를 이용하여 \\({\\boldsymbol v}\\)를 만드는 코드와 \\({\\boldsymbol v}\\)를 이용하여 \\(y\\)를 만드는 코드를 작성하라.\n(3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라.\n(곤이) (2) 모형은 활성화함수로 relu를 사용하였다.\n(철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다.\n(아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다.\n(짝귀) (1) 의 모형은 오버피팅의 위험이 있다.\n\n\n\n(1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다.\n(2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다.\n(3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다.\n(4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다.\n(5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html#경사하강법과-tf.gradienttape의-사용방법-30점",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html#경사하강법과-tf.gradienttape의-사용방법-30점",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "(1) 아래는 \\(X_i \\overset{iid}{\\sim} N(3,2^2)\\) 를 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tnp.random.randn(10000)*2+3\nx\n\n&lt;tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])&gt;\n\n\n함수 \\(L(\\mu,\\sigma)\\)을 최대화하는 \\((\\mu,\\sigma)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\mu\\)의 초기값은 2로 \\(\\sigma\\)의 초기값은 3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2}\\]\n(2) 아래는 \\(X_i \\overset{iid}{\\sim} Ber(0.8)\\)을 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tf.constant(np.random.binomial(1,0.8,(10000,)))\nx\n\n&lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([0, 0, 1, ..., 1, 1, 1])&gt;\n\n\n함수 \\(L(p)\\)을 최대화하는 \\(p\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(p\\)의 초기값은 0.3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=p^{x_i}(1-p)^{1-x_i}\\]\n(3) 아래의 모형에 따라서 \\(\\{Y_i\\}_{i=1}^{10000}\\)를 생성하는 코드를 작성하라. - \\(Y_i \\overset{iid}{\\sim} N(\\mu_i,1)\\) - \\(\\mu_i = \\beta_0 + \\beta_1 x_i = 0.5 + 2 x_i\\) , where \\(x_i = \\frac{i}{10000}\\)\n함수 \\(L(\\beta_0,\\beta_1)\\)을 최대화하는 \\((\\beta_0,\\beta_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\beta_0,\\beta_1\\)의 초기값은 모두 1로 설정할 것)\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html#회귀분석의-이론적해와-tf.keras.optimizer-이용방법-20점",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html#회귀분석의-이론적해와-tf.keras.optimizer-이용방법-20점",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "아래와 같은 선형모형을 고려하자.\n\\[y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i.\\]\n이때 오차항은 정규분포로 가정한다. 즉 \\(\\epsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)\\)라고 가정한다.\n관측데이터가 아래와 같을때 아래의 물음에 답하라.\n\nx= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\ny= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 ,\n              63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286])\n# X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4],\n#               [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]])\n\n(1) MSE loss를 최소화 하는 \\(\\beta_0,\\beta_1\\)의 해석해를 구하라.\n(2) 경사하강법과 MSE loss의 도함수를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것.\n(3) tf.keras.optimizers의 apply_gradients()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n(4) tf.keras.optimizers의 minimize()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html#keras를-이용한-풀이-30점",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html#keras를-이용한-풀이-30점",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i, \\quad i=0,1,\\dots, 999\\]\n여기에서 \\(t_i=\\frac{2\\pi i}{1000}\\) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.array(range(1000))* np.pi/1000\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.2)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n(2) 아래와 같은 모형을 고려하자.\n\\[y_i \\sim Ber(\\pi_i), ~ \\text{where} ~ \\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\]\n위의 모형에서 관측한 데이터는 아래와 같다.\n\ntf.random.set_seed(43052)\nx = tnp.linspace(-1,1,2000)\ny = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64)\nplt.plot(x,y,'.',alpha=0.05)\n\n\n\n\ntf.keras를 이용하여 \\(w_0,w_1\\)을 추정하라. (참고: \\(w_0, w_1\\)에 대한 참값은 -1과 5이다.)"
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html#piecewise-linear-regression-15점",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html#piecewise-linear-regression-15점",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "아래의 모형을 고려하자.\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x&gt;0 \\end{cases}\\)\n아래는 위의 모형에서 생성한 샘플이다.\n\n## data\nnp.random.seed(43052)\nN=100\nx= np.linspace(-1,1,N).reshape(N,1)\ny= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1)\n\n(1) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n$ = _0+_1x $\n\n\ntf.random.set_seed(43054)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n&lt;keras.callbacks.History at 0x7f6b142800d0&gt;\n\n\n케라스에 의해 추정된 \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\)을 구하라.\n(2) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n\\(\\boldsymbol{u}= x\\boldsymbol{W}^{(1)}+\\boldsymbol{b}^{(1)}\\)\n\\(\\boldsymbol{v}= \\text{relu}(u)\\)\n\\(y= \\boldsymbol{v}\\boldsymbol{W}^{(2)}+b^{(2)}\\)\n\n\ntf.random.set_seed(43056)\n## 1단계\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2))\nnet.add(tf.keras.layers.Activation('relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n&lt;keras.callbacks.History at 0x7f6af6c39f30&gt;\n\n\n\\({\\boldsymbol u}\\)를 이용하여 \\({\\boldsymbol v}\\)를 만드는 코드와 \\({\\boldsymbol v}\\)를 이용하여 \\(y\\)를 만드는 코드를 작성하라.\n(3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라.\n(곤이) (2) 모형은 활성화함수로 relu를 사용하였다.\n(철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다.\n(아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다.\n(짝귀) (1) 의 모형은 오버피팅의 위험이 있다."
  },
  {
    "objectID": "posts/3_STBDA2022/2022_04_25_중간고사.html#다음을-잘-읽고-참과-거짓을-판단하라.-5점",
    "href": "posts/3_STBDA2022/2022_04_25_중간고사.html#다음을-잘-읽고-참과-거짓을-판단하라.-5점",
    "title": "[STBDA] 중간고사",
    "section": "",
    "text": "(1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다.\n(2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다.\n(3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다.\n(4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다.\n(5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다."
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "",
    "text": "Autogluon (Fsize, Drop SibSp/Parch)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#a.-데이터",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#a.-데이터",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"./titanic/train.csv\")\ntst = TabularDataset(\"./titanic/test.csv\")\n\n- 피처엔지니어링\n\n_tr = tr.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)\n_tst = tst.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#b.-predictor-생성",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230913_063109/\""
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#c.-적합fit",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#c.-적합fit",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(_tr) # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230913_063109/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.16\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   675.47 GB / 982.82 GB (68.7%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    36873.71 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    10 features in original data used to generate 27 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.2s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8101   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8268   = Validation score   (accuracy)\n    0.18s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.56s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.7989   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8268   = Validation score   (accuracy)\n    1.14s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8212   = Validation score   (accuracy)\n    0.11s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.838    = Validation score   (accuracy)\n    1.62s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8268   = Validation score   (accuracy)\n    0.32s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 5.93s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230913_063109/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fc481da6880&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.036541  2.535042                0.000440           0.277847            2       True         14\n1        NeuralNetTorch   0.837989       0.007182  1.621377                0.007182           1.621377            1       True         12\n2         LightGBMLarge   0.826816       0.002590  0.316955                0.002590           0.316955            1       True         13\n3              LightGBM   0.826816       0.002826  0.182086                0.002826           0.182086            1       True          4\n4              CatBoost   0.826816       0.003359  0.564035                0.003359           0.564035            1       True          7\n5       NeuralNetFastAI   0.826816       0.007859  1.137854                0.007859           1.137854            1       True         10\n6               XGBoost   0.821229       0.003699  0.113863                0.003699           0.113863            1       True         11\n7      RandomForestEntr   0.821229       0.021228  0.249363                0.021228           0.249363            1       True          6\n8      RandomForestGini   0.815642       0.021727  0.271968                0.021727           0.271968            1       True          5\n9            LightGBMXT   0.810056       0.003175  0.201514                0.003175           0.201514            1       True          3\n10       ExtraTreesGini   0.804469       0.021523  0.275308                0.021523           0.275308            1       True          8\n11       ExtraTreesEntr   0.798883       0.022917  0.252218                0.022917           0.252218            1       True          9\n12       KNeighborsDist   0.653631       0.003518  0.007811                0.003518           0.007811            1       True          2\n13       KNeighborsUnif   0.653631       0.026963  0.007720                0.026963           0.007720            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.036541\n2.535042\n0.000440\n0.277847\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.007182\n1.621377\n0.007182\n1.621377\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.826816\n0.002590\n0.316955\n0.002590\n0.316955\n1\nTrue\n13\n\n\n3\nLightGBM\n0.826816\n0.002826\n0.182086\n0.002826\n0.182086\n1\nTrue\n4\n\n\n4\nCatBoost\n0.826816\n0.003359\n0.564035\n0.003359\n0.564035\n1\nTrue\n7\n\n\n5\nNeuralNetFastAI\n0.826816\n0.007859\n1.137854\n0.007859\n1.137854\n1\nTrue\n10\n\n\n6\nXGBoost\n0.821229\n0.003699\n0.113863\n0.003699\n0.113863\n1\nTrue\n11\n\n\n7\nRandomForestEntr\n0.821229\n0.021228\n0.249363\n0.021228\n0.249363\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.815642\n0.021727\n0.271968\n0.021727\n0.271968\n1\nTrue\n5\n\n\n9\nLightGBMXT\n0.810056\n0.003175\n0.201514\n0.003175\n0.201514\n1\nTrue\n3\n\n\n10\nExtraTreesGini\n0.804469\n0.021523\n0.275308\n0.021523\n0.275308\n1\nTrue\n8\n\n\n11\nExtraTreesEntr\n0.798883\n0.022917\n0.252218\n0.022917\n0.252218\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.003518\n0.007811\n0.003518\n0.007811\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.026963\n0.007720\n0.026963\n0.007720\n1\nTrue\n1\n\n\n\n\n\n\n\n- validation set의 의미:"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#d.-예측-predict",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(_tr)).mean()\n\n0.9438832772166106\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(_tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"./titanic_sub/autogluon(Fsize,Drop)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#submission-result",
    "href": "posts/5_STBDA2023/02wk-007-타이타닉, Autogluon (Fsize,Drop).out.html#submission-result",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "Submission Result",
    "text": "Submission Result\n\n\n\nSub and Description\nPublic Score\n\n\n\n\nautogulon_sub\n0.75358\n\n\nautogulon(Fsize)_sub\n0.77272\n\n\nautogulon(Fsize,Drop)\n0.78947\n\n\n\n- drop 했을 때 성능이 올라가는 이유?\n\nver2 : autogluon // 그냥\nver3 : autogluon // Fsize X,Y –&gt; X+Y+l = Z\nver4 : autogluon // Fsize, Drop"
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html",
    "href": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "",
    "text": "- 왜 여자만 살까?\n\nGTP 답변\ngender_submission.csv파일은 단순히 모든 여성 승객이 생존한다고 가정한 예측을 담은 파일로, 실제 데이터에 기반한 예측이 아닙니다. 이 파일은 제출 파일의 형식과 구성을 보여주기 위한 예시입니다. 실제로는 성별이나 다른 특성에 따라 생존 여부가 결정되는 것이 아닌, 여러 다양한 변수들이 생존에 영향을 미치게 됩니다.\n실제로 타이타닉호에서는 여성들이 생존율이 높았습니다. 타이타닉호 침몰 사건 당시 여성과 아이들을 먼저 구조하고 구명보트에 태우는 등의 우선순위가 있었기 때문입니다. 따라서 “gender_submission.csv” 파일은 예시로서 제공되었고, 실제 예측 모델을 개발할 때에는 여러 다른 변수들도 고려하여 예측해야 합니다.\n\n\n\n\n- 항구는 승객의 배경과 사회적 상황을 반영할 수 있다.\n\nGPT 답변\n네, 항구별로 승객들의 배경과 사회적 상황이 다를 수 있습니다. 다양한 항구에서 탑승한 승객들은 지역적, 경제적, 문화적인 차이를 반영할 수 있습니다. 몇 가지 예시를 들어보겠습니다:\n\n경제적 차이: 서로 다른 항구에서 탑승한 승객들은 다른 경제적 배경을 가지고 있을 수 있습니다. 상류층 승객이나 부유한 승객들이 탑승한 항구일수록 1등급 티켓을 선택한 경우가 많을 것입니다.\n문화적 차이: 다른 항구에서 탑승한 승객들은 다른 문화적 배경을 가지고 있을 수 있습니다. 이로 인해 승객들의 습관, 언어, 관습 등이 다를 수 있습니다.\n지역적 차이: 항구의 위치와 지역적 특성에 따라 승객들의 배경이 달라질 수 있습니다. 해당 지역의 주요 산업이나 경제 활동에 따라 승객들의 직업이나 사회적 지위가 다를 수 있습니다.\n가족 관계: 특정 항구에서 가족들이 함께 탑승한 경우, 해당 지역의 가족 구조와 관련된 차이가 반영될 수 있습니다.\n\n이러한 다양한 차이를 분석하여 항구별로 생존율이나 다른 변수들과의 상관 관계를 파악할 수 있습니다. 이는 모델링 과정에서 추가적인 특성 엔지니어링이나 분석에 활용될 수 있습니다.\n\n- 항구는 국적에 대한 정보를 내포할 수도 있다.\n\nGPT 답변\n타이타닉호의 데이터에서는 승객들의 국적에 대한 직접적인 정보는 제공되지 않습니다. 따라서 승객들의 국적에 대한 정보를 확인하는 것은 어렵습니다. 그러나 항구별로 승객들의 출발지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수는 있습니다.\n예를 들어, Cherbourg 항구가 프랑스에 위치하며, Queenstown 항구가 아일랜드에 위치해 있습니다. 따라서 해당 항구에서 탑승한 승객들 중 일부는 해당 국가의 국적을 가질 가능성이 높을 것입니다. 그러나 국적은 항상 탑승 항구와 일치하지 않을 수 있으며, 탑승한 승객들의 다양한 출신지와 배경을 고려해야 합니다.\n요약하면, 타이타닉호 데이터에서는 직접적인 국적 정보는 제공되지 않지만, 항구별로 승객들의 출신지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수 있습니다.\n\n\n\n\n- 두 변수 sibsp와 parch를 합쳐 family_size라는 하나의 변수를 만들고, 이를 활용하여 y를 예측할 수 있다.\n\n데이터를 봐야 하겠지만 가족단위로 보트를 줬을 확률도 큼\n가족단위로 보트를 실제로 주었을 경우 family_size라는 변수는 매우 의미있는 해석이 됨\n\n\nGPT 답변\n가족 규모 특성 생성: ‘sibsp’와 ’parch’ 변수를 활용하여 승객의 가족 규모를 나타내는 새로운 변수를 만들 수 있습니다. 예를 들어, ‘sibsp’와 ’parch’ 값을 더한 후 1을 더해 ‘family_size’ 변수로 만들 수 있습니다. 이를 통해 가족 규모가 큰 승객이 생존 여부에 영향을 미칠 수 있는지 확인할 수 있습니다.\n\n\n\n\n- 경우에 따라서 두가지 범주를 하나도 합쳐서 만든 새로운 변수가 매우 의미있을 수 있음\n\n경우에 따라서 특정 국적 (프랑스)의 1등급 손님이 구명보트를 많이 얻었을 수도 있음.\n이럴경우 embarked와 class를 독립적으로 생각하는것보다 둘을 합친 변수가 훨씬 유의미한 변수임\n\n\nGPT 답변\n물론, 자세한 설명을 드리겠습니다. “항구와 클래스의 조합 특성 생성”이라는 개념은 ‘embarked’ (탑승 항구)와 ‘pclass’ (티켓 클래스) 두 변수를 활용하여 새로운 특성을 만들어 분석하는 것을 의미합니다.\n예를 들어, “embarked” 변수에는 ‘C’, ‘Q’, ‘S’와 같은 항구 정보가 있으며, “pclass” 변수에는 ’1’, ‘2’, ’3’과 같은 티켓 클래스 정보가 있습니다. 이 두 변수를 조합하여 새로운 변수를 만들어보겠습니다.\n생성할 새로운 변수의 예시:\n“embarked_pclass”: ‘C_1’, ‘C_2’, ‘C_3’, ‘Q_1’, ‘Q_2’, ‘Q_3’, ‘S_1’, ‘S_2’, ‘S_3’ 위와 같이 “embarked”와 “pclass”를 조합하여 새로운 변수 값을 만듭니다. 예를 들어 ’embarked’가 ’C’이고 ’pclass’가 ’1’인 경우, “embarked_pclass” 변수 값은 ’C_1’이 됩니다. 이렇게 조합된 변수는 해당 승객이 어떤 항구에서 탑승했고 어떤 티켓 클래스를 사용했는지를 동시에 나타내는 특성입니다.\n이렇게 생성된 “embarked_pclass” 변수를 사용하여 특정 항구와 클래스의 조합이 생존 여부에 어떤 영향을 미치는지를 분석할 수 있습니다. 예를 들어, ‘C_1’ 조합이 생존율과 관련이 있는지, ‘Q_3’ 조합이 생존 여부에 영향을 미치는지 등을 확인하여 해당 특성이 모델링에 유용한 정보를 제공하는지 알아볼 수 있습니다.\n\n\nGPT 답변 2 – 그냥 “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것과 무슨차이냐는 질문에 대한 답변\n네, “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것도 가능합니다. 그러나 새로운 변수를 만드는 것은 데이터를 조금 더 유용하게 활용하는 방법 중 하나입니다. 여기에 몇 가지 이유가 있습니다:\n상호작용 효과 강조: 새로운 변수를 만들면 “embarked”와 “pclass”의 조합이 생존 여부에 미치는 영향을 강조할 수 있습니다. 단순히 각각의 변수를 개별적으로 사용하는 것보다 조합된 변수를 통해 서로 간의 상호작용 효과를 쉽게 확인할 수 있습니다.\n더 풍부한 패턴 탐색: 새로운 변수를 만들면 서로 다른 항구와 클래스 조합들 간의 생존률 차이를 더 세부적으로 분석할 수 있습니다. 이를 통해 특정 항구와 클래스의 승객들이 생존 여부에 미치는 영향을 더 자세히 이해할 수 있습니다.\n다양한 조합 고려: 조합된 변수를 만들면 “embarked”와 “pclass”의 가능한 모든 조합을 커버할 수 있습니다. 이는 분석의 전체적인 시각을 확장하고, 어떤 조합이 중요한지를 더 효과적으로 파악할 수 있도록 도와줍니다.\n모델 학습에 더 적합한 데이터 생성: 몇몇 머신 러닝 알고리즘은 상호작용 변수를 활용하여 모델의 성능을 향상시킬 수 있습니다. 이러한 알고리즘들은 새로운 변수를 활용하여 더 복잡한 패턴을 학습할 수 있습니다.\n새로운 변수를 만들어 분석하는 것은 분석의 창의성과 도메인 지식을 활용하여 데이터로부터 더 많은 의미를 도출하려는 노력의 일부입니다. 분석의 목적에 따라 각 변수를 개별적으로 사용하거나 조합하여 새로운 변수를 만들 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#a.-gender_submission.csv",
    "href": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#a.-gender_submission.csv",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "",
    "text": "- 왜 여자만 살까?\n\nGTP 답변\ngender_submission.csv파일은 단순히 모든 여성 승객이 생존한다고 가정한 예측을 담은 파일로, 실제 데이터에 기반한 예측이 아닙니다. 이 파일은 제출 파일의 형식과 구성을 보여주기 위한 예시입니다. 실제로는 성별이나 다른 특성에 따라 생존 여부가 결정되는 것이 아닌, 여러 다양한 변수들이 생존에 영향을 미치게 됩니다.\n실제로 타이타닉호에서는 여성들이 생존율이 높았습니다. 타이타닉호 침몰 사건 당시 여성과 아이들을 먼저 구조하고 구명보트에 태우는 등의 우선순위가 있었기 때문입니다. 따라서 “gender_submission.csv” 파일은 예시로서 제공되었고, 실제 예측 모델을 개발할 때에는 여러 다른 변수들도 고려하여 예측해야 합니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#b.-embarked",
    "href": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#b.-embarked",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "",
    "text": "- 항구는 승객의 배경과 사회적 상황을 반영할 수 있다.\n\nGPT 답변\n네, 항구별로 승객들의 배경과 사회적 상황이 다를 수 있습니다. 다양한 항구에서 탑승한 승객들은 지역적, 경제적, 문화적인 차이를 반영할 수 있습니다. 몇 가지 예시를 들어보겠습니다:\n\n경제적 차이: 서로 다른 항구에서 탑승한 승객들은 다른 경제적 배경을 가지고 있을 수 있습니다. 상류층 승객이나 부유한 승객들이 탑승한 항구일수록 1등급 티켓을 선택한 경우가 많을 것입니다.\n문화적 차이: 다른 항구에서 탑승한 승객들은 다른 문화적 배경을 가지고 있을 수 있습니다. 이로 인해 승객들의 습관, 언어, 관습 등이 다를 수 있습니다.\n지역적 차이: 항구의 위치와 지역적 특성에 따라 승객들의 배경이 달라질 수 있습니다. 해당 지역의 주요 산업이나 경제 활동에 따라 승객들의 직업이나 사회적 지위가 다를 수 있습니다.\n가족 관계: 특정 항구에서 가족들이 함께 탑승한 경우, 해당 지역의 가족 구조와 관련된 차이가 반영될 수 있습니다.\n\n이러한 다양한 차이를 분석하여 항구별로 생존율이나 다른 변수들과의 상관 관계를 파악할 수 있습니다. 이는 모델링 과정에서 추가적인 특성 엔지니어링이나 분석에 활용될 수 있습니다.\n\n- 항구는 국적에 대한 정보를 내포할 수도 있다.\n\nGPT 답변\n타이타닉호의 데이터에서는 승객들의 국적에 대한 직접적인 정보는 제공되지 않습니다. 따라서 승객들의 국적에 대한 정보를 확인하는 것은 어렵습니다. 그러나 항구별로 승객들의 출발지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수는 있습니다.\n예를 들어, Cherbourg 항구가 프랑스에 위치하며, Queenstown 항구가 아일랜드에 위치해 있습니다. 따라서 해당 항구에서 탑승한 승객들 중 일부는 해당 국가의 국적을 가질 가능성이 높을 것입니다. 그러나 국적은 항상 탑승 항구와 일치하지 않을 수 있으며, 탑승한 승객들의 다양한 출신지와 배경을 고려해야 합니다.\n요약하면, 타이타닉호 데이터에서는 직접적인 국적 정보는 제공되지 않지만, 항구별로 승객들의 출신지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "href": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "",
    "text": "- 두 변수 sibsp와 parch를 합쳐 family_size라는 하나의 변수를 만들고, 이를 활용하여 y를 예측할 수 있다.\n\n데이터를 봐야 하겠지만 가족단위로 보트를 줬을 확률도 큼\n가족단위로 보트를 실제로 주었을 경우 family_size라는 변수는 매우 의미있는 해석이 됨\n\n\nGPT 답변\n가족 규모 특성 생성: ‘sibsp’와 ’parch’ 변수를 활용하여 승객의 가족 규모를 나타내는 새로운 변수를 만들 수 있습니다. 예를 들어, ‘sibsp’와 ’parch’ 값을 더한 후 1을 더해 ‘family_size’ 변수로 만들 수 있습니다. 이를 통해 가족 규모가 큰 승객이 생존 여부에 영향을 미칠 수 있는지 확인할 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "href": "posts/5_STBDA2023/01wk-002-타이타닉, 데이터의 이해.out.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "",
    "text": "- 경우에 따라서 두가지 범주를 하나도 합쳐서 만든 새로운 변수가 매우 의미있을 수 있음\n\n경우에 따라서 특정 국적 (프랑스)의 1등급 손님이 구명보트를 많이 얻었을 수도 있음.\n이럴경우 embarked와 class를 독립적으로 생각하는것보다 둘을 합친 변수가 훨씬 유의미한 변수임\n\n\nGPT 답변\n물론, 자세한 설명을 드리겠습니다. “항구와 클래스의 조합 특성 생성”이라는 개념은 ‘embarked’ (탑승 항구)와 ‘pclass’ (티켓 클래스) 두 변수를 활용하여 새로운 특성을 만들어 분석하는 것을 의미합니다.\n예를 들어, “embarked” 변수에는 ‘C’, ‘Q’, ‘S’와 같은 항구 정보가 있으며, “pclass” 변수에는 ’1’, ‘2’, ’3’과 같은 티켓 클래스 정보가 있습니다. 이 두 변수를 조합하여 새로운 변수를 만들어보겠습니다.\n생성할 새로운 변수의 예시:\n“embarked_pclass”: ‘C_1’, ‘C_2’, ‘C_3’, ‘Q_1’, ‘Q_2’, ‘Q_3’, ‘S_1’, ‘S_2’, ‘S_3’ 위와 같이 “embarked”와 “pclass”를 조합하여 새로운 변수 값을 만듭니다. 예를 들어 ’embarked’가 ’C’이고 ’pclass’가 ’1’인 경우, “embarked_pclass” 변수 값은 ’C_1’이 됩니다. 이렇게 조합된 변수는 해당 승객이 어떤 항구에서 탑승했고 어떤 티켓 클래스를 사용했는지를 동시에 나타내는 특성입니다.\n이렇게 생성된 “embarked_pclass” 변수를 사용하여 특정 항구와 클래스의 조합이 생존 여부에 어떤 영향을 미치는지를 분석할 수 있습니다. 예를 들어, ‘C_1’ 조합이 생존율과 관련이 있는지, ‘Q_3’ 조합이 생존 여부에 영향을 미치는지 등을 확인하여 해당 특성이 모델링에 유용한 정보를 제공하는지 알아볼 수 있습니다.\n\n\nGPT 답변 2 – 그냥 “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것과 무슨차이냐는 질문에 대한 답변\n네, “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것도 가능합니다. 그러나 새로운 변수를 만드는 것은 데이터를 조금 더 유용하게 활용하는 방법 중 하나입니다. 여기에 몇 가지 이유가 있습니다:\n상호작용 효과 강조: 새로운 변수를 만들면 “embarked”와 “pclass”의 조합이 생존 여부에 미치는 영향을 강조할 수 있습니다. 단순히 각각의 변수를 개별적으로 사용하는 것보다 조합된 변수를 통해 서로 간의 상호작용 효과를 쉽게 확인할 수 있습니다.\n더 풍부한 패턴 탐색: 새로운 변수를 만들면 서로 다른 항구와 클래스 조합들 간의 생존률 차이를 더 세부적으로 분석할 수 있습니다. 이를 통해 특정 항구와 클래스의 승객들이 생존 여부에 미치는 영향을 더 자세히 이해할 수 있습니다.\n다양한 조합 고려: 조합된 변수를 만들면 “embarked”와 “pclass”의 가능한 모든 조합을 커버할 수 있습니다. 이는 분석의 전체적인 시각을 확장하고, 어떤 조합이 중요한지를 더 효과적으로 파악할 수 있도록 도와줍니다.\n모델 학습에 더 적합한 데이터 생성: 몇몇 머신 러닝 알고리즘은 상호작용 변수를 활용하여 모델의 성능을 향상시킬 수 있습니다. 이러한 알고리즘들은 새로운 변수를 활용하여 더 복잡한 패턴을 학습할 수 있습니다.\n새로운 변수를 만들어 분석하는 것은 분석의 창의성과 도메인 지식을 활용하여 데이터로부터 더 많은 의미를 도출하려는 노력의 일부입니다. 분석의 목적에 따라 각 변수를 개별적으로 사용하거나 조합하여 새로운 변수를 만들 수 있습니다."
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html",
    "href": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html",
    "title": "타이타닉 튜토리얼",
    "section": "",
    "text": "I decided to write this kernel because Titanic: Machine Learning from Disaster is one of my favorite competitions on Kaggle. This is a beginner level kernel which focuses on Exploratory Data Analysis and Feature Engineering. A lot of people start Kaggle with this competition and they get lost in extremely long tutorial kernels. This is a short kernel compared to the other ones. I hope this will be a good guide for starters and inspire them with new feature engineering ideas.\nTitanic: Machine Learning from Disaster is a great competition to apply domain knowledge for feature engineering, so I made a research and learned a lot about Titanic. There are many secrets to be revealed beneath the Titanic dataset. I tried to find out some of those secret factors that had affected the survival of passengers when the Titanic was sinking. I believe there are other features still waiting to be discovered.\nThis kernel has 3 main sections; Exploratory Data Analysis, Feature Engineering and Model, and it can achieve top 2% (0.83732) public leaderboard score with a tuned Random Forest Classifier. It takes 60 seconds to run whole notebook. If you have any idea that might improve this kernel, please be sure to comment, or fork and experiment as you like. If you didn’t understand any part, feel free to ask.\n::: {#cell-2 .cell _cell_guid=‘79c7e3d0-c299-4dcb-8224-4455121ee9b0’ _uuid=‘d629ff2d2480ee46fbb7e2d37f6b5fab8052498a’ execution_count=1}\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42\n:::\n\nTraining set has 891 rows and test set has 418 rows\nTraining set have 12 features and test set have 11 features\nOne extra feature in training set is Survived feature, which is the target variable\n\n::: {#cell-4 .cell _uuid=‘467443fda7135a8ce89c4d537da3f3a8546e2384’ execution_count=2}\ndef concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('titanic/train.csv')\ndf_test = pd.read_csv('titanic/test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)\n\nNumber of Training Examples = 891\nNumber of Test Examples = 418\n\nTraining X Shape = (891, 12)\nTraining y Shape = 891\n\nTest X Shape = (418, 11)\nTest y Shape = 418\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\nIndex(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n:::"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#introduction",
    "href": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#introduction",
    "title": "타이타닉 튜토리얼",
    "section": "",
    "text": "I decided to write this kernel because Titanic: Machine Learning from Disaster is one of my favorite competitions on Kaggle. This is a beginner level kernel which focuses on Exploratory Data Analysis and Feature Engineering. A lot of people start Kaggle with this competition and they get lost in extremely long tutorial kernels. This is a short kernel compared to the other ones. I hope this will be a good guide for starters and inspire them with new feature engineering ideas.\nTitanic: Machine Learning from Disaster is a great competition to apply domain knowledge for feature engineering, so I made a research and learned a lot about Titanic. There are many secrets to be revealed beneath the Titanic dataset. I tried to find out some of those secret factors that had affected the survival of passengers when the Titanic was sinking. I believe there are other features still waiting to be discovered.\nThis kernel has 3 main sections; Exploratory Data Analysis, Feature Engineering and Model, and it can achieve top 2% (0.83732) public leaderboard score with a tuned Random Forest Classifier. It takes 60 seconds to run whole notebook. If you have any idea that might improve this kernel, please be sure to comment, or fork and experiment as you like. If you didn’t understand any part, feel free to ask.\n::: {#cell-2 .cell _cell_guid=‘79c7e3d0-c299-4dcb-8224-4455121ee9b0’ _uuid=‘d629ff2d2480ee46fbb7e2d37f6b5fab8052498a’ execution_count=1}\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42\n:::\n\nTraining set has 891 rows and test set has 418 rows\nTraining set have 12 features and test set have 11 features\nOne extra feature in training set is Survived feature, which is the target variable\n\n::: {#cell-4 .cell _uuid=‘467443fda7135a8ce89c4d537da3f3a8546e2384’ execution_count=2}\ndef concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('titanic/train.csv')\ndf_test = pd.read_csv('titanic/test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)\n\nNumber of Training Examples = 891\nNumber of Test Examples = 418\n\nTraining X Shape = (891, 12)\nTraining y Shape = 891\n\nTest X Shape = (418, 11)\nTest y Shape = 418\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\nIndex(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\n:::"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#exploratory-data-analysis",
    "href": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#exploratory-data-analysis",
    "title": "타이타닉 튜토리얼",
    "section": "1. Exploratory Data Analysis",
    "text": "1. Exploratory Data Analysis\n\n1.1 Overview\n\nPassengerId is the unique id of the row and it doesn’t have any effect on target\nSurvived is the target variable we are trying to predict (0 or 1):\n\n1 = Survived\n0 = Not Survived\n\nPclass (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has 3 unique values (1, 2 or 3):\n\n1 = Upper Class\n2 = Middle Class\n3 = Lower Class\n\nName, Sex and Age are self-explanatory\nSibSp is the total number of the passengers’ siblings and spouse\nParch is the total number of the passengers’ parents and children\nTicket is the ticket number of the passenger\nFare is the passenger fare\nCabin is the cabin number of the passenger\nEmbarked is port of embarkation and it is a categorical feature which has 3 unique values (C, Q or S):\n\nC = Cherbourg\nQ = Queenstown\nS = Southampton\n\n\n::: {#cell-7 .cell _uuid=‘f02f321f8fd8b8c7c2a4aedb36ebe868ae51004e’ execution_count=3}\nprint(df_train.info())\ndf_train.sample(3)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\nNone\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n633\n634\n0\n1\nParr, Mr. William Henry Marsh\nmale\nNaN\n0\n0\n112052\n0.00\nNaN\nS\n\n\n801\n802\n1\n2\nCollyer, Mrs. Harvey (Charlotte Annie Tate)\nfemale\n31.0\n1\n1\nC.A. 31921\n26.25\nNaN\nS\n\n\n200\n201\n0\n3\nVande Walle, Mr. Nestor Cyriel\nmale\n28.0\n0\n0\n345770\n9.50\nNaN\nS\n\n\n\n\n\n\n:::\n::: {#cell-8 .cell _uuid=‘851ccf74127831d31ea0d7273b686f9a7cf20eee’ execution_count=4}\nprint(df_test.info())\ndf_test.sample(3)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 418 entries, 0 to 417\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  418 non-null    int64  \n 1   Pclass       418 non-null    int64  \n 2   Name         418 non-null    object \n 3   Sex          418 non-null    object \n 4   Age          332 non-null    float64\n 5   SibSp        418 non-null    int64  \n 6   Parch        418 non-null    int64  \n 7   Ticket       418 non-null    object \n 8   Fare         417 non-null    float64\n 9   Cabin        91 non-null     object \n 10  Embarked     418 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 36.0+ KB\nNone\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n35\n927\n3\nKatavelas, Mr. Vassilios (Catavelas Vassilios\")\"\nmale\n18.5\n0\n0\n2682\n7.2292\nNaN\nC\n\n\n29\n921\n3\nSamaan, Mr. Elias\nmale\nNaN\n2\n0\n2662\n21.6792\nNaN\nC\n\n\n135\n1027\n3\nCarlsson, Mr. Carl Robert\nmale\n24.0\n0\n0\n350409\n7.8542\nNaN\nS\n\n\n\n\n\n\n:::\n\n\n1.2 Missing Values\nAs seen from below, some columns have missing values. display_missing function shows the count of missing values in every column in both training and test set. * Training set have missing values in Age, Cabin and Embarked columns * Test set have missing values in Age, Cabin and Fare columns\nIt is convenient to work on concatenated training and test set while dealing with missing values, otherwise filled data may overfit to training or test set samples. The count of missing values in Age, Embarked and Fare are smaller compared to total sample, but roughly 80% of the Cabin is missing. Missing values in Age, Embarked and Fare can be filled with descriptive statistical measures but that wouldn’t work for Cabin.\n::: {#cell-10 .cell _kg_hide-input=‘true’ _uuid=‘d4e8f7b72e2bd165cafa71d67c95f008e7c6101d’ execution_count=5}\ndef display_missing(df):    \n    for col in df.columns.tolist():          \n        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n    print('\\n')\n    \nfor df in dfs:\n    print('{}'.format(df.name))\n    display_missing(df)\n\nTraining Set\nPassengerId column missing values: 0\nSurvived column missing values: 0\nPclass column missing values: 0\nName column missing values: 0\nSex column missing values: 0\nAge column missing values: 177\nSibSp column missing values: 0\nParch column missing values: 0\nTicket column missing values: 0\nFare column missing values: 0\nCabin column missing values: 687\nEmbarked column missing values: 2\n\n\nTest Set\nPassengerId column missing values: 0\nPclass column missing values: 0\nName column missing values: 0\nSex column missing values: 0\nAge column missing values: 86\nSibSp column missing values: 0\nParch column missing values: 0\nTicket column missing values: 0\nFare column missing values: 1\nCabin column missing values: 327\nEmbarked column missing values: 0\n\n\n\n:::\n\n1.2.1 Age\nMissing values in Age are filled with median age, but using median age of the whole data set is not a good choice. Median age of Pclass groups is the best choice because of its high correlation with Age (0.408106) and Survived (0.338481). It is also more logical to group ages by passenger classes instead of other features.\n::: {#cell-12 .cell _kg_hide-input=‘true’ execution_count=6}\ndf_all_corr = df_all.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'Age']\n\n\n\n\n\n\n\n\nFeature 1\nFeature 2\nCorrelation Coefficient\n\n\n\n\n0\nAge\nAge\n1.000000\n\n\n9\nAge\nPclass\n0.408106\n\n\n18\nAge\nSibSp\n0.243699\n\n\n21\nAge\nFare\n0.178740\n\n\n26\nAge\nParch\n0.150917\n\n\n30\nAge\nSurvived\n0.077221\n\n\n41\nAge\nPassengerId\n0.028814\n\n\n\n\n\n\n:::\nIn order to be more accurate, Sex feature is used as the second level of groupby while filling the missing Age values. As seen from below, Pclass and Sex groups have distinct median Age values. When passenger class increases, the median age for both males and females also increases. However, females tend to have slightly lower median Age than males. The median ages below are used for filling the missing values in Age feature.\n\nage_by_pclass_sex = df_all.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('Median age of all passengers: {}'.format(df_all['Age'].median()))\n\n# Filling the missing values in Age with the medians of Sex and Pclass groups\ndf_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n\nMedian age of Pclass 1 females: 36.0\nMedian age of Pclass 1 males: 42.0\nMedian age of Pclass 2 females: 28.0\nMedian age of Pclass 2 males: 29.5\nMedian age of Pclass 3 females: 22.0\nMedian age of Pclass 3 males: 25.0\nMedian age of all passengers: 28.0\n\n\n\n\n1.2.2 Embarked\nEmbarked is a categorical feature and there are only 2 missing values in whole data set. Both of those passengers are female, upper class and they have the same ticket number. This means that they know each other and embarked from the same port together. The mode Embarked value for an upper class female passenger is C (Cherbourg), but this doesn’t necessarily mean that they embarked from that port.\n::: {#cell-16 .cell _kg_hide-input=‘true’ execution_count=8}\ndf_all[df_all['Embarked'].isnull()]\n\n\n\n\n\n\n\n\nAge\nCabin\nEmbarked\nFare\nName\nParch\nPassengerId\nPclass\nSex\nSibSp\nSurvived\nTicket\n\n\n\n\n61\n38.0\nB28\nNaN\n80.0\nIcard, Miss. Amelie\n0\n62\n1\nfemale\n0\n1.0\n113572\n\n\n829\n62.0\nB28\nNaN\n80.0\nStone, Mrs. George Nelson (Martha Evelyn)\n0\n830\n1\nfemale\n0\n1.0\n113572\n\n\n\n\n\n\n:::\nWhen I googled Stone, Mrs. George Nelson (Martha Evelyn), I found that she embarked from S (Southampton) with her maid Amelie Icard, in this page Martha Evelyn Stone: Titanic Survivor.\n\nMrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28.\n\nMissing values in Embarked are filled with S with this information.\n\n# Filling the missing values in Embarked with S\ndf_all['Embarked'] = df_all['Embarked'].fillna('S')\n\n\n\n1.2.3 Fare\nThere is only one passenger with missing Fare value. We can assume that Fare is related to family size (Parch and SibSp) and Pclass features. Median Fare value of a male with a third class ticket and no family is a logical choice to fill the missing value.\n::: {#cell-20 .cell _kg_hide-input=‘true’ execution_count=10}\ndf_all[df_all['Fare'].isnull()]\n\n\n\n\n\n\n\n\nAge\nCabin\nEmbarked\nFare\nName\nParch\nPassengerId\nPclass\nSex\nSibSp\nSurvived\nTicket\n\n\n\n\n1043\n60.5\nNaN\nS\nNaN\nStorey, Mr. Thomas\n0\n1044\n3\nmale\n0\nNaN\n3701\n\n\n\n\n\n\n:::\n\nmed_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\ndf_all['Fare'] = df_all['Fare'].fillna(med_fare)\n\n\n\n1.2.4 Cabin\nCabin feature is little bit tricky and it needs further exploration. The large portion of the Cabin feature is missing and the feature itself can’t be ignored completely because some the cabins might have higher survival rates. It turns out to be the first letter of the Cabin values are the decks in which the cabins are located. Those decks were mainly separated for one passenger class, but some of them were used by multiple passenger classes.  * On the Boat Deck there were 6 rooms labeled as T, U, W, X, Y, Z but only the T cabin is present in the dataset * A, B and C decks were only for 1st class passengers * D and E decks were for all classes * F and G decks were for both 2nd and 3rd class passengers * From going A to G, distance to the staircase increases which might be a factor of survival\n::: {#cell-23 .cell _kg_hide-input=‘true’ execution_count=12}\n# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\ndf_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ndf_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n                                                                        'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n\ndef get_pclass_dist(df):\n    \n    # Creating a dictionary for every passenger class count in every deck\n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    decks = df.columns.levels[0]    \n    \n    for deck in decks:\n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count \n            except KeyError:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)    \n    deck_percentages = {}\n\n    # Creating a dictionary for every passenger class percentage in every deck\n    for col in df_decks.columns:\n        deck_percentages[col] = [(count / df_decks[col].sum()) * 100 for count in df_decks[col]]\n        \n    return deck_counts, deck_percentages\n\ndef display_pclass_dist(percentages):\n    \n    df_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, pclass1, color='#b5ffb9', edgecolor='white', width=bar_width, label='Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#f9bc86', edgecolor='white', width=bar_width, label='Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#a3acff', edgecolor='white', width=bar_width, label='Passenger Class 3')\n\n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Passenger Class Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Passenger Class Distribution in Decks', size=18, y=1.05)   \n    \n    plt.show()    \n\nall_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\ndisplay_pclass_dist(all_deck_per)\n\n\n\n:::\n\n100% of A, B and C decks are 1st class passengers\nDeck D has 87% 1st class and 13% 2nd class passengers\nDeck E has 83% 1st class, 10% 2nd class and 7% 3rd class passengers\nDeck F has 62% 2nd class and 38% 3rd class passengers\n100% of G deck are 3rd class passengers\nThere is one person on the boat deck in T cabin and he is a 1st class passenger. T cabin passenger has the closest resemblance to A deck passengers so he is grouped with A deck\nPassengers labeled as M are the missing values in Cabin feature. I don’t think it is possible to find those passengers’ real Deck so I decided to use M like a deck\n\n\n# Passenger in the T deck is changed to A\nidx = df_all[df_all['Deck'] == 'T'].index\ndf_all.loc[idx, 'Deck'] = 'A'\n\n::: {#cell-26 .cell _kg_hide-input=‘true’ execution_count=14}\ndf_all_decks_survived = df_all.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n                                                                                   'Embarked', 'Pclass', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name':'Count'}).transpose()\n\ndef get_survived_dist(df):\n    \n    # Creating a dictionary for every survival count in every deck\n    surv_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}}\n    decks = df.columns.levels[0]    \n\n    for deck in decks:\n        for survive in range(0, 2):\n            surv_counts[deck][survive] = df[deck][survive][0]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count / df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    return surv_counts, surv_percentages\n\ndef display_surv_dist(percentages):\n    \n    df_survived_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85    \n\n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, not_survived, color='#b5ffb9', edgecolor='white', width=bar_width, label=\"Not Survived\")\n    plt.bar(bar_count, survived, bottom=not_survived, color='#f9bc86', edgecolor='white', width=bar_width, label=\"Survived\")\n \n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Survival Percentage in Decks', size=18, y=1.05)\n    \n    plt.show()\n\nall_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\ndisplay_surv_dist(all_surv_per)\n\n\n\n:::\nAs I suspected, every deck has different survival rates and that information can’t be discarded. Deck B, C, D and E have the highest survival rates. Those decks are mostly occupied by 1st class passengers. M has the lowest survival rate which is mostly occupied by 2nd and 3rd class passengers. To conclude, cabins used by 1st class passengers have higher survival rates than cabins used by 2nd and 3rd class passengers. In my opinion M (Missing Cabin values) has the lowest survival rate because they couldn’t retrieve the cabin data of the victims. That’s why I believe labeling that group as M is a reasonable way to handle the missing data. It is a unique group with shared characteristics. Deck feature has high-cardinality right now so some of the values are grouped with each other based on their similarities. * A, B and C decks are labeled as ABC because all of them have only 1st class passengers * D and E decks are labeled as DE because both of them have similar passenger class distribution and same survival rate * F and G decks are labeled as FG because of the same reason above * M deck doesn’t need to be grouped with other decks because it is very different from others and has the lowest survival rate.\n\ndf_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\ndf_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')\n\ndf_all['Deck'].value_counts()\n\nM      1014\nABC     182\nDE       87\nFG       26\nName: Deck, dtype: int64\n\n\nAfter filling the missing values in Age, Embarked, Fare and Deck features, there is no missing value left in both training and test set. Cabin is dropped because Deck feature is used instead of it.\n::: {#cell-30 .cell _kg_hide-input=‘true’ execution_count=16}\n# Dropping the Cabin feature\ndf_all.drop(['Cabin'], inplace=True, axis=1)\n\ndf_train, df_test = divide_df(df_all)\ndfs = [df_train, df_test]\n\nfor df in dfs:\n    display_missing(df)\n\nAge column missing values: 0\nEmbarked column missing values: 0\nFare column missing values: 0\nName column missing values: 0\nParch column missing values: 0\nPassengerId column missing values: 0\nPclass column missing values: 0\nSex column missing values: 0\nSibSp column missing values: 0\nSurvived column missing values: 0\nTicket column missing values: 0\nDeck column missing values: 0\n\n\nAge column missing values: 0\nEmbarked column missing values: 0\nFare column missing values: 0\nName column missing values: 0\nParch column missing values: 0\nPassengerId column missing values: 0\nPclass column missing values: 0\nSex column missing values: 0\nSibSp column missing values: 0\nTicket column missing values: 0\nDeck column missing values: 0\n\n\n\n:::\n\n\n\n1.3 Target Distribution\n\n38.38% (342/891) of training set is Class 1\n61.62% (549/891) of training set is Class 0\n\n::: {#cell-32 .cell _kg_hide-input=‘true’ _uuid=‘c70aa13b7a552beb976574d52c1cd3da1cc1ee5c’ execution_count=17}\nsurvived = df_train['Survived'].value_counts()[1]\nnot_survived = df_train['Survived'].value_counts()[0]\nsurvived_per = survived / df_train.shape[0] * 100\nnot_survived_per = not_survived / df_train.shape[0] * 100\n\nprint('{} of {} passengers survived and it is the {:.2f}% of the training set.'.format(survived, df_train.shape[0], survived_per))\nprint('{} of {} passengers didnt survive and it is the {:.2f}% of the training set.'.format(not_survived, df_train.shape[0], not_survived_per))\n\nplt.figure(figsize=(10, 8))\nsns.countplot(df_train['Survived'])\n\nplt.xlabel('Survival', size=15, labelpad=15)\nplt.ylabel('Passenger Count', size=15, labelpad=15)\nplt.xticks((0, 1), ['Not Survived ({0:.2f}%)'.format(not_survived_per), 'Survived ({0:.2f}%)'.format(survived_per)])\nplt.tick_params(axis='x', labelsize=13)\nplt.tick_params(axis='y', labelsize=13)\n\nplt.title('Training Set Survival Distribution', size=15, y=1.05)\n\nplt.show()\n\n342 of 891 passengers survived and it is the 38.38% of the training set.\n549 of 891 passengers didnt survive and it is the 61.62% of the training set.\n\n\n\n\n:::\n\n\n1.4 Correlations\nFeatures are highly correlated with each other and dependent to each other. The highest correlation between features is 0.549500 in training set and 0.577147 in test set (between Fare and Pclass). The other features are also highly correlated. There are 9 correlations in training set and 6 correlations in test set that are higher than 0.1.\n::: {#cell-34 .cell _kg_hide-input=‘true’ execution_count=18}\ndf_train_corr = df_train.drop(['PassengerId'], axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\ndf_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n\ndf_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\ndf_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)\n:::\n\n# Training set high correlations\ncorr = df_train_corr_nd['Correlation Coefficient'] &gt; 0.1\ndf_train_corr_nd[corr]\n\n\n\n\n\n\n\n\nFeature 1\nFeature 2\nCorrelation Coefficient\n\n\n\n\n6\nFare\nPclass\n0.549500\n\n\n8\nAge\nPclass\n0.417667\n\n\n10\nParch\nSibSp\n0.414838\n\n\n12\nPclass\nSurvived\n0.338481\n\n\n14\nFare\nSurvived\n0.257307\n\n\n16\nAge\nSibSp\n0.249747\n\n\n18\nParch\nFare\n0.216225\n\n\n20\nParch\nAge\n0.176733\n\n\n22\nSibSp\nFare\n0.159651\n\n\n24\nAge\nFare\n0.124061\n\n\n\n\n\n\n\n\n# Test set high correlations\ncorr = df_test_corr_nd['Correlation Coefficient'] &gt; 0.1\ndf_test_corr_nd[corr]\n\n\n\n\n\n\n\n\nFeature 1\nFeature 2\nCorrelation Coefficient\n\n\n\n\n6\nPclass\nFare\n0.577489\n\n\n8\nAge\nPclass\n0.526789\n\n\n10\nAge\nFare\n0.345347\n\n\n12\nSibSp\nParch\n0.306895\n\n\n14\nParch\nFare\n0.230410\n\n\n16\nFare\nSibSp\n0.172032\n\n\n\n\n\n\n\n::: {#cell-37 .cell _kg_hide-input=‘true’ execution_count=21}\nfig, axs = plt.subplots(nrows=2, figsize=(20, 20))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n\nfor i in range(2):    \n    axs[i].tick_params(axis='x', labelsize=14)\n    axs[i].tick_params(axis='y', labelsize=14)\n    \naxs[0].set_title('Training Set Correlations', size=15)\naxs[1].set_title('Test Set Correlations', size=15)\n\nplt.show()\n\n\n\n:::\n\n\n1.5 Target Distribution in Features\n\n1.5.1 Continuous Features\nBoth of the continuous features (Age and Fare) have good split points and spikes for a decision tree to learn. One potential problem for both features is, the distribution has more spikes and bumps in training set, but it is smoother in test set. Model may not be able to generalize to test set because of this reason.\n\nDistribution of Age feature clearly shows that children younger than 15 has a higher survival rate than any of the other age groups\nIn distribution of Fare feature, the survival rate is higher on distribution tails. The distribution also has positive skew because of the extremely large outliers\n\n::: {#cell-40 .cell _kg_hide-input=‘true’ execution_count=22}\ncont_features = ['Age', 'Fare']\nsurv = df_train['Survived'] == 1\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\nplt.subplots_adjust(right=1.5)\n\nfor i, feature in enumerate(cont_features):    \n    # Distribution of survival in feature\n    sns.distplot(df_train[~surv][feature], label='Not Survived', hist=True, color='#e74c3c', ax=axs[0][i])\n    sns.distplot(df_train[surv][feature], label='Survived', hist=True, color='#2ecc71', ax=axs[0][i])\n    \n    # Distribution of feature in dataset\n    sns.distplot(df_train[feature], label='Training Set', hist=False, color='#e74c3c', ax=axs[1][i])\n    sns.distplot(df_test[feature], label='Test Set', hist=False, color='#2ecc71', ax=axs[1][i])\n    \n    axs[0][i].set_xlabel('')\n    axs[1][i].set_xlabel('')\n    \n    for j in range(2):        \n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n    \n    axs[0][i].legend(loc='upper right', prop={'size': 20})\n    axs[1][i].legend(loc='upper right', prop={'size': 20})\n    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n\naxs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\naxs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n        \nplt.show()\n\n\n\n:::\n\n\n1.5.2 Categorical Features\nEvery categorical feature has at least one class with high mortality rate. Those classes are very helpful to predict whether the passenger is a survivor or victim. Best categorical features are Pclass and Sex because they have the most homogenous distributions.\n\nPassengers boarded from Southampton has a lower survival rate unlike other ports. More than half of the passengers boarded from Cherbourg had survived. This observation could be related to Pclass feature\nParch and SibSp features show that passengers with only one family member has a higher survival rate\n\n::: {#cell-42 .cell _kg_hide-input=‘true’ execution_count=23}\ncat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=df_train)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n\nplt.show()\n\n\n\n:::\n\n\n\n1.6 Conclusion\nMost of the features are correlated with each other. This relationship can be used to create new features with feature transformation and feature interaction. Target encoding could be very useful as well because of the high correlations with Survived feature.\nSplit points and spikes are visible in continuous features. They can be captured easily with a decision tree model, but linear models may not be able to spot them.\nCategorical features have very distinct distributions with different survival rates. Those features can be one-hot encoded. Some of those features may be combined with each other to make new features.\nCreated a new feature called Deck and dropped Cabin feature at the Exploratory Data Analysis part.\n::: {#cell-44 .cell _kg_hide-input=‘true’ execution_count=24}\ndf_all = concat_df(df_train, df_test)\ndf_all.head()\n\n\n\n\n\n\n\n\nAge\nDeck\nEmbarked\nFare\nName\nParch\nPassengerId\nPclass\nSex\nSibSp\nSurvived\nTicket\n\n\n\n\n0\n22.0\nM\nS\n7.2500\nBraund, Mr. Owen Harris\n0\n1\n3\nmale\n1\n0.0\nA/5 21171\n\n\n1\n38.0\nABC\nC\n71.2833\nCumings, Mrs. John Bradley (Florence Briggs Th...\n0\n2\n1\nfemale\n1\n1.0\nPC 17599\n\n\n2\n26.0\nM\nS\n7.9250\nHeikkinen, Miss. Laina\n0\n3\n3\nfemale\n0\n1.0\nSTON/O2. 3101282\n\n\n3\n35.0\nABC\nS\n53.1000\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n0\n4\n1\nfemale\n1\n1.0\n113803\n\n\n4\n35.0\nM\nS\n8.0500\nAllen, Mr. William Henry\n0\n5\n3\nmale\n0\n0.0\n373450\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#feature-engineering",
    "href": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#feature-engineering",
    "title": "타이타닉 튜토리얼",
    "section": "2. Feature Engineering",
    "text": "2. Feature Engineering\n\n2.1 Binning Continuous Features\n\n2.1.1 Fare\nFare feature is positively skewed and survival rate is extremely high on the right end. 13 quantile based bins are used for Fare feature. Even though the bins are too much, they provide decent amount of information gain. The groups at the left side of the graph has the lowest survival rate and the groups at the right side of the graph has the highest survival rate. This high survival rate was not visible in the distribution graph. There is also an unusual group (15.742, 23.25] in the middle with high survival rate that is captured in this process.\n\ndf_all['Fare'] = pd.qcut(df_all['Fare'], 13)\n\n::: {#cell-49 .cell _kg_hide-input=‘true’ execution_count=26}\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=df_all)\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()\n\n\n\n:::\n\n\n2.1.2 Age\nAge feature has a normal distribution with some spikes and bumps and 10 quantile based bins are used for Age. The first bin has the highest survival rate and 4th bin has the lowest survival rate. Those were the biggest spikes in the distribution. There is also an unusual group (34.0, 40.0] with high survival rate that is captured in this process.\n\ndf_all['Age'] = pd.qcut(df_all['Age'], 10)\n\n::: {#cell-52 .cell _kg_hide-input=‘true’ execution_count=28}\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=df_all)\n\nplt.xlabel('Age', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()\n\n\n\n:::\n\n\n\n2.2 Frequency Encoding\nFamily_Size is created by adding SibSp, Parch and 1. SibSp is the count of siblings and spouse, and Parch is the count of parents and children. Those columns are added in order to find the total size of families. Adding 1 at the end, is the current passenger. Graphs have clearly shown that family size is a predictor of survival because different values have different survival rates. * Family Size with 1 are labeled as Alone * Family Size with 2, 3 and 4 are labeled as Small * Family Size with 5 and 6 are labeled as Medium * Family Size with 7, 8 and 11 are labeled as Large\n::: {#cell-54 .cell _kg_hide-input=‘true’ execution_count=29}\ndf_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0][0])\nsns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()\n\n\n\n:::\nThere are too many unique Ticket values to analyze, so grouping them up by their frequencies makes things easier.\nHow is this feature different than Family_Size? Many passengers travelled along with groups. Those groups consist of friends, nannies, maids and etc. They weren’t counted as family, but they used the same ticket.\nWhy not grouping tickets by their prefixes? If prefixes in Ticket feature has any meaning, then they are already captured in Pclass or Embarked features because that could be the only logical information which can be derived from the Ticket feature.\nAccording to the graph below, groups with 2,3 and 4 members had a higher survival rate. Passengers who travel alone has the lowest survival rate. After 4 group members, survival rate decreases drastically. This pattern is very similar to Family_Size feature but there are minor differences. Ticket_Frequency values are not grouped like Family_Size because that would basically create the same feature with perfect correlation. This kind of feature wouldn’t provide any additional information gain.\n\ndf_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')\n\n::: {#cell-57 .cell _kg_hide-input=‘true’ execution_count=31}\nfig, axs = plt.subplots(figsize=(12, 9))\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=df_all)\n\nplt.xlabel('Ticket Frequency', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=15, y=1.05)\n\nplt.show()\n\n\n\n:::\n\n\n2.3 Title & Is Married\nTitle is created by extracting the prefix before Name feature. According to graph below, there are many titles that are occuring very few times. Some of those titles doesn’t seem correct and they need to be replaced. Miss, Mrs, Ms, Mlle, Lady, Mme, the Countess, Dona titles are replaced with Miss/Mrs/Ms because all of them are female. Values like Mlle, Mme and Dona are actually the name of the passengers, but they are classified as titles because Name feature is split by comma. Dr, Col, Major, Jonkheer, Capt, Sir, Don and Rev titles are replaced with Dr/Military/Noble/Clergy because those passengers have similar characteristics. Master is a unique title. It is given to male passengers below age 26. They have the highest survival rate among all males.\nIs_Married is a binary feature based on the Mrs title. Mrs title has the highest survival rate among other female titles. This title needs to be a feature because all female titles are grouped with each other.\n\ndf_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1\n\n::: {#cell-60 .cell _kg_hide-input=‘true’ execution_count=33}\nfig, axs = plt.subplots(nrows=2, figsize=(20, 20))\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[0])\n\naxs[0].tick_params(axis='x', labelsize=10)\naxs[1].tick_params(axis='x', labelsize=15)\n\nfor i in range(2):    \n    axs[i].tick_params(axis='y', labelsize=15)\n\naxs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')\n\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n\nplt.show()\n\n\n\n:::\n\n\n2.4 Target Encoding\nextract_surname function is used for extracting surnames of passengers from the Name feature. Family feature is created with the extracted surname. This is necessary for grouping passengers in the same family.\n\ndef extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families\n\ndf_all['Family'] = extract_surname(df_all['Name'])\ndf_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]\n\nFamily_Survival_Rate is calculated from families in training set since there is no Survived feature in test set. A list of family names that are occuring in both training and test set (non_unique_families), is created. The survival rate is calculated for families with more than 1 members in that list, and stored in Family_Survival_Rate feature.\nAn extra binary feature Family_Survival_Rate_NA is created for families that are unique to the test set. This feature is also necessary because there is no way to calculate those families’ survival rate. This feature implies that family survival rate is not applicable to those passengers because there is no way to retrieve their survival rate.\nTicket_Survival_Rate and Ticket_Survival_Rate_NA features are also created with the same method. Ticket_Survival_Rate and Family_Survival_Rate are averaged and become Survival_Rate, and Ticket_Survival_Rate_NA and Family_Survival_Rate_NA are also averaged and become Survival_Rate_NA.\n\n# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] &gt; 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] &gt; 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]\n\n\nmean_survival_rate = np.mean(df_train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA\n\n\nfor df in [df_train, df_test]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) / 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) / 2    \n\n\n\n2.5 Feature Transformation\n\n2.5.1 Label Encoding Non-Numerical Features\nEmbarked, Sex, Deck , Title and Family_Size_Grouped are object type, and Age and Fare features are category type. They are converted to numerical type with LabelEncoder. LabelEncoder basically labels the classes from 0 to n. This process is necessary for models to learn from those features.\n\nnon_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n\nfor df in dfs:\n    for feature in non_numeric_features:        \n        df[feature] = LabelEncoder().fit_transform(df[feature])\n\n\n\n2.5.2 One-Hot Encoding the Categorical Features\nThe categorical features (Pclass, Sex, Deck, Embarked, Title) are converted to one-hot encoded features with OneHotEncoder. Age and Fare features are not converted because they are ordinal unlike the previous ones.\n\ncat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\nencoded_features = []\n\nfor df in dfs:\n    for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n\ndf_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[6:]], axis=1)\n\n\n\n\n2.6 Conclusion\nAge and Fare features are binned. Binning helped dealing with outliers and it revealed some homogeneous groups in those features. Family_Size is created by adding Parch and SibSp features and 1. Ticket_Frequency is created by counting the occurence of Ticket values.\nName feature is very useful. First, Title and Is_Married features are created from the title prefix in the names. Second, Family_Survival_Rate and Family_Survival_Rate_NA features are created by target encoding the surname of the passengers. Ticket_Survival_Rate is created by target encoding the Ticket feature. Survival_Rate feature is created by averaging the Family_Survival_Rate and Ticket_Survival_Rate features.\nFinally, the non-numeric type features are label encoded and categorical features are one-hot encoded. Created 5 new features (Family_Size, Title, Is_Married, Survival_Rate and Survival_Rate_NA) and dropped the useless features after encoding.\n::: {#cell-73 .cell _kg_hide-input=‘true’ execution_count=40}\ndf_all = concat_df(df_train, df_test)\ndrop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n\ndf_all.drop(columns=drop_cols, inplace=True)\n\ndf_all.head()\n\n\n\n\n\n\n\n\nAge\nDeck_1\nDeck_2\nDeck_3\nDeck_4\nEmbarked_1\nEmbarked_2\nEmbarked_3\nFamily_Size_Grouped_1\nFamily_Size_Grouped_2\n...\nPclass_3\nSex_1\nSex_2\nSurvival_Rate\nSurvival_Rate_NA\nTicket_Frequency\nTitle_1\nTitle_2\nTitle_3\nTitle_4\n\n\n\n\n0\n2\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n1.0\n0.0\n1.0\n0.383838\n0.0\n1\n0.0\n0.0\n0.0\n1.0\n\n\n1\n7\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n1.000000\n1.0\n2\n0.0\n0.0\n1.0\n0.0\n\n\n2\n4\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n1.0\n1.0\n0.0\n0.383838\n0.0\n1\n0.0\n0.0\n1.0\n0.0\n\n\n3\n7\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.383838\n0.0\n2\n0.0\n0.0\n1.0\n0.0\n\n\n4\n7\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n1.0\n0.0\n1.0\n0.383838\n0.0\n1\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n5 rows × 26 columns\n\n\n:::"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#model",
    "href": "posts/5_STBDA2023/02wk-00-itanic-advanced-feature-engineering-tutorial.html#model",
    "title": "타이타닉 튜토리얼",
    "section": "3. Model",
    "text": "3. Model\n\nX_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\ny_train = df_train['Survived'].values\nX_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\n\nX_train shape: (891, 26)\ny_train shape: (891,)\nX_test shape: (418, 26)\n\n\n\n3.1 Random Forest\nCreated 2 RandomForestClassifier’s. One of them is a single model and the other is for k-fold cross validation.\nThe highest accuracy of the single_best_model is 0.82775 in public leaderboard. However, it doesn’t perform better in k-fold cross validation. It is a good model to start experimenting and hyperparameter tuning.\nThe highest accuracy of leaderboard_model is 0.83732 in public leaderboard with 5-fold cross validation. This model is created for leaderboard score and it is tuned to overfit slightly. It is designed to overfit because the estimated probabilities of X_test in every fold are going to be divided by N (fold count). If this model is used as a single model, it would struggle to predict lots of samples correctly.\nWhich model should I use? * leaderboard_model overfits to test set so it’s not suggested to use models like this in real life projects. * single_best_model is a good model to start experimenting and learning about decision trees.\n\nsingle_best_model = RandomForestClassifier(criterion='gini', \n                                           n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1)\n\nleaderboard_model = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1) \n\nStratifiedKFold is used for stratifying the target variable. The folds are made by preserving the percentage of samples for each class in target variable (Survived).\n\nN = 5\noob = 0\nprobs = pd.DataFrame(np.zeros((len(X_test), N * 2)), columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N + 1) for j in range(2)])\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)], index=df_all.columns)\nfprs, tprs, scores = [], [], []\n\nskf = StratifiedKFold(n_splits=N, random_state=N, shuffle=True)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n    print('Fold {}\\n'.format(fold))\n    \n    # Fitting the model\n    leaderboard_model.fit(X_train[trn_idx], y_train[trn_idx])\n    \n    # Computing Train AUC score\n    trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_train[trn_idx], leaderboard_model.predict_proba(X_train[trn_idx])[:, 1])\n    trn_auc_score = auc(trn_fpr, trn_tpr)\n    # Computing Validation AUC score\n    val_fpr, val_tpr, val_thresholds = roc_curve(y_train[val_idx], leaderboard_model.predict_proba(X_train[val_idx])[:, 1])\n    val_auc_score = auc(val_fpr, val_tpr)  \n      \n    scores.append((trn_auc_score, val_auc_score))\n    fprs.append(val_fpr)\n    tprs.append(val_tpr)\n    \n    # X_test probabilities\n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = leaderboard_model.predict_proba(X_test)[:, 1]\n    importances.iloc[:, fold - 1] = leaderboard_model.feature_importances_\n        \n    oob += leaderboard_model.oob_score_ / N\n    print('Fold {} OOB Score: {}\\n'.format(fold, leaderboard_model.oob_score_))   \n    \nprint('Average OOB Score: {}'.format(oob))\n\nFold 1\n\nFold 1 OOB Score: 0.8553370786516854\n\nFold 2\n\nFold 2 OOB Score: 0.844319775596073\n\nFold 3\n\nFold 3 OOB Score: 0.8513323983169705\n\nFold 4\n\nFold 4 OOB Score: 0.8359046283309958\n\nFold 5\n\nFold 5 OOB Score: 0.8260869565217391\n\nAverage OOB Score: 0.8425961674834928\n\n\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    1.3s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    1.2s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    1.2s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    1.2s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    0.5s\n[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=-1)]: Done 1750 out of 1750 | elapsed:    1.2s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=16)]: Done 1750 out of 1750 | elapsed:    0.1s finished\n\n\n\n\n3.2 Feature Importance\n::: {#cell-81 .cell _kg_hide-input=‘true’ execution_count=44}\nimportances['Mean_Importance'] = importances.mean(axis=1)\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 20))\nsns.barplot(x='Mean_Importance', y=importances.index, data=importances)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Random Forest Classifier Mean Feature Importance Between Folds', size=15)\n\nplt.show()\n\n\n\n:::\n\n\n3.3 ROC Curve\n::: {#cell-83 .cell _kg_hide-input=‘true’ execution_count=45}\ndef plot_roc_curve(fprs, tprs):\n    \n    tprs_interp = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    f, ax = plt.subplots(figsize=(15, 15))\n    \n    # Plotting ROC for each fold and computing AUC scores\n    for i, (fpr, tpr) in enumerate(zip(fprs, tprs), 1):\n        tprs_interp.append(np.interp(mean_fpr, fpr, tpr))\n        tprs_interp[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        ax.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC Fold {} (AUC = {:.3f})'.format(i, roc_auc))\n        \n    # Plotting ROC for random guessing\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8, label='Random Guessing')\n    \n    mean_tpr = np.mean(tprs_interp, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    \n    # Plotting the mean ROC\n    ax.plot(mean_fpr, mean_tpr, color='b', label='Mean ROC (AUC = {:.3f} $\\pm$ {:.3f})'.format(mean_auc, std_auc), lw=2, alpha=0.8)\n    \n    # Plotting the standard deviation around the mean ROC Curve\n    std_tpr = np.std(tprs_interp, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='$\\pm$ 1 std. dev.')\n    \n    ax.set_xlabel('False Positive Rate', size=15, labelpad=20)\n    ax.set_ylabel('True Positive Rate', size=15, labelpad=20)\n    ax.tick_params(axis='x', labelsize=15)\n    ax.tick_params(axis='y', labelsize=15)\n    ax.set_xlim([-0.05, 1.05])\n    ax.set_ylim([-0.05, 1.05])\n\n    ax.set_title('ROC Curves of Folds', size=20, y=1.02)\n    ax.legend(loc='lower right', prop={'size': 13})\n    \n    plt.show()\n\nplot_roc_curve(fprs, tprs)\n\n\n\n:::\n\n\n3.4 Submission\n\nclass_survived = [col for col in probs.columns if col.endswith('Prob_1')]\nprobs['1'] = probs[class_survived].sum(axis=1) / N\nprobs['0'] = probs.drop(columns=class_survived).sum(axis=1) / N\nprobs['pred'] = 0\npos = probs[probs['1'] &gt;= 0.5].index\nprobs.loc[pos, 'pred'] = 1\n\ny_pred = probs['pred'].astype(int)\n\nsubmission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = df_test['PassengerId']\nsubmission_df['Survived'] = y_pred.values\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head(10)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n891\n892\n0\n\n\n892\n893\n1\n\n\n893\n894\n0\n\n\n894\n895\n0\n\n\n895\n896\n1\n\n\n896\n897\n0\n\n\n897\n898\n1\n\n\n898\n899\n0\n\n\n899\n900\n1\n\n\n900\n901\n0"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-x6USW3HM9Lm-B19o9qrm19&si=EFy8hdlgDJ-LUFHi"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#a.-데이터",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#a.-데이터",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"titanic/train.csv\")\ntst = TabularDataset(\"titanic/test.csv\")\n\n- 피처엔지니어링\n\n_tr = tr.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)\n_tst = tst.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#b.-predictor-생성",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230913_070925/\""
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#c.-적합fit",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#c.-적합fit",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(_tr,presets='best_quality') # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230913_070925/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.16\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   675.38 GB / 982.82 GB (68.7%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    35836.53 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.1s = Fit runtime\n    10 features in original data used to generate 27 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.15s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6308   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6364   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8418   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8496   = Validation score   (accuracy)\n    0.44s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8384   = Validation score   (accuracy)\n    0.36s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8238   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.853    = Validation score   (accuracy)\n    1.42s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8227   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.826    = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8519   = Validation score   (accuracy)\n    2.25s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.844    = Validation score   (accuracy)\n    0.68s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8418   = Validation score   (accuracy)\n    3.75s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.844    = Validation score   (accuracy)\n    1.0s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.853    = Validation score   (accuracy)\n    0.41s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 19.83s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230913_070925/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f0c4def4ee0&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.852974       0.031577  1.419052                0.031577           1.419052            1       True          7\n1       WeightedEnsemble_L2   0.852974       0.032645  1.832624                0.001068           0.413572            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.851852       0.063553  2.253746                0.063553           2.253746            1       True         10\n3           LightGBM_BAG_L1   0.849607       0.023809  0.435157                0.023809           0.435157            1       True          4\n4      LightGBMLarge_BAG_L1   0.843996       0.026434  1.002698                0.026434           1.002698            1       True         13\n5            XGBoost_BAG_L1   0.843996       0.033381  0.684330                0.033381           0.684330            1       True         11\n6         LightGBMXT_BAG_L1   0.841751       0.024597  0.384963                0.024597           0.384963            1       True          3\n7     NeuralNetTorch_BAG_L1   0.841751       0.077535  3.745817                0.077535           3.745817            1       True         12\n8   RandomForestGini_BAG_L1   0.838384       0.054624  0.364527                0.054624           0.364527            1       True          5\n9     ExtraTreesEntr_BAG_L1   0.826038       0.055092  0.248551                0.055092           0.248551            1       True          9\n10  RandomForestEntr_BAG_L1   0.823793       0.062297  0.314906                0.062297           0.314906            1       True          6\n11    ExtraTreesGini_BAG_L1   0.822671       0.056287  0.271098                0.056287           0.271098            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.004575  0.002916                0.004575           0.002916            1       True          2\n13    KNeighborsUnif_BAG_L1   0.630752       0.027986  0.002342                0.027986           0.002342            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.852974\n0.031577\n1.419052\n0.031577\n1.419052\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.852974\n0.032645\n1.832624\n0.001068\n0.413572\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.851852\n0.063553\n2.253746\n0.063553\n2.253746\n1\nTrue\n10\n\n\n3\nLightGBM_BAG_L1\n0.849607\n0.023809\n0.435157\n0.023809\n0.435157\n1\nTrue\n4\n\n\n4\nLightGBMLarge_BAG_L1\n0.843996\n0.026434\n1.002698\n0.026434\n1.002698\n1\nTrue\n13\n\n\n5\nXGBoost_BAG_L1\n0.843996\n0.033381\n0.684330\n0.033381\n0.684330\n1\nTrue\n11\n\n\n6\nLightGBMXT_BAG_L1\n0.841751\n0.024597\n0.384963\n0.024597\n0.384963\n1\nTrue\n3\n\n\n7\nNeuralNetTorch_BAG_L1\n0.841751\n0.077535\n3.745817\n0.077535\n3.745817\n1\nTrue\n12\n\n\n8\nRandomForestGini_BAG_L1\n0.838384\n0.054624\n0.364527\n0.054624\n0.364527\n1\nTrue\n5\n\n\n9\nExtraTreesEntr_BAG_L1\n0.826038\n0.055092\n0.248551\n0.055092\n0.248551\n1\nTrue\n9\n\n\n10\nRandomForestEntr_BAG_L1\n0.823793\n0.062297\n0.314906\n0.062297\n0.314906\n1\nTrue\n6\n\n\n11\nExtraTreesGini_BAG_L1\n0.822671\n0.056287\n0.271098\n0.056287\n0.271098\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.004575\n0.002916\n0.004575\n0.002916\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.630752\n0.027986\n0.002342\n0.027986\n0.002342\n1\nTrue\n1\n\n\n\n\n\n\n\n\nCatBoost_BAG_L1"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#d.-예측-predict",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(_tr)).mean()\n\n0.9113355780022446\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(_tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"./titanic_sub/autogluon(Fsize,Drop,best_quality)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#submission-result",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out-Copy1.html#submission-result",
    "title": "02wk-008: 타이타닉, Autogluon (Fsize,Drop,best_quality)",
    "section": "Submission Result",
    "text": "Submission Result\n\n\n\nSub and Description\nPublic Score\n\n\n\n\nautogulon_sub\n0.75358\n\n\nautogulon(Fsize)_sub\n0.77272\n\n\nautogulon(Fsize,Drop)\n0.78947\n\n\nautogulon(best_quality)\n0.80143"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-yR3MqhN9-OgAtewojoYoKD&si=U1GTMdGiWFvlppYH"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-1-get-started",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-1-get-started",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 1: Get started",
    "text": "Part 1: Get started\nIn this section, you’ll learn more about the competition and make your first submission.\n\nJoin the competition!\nThe first thing to do is to join the competition! Open a new window with the competition page, and click on the “Join Competition” button, if you haven’t already. (If you see a “Submit Predictions” button instead of a “Join Competition” button, you have already joined the competition, and don’t need to do so again.)\n\nThis takes you to the rules acceptance page. You must accept the competition rules in order to participate. These rules govern how many submissions you can make per day, the maximum team size, and other competition-specific details. Then, click on “I Understand and Accept” to indicate that you will abide by the competition rules.\n\n\nThe challenge\nThe competition is simple: we want you to use the Titanic passenger data (name, age, price of ticket, etc) to try to predict who will survive and who will die.\n\n\nThe data\nTo take a look at the competition data, click on the Data tab at the top of the competition page. Then, scroll down to find the list of files.\nThere are three files in the data: (1) train.csv, (2) test.csv, and (3) gender_submission.csv.\n\n(1) train.csv\ntrain.csv contains the details of a subset of the passengers on board (891 passengers, to be exact – where each passenger gets a different row in the table). To investigate this data, click on the name of the file on the left of the screen. Once you’ve done this, you can view all of the data in the window.\n\nThe values in the second column (“Survived”) can be used to determine whether each passenger survived or not: - if it’s a “1”, the passenger survived. - if it’s a “0”, the passenger died.\nFor instance, the first passenger listed in train.csv is Mr. Owen Harris Braund. He was 22 years old when he died on the Titanic.\n\n\n(2) test.csv\nUsing the patterns you find in train.csv, you have to predict whether the other 418 passengers on board (in test.csv) survived.\nClick on test.csv (on the left of the screen) to examine its contents. Note that test.csv does not have a “Survived” column - this information is hidden from you, and how well you do at predicting these hidden values will determine how highly you score in the competition!\n\n\n(3) gender_submission.csv\nThe gender_submission.csv file is provided as an example that shows how you should structure your predictions. It predicts that all female passengers survived, and all male passengers died. Your hypotheses regarding survival will probably be different, which will lead to a different submission file. But, just like this file, your submission should have: - a “PassengerId” column containing the IDs of each passenger from test.csv. - a “Survived” column (that you will create!) with a “1” for the rows where you think the passenger survived, and a “0” where you predict that the passenger died."
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-2-your-coding-environment",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-2-your-coding-environment",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 2: Your coding environment",
    "text": "Part 2: Your coding environment\nIn this section, you’ll train your own machine learning model to improve your predictions. If you’ve never written code before or don’t have any experience with machine learning, don’t worry! We don’t assume any prior experience in this tutorial.\n\nThe Notebook\nThe first thing to do is to create a Kaggle Notebook where you’ll store all of your code. You can use Kaggle Notebooks to getting up and running with writing code quickly, and without having to install anything on your computer. (If you are interested in deep learning, we also offer free GPU access!)\nBegin by clicking on the Code tab on the competition page. Then, click on “New Notebook”.\n\nYour notebook will take a few seconds to load. In the top left corner, you can see the name of your notebook – something like “kernel2daed3cd79”.\n\nYou can edit the name by clicking on it. Change it to something more descriptive, like “Getting Started with Titanic”.\n\n\n\nYour first lines of code\nWhen you start a new notebook, it has two gray boxes for storing code. We refer to these gray boxes as “code cells”.\n\nThe first code cell already has some code in it. To run this code, put your cursor in the code cell. (If your cursor is in the right place, you’ll notice a blue vertical line to the left of the gray box.) Then, either hit the play button (which appears to the left of the blue line), or hit [Shift] + [Enter] on your keyboard.\nIf the code runs successfully, three lines of output are returned. Below, you can see the same code that you just ran, along with the output that you should see in your notebook.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nThis shows us where the competition data is stored, so that we can load the files into the notebook. We’ll do that next.\n\n\nLoad the data\nThe second code cell in your notebook now appears below the three lines of output with the file locations.\n\nType the two lines of code below into your second code cell. Then, once you’re done, either click on the blue play button, or hit [Shift] + [Enter].\n\n# train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data = pd.read_csv('./titanic/train.csv')\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nYour code should return the output above, which corresponds to the first five rows of the table in train.csv. It’s very important that you see this output in your notebook before proceeding with the tutorial! &gt; If your code does not produce this output, double-check that your code is identical to the two lines above. And, make sure your cursor is in the code cell before hitting [Shift] + [Enter].\nThe code that you’ve just written is in the Python programming language. It uses a Python “module” called pandas (abbreviated as pd) to load the table from the train.csv file into the notebook. To do this, we needed to plug in the location of the file (which we saw was /kaggle/input/titanic/train.csv).\n&gt; If you’re not already familiar with Python (and pandas), the code shouldn’t make sense to you – but don’t worry! The point of this tutorial is to (quickly!) make your first submission to the competition. At the end of the tutorial, we suggest resources to continue your learning.\nAt this point, you should have at least three code cells in your notebook.\n\nCopy the code below into the third code cell of your notebook to load the contents of the test.csv file. Don’t forget to click on the play button (or hit [Shift] + [Enter])!\n\nimport pandas as pd\n\n\n# test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data = pd.read_csv('./titanic/test.csv')\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nAs before, make sure that you see the output above in your notebook before continuing.\nOnce all of the code runs successfully, all of the data (in train.csv and test.csv) is loaded in the notebook. (The code above shows only the first 5 rows of each table, but all of the data is there – all 891 rows of train.csv and all 418 rows of test.csv!)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-3-your-first-submission",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-3-your-first-submission",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 3: Your first submission",
    "text": "Part 3: Your first submission\nRemember our goal: we want to find patterns in train.csv that help us predict whether the passengers in test.csv survived.\nIt might initially feel overwhelming to look for patterns, when there’s so much data to sort through. So, we’ll start simple.\n\nExplore a pattern\nRemember that the sample submission file in gender_submission.csv assumes that all female passengers survived (and all male passengers died).\nIs this a reasonable first guess? We’ll check if this pattern holds true in the data (in train.csv).\nCopy the code below into a new code cell. Then, run the cell.\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n\n% of women who survived: 0.7420382165605095\n\n\n\n여성의 생존률을 구하는 코드입니다, 이전에 accuracy를 구하던 테크닉을 활용하면 아래의 코드도 가능합니다\n\ntrain_data[train_data.Sex == 'female'].Survived.mean()\n\n0.7420382165605095\n\n\n\nBefore moving on, make sure that your code returns the output above. The code above calculates the percentage of female passengers (in train.csv) who survived.\nThen, run the code below in another code cell:\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n\n% of men who survived: 0.18890814558058924\n\n\nThe code above calculates the percentage of male passengers (in train.csv) who survived.\nFrom this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in gender_submission.csv is not a bad first guess!\nBut at the end of the day, this gender-based submission bases its predictions on only a single column. As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions. Since it is quite difficult to consider several columns at once (or, it would take a long time to consider all possible patterns in many different columns simultaneously), we’ll use machine learning to automate this for us."
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#your-first-machine-learning-model",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#your-first-machine-learning-model",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Your first machine learning model",
    "text": "Your first machine learning model\nWe’ll build what’s known as a random forest model. This model is constructed of several “trees” (there are three trees in the picture below, but we’ll construct 100!) that will individually consider each passenger’s data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\n\nThe code cell below looks for patterns in four different columns (“Pclass”, “Sex”, “SibSp”, and “Parch”) of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file submission.csv.\nCopy this code into your notebook, and run it in a new code cell.\n\nimport pandas as pd\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('titanic_sub/submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\nMake sure that your notebook outputs the same message above (Your submission was successfully saved!) before moving on. &gt; Again, don’t worry if this code doesn’t make sense to you! For now, we’ll focus on how to generate and submit predictions.\nOnce you’re ready, click on the “Save Version” button in the top right corner of your notebook. This will generate a pop-up window.\n- Ensure that the “Save and Run All” option is selected, and then click on the “Save” button. - This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the “Save Version” button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (…) to the right of the most recent version, and select Open in Viewer.\n- Click on the Data tab on the top of the screen. Then, click on the “Submit” button to submit your results.\n\nCongratulations for making your first submission to a Kaggle competition! Within ten minutes, you should receive a message providing your spot on the leaderboard. Great work!"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-4-learn-more",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#part-4-learn-more",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 4: Learn more!",
    "text": "Part 4: Learn more!\nIf you’re interested in learning more, we strongly suggest our (3-hour) Intro to Machine Learning course, which will help you fully understand all of the code that we’ve presented here. You’ll also know enough to generate even better predictions!"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?",
    "text": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?\n- 원래코드\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\n\nlen(predictions), len(X_test)\n\n- 이렇게 수정하면 될 듯\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\nlen(predictions),len(y)\n\n\n(predictions == y).mean()"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#b.-alexis-cook의-코드를-수정해보자",
    "href": "posts/5_STBDA2023/02wk-004-타이타닉, Alexis Cook의 코드.out.html#b.-alexis-cook의-코드를-수정해보자",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "B. Alexis Cook의 코드를 수정해보자!",
    "text": "B. Alexis Cook의 코드를 수정해보자!\n- 코드를 수정해보자.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=5000, max_depth=1000, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\n(predictions == y).mean()\n\n0.8170594837261503\n\n\n\n내가 만든게 더 좋은데??\n\n- 이것도 제출결과로 만들어보자.\n\npredictions = model.predict(X_test)\n\n- 아래와 같이 제출하면 에러가 발생\n\npd.read_csv(\"./titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정_submission.csv\")\n\n- 아래와 같이 제출파일을 저장해야 한다.\n\npd.read_csv(\"titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정2_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "",
    "text": "https://youtu.be/playlist?list=PLQqh36zP38-x6USW3HM9Lm-B19o9qrm19&si=EFy8hdlgDJ-LUFHi"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#a.-데이터",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#a.-데이터",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"titanic/train.csv\")\ntst = TabularDataset(\"titanic/test.csv\")"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#b.-predictor-생성",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#b.-predictor-생성",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230913_064329/\""
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#c.-적합fit",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#c.-적합fit",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr,presets='best_quality') # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230913_064329/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.16\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   675.44 GB / 982.82 GB (68.7%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    36523.72 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.18s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6308   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6364   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.835    = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    0.43s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8373   = Validation score   (accuracy)\n    0.35s    = Training   runtime\n    0.06s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8361   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8608   = Validation score   (accuracy)\n    1.59s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8294   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8328   = Validation score   (accuracy)\n    0.26s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.853    = Validation score   (accuracy)\n    2.15s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    0.57s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8462   = Validation score   (accuracy)\n    3.68s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    1.06s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8608   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 19.56s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230913_064329/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fb1783c2160&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.860831       0.034114  1.593431                0.034114           1.593431            1       True          7\n1       WeightedEnsemble_L2   0.860831       0.035236  2.013822                0.001122           0.420391            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.852974       0.067540  2.145609                0.067540           2.145609            1       True         10\n3     NeuralNetTorch_BAG_L1   0.846240       0.073878  3.684342                0.073878           3.684342            1       True         12\n4           LightGBM_BAG_L1   0.840629       0.024179  0.434251                0.024179           0.434251            1       True          4\n5      LightGBMLarge_BAG_L1   0.840629       0.027767  1.059269                0.027767           1.059269            1       True         13\n6            XGBoost_BAG_L1   0.840629       0.035949  0.567940                0.035949           0.567940            1       True         11\n7   RandomForestGini_BAG_L1   0.837262       0.056912  0.350744                0.056912           0.350744            1       True          5\n8   RandomForestEntr_BAG_L1   0.836139       0.054147  0.253396                0.054147           0.253396            1       True          6\n9         LightGBMXT_BAG_L1   0.835017       0.026695  0.332248                0.026695           0.332248            1       True          3\n10    ExtraTreesEntr_BAG_L1   0.832772       0.075741  0.255411                0.075741           0.255411            1       True          9\n11    ExtraTreesGini_BAG_L1   0.829405       0.054623  0.255257                0.054623           0.255257            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.005361  0.003128                0.005361           0.003128            1       True          2\n13    KNeighborsUnif_BAG_L1   0.630752       0.033042  0.002380                0.033042           0.002380            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.860831\n0.034114\n1.593431\n0.034114\n1.593431\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.860831\n0.035236\n2.013822\n0.001122\n0.420391\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.852974\n0.067540\n2.145609\n0.067540\n2.145609\n1\nTrue\n10\n\n\n3\nNeuralNetTorch_BAG_L1\n0.846240\n0.073878\n3.684342\n0.073878\n3.684342\n1\nTrue\n12\n\n\n4\nLightGBM_BAG_L1\n0.840629\n0.024179\n0.434251\n0.024179\n0.434251\n1\nTrue\n4\n\n\n5\nLightGBMLarge_BAG_L1\n0.840629\n0.027767\n1.059269\n0.027767\n1.059269\n1\nTrue\n13\n\n\n6\nXGBoost_BAG_L1\n0.840629\n0.035949\n0.567940\n0.035949\n0.567940\n1\nTrue\n11\n\n\n7\nRandomForestGini_BAG_L1\n0.837262\n0.056912\n0.350744\n0.056912\n0.350744\n1\nTrue\n5\n\n\n8\nRandomForestEntr_BAG_L1\n0.836139\n0.054147\n0.253396\n0.054147\n0.253396\n1\nTrue\n6\n\n\n9\nLightGBMXT_BAG_L1\n0.835017\n0.026695\n0.332248\n0.026695\n0.332248\n1\nTrue\n3\n\n\n10\nExtraTreesEntr_BAG_L1\n0.832772\n0.075741\n0.255411\n0.075741\n0.255411\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.829405\n0.054623\n0.255257\n0.054623\n0.255257\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.005361\n0.003128\n0.005361\n0.003128\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.630752\n0.033042\n0.002380\n0.033042\n0.002380\n1\nTrue\n1\n\n\n\n\n\n\n\n\nCatBoost_BAG_L1"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#d.-예측-predict",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#d.-예측-predict",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.898989898989899\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"./titanic_sub/autogluon(best_quality)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#submission-result",
    "href": "posts/5_STBDA2023/02wk-008-타이타닉, Autogluon (best_quality).out.html#submission-result",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "Submission Result",
    "text": "Submission Result\n\n\n\nSub and Description\nPublic Score\n\n\n\n\nautogulon_sub\n0.75358\n\n\nautogulon(Fsize)_sub\n0.77272\n\n\nautogulon(Fsize,Drop)\n0.78947\n\n\nautogulon(best_quality)\n0.80143"
  },
  {
    "objectID": "posts/6_note/2023-09-09_smart_farm_eda.html",
    "href": "posts/6_note/2023-09-09_smart_farm_eda.html",
    "title": "연습장1",
    "section": "",
    "text": "- 하우스 내부\nCO2와 일사량 사이의 관계 곡선을 통해 온실 내부 환경을 제어한다. 즉, 맞추어져 있는 환경이 아니라 현재의 환경을 가지고 최적화 시키는 방식으로 되어져야 한다.\n센서로부터 취득한 데이터는 그 숫자의 의미가 매우 가변적이므로 온도, 습도, 일사 더 나아가 CO2 등 온실에서 계측되는 데이터들은 단일계로서의 의미는 매우 적게 활용되어 진다.\n즉, 온도와 습도가 짝을 이루고, pH와 EC가 짝을 이룬다.\n또한 양액을 공급하기 위한 토양의 습도와 온도 일사도 짝을 이루게 된다.\n평창과 같이 겨울에 기온이 상당히 내려가는 지역에 있어서는 풍향과 풍속도 짝을 이루어서 관리를 해야한다.\n만약 단편적으로 온도를 낮추고 온도를 높이기 위해서 천창, 팬, 난방기 이런 것들을 가동하게 된다면 즉, 온도라는 조건만 가지고 가동하게 된다면 습도에 문제가 생기고, 대지가 마르는 현상이 나타날 수 있다. (복합적으로 판단하여 복합적인 요소로 관리하는 것이 매우 중요하다.)\n천창을 열건지, 팬을 돌릴것인지, 온풍기를 가동할 것인지, 양액을 어떻게 조성할 것인지 등은 지극히 사람들의 경험에 의해서 이루어지고 있다. (AI가 대체할 수 있을까?)\n- 노지\n온도 습도 반비례, 온도 일사량 비례하는 관계가 나타난다.\n겨울철에 나타나는 결로현상1\n결로가 발생할 수 있는 어떤 지점, 장치, 시간대들을 계산하여서 최대한 결로현상을 줄여나가는 것도 관건.\n- 언제 작물에 물을 줄까?\n스파트팜을 이용해서 토경재배를 할 때 언제 물을 주었는지도 중요한 포인트이지만 얼마만큼의 속도로 수분량이 줄어들고 있는지가 더 중요한 요소 중 하나이다. (진흙에 가까운 토양일수록 물빠짐이 나쁘기 때문에 곡선이 완만할 것.)\n가지고 있는 토양의 조건이 어떠냐에 따라서 여기에는 어떤 작물을 선택할 것이며, 또 어떤 패턴의 관리 기법이 들어가야 될 것인지도 동일하게 검토되어야 할 부분이다.\n즉, 물을 주는 것도 중요하지만 물빠짐 현상을 해석하는 것도 중요한 요소 중 하나이다.\n- 적산온도\n예시) 수박의 적산온도로 수박을 언제쯤 수확하면 되겠다~ // 수확을 하기 위해서 최적의 적산온도는 1000도씨. (수박의 경우)\n적산온도로 정확한 출하시기/수확시기를 예측할 수 있는 모델도 고려할 수 있음.\n- 1일 온도와 습도의 변화폭의 관계\n천천히 열던지, 조금열고 많이 기다리고\n바깥쪽의 찬 공기가 작물에 영향을 주었다는 것을 온도변화 폭을 보고 알 수 있다.\n- 온도와 습도 그리고 CO2 변화의 관계\nCO2 공급기로 약 1800에서 2000ppm정도의 co2를 공급했음에도(낮에) 불구하고 작물이 정상적으로 광학성을 하고 있다고 게시할 만한 차트의 변화가 없다. 즉, co2는 공급이 되고 있는데 식물들의 반응이 없었다는 말.\n광합성을 위한 co2가 적정이상의 좋은 조건으로 공급되고 있음에도 불구하고 식물들에게 반응이 없다는 것은 어떤 다른 조건이 만족하고 있지 않다는 것을 말한다.\n예를들어, 식물이 생존하기 위해 습도가 너무 낮거나 공권부에서 흡수되는 수분의 양이 적게 되면 자신이 갖고 있는 수분량을 빼앗기지 않기 위해 기공을 닫고있게 된다. 즉, 호흡을 하지 않는다. 따라서 이 상태에서 아무리 CO2 조건을 좋게 하더라도 식물에게는 좋은 영향을 미칠 수 없다.\n즉, 습도와 CO2가 잘 맞아야지 유의미한 결과가 나온다.\n- 제안 (예시)\n시간에 있어서는 일몰과 일출을 기준으로 접근하는 것이 필요.\n표준화된 구간 안에서 온실 내 환경을 잘 맞출 수 있는 솔루션 개발 필요.\n추가적으로, 우리나라는 여름철/겨울철의 일몰과 일출시간이 다르다. 즉, 몇시에서부터 몇시까지의 개념이 반드시 일몰과 일출의 개념으로 시작 및 종료 시간이 정해져야 한다. (오늘의 6시와 6개월 후의 6시는 일조량에 있어서 전혀 다른 의미에서의 6시)"
  },
  {
    "objectID": "posts/6_note/2023-09-09_smart_farm_eda.html#import",
    "href": "posts/6_note/2023-09-09_smart_farm_eda.html#import",
    "title": "연습장1",
    "section": "Import",
    "text": "Import\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n# 데이터 불러오기\ninputs = pd.read_csv('./farm/train_input.csv')\noutputs = pd.read_csv('./farm/train_output.csv')\n\n\ninputs.head()\n\n\n\n\n\n\n\n\nSample_no\n시설ID\n일\n주차\n내부CO2\n내부습도\n내부온도\n지온\n강우감지\n일사량\n외부온도\n외부풍향\n외부풍속\n지습\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n품종\n재배형태\n\n\n\n\n0\n0\nfarm25\n20220323\n30주차\n517.041667\n84.985417\n20.610833\n0.0\nNaN\n1879\n11.166667\n195.0\n0.083333\n0.0\n14\n2.68\n4.42\n88\ntomato09\nNaN\n\n\n1\n0\nfarm25\n20220324\n30주차\n514.416667\n88.291250\n20.695000\n0.0\nNaN\n1411\n12.708333\n142.5\n0.000000\n0.0\n14\n2.78\n5.63\n97\ntomato09\nNaN\n\n\n2\n0\nfarm25\n20220326\n30주차\n471.875000\n83.514583\n20.402500\n0.0\nNaN\n1955\n8.791667\n202.5\n0.000000\n0.0\n14\n2.69\n4.25\n101\ntomato09\nNaN\n\n\n3\n0\nfarm25\n20220327\n30주차\n469.250000\n80.916250\n20.139167\n0.0\nNaN\n2231\n8.041667\n180.0\n0.000000\n0.0\n14\n2.70\n4.25\n99\ntomato09\nNaN\n\n\n4\n0\nfarm25\n20220328\n30주차\n465.750000\n82.026250\n17.653333\n0.0\nNaN\n2284\n9.000000\n97.5\n0.041667\n0.0\n13\n2.66\n4.21\n94\ntomato09\nNaN\n\n\n\n\n\n\n\n\noutputs.head()\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n0\n20220330\n30주차\n208.0\n6.9\n16.67\n\n\n1\n1\n20220330\n30주차\n172.0\n6.8\n17.33\n\n\n2\n2\n20220330\n30주차\n150.0\n9.3\n16.00\n\n\n3\n3\n20220330\n30주차\n121.0\n5.9\n16.20\n\n\n4\n4\n20220406\n31주차\n175.0\n5.8\n17.40\n\n\n\n\n\n\n\n\n생장길이 = 지난주 생장점에서 금주 생장점 길이\n개화군 = 꽃이 핀 개수를 점수로 측정\n\n\n# pd.DataFrame(inputs['시설ID'].value_counts()).sort_index() # farm01~farm36\n\n\ninputs.isna().sum()\n\nSample_no        0\n시설ID             0\n일                0\n주차               0\n내부CO2            0\n내부습도             0\n내부온도             0\n지온            1749\n강우감지          1505\n일사량              0\n외부온도           201\n외부풍향          6993\n외부풍속           670\n지습            5873\n급액횟수             0\n급액EC(dS/m)       0\n급액pH             0\n급액량(회당)          0\n품종            7114\n재배형태          2408\ndtype: int64\n\n\n\n# nan 제거  -- 베이스라인이므로 간단한 처리를 위해 nan 항목 보간 없이 학습\ninputs = inputs.dropna(axis=1)\n\n\n# 주차 정보 수치 변환\ninputs['주차'] = [int(i.replace('주차', \"\")) for i in inputs['주차']]\n\n\n# scaler\ninput_scaler = MinMaxScaler()\noutput_scaler = MinMaxScaler()\n\n\ninputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n주차\n내부CO2\n내부습도\n내부온도\n일사량\n급액횟수\n급액EC(dS/m)\n급액pH\n급액량(회당)\n\n\n\n\n0\n30\n517.041667\n84.985417\n20.610833\n1879\n14\n2.68\n4.42\n88\n\n\n1\n30\n514.416667\n88.291250\n20.695000\n1411\n14\n2.78\n5.63\n97\n\n\n2\n30\n471.875000\n83.514583\n20.402500\n1955\n14\n2.69\n4.25\n101\n\n\n3\n30\n469.250000\n80.916250\n20.139167\n2231\n14\n2.70\n4.25\n99\n\n\n4\n30\n465.750000\n82.026250\n17.653333\n2284\n13\n2.66\n4.21\n94\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10107\n7\n334.684002\n65.565417\n21.985833\n979\n26\n2.06\n5.80\n81\n\n\n10108\n7\n333.726601\n61.144167\n22.530833\n2515\n28\n2.43\n4.42\n32\n\n\n10109\n7\n344.862883\n72.867917\n20.397917\n1972\n21\n2.71\n5.88\n27\n\n\n10110\n7\n372.708516\n66.672917\n24.401667\n1314\n18\n2.50\n5.39\n82\n\n\n10111\n7\n372.612192\n59.257083\n28.352500\n1310\n16\n2.50\n5.39\n82\n\n\n\n\n10112 rows × 9 columns\n\n\n\n\noutputs.iloc[:,3:]\n\n\n\n\n\n\n\n\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n208.0\n6.90\n16.67\n\n\n1\n172.0\n6.80\n17.33\n\n\n2\n150.0\n9.30\n16.00\n\n\n3\n121.0\n5.90\n16.20\n\n\n4\n175.0\n5.80\n17.40\n\n\n...\n...\n...\n...\n\n\n1513\n150.0\n6.95\n2.20\n\n\n1514\n140.0\n10.13\n1.40\n\n\n1515\n200.0\n9.61\n1.40\n\n\n1516\n210.0\n8.47\n2.20\n\n\n1517\n150.0\n9.16\n3.20\n\n\n\n\n1518 rows × 3 columns\n\n\n\n\n# scaling\ninput_sc = input_scaler.fit_transform(inputs.iloc[:,3:].to_numpy())\noutput_sc = output_scaler.fit_transform(outputs.iloc[:,3:].to_numpy())\n\n\nlen(inputs['Sample_no'].unique()) \n\n1518\n\n\n\n# 입력 시계열화\ninput_ts = []\nfor i in outputs['Sample_no']:\n    sample = input_sc[inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    input_ts.append(sample)\ninput_ts = np.concatenate(input_ts, axis=0)\n\n\ninput_ts.shape\n\n(1518, 7, 9)\n\n\n\n# 셋 분리\ntrain_x, val_x, train_y, val_y = train_test_split(input_ts, output_sc, test_size=0.2,\n                                                  shuffle=True, random_state=0)\n\n\ntrain_x.shape, val_x.shape, train_y.shape, val_y.shape\n\n((1214, 7, 9), (304, 7, 9), (1214, 3), (304, 3))\n\n\n\n# 모델 정의\ndef create_model():\n    x = Input(shape=[7, 9])\n    l1 = LSTM(64)(x)\n    out = Dense(3, activation='tanh')(l1)\n    return Model(inputs=x, outputs=out)\n\nmodel = create_model()\nmodel.summary()\ncheckpointer = ModelCheckpoint(monitor='val_loss', filepath='baseline.h5',\n                               verbose=1, save_best_only=True, save_weights_only=True)\n\nmodel.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['mse'])\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 7, 9)]            0         \n                                                                 \n lstm (LSTM)                 (None, 64)                18944     \n                                                                 \n dense (Dense)               (None, 3)                 195       \n                                                                 \n=================================================================\nTotal params: 19,139\nTrainable params: 19,139\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n\n\n\n# 학습\nhist = model.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[checkpointer])\n\nEpoch 1/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0170 - mse: 0.0170 \nEpoch 1: val_loss improved from inf to 0.00995, saving model to baseline.h5\n38/38 [==============================] - 1s 9ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.0099 - val_mse: 0.0099\nEpoch 2/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0117 - mse: 0.0117\nEpoch 2: val_loss improved from 0.00995 to 0.00839, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0084 - val_mse: 0.0084\nEpoch 3/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0109 - mse: 0.0109\nEpoch 3: val_loss improved from 0.00839 to 0.00777, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 4/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0106 - mse: 0.0106\nEpoch 4: val_loss did not improve from 0.00777\n38/38 [==============================] - 0s 2ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0078 - val_mse: 0.0078\nEpoch 5/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0101 - mse: 0.0101\nEpoch 5: val_loss improved from 0.00777 to 0.00757, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0076 - val_mse: 0.0076\nEpoch 6/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0105 - mse: 0.0105\nEpoch 6: val_loss improved from 0.00757 to 0.00751, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0075 - val_mse: 0.0075\nEpoch 7/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0098 - mse: 0.0098\nEpoch 7: val_loss improved from 0.00751 to 0.00738, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0074 - val_mse: 0.0074\nEpoch 8/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 8: val_loss improved from 0.00738 to 0.00709, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0071 - val_mse: 0.0071\nEpoch 9/50\n31/38 [=======================&gt;......] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 9: val_loss improved from 0.00709 to 0.00702, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 10/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 10: val_loss did not improve from 0.00702\n38/38 [==============================] - 0s 2ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0072 - val_mse: 0.0072\nEpoch 11/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 11: val_loss improved from 0.00702 to 0.00696, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 12/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0095 - mse: 0.0095\nEpoch 12: val_loss improved from 0.00696 to 0.00682, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 13/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 13: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 14/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0092 - mse: 0.0092\nEpoch 14: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 15/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0094 - mse: 0.0094\nEpoch 15: val_loss did not improve from 0.00682\n38/38 [==============================] - 0s 2ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 16/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 16: val_loss improved from 0.00682 to 0.00674, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 17/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 17: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 18/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 18: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 19/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 19: val_loss did not improve from 0.00674\n38/38 [==============================] - 0s 2ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0069 - val_mse: 0.0069\nEpoch 20/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 20: val_loss improved from 0.00674 to 0.00670, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 21/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0093 - mse: 0.0093\nEpoch 21: val_loss improved from 0.00670 to 0.00659, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 22/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 22: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0070 - val_mse: 0.0070\nEpoch 23/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 23: val_loss did not improve from 0.00659\n38/38 [==============================] - 0s 2ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0067 - val_mse: 0.0067\nEpoch 24/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0090 - mse: 0.0090\nEpoch 24: val_loss improved from 0.00659 to 0.00650, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 25/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 25: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 26/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0086 - mse: 0.0086\nEpoch 26: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0068 - val_mse: 0.0068\nEpoch 27/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0091 - mse: 0.0091\nEpoch 27: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 28/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0089 - mse: 0.0089\nEpoch 28: val_loss did not improve from 0.00650\n38/38 [==============================] - 0s 2ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 29/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0088 - mse: 0.0088\nEpoch 29: val_loss improved from 0.00650 to 0.00637, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0064 - val_mse: 0.0064\nEpoch 30/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0084 - mse: 0.0084\nEpoch 30: val_loss improved from 0.00637 to 0.00632, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0063 - val_mse: 0.0063\nEpoch 31/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 31: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0066 - val_mse: 0.0066\nEpoch 32/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0077 - mse: 0.0077\nEpoch 32: val_loss did not improve from 0.00632\n38/38 [==============================] - 0s 2ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0065 - val_mse: 0.0065\nEpoch 33/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0085 - mse: 0.0085\nEpoch 33: val_loss improved from 0.00632 to 0.00623, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 34/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0082 - mse: 0.0082\nEpoch 34: val_loss improved from 0.00623 to 0.00614, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0061 - val_mse: 0.0061\nEpoch 35/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0080 - mse: 0.0080\nEpoch 35: val_loss improved from 0.00614 to 0.00604, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 36/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 36: val_loss did not improve from 0.00604\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0062 - val_mse: 0.0062\nEpoch 37/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - mse: 0.0074\nEpoch 37: val_loss improved from 0.00604 to 0.00601, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 38/50\n32/38 [========================&gt;.....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 38: val_loss improved from 0.00601 to 0.00586, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0059 - val_mse: 0.0059\nEpoch 39/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0078 - mse: 0.0078\nEpoch 39: val_loss did not improve from 0.00586\n38/38 [==============================] - 0s 2ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0060 - val_mse: 0.0060\nEpoch 40/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0075 - mse: 0.0075\nEpoch 40: val_loss improved from 0.00586 to 0.00551, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 41/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 41: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 42/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0068 - mse: 0.0068\nEpoch 42: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 43/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 43: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 44/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0071 - mse: 0.0071\nEpoch 44: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0057 - val_mse: 0.0057\nEpoch 45/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 45: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0056 - val_mse: 0.0056\nEpoch 46/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 46: val_loss did not improve from 0.00551\n38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 47/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0070 - mse: 0.0070\nEpoch 47: val_loss improved from 0.00551 to 0.00509, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 48/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 48: val_loss did not improve from 0.00509\n38/38 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0054 - val_mse: 0.0054\nEpoch 49/50\n33/38 [=========================&gt;....] - ETA: 0s - loss: 0.0062 - mse: 0.0062\nEpoch 49: val_loss improved from 0.00509 to 0.00507, saving model to baseline.h5\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 50/50\n34/38 [=========================&gt;....] - ETA: 0s - loss: 0.0067 - mse: 0.0067\nEpoch 50: val_loss did not improve from 0.00507\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0056 - val_mse: 0.0056\n\n\n\n# loss 히스토리 확인\nfig, loss_ax = plt.subplots()\nloss_ax.plot(hist.history['loss'], 'r', label='loss')\nloss_ax.plot(hist.history['val_loss'], 'g', label='val_loss')\nloss_ax.set_xlabel('epoch')\nloss_ax.set_ylabel('loss')\nloss_ax.legend()\nplt.title('Training loss - Validation loss plot')\nplt.show()\n\n\n\n\n\n# 저장된 가중치 불러오기\nmodel.load_weights('baseline.h5')\n\n\n# 테스트셋 전처리 및 추론\ntest_inputs = pd.read_csv('./farm/test_input.csv')\noutput_sample = pd.read_csv('./farm/answer_sample.csv')\n\ntest_inputs = test_inputs[inputs.columns]\ntest_inputs['주차'] = [int(i.replace('주차', \"\")) for i in test_inputs['주차']]\ntest_input_sc = input_scaler.transform(test_inputs.iloc[:,3:].to_numpy())\n\ntest_input_ts = []\nfor i in output_sample['Sample_no']:\n    sample = test_input_sc[test_inputs['Sample_no'] == i]\n    if len(sample &lt; 7):\n        sample = np.append(np.zeros((7-len(sample), sample.shape[-1])), sample,\n                           axis=0)\n    sample = np.expand_dims(sample, axis=0)\n    test_input_ts.append(sample)\ntest_input_ts = np.concatenate(test_input_ts, axis=0)\n\n\ntest_input_ts.shape\n\n(506, 7, 9)\n\n\n\nprediction = model.predict(test_input_ts)\n\nprediction = output_scaler.inverse_transform(prediction)\noutput_sample[['생장길이', '줄기직경', '개화군']] = prediction\n\n16/16 [==============================] - 0s 793us/step\n\n\n\noutput_sample\n\n\n\n\n\n\n\n\nSample_no\n조사일\n주차\n생장길이\n줄기직경\n개화군\n\n\n\n\n0\n9\n20220413\n32주차\n47.151882\n7.198693\n13.229403\n\n\n1\n12\n20170312\n30주차\n420.956116\n3.321363\n10.489825\n\n\n2\n19\n20170319\n31주차\n589.641235\n4.079537\n7.553223\n\n\n3\n23\n20170326\n32주차\n281.593994\n4.563877\n7.977988\n\n\n4\n27\n20170430\n37주차\n89.870880\n7.789731\n4.768530\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n501\n2015\n20160508\n14주차\n188.253265\n12.274371\n5.467629\n\n\n502\n2016\n20160529\n17주차\n1998.461670\n4.776089\n1.905745\n\n\n503\n2024\n20160828\n7주차\n49.577644\n13.211569\n2.025992\n\n\n504\n2025\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n505\n2026\n20160828\n7주차\n49.577755\n13.211572\n2.025991\n\n\n\n\n506 rows × 6 columns\n\n\n\n\n# 제출할 추론 결과 저장\noutput_sample.to_csv('prediction.csv', index=False)\n\n\n# 학습\ncb1 = tf.keras.callbacks.TensorBoard()\nmodel.fit(train_x, train_y, batch_size=32, epochs=50, validation_data=(val_x, val_y), callbacks=[cb1])\n\nEpoch 1/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 2/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 3/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 4/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0049 - val_mse: 0.0049\nEpoch 5/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0051 - val_mse: 0.0051\nEpoch 6/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 7/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 8/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 9/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 10/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 11/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0055 - val_mse: 0.0055\nEpoch 12/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0052 - val_mse: 0.0052\nEpoch 13/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 14/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 15/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0048 - val_mse: 0.0048\nEpoch 16/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 17/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0044 - val_mse: 0.0044\nEpoch 18/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 19/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 20/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0050 - val_mse: 0.0050\nEpoch 21/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 22/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 23/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 24/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0047 - val_mse: 0.0047\nEpoch 25/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 26/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 27/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0043 - val_mse: 0.0043\nEpoch 28/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 29/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0045 - val_mse: 0.0045\nEpoch 30/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 31/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 32/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0046 - val_mse: 0.0046\nEpoch 33/50\n38/38 [==============================] - 0s 3ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 34/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 35/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 36/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 37/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 38/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 39/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 40/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 41/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 42/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 43/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0042 - val_mse: 0.0042\nEpoch 44/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 45/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0041 - val_mse: 0.0041\nEpoch 46/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0037 - val_mse: 0.0037\nEpoch 47/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0040 - val_mse: 0.0040\nEpoch 48/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0039 - val_mse: 0.0039\nEpoch 49/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0038 - val_mse: 0.0038\nEpoch 50/50\n38/38 [==============================] - 0s 2ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0036 - val_mse: 0.0036\n\n\n&lt;keras.callbacks.History at 0x7f2df8e88b80&gt;\n\n\n\n%load_ext tensorboard\n%tensorboard --logdir logs --host \n\nThe tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n\n\nReusing TensorBoard on port 6006 (pid 109833), started 0:04:21 ago. (Use '!kill 109833' to kill it.)"
  },
  {
    "objectID": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html",
    "href": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html",
    "title": "트랜스포머 (전력사용량 데이터)",
    "section": "",
    "text": "Click to run on colab (if you’re not already there): \nThe goal of this notebook is to illustrate the use of a transformer for timeseries prediction. This notebook was built by Alice Martin and adapted to pytorch by Charles Ollion\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport time\nimport matplotlib.pyplot as plt\n\n\n\nEnergy consumption dataset from https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction * gathers 10-min measurements of household appliances energy consumption (20 first features), coupled with local meteorological data (8 last features). * The time-series forecasting task is to predict the first 20 features, given as input data the 28 features. A window of observations of 12 time steps is considered to predict the next series of observations (this corresponds to a 2-hours window of observations.\nyou may get the dataset (a single csv file) by running the following cell:\n\n!wget https://raw.githubusercontent.com/LuisM78/Appliances-energy-prediction-data/master/energydata_complete.csv\n\n\n# load and preprocess the energy dataset:\ndef convert_col_into_float(df, list_cols):\n    for col in list_cols:\n        df[col] = df[col].astype(str)\n        df[col] = df[col].str.replace(',', '.')\n        df[col] = df[col].astype(np.float32)\n    return df\ndf = pd.read_csv(\"energydata_complete.csv\", index_col='date', parse_dates=['date'])\nprint(df.head())\nlist_cols = list(df.columns)\n# gathers 10-min measurements of household appliances energy consumption (20 first features), coupled with local meteorological data. (8 last features)\nprint(\"dataset variables\", list_cols)\ndf = convert_col_into_float(df, list_cols)\ndata = df.values\n\n\ndef split_dataset_into_seq(dataset, start_index=0, end_index=None, history_size=13, step=1):\n    '''split the dataset to have sequence of observations of length history size'''\n    data = []\n    start_index = start_index + history_size\n    if end_index is None:\n        end_index = len(dataset)\n    for i in range(start_index, end_index):\n        indices = range(i - history_size, i, step)\n        data.append(dataset[indices])\n    return np.array(data)\n\n\ndef split_dataset(data, TRAIN_SPLIT=0.7, VAL_SPLIT=0.5, save_path=None):\n    '''split the dataset into train, val and test splits'''\n    # normalization\n    data_mean = data.mean(axis=0)\n    data_std = data.std(axis=0)\n    data = (data - data_mean) / data_std\n    stats = (data_mean, data_std)\n\n    data_in_seq = split_dataset_into_seq(data, start_index=0, end_index=None, history_size=13, step=1)\n\n    # split between validation dataset and test set:\n    train_data, val_data = train_test_split(data_in_seq, train_size=TRAIN_SPLIT, shuffle=True, random_state=123)\n    val_data, test_data = train_test_split(val_data, train_size=VAL_SPLIT, shuffle=True, random_state=123)\n\n    return train_data, val_data, test_data\n\n\ndef split_fn(chunk):\n    \"\"\"to split the dataset sequences into input and targets sequences\"\"\"\n    inputs = torch.tensor(chunk[:, :-1, :], device=device)\n    targets = torch.tensor(chunk[:, 1:, :], device=device)\n    return inputs, targets\n\n\ndef data_to_dataset(train_data, val_data, test_data, batch_size=32, target_features=list(range(20))):\n    '''\n    split each train split into inputs and targets\n    convert each train split into a tf.dataset\n    '''\n    x_train, y_train = split_fn(train_data)\n    x_val, y_val = split_fn(val_data)\n    x_test, y_test = split_fn(test_data)\n    # selecting only the first 20 features for prediction:\n    y_train = y_train[:, :, target_features]\n    y_val = y_val[:, :, target_features]\n    y_test = y_test[:, :, target_features]\n    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n    test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n    return train_loader, val_loader, test_loader\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\ntrain_data, val_data, test_data = split_dataset(data)\ntrain_dataset, val_dataset, test_dataset = data_to_dataset(train_data, val_data, test_data)\n\n\n\n\n\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    '''Multi-head self-attention module'''\n    def __init__(self, D, H):\n        super(MultiHeadAttention, self).__init__()\n        self.H = H # number of heads\n        self.D = D # dimension\n\n        self.wq = nn.Linear(D, D*H)\n        self.wk = nn.Linear(D, D*H)\n        self.wv = nn.Linear(D, D*H)\n\n        self.dense = nn.Linear(D*H, D)\n\n    def concat_heads(self, x):\n        '''(B, H, S, D) =&gt; (B, S, D*H)'''\n        B, H, S, D = x.shape\n        x = x.permute((0, 2, 1, 3)).contiguous()  # (B, S, H, D)\n        x = x.reshape((B, S, H*D))   # (B, S, D*H)\n        return x\n\n    def split_heads(self, x):\n        '''(B, S, D*H) =&gt; (B, H, S, D)'''\n        B, S, D_H = x.shape\n        x = x.reshape(B, S, self.H, self.D)    # (B, S, H, D)\n        x = x.permute((0, 2, 1, 3))  # (B, H, S, D)\n        return x\n\n    def forward(self, x, mask):\n\n        q = self.wq(x)  # (B, S, D*H)\n        k = self.wk(x)  # (B, S, D*H)\n        v = self.wv(x)  # (B, S, D*H)\n\n        q = self.split_heads(q)  # (B, H, S, D)\n        k = self.split_heads(k)  # (B, H, S, D)\n        v = self.split_heads(v)  # (B, H, S, D)\n\n        attention_scores = torch.matmul(q, k.transpose(-1, -2)) #(B,H,S,S)\n        attention_scores = attention_scores / math.sqrt(self.D)\n\n        # add the mask to the scaled tensor.\n        if mask is not None:\n            attention_scores += (mask * -1e9)\n\n        attention_weights = nn.Softmax(dim=-1)(attention_scores)\n        scaled_attention = torch.matmul(attention_weights, v)  # (B, H, S, D)\n        concat_attention = self.concat_heads(scaled_attention) # (B, S, D*H)\n        output = self.dense(concat_attention)  # (B, S, D)\n\n        return output, attention_weights\n\n\nB, S, H, D = 9, 11, 5, 8\nmha = MultiHeadAttention(D, H)\nout, att = mha.forward(torch.zeros(B, S, D), mask=None)\nout.shape, att.shape\n\n\n# Positional encodings\ndef get_angles(pos, i, D):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(D))\n    return pos * angle_rates\n\n\ndef positional_encoding(D, position=20, dim=3, device=device):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                            np.arange(D)[np.newaxis, :],\n                            D)\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    if dim == 3:\n        pos_encoding = angle_rads[np.newaxis, ...]\n    elif dim == 4:\n        pos_encoding = angle_rads[np.newaxis,np.newaxis,  ...]\n    return torch.tensor(pos_encoding, device=device)\n\n\n# function that implement the look_ahead mask for masking future time steps.\ndef create_look_ahead_mask(size, device=device):\n    mask = torch.ones((size, size), device=device)\n    mask = torch.triu(mask, diagonal=1)\n    return mask  # (size, size)\n\n\ncreate_look_ahead_mask(6)\n\n\nclass TransformerLayer(nn.Module):\n    def __init__(self, D, H, hidden_mlp_dim, dropout_rate):\n        super(TransformerLayer, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.mlp_hidden = nn.Linear(D, hidden_mlp_dim)\n        self.mlp_out = nn.Linear(hidden_mlp_dim, D)\n        self.layernorm1 = nn.LayerNorm(D, eps=1e-9)\n        self.layernorm2 = nn.LayerNorm(D, eps=1e-9)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.mha = MultiHeadAttention(D, H)\n\n\n    def forward(self, x, look_ahead_mask):\n\n        attn, attn_weights = self.mha(x, look_ahead_mask)  # (B, S, D)\n        attn = self.dropout1(attn) # (B,S,D)\n        attn = self.layernorm1(attn + x) # (B,S,D)\n\n        mlp_act = torch.relu(self.mlp_hidden(attn))\n        mlp_act = self.mlp_out(mlp_act)\n        mlp_act = self.dropout2(mlp_act)\n\n        output = self.layernorm2(mlp_act + attn)  # (B, S, D)\n\n        return output, attn_weights\n\n\ndl = TransformerLayer(16, 3, 32, 0.1)\nout, attn = dl(x=torch.zeros(5, 7, 16), look_ahead_mask=None)\nout.shape, attn.shape\n\n\nclass Transformer(nn.Module):\n    '''Transformer Decoder Implementating several Decoder Layers.\n    '''\n    def __init__(self, num_layers, D, H, hidden_mlp_dim, inp_features, out_features, dropout_rate):\n        super(Transformer, self).__init__()\n        self.sqrt_D = torch.tensor(math.sqrt(D))\n        self.num_layers = num_layers\n        self.input_projection = nn.Linear(inp_features, D) # multivariate input\n        self.output_projection = nn.Linear(D, out_features) # multivariate output\n        self.pos_encoding = positional_encoding(D)\n        self.dec_layers = nn.ModuleList([TransformerLayer(D, H, hidden_mlp_dim,\n                                        dropout_rate=dropout_rate\n                                       ) for _ in range(num_layers)])\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, mask):\n        B, S, D = x.shape\n        attention_weights = {}\n        x = self.input_projection(x)\n        x *= self.sqrt_D\n\n        x += self.pos_encoding[:, :S, :]\n\n        x = self.dropout(x)\n\n        for i in range(self.num_layers):\n            x, block = self.dec_layers[i](x=x,\n                                          look_ahead_mask=mask)\n            attention_weights['decoder_layer{}'.format(i + 1)] = block\n\n        x = self.output_projection(x)\n\n        return x, attention_weights # (B,S,S)\n\n\n# Test Forward pass on the Transformer:\ntransformer = Transformer(num_layers=1, D=32, H=1, hidden_mlp_dim=32,\n                                       inp_features=28, out_features=20, dropout_rate=0.1)\ntransformer.to(device)\n(inputs, targets) = next(iter(train_dataset))\n\nS = inputs.shape[1]\nmask = create_look_ahead_mask(S)\nout, attn = transformer (x=inputs, mask=mask)\nout.shape, attn[\"decoder_layer1\"].shape"
  },
  {
    "objectID": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html#transformers-for-timeseries",
    "href": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html#transformers-for-timeseries",
    "title": "트랜스포머 (전력사용량 데이터)",
    "section": "",
    "text": "Click to run on colab (if you’re not already there): \nThe goal of this notebook is to illustrate the use of a transformer for timeseries prediction. This notebook was built by Alice Martin and adapted to pytorch by Charles Ollion\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport time\nimport matplotlib.pyplot as plt\n\n\n\nEnergy consumption dataset from https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction * gathers 10-min measurements of household appliances energy consumption (20 first features), coupled with local meteorological data (8 last features). * The time-series forecasting task is to predict the first 20 features, given as input data the 28 features. A window of observations of 12 time steps is considered to predict the next series of observations (this corresponds to a 2-hours window of observations.\nyou may get the dataset (a single csv file) by running the following cell:\n\n!wget https://raw.githubusercontent.com/LuisM78/Appliances-energy-prediction-data/master/energydata_complete.csv\n\n\n# load and preprocess the energy dataset:\ndef convert_col_into_float(df, list_cols):\n    for col in list_cols:\n        df[col] = df[col].astype(str)\n        df[col] = df[col].str.replace(',', '.')\n        df[col] = df[col].astype(np.float32)\n    return df\ndf = pd.read_csv(\"energydata_complete.csv\", index_col='date', parse_dates=['date'])\nprint(df.head())\nlist_cols = list(df.columns)\n# gathers 10-min measurements of household appliances energy consumption (20 first features), coupled with local meteorological data. (8 last features)\nprint(\"dataset variables\", list_cols)\ndf = convert_col_into_float(df, list_cols)\ndata = df.values\n\n\ndef split_dataset_into_seq(dataset, start_index=0, end_index=None, history_size=13, step=1):\n    '''split the dataset to have sequence of observations of length history size'''\n    data = []\n    start_index = start_index + history_size\n    if end_index is None:\n        end_index = len(dataset)\n    for i in range(start_index, end_index):\n        indices = range(i - history_size, i, step)\n        data.append(dataset[indices])\n    return np.array(data)\n\n\ndef split_dataset(data, TRAIN_SPLIT=0.7, VAL_SPLIT=0.5, save_path=None):\n    '''split the dataset into train, val and test splits'''\n    # normalization\n    data_mean = data.mean(axis=0)\n    data_std = data.std(axis=0)\n    data = (data - data_mean) / data_std\n    stats = (data_mean, data_std)\n\n    data_in_seq = split_dataset_into_seq(data, start_index=0, end_index=None, history_size=13, step=1)\n\n    # split between validation dataset and test set:\n    train_data, val_data = train_test_split(data_in_seq, train_size=TRAIN_SPLIT, shuffle=True, random_state=123)\n    val_data, test_data = train_test_split(val_data, train_size=VAL_SPLIT, shuffle=True, random_state=123)\n\n    return train_data, val_data, test_data\n\n\ndef split_fn(chunk):\n    \"\"\"to split the dataset sequences into input and targets sequences\"\"\"\n    inputs = torch.tensor(chunk[:, :-1, :], device=device)\n    targets = torch.tensor(chunk[:, 1:, :], device=device)\n    return inputs, targets\n\n\ndef data_to_dataset(train_data, val_data, test_data, batch_size=32, target_features=list(range(20))):\n    '''\n    split each train split into inputs and targets\n    convert each train split into a tf.dataset\n    '''\n    x_train, y_train = split_fn(train_data)\n    x_val, y_val = split_fn(val_data)\n    x_test, y_test = split_fn(test_data)\n    # selecting only the first 20 features for prediction:\n    y_train = y_train[:, :, target_features]\n    y_val = y_val[:, :, target_features]\n    y_test = y_test[:, :, target_features]\n    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n    test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n    return train_loader, val_loader, test_loader\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\ntrain_data, val_data, test_data = split_dataset(data)\ntrain_dataset, val_dataset, test_dataset = data_to_dataset(train_data, val_data, test_data)\n\n\n\n\n\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    '''Multi-head self-attention module'''\n    def __init__(self, D, H):\n        super(MultiHeadAttention, self).__init__()\n        self.H = H # number of heads\n        self.D = D # dimension\n\n        self.wq = nn.Linear(D, D*H)\n        self.wk = nn.Linear(D, D*H)\n        self.wv = nn.Linear(D, D*H)\n\n        self.dense = nn.Linear(D*H, D)\n\n    def concat_heads(self, x):\n        '''(B, H, S, D) =&gt; (B, S, D*H)'''\n        B, H, S, D = x.shape\n        x = x.permute((0, 2, 1, 3)).contiguous()  # (B, S, H, D)\n        x = x.reshape((B, S, H*D))   # (B, S, D*H)\n        return x\n\n    def split_heads(self, x):\n        '''(B, S, D*H) =&gt; (B, H, S, D)'''\n        B, S, D_H = x.shape\n        x = x.reshape(B, S, self.H, self.D)    # (B, S, H, D)\n        x = x.permute((0, 2, 1, 3))  # (B, H, S, D)\n        return x\n\n    def forward(self, x, mask):\n\n        q = self.wq(x)  # (B, S, D*H)\n        k = self.wk(x)  # (B, S, D*H)\n        v = self.wv(x)  # (B, S, D*H)\n\n        q = self.split_heads(q)  # (B, H, S, D)\n        k = self.split_heads(k)  # (B, H, S, D)\n        v = self.split_heads(v)  # (B, H, S, D)\n\n        attention_scores = torch.matmul(q, k.transpose(-1, -2)) #(B,H,S,S)\n        attention_scores = attention_scores / math.sqrt(self.D)\n\n        # add the mask to the scaled tensor.\n        if mask is not None:\n            attention_scores += (mask * -1e9)\n\n        attention_weights = nn.Softmax(dim=-1)(attention_scores)\n        scaled_attention = torch.matmul(attention_weights, v)  # (B, H, S, D)\n        concat_attention = self.concat_heads(scaled_attention) # (B, S, D*H)\n        output = self.dense(concat_attention)  # (B, S, D)\n\n        return output, attention_weights\n\n\nB, S, H, D = 9, 11, 5, 8\nmha = MultiHeadAttention(D, H)\nout, att = mha.forward(torch.zeros(B, S, D), mask=None)\nout.shape, att.shape\n\n\n# Positional encodings\ndef get_angles(pos, i, D):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(D))\n    return pos * angle_rates\n\n\ndef positional_encoding(D, position=20, dim=3, device=device):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                            np.arange(D)[np.newaxis, :],\n                            D)\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    if dim == 3:\n        pos_encoding = angle_rads[np.newaxis, ...]\n    elif dim == 4:\n        pos_encoding = angle_rads[np.newaxis,np.newaxis,  ...]\n    return torch.tensor(pos_encoding, device=device)\n\n\n# function that implement the look_ahead mask for masking future time steps.\ndef create_look_ahead_mask(size, device=device):\n    mask = torch.ones((size, size), device=device)\n    mask = torch.triu(mask, diagonal=1)\n    return mask  # (size, size)\n\n\ncreate_look_ahead_mask(6)\n\n\nclass TransformerLayer(nn.Module):\n    def __init__(self, D, H, hidden_mlp_dim, dropout_rate):\n        super(TransformerLayer, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.mlp_hidden = nn.Linear(D, hidden_mlp_dim)\n        self.mlp_out = nn.Linear(hidden_mlp_dim, D)\n        self.layernorm1 = nn.LayerNorm(D, eps=1e-9)\n        self.layernorm2 = nn.LayerNorm(D, eps=1e-9)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.mha = MultiHeadAttention(D, H)\n\n\n    def forward(self, x, look_ahead_mask):\n\n        attn, attn_weights = self.mha(x, look_ahead_mask)  # (B, S, D)\n        attn = self.dropout1(attn) # (B,S,D)\n        attn = self.layernorm1(attn + x) # (B,S,D)\n\n        mlp_act = torch.relu(self.mlp_hidden(attn))\n        mlp_act = self.mlp_out(mlp_act)\n        mlp_act = self.dropout2(mlp_act)\n\n        output = self.layernorm2(mlp_act + attn)  # (B, S, D)\n\n        return output, attn_weights\n\n\ndl = TransformerLayer(16, 3, 32, 0.1)\nout, attn = dl(x=torch.zeros(5, 7, 16), look_ahead_mask=None)\nout.shape, attn.shape\n\n\nclass Transformer(nn.Module):\n    '''Transformer Decoder Implementating several Decoder Layers.\n    '''\n    def __init__(self, num_layers, D, H, hidden_mlp_dim, inp_features, out_features, dropout_rate):\n        super(Transformer, self).__init__()\n        self.sqrt_D = torch.tensor(math.sqrt(D))\n        self.num_layers = num_layers\n        self.input_projection = nn.Linear(inp_features, D) # multivariate input\n        self.output_projection = nn.Linear(D, out_features) # multivariate output\n        self.pos_encoding = positional_encoding(D)\n        self.dec_layers = nn.ModuleList([TransformerLayer(D, H, hidden_mlp_dim,\n                                        dropout_rate=dropout_rate\n                                       ) for _ in range(num_layers)])\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, mask):\n        B, S, D = x.shape\n        attention_weights = {}\n        x = self.input_projection(x)\n        x *= self.sqrt_D\n\n        x += self.pos_encoding[:, :S, :]\n\n        x = self.dropout(x)\n\n        for i in range(self.num_layers):\n            x, block = self.dec_layers[i](x=x,\n                                          look_ahead_mask=mask)\n            attention_weights['decoder_layer{}'.format(i + 1)] = block\n\n        x = self.output_projection(x)\n\n        return x, attention_weights # (B,S,S)\n\n\n# Test Forward pass on the Transformer:\ntransformer = Transformer(num_layers=1, D=32, H=1, hidden_mlp_dim=32,\n                                       inp_features=28, out_features=20, dropout_rate=0.1)\ntransformer.to(device)\n(inputs, targets) = next(iter(train_dataset))\n\nS = inputs.shape[1]\nmask = create_look_ahead_mask(S)\nout, attn = transformer (x=inputs, mask=mask)\nout.shape, attn[\"decoder_layer1\"].shape"
  },
  {
    "objectID": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html#training-the-transformer",
    "href": "posts/7_study/5_etc/Copy_of_Transformers_for_timeseries.html#training-the-transformer",
    "title": "트랜스포머 (전력사용량 데이터)",
    "section": "Training the Transformer",
    "text": "Training the Transformer\n\nparam_sizes = [p.numel() for p in transformer.parameters()]\nprint(f\"number of weight/biases matrices: {len(param_sizes)} \"\n      f\"for a total of {np.sum(param_sizes)} parameters \")\n\n\ntransformer = Transformer(num_layers=1, D=32, H=4, hidden_mlp_dim=32,\n                          inp_features=28, out_features=20, dropout_rate=0.1).to(device)\noptimizer = torch.optim.RMSprop(transformer.parameters(),\n                                lr=0.00005)\n\n\nfrom tqdm import tqdm\n\nn_epochs = 20\nniter = len(train_dataset)\nlosses, val_losses = [], []\n\nfor e in tqdm(range(n_epochs)):\n\n    # one epoch on train set\n    transformer.train()\n    sum_train_loss = 0.0\n    for x,y in train_dataset:\n        S = x.shape[1]\n        mask = create_look_ahead_mask(S)\n        out, _ = transformer(x, mask)\n        loss = torch.nn.MSELoss()(out, y)\n        sum_train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n    losses.append(sum_train_loss / niter)\n\n    # Evaluate on val set\n    transformer.eval()\n    sum_val_loss = 0.0\n    for i, (x, y) in enumerate(val_dataset):\n        S = x.shape[1]\n        mask = create_look_ahead_mask(S)\n        out, _ = transformer(x, mask)\n        loss = torch.nn.MSELoss()(out, y)\n        sum_val_loss += loss.item()\n    val_losses.append(sum_val_loss / (i + 1))\n\n\nplt.plot(losses)\nplt.plot(val_losses);\n\n\nEvaluation on Test set\n\ntest_losses, test_preds  = [], []\ntransformer.eval()\nfor (x, y) in test_dataset:\n    S = x.shape[-2]\n    y_pred, _ = transformer(x,\n                            mask=create_look_ahead_mask(S))\n    loss_test = torch.nn.MSELoss()(y_pred, y)  # (B,S)\n    test_losses.append(loss_test.item())\n    test_preds.append(y_pred.detach().cpu().numpy())\ntest_preds = np.vstack(test_preds)\nnp.mean(test_losses)\n\n\n# Display predictions vs ground truth:\n# we'll take one random element of the first batch\n# and display the first feature\nseq_len = 12\nindex = np.random.randint(32)\nfeature_num = 0\n\nx_test, _ = test_dataset.dataset.tensors\nx_test = x_test[index, :, feature_num].cpu().numpy()\npred = test_preds[index, :, feature_num]\nx = np.linspace(1, seq_len, seq_len)\nplt.plot(x, pred, 'red', lw=2, label='predictions for sample: {}'.format(index))\nplt.plot(x, x_test, 'cyan', lw=2, label='ground-truth for sample: {}'.format(index))\nplt.legend(fontsize=10)\nplt.show()"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-08-10-transformer-ts.html",
    "href": "posts/7_study/5_etc/2023-08-10-transformer-ts.html",
    "title": "Time Series Transformere (Stock Price)",
    "section": "",
    "text": "Training a transformer model to forecast time series sequence of stock closing price Using 10 timesteps to forecast 1 forward timestep\n\n\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport time\nimport math\nimport matplotlib.pyplot as plt\n\n\n\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/ctxj/Time-Series-Transformer-Pytorch/main/FB_raw.csv')\ndf.head()\n\n\n\n\n\n\n\n\ndate\nminute\nlabel\nhigh\nlow\nopen\nclose\naverage\nvolume\nnotional\n...\nrsi\nlong\nexit_long\nshort\nexit_short\nlong_pnl\nshort_pnl\ntotal_pnl\nlong_labels\nshort_labels\n\n\n\n\n0\n2019-04-01\n09:30\n09:30 AM\n168.000\n167.52\n167.925\n167.82\n167.775\n9294.0\n1559300.720\n...\nNaN\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n1\n2019-04-01\n09:31\n09:31 AM\n168.190\n167.76\n167.760\n168.19\n167.908\n863.0\n144904.500\n...\n100.000000\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n2\n2019-04-01\n09:32\n09:32 AM\n168.630\n168.36\n168.390\n168.36\n168.490\n2707.0\n456101.680\n...\n100.000000\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n3\n2019-04-01\n09:33\n09:33 AM\n168.135\n167.94\n168.135\n168.04\n168.132\n5503.0\n925229.920\n...\n46.468401\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n4\n2019-04-01\n09:34\n09:34 AM\n168.200\n168.00\n168.045\n168.01\n168.189\n15236.0\n2562520.845\n...\n43.215212\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n\n\n5 rows × 21 columns\n\n\n\n- global variables\n\ninput_window = 10 # number of input steps\noutput_window = 1 # number of prediction steps // 여기서는 1로 고정\nbatch_size = 250\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice = torch.device('cpu')\n\n\nclose = np.array(df['close'])\nlogreturn = np.diff(np.log(close)) # transform closing price to log returns\ncsum_logreturn = logreturn.cumsum() # cumulative sum of log returns\n\n- plot\n\nfig, axes = plt.subplots(2,1)\naxes[0].plot(close, color='red')\naxes[0].set_title('Closing Price')\naxes[0].set_ylabel('Close Price')\naxes[0].set_xlabel('Time Steps')\n\naxes[1].plot(csum_logreturn, color='green')\naxes[1].set_title('Cumulative Sum of Log Returns')\naxes[1].set_xlabel('Time Steps')\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()       \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:x.size(0), :]                          \n\n\n\n\n\nclass TransAm(nn.Module):\n    def __init__(self, feature_size=250, num_layers=1, dropout=0.1):\n        super(TransAm, self).__init__()\n        self.model_type = 'Transformer'\n        \n        self.src_mask = None\n        self.pos_encoder = PositionalEncoding(feature_size)\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n        self.decoder = nn.Linear(feature_size,1)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1    \n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self,src):\n        if self.src_mask is None or self.src_mask.size(0) != len(src):\n            device = src.device\n            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n            self.src_mask = mask\n\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src,self.src_mask)\n        output = self.decoder(output)\n        return output\n\n    def _generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n- Window function\nsplit data into sequence window\n\ndef create_inout_sequences(input_data, tw):\n    inout_seq = []\n    L = len(input_data)\n    for i in range(L-tw):\n        train_seq = input_data[i:i+tw]\n        train_label = input_data[i+output_window:i+tw+output_window]\n        inout_seq.append((train_seq ,train_label))\n    return torch.FloatTensor(inout_seq)\n\nSplit data in training and testing, prepared in windowed sequences and pass through GPU\n\ndef get_data(data, split):\n    \"\"\"Split ratio of training data\"\"\"\n\n    series = data\n    \n    split = round(split*len(series))\n    train_data = series[:split]\n    test_data = series[split:]\n\n    train_data = train_data.cumsum()\n    train_data = 2*train_data # Training data augmentation, increase amplitude for the model to better generalize.(Scaling by 2 is aribitrary)\n                              # Similar to image transformation to allow model to train on wider data sets\n\n    test_data = test_data.cumsum()\n\n    train_sequence = create_inout_sequences(train_data,input_window)\n    train_sequence = train_sequence[:-output_window]\n\n    test_data = create_inout_sequences(test_data,input_window)\n    test_data = test_data[:-output_window]\n\n    return train_sequence.to(device), test_data.to(device)\n\nSplit into training batches\n\ndef get_batch(source, i, batch_size):\n    seq_len = min(batch_size, len(source) - 1 - i)\n    data = source[i:i+seq_len]    \n    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window, 1))\n    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window, 1))\n    return input, target\n\nTraining function\n\ndef train(train_data):\n    model.train() # Turn on the evaluation mode\n    total_loss = 0.\n    start_time = time.time()\n\n    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n        data, targets = get_batch(train_data, i,batch_size)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, targets)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n        optimizer.step()\n\n        total_loss += loss.item()\n        log_interval = int(len(train_data) / batch_size / 5)\n        if batch % log_interval == 0 and batch &gt; 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n                  'lr {:02.10f} | {:5.2f} ms | '\n                  'loss {:5.7f}'.format(\n                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n                    elapsed * 1000 / log_interval,\n                    cur_loss))\n            total_loss = 0\n            start_time = time.time()\n\nEvaluation function for model after training\n\ndef evaluate(eval_model, data_source):\n    eval_model.eval() # Turn on the evaluation mode\n    total_loss = 0.\n    eval_batch_size = 1000\n    with torch.no_grad():\n        for i in range(0, len(data_source) - 1, eval_batch_size):\n            data, targets = get_batch(data_source, i, eval_batch_size)\n            output = eval_model(data)            \n            total_loss += len(data[0])* criterion(output, targets).cpu().item()\n    return total_loss / len(data_source)\n\nFunction to forecast 1 time step from window sequence\n\ndef model_forecast(model, seqence):\n    model.eval() \n    total_loss = 0.\n    test_result = torch.Tensor(0)    \n    truth = torch.Tensor(0)\n\n    seq = np.pad(seqence, (0, 3), mode='constant', constant_values=(0, 0))\n    seq = create_inout_sequences(seq, input_window)\n    seq = seq[:-output_window].to(device)\n\n    seq, _ = get_batch(seq, 0, 1)\n    with torch.no_grad():\n        for i in range(0, output_window):            \n            output = model(seq[-output_window:])                        \n            seq = torch.cat((seq, output[-1:]))\n\n    seq = seq.cpu().view(-1).numpy()\n\n    return seq\n\nFunction to forecast entire sequence\n\ndef forecast_seq(model, sequences):\n    \"\"\"Sequences data has to been windowed and passed through device\"\"\"\n    start_timer = time.time()\n    model.eval() \n    forecast_seq = torch.Tensor(0)    \n    actual = torch.Tensor(0)\n    with torch.no_grad():\n        for i in range(0, len(sequences) - 1):\n            data, target = get_batch(sequences, i, 1)\n            output = model(data)            \n            forecast_seq = torch.cat((forecast_seq, output[-1].view(-1).cpu()), 0)\n            actual = torch.cat((actual, target[-1].view(-1).cpu()), 0)\n    timed = time.time()-start_timer\n    print(f\"{timed} sec\")\n\n    return forecast_seq, actual\n\nPrepare data for training model\n\ntrain_data, val_data = get_data(logreturn, 0.6) # 60% train, 40% test split\nmodel = TransAm().to(device)\n\n\ncriterion = nn.MSELoss() # Loss function\nlr = 0.00005 # learning rate\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n\nepochs =  10 # Number of epochs 150\n\n\nfor epoch in range(1, epochs + 1):\n    epoch_start_time = time.time()\n    train(train_data)\n    \n    if(epoch % epochs == 0): # Valid model after last training epoch\n        val_loss = evaluate(model, val_data)\n        print('-' * 80)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss: {:5.7f}'.format(epoch, (time.time() - epoch_start_time), val_loss))\n        print('-' * 80)\n\n    else:   \n        print('-' * 80)\n        print('| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time)))\n        print('-' * 80)\n\n    scheduler.step() \n\n| epoch   1 |    77/  385 batches | lr 0.0000500000 | 57.22 ms | loss 0.0136349\n| epoch   1 |   154/  385 batches | lr 0.0000500000 | 56.69 ms | loss 0.0101929\n| epoch   1 |   231/  385 batches | lr 0.0000500000 | 56.71 ms | loss 0.0057727\n| epoch   1 |   308/  385 batches | lr 0.0000500000 | 56.94 ms | loss 0.0042714\n| epoch   1 |   385/  385 batches | lr 0.0000500000 | 56.60 ms | loss 0.0135543\n--------------------------------------------------------------------------------\n| end of epoch   1 | time: 21.88s\n--------------------------------------------------------------------------------\n| epoch   2 |    77/  385 batches | lr 0.0000451250 | 57.75 ms | loss 0.0058133\n| epoch   2 |   154/  385 batches | lr 0.0000451250 | 57.89 ms | loss 0.0048406\n| epoch   2 |   231/  385 batches | lr 0.0000451250 | 56.89 ms | loss 0.0023565\n| epoch   2 |   308/  385 batches | lr 0.0000451250 | 57.11 ms | loss 0.0025112\n| epoch   2 |   385/  385 batches | lr 0.0000451250 | 56.61 ms | loss 0.0074962\n--------------------------------------------------------------------------------\n| end of epoch   2 | time: 22.04s\n--------------------------------------------------------------------------------\n| epoch   3 |    77/  385 batches | lr 0.0000428687 | 57.97 ms | loss 0.0040375\n| epoch   3 |   154/  385 batches | lr 0.0000428687 | 57.34 ms | loss 0.0036393\n| epoch   3 |   231/  385 batches | lr 0.0000428687 | 57.57 ms | loss 0.0015818\n| epoch   3 |   308/  385 batches | lr 0.0000428687 | 57.03 ms | loss 0.0035247\n| epoch   3 |   385/  385 batches | lr 0.0000428687 | 56.50 ms | loss 0.0056444\n--------------------------------------------------------------------------------\n| end of epoch   3 | time: 22.05s\n--------------------------------------------------------------------------------\n| epoch   4 |    77/  385 batches | lr 0.0000407253 | 57.57 ms | loss 0.0046868\n| epoch   4 |   154/  385 batches | lr 0.0000407253 | 56.73 ms | loss 0.0026414\n| epoch   4 |   231/  385 batches | lr 0.0000407253 | 56.21 ms | loss 0.0018956\n| epoch   4 |   308/  385 batches | lr 0.0000407253 | 58.26 ms | loss 0.0027017\n| epoch   4 |   385/  385 batches | lr 0.0000407253 | 56.28 ms | loss 0.0050008\n--------------------------------------------------------------------------------\n| end of epoch   4 | time: 21.95s\n--------------------------------------------------------------------------------\n| epoch   5 |    77/  385 batches | lr 0.0000386890 | 57.32 ms | loss 0.0052833\n| epoch   5 |   154/  385 batches | lr 0.0000386890 | 56.76 ms | loss 0.0036348\n| epoch   5 |   231/  385 batches | lr 0.0000386890 | 57.19 ms | loss 0.0021735\n| epoch   5 |   308/  385 batches | lr 0.0000386890 | 57.64 ms | loss 0.0020005\n| epoch   5 |   385/  385 batches | lr 0.0000386890 | 56.69 ms | loss 0.0055897\n--------------------------------------------------------------------------------\n| end of epoch   5 | time: 21.99s\n--------------------------------------------------------------------------------\n| epoch   6 |    77/  385 batches | lr 0.0000367546 | 57.75 ms | loss 0.0070904\n| epoch   6 |   154/  385 batches | lr 0.0000367546 | 56.02 ms | loss 0.0022925\n| epoch   6 |   231/  385 batches | lr 0.0000367546 | 56.53 ms | loss 0.0018821\n| epoch   6 |   308/  385 batches | lr 0.0000367546 | 57.27 ms | loss 0.0022888\n| epoch   6 |   385/  385 batches | lr 0.0000367546 | 56.54 ms | loss 0.0090193\n--------------------------------------------------------------------------------\n| end of epoch   6 | time: 21.88s\n--------------------------------------------------------------------------------\n| epoch   7 |    77/  385 batches | lr 0.0000349169 | 57.88 ms | loss 0.0068115\n| epoch   7 |   154/  385 batches | lr 0.0000349169 | 57.39 ms | loss 0.0021631\n| epoch   7 |   231/  385 batches | lr 0.0000349169 | 58.06 ms | loss 0.0018259\n| epoch   7 |   308/  385 batches | lr 0.0000349169 | 57.22 ms | loss 0.0022266\n| epoch   7 |   385/  385 batches | lr 0.0000349169 | 56.09 ms | loss 0.0118806\n--------------------------------------------------------------------------------\n| end of epoch   7 | time: 22.07s\n--------------------------------------------------------------------------------\n| epoch   8 |    77/  385 batches | lr 0.0000331710 | 58.05 ms | loss 0.0077954\n| epoch   8 |   154/  385 batches | lr 0.0000331710 | 56.74 ms | loss 0.0031891\n| epoch   8 |   231/  385 batches | lr 0.0000331710 | 56.75 ms | loss 0.0040912\n| epoch   8 |   308/  385 batches | lr 0.0000331710 | 56.93 ms | loss 0.0023997\n| epoch   8 |   385/  385 batches | lr 0.0000331710 | 56.25 ms | loss 0.0081000\n--------------------------------------------------------------------------------\n| end of epoch   8 | time: 21.92s\n--------------------------------------------------------------------------------\n| epoch   9 |    77/  385 batches | lr 0.0000315125 | 57.79 ms | loss 0.0066046\n| epoch   9 |   154/  385 batches | lr 0.0000315125 | 56.83 ms | loss 0.0039425\n| epoch   9 |   231/  385 batches | lr 0.0000315125 | 56.75 ms | loss 0.0037167\n| epoch   9 |   308/  385 batches | lr 0.0000315125 | 57.20 ms | loss 0.0019323\n| epoch   9 |   385/  385 batches | lr 0.0000315125 | 57.33 ms | loss 0.0132667\n--------------------------------------------------------------------------------\n| end of epoch   9 | time: 22.02s\n--------------------------------------------------------------------------------\n| epoch  10 |    77/  385 batches | lr 0.0000299368 | 58.24 ms | loss 0.0085330\n| epoch  10 |   154/  385 batches | lr 0.0000299368 | 57.27 ms | loss 0.0032725\n| epoch  10 |   231/  385 batches | lr 0.0000299368 | 70.65 ms | loss 0.0034041\n| epoch  10 |   308/  385 batches | lr 0.0000299368 | 70.60 ms | loss 0.0015592\n| epoch  10 |   385/  385 batches | lr 0.0000299368 | 68.78 ms | loss 0.0146613\n--------------------------------------------------------------------------------\n| end of epoch  10 | time: 38.02s | valid loss: 0.2860207\n--------------------------------------------------------------------------------\n\n\n\nmodel\n\nTransAm(\n  (pos_encoder): PositionalEncoding()\n  (encoder_layer): TransformerEncoderLayer(\n    (self_attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=250, out_features=250, bias=True)\n    )\n    (linear1): Linear(in_features=250, out_features=2048, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linear2): Linear(in_features=2048, out_features=250, bias=True)\n    (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n    (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n    (dropout1): Dropout(p=0.1, inplace=False)\n    (dropout2): Dropout(p=0.1, inplace=False)\n  )\n  (transformer_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=250, out_features=250, bias=True)\n        )\n        (linear1): Linear(in_features=250, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=250, bias=True)\n        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): Linear(in_features=250, out_features=1, bias=True)\n)\n\n\n\ntest_result, truth = forecast_seq(model, val_data)\n\n1600.3889005184174 sec\n\n\n- Plot forecasted sequence vs. actual\n\nplt.plot(truth, color='red', alpha=0.7)\nplt.plot(test_result, color='blue', linewidth=0.7)\nplt.title('Actual vs Forecast')\nplt.legend(['Actual', 'Forecast'])\nplt.xlabel('Time Steps')\nplt.show()\n\n\n\n\n\n에폭 수가 너무 적어서 학습이 잘 안된 듯..\n\n- Test random sequence\n\nr = np.random.randint(100000, 160000)\ntest_forecast = model_forecast(model, csum_logreturn[r: r+10]) # random 10 sequence length\n\nprint(f\"forecast sequence: {test_forecast}\")\nprint(f\"Actual sequence: {csum_logreturn[r: r+11]}\")\n\nforecast sequence: [ 0.45956996  0.45955116  0.45804468  0.45834616  0.46043515  0.45947587\n  0.45996502  0.45966405  0.46049154  0.45876053 -0.08363973]\nActual sequence: [0.45956996 0.45955115 0.45804467 0.45834615 0.46043515 0.45947588\n 0.45996503 0.45966404 0.46049154 0.45876053 0.45747915]\n\n\n\nref: https://github.com/ctxj/Time-Series-Transformer-Pytorch/tree/main"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-08-10-transformer-ts.html#import",
    "href": "posts/7_study/5_etc/2023-08-10-transformer-ts.html#import",
    "title": "Time Series Transformere (Stock Price)",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport time\nimport math\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/7_study/5_etc/2023-08-10-transformer-ts.html#load-data",
    "href": "posts/7_study/5_etc/2023-08-10-transformer-ts.html#load-data",
    "title": "Time Series Transformere (Stock Price)",
    "section": "",
    "text": "df = pd.read_csv('https://raw.githubusercontent.com/ctxj/Time-Series-Transformer-Pytorch/main/FB_raw.csv')\ndf.head()\n\n\n\n\n\n\n\n\ndate\nminute\nlabel\nhigh\nlow\nopen\nclose\naverage\nvolume\nnotional\n...\nrsi\nlong\nexit_long\nshort\nexit_short\nlong_pnl\nshort_pnl\ntotal_pnl\nlong_labels\nshort_labels\n\n\n\n\n0\n2019-04-01\n09:30\n09:30 AM\n168.000\n167.52\n167.925\n167.82\n167.775\n9294.0\n1559300.720\n...\nNaN\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n1\n2019-04-01\n09:31\n09:31 AM\n168.190\n167.76\n167.760\n168.19\n167.908\n863.0\n144904.500\n...\n100.000000\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n2\n2019-04-01\n09:32\n09:32 AM\n168.630\n168.36\n168.390\n168.36\n168.490\n2707.0\n456101.680\n...\n100.000000\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n3\n2019-04-01\n09:33\n09:33 AM\n168.135\n167.94\n168.135\n168.04\n168.132\n5503.0\n925229.920\n...\n46.468401\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n4\n2019-04-01\n09:34\n09:34 AM\n168.200\n168.00\n168.045\n168.01\n168.189\n15236.0\n2562520.845\n...\n43.215212\nNaN\n167.925\nNaN\n167.805\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n\n\n5 rows × 21 columns\n\n\n\n- global variables\n\ninput_window = 10 # number of input steps\noutput_window = 1 # number of prediction steps // 여기서는 1로 고정\nbatch_size = 250\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice = torch.device('cpu')\n\n\nclose = np.array(df['close'])\nlogreturn = np.diff(np.log(close)) # transform closing price to log returns\ncsum_logreturn = logreturn.cumsum() # cumulative sum of log returns\n\n- plot\n\nfig, axes = plt.subplots(2,1)\naxes[0].plot(close, color='red')\naxes[0].set_title('Closing Price')\naxes[0].set_ylabel('Close Price')\naxes[0].set_xlabel('Time Steps')\n\naxes[1].plot(csum_logreturn, color='green')\naxes[1].set_title('Cumulative Sum of Log Returns')\naxes[1].set_xlabel('Time Steps')\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()       \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:x.size(0), :]                          \n\n\n\n\n\nclass TransAm(nn.Module):\n    def __init__(self, feature_size=250, num_layers=1, dropout=0.1):\n        super(TransAm, self).__init__()\n        self.model_type = 'Transformer'\n        \n        self.src_mask = None\n        self.pos_encoder = PositionalEncoding(feature_size)\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n        self.decoder = nn.Linear(feature_size,1)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1    \n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self,src):\n        if self.src_mask is None or self.src_mask.size(0) != len(src):\n            device = src.device\n            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n            self.src_mask = mask\n\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src,self.src_mask)\n        output = self.decoder(output)\n        return output\n\n    def _generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n- Window function\nsplit data into sequence window\n\ndef create_inout_sequences(input_data, tw):\n    inout_seq = []\n    L = len(input_data)\n    for i in range(L-tw):\n        train_seq = input_data[i:i+tw]\n        train_label = input_data[i+output_window:i+tw+output_window]\n        inout_seq.append((train_seq ,train_label))\n    return torch.FloatTensor(inout_seq)\n\nSplit data in training and testing, prepared in windowed sequences and pass through GPU\n\ndef get_data(data, split):\n    \"\"\"Split ratio of training data\"\"\"\n\n    series = data\n    \n    split = round(split*len(series))\n    train_data = series[:split]\n    test_data = series[split:]\n\n    train_data = train_data.cumsum()\n    train_data = 2*train_data # Training data augmentation, increase amplitude for the model to better generalize.(Scaling by 2 is aribitrary)\n                              # Similar to image transformation to allow model to train on wider data sets\n\n    test_data = test_data.cumsum()\n\n    train_sequence = create_inout_sequences(train_data,input_window)\n    train_sequence = train_sequence[:-output_window]\n\n    test_data = create_inout_sequences(test_data,input_window)\n    test_data = test_data[:-output_window]\n\n    return train_sequence.to(device), test_data.to(device)\n\nSplit into training batches\n\ndef get_batch(source, i, batch_size):\n    seq_len = min(batch_size, len(source) - 1 - i)\n    data = source[i:i+seq_len]    \n    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window, 1))\n    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window, 1))\n    return input, target\n\nTraining function\n\ndef train(train_data):\n    model.train() # Turn on the evaluation mode\n    total_loss = 0.\n    start_time = time.time()\n\n    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n        data, targets = get_batch(train_data, i,batch_size)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, targets)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n        optimizer.step()\n\n        total_loss += loss.item()\n        log_interval = int(len(train_data) / batch_size / 5)\n        if batch % log_interval == 0 and batch &gt; 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n                  'lr {:02.10f} | {:5.2f} ms | '\n                  'loss {:5.7f}'.format(\n                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n                    elapsed * 1000 / log_interval,\n                    cur_loss))\n            total_loss = 0\n            start_time = time.time()\n\nEvaluation function for model after training\n\ndef evaluate(eval_model, data_source):\n    eval_model.eval() # Turn on the evaluation mode\n    total_loss = 0.\n    eval_batch_size = 1000\n    with torch.no_grad():\n        for i in range(0, len(data_source) - 1, eval_batch_size):\n            data, targets = get_batch(data_source, i, eval_batch_size)\n            output = eval_model(data)            \n            total_loss += len(data[0])* criterion(output, targets).cpu().item()\n    return total_loss / len(data_source)\n\nFunction to forecast 1 time step from window sequence\n\ndef model_forecast(model, seqence):\n    model.eval() \n    total_loss = 0.\n    test_result = torch.Tensor(0)    \n    truth = torch.Tensor(0)\n\n    seq = np.pad(seqence, (0, 3), mode='constant', constant_values=(0, 0))\n    seq = create_inout_sequences(seq, input_window)\n    seq = seq[:-output_window].to(device)\n\n    seq, _ = get_batch(seq, 0, 1)\n    with torch.no_grad():\n        for i in range(0, output_window):            \n            output = model(seq[-output_window:])                        \n            seq = torch.cat((seq, output[-1:]))\n\n    seq = seq.cpu().view(-1).numpy()\n\n    return seq\n\nFunction to forecast entire sequence\n\ndef forecast_seq(model, sequences):\n    \"\"\"Sequences data has to been windowed and passed through device\"\"\"\n    start_timer = time.time()\n    model.eval() \n    forecast_seq = torch.Tensor(0)    \n    actual = torch.Tensor(0)\n    with torch.no_grad():\n        for i in range(0, len(sequences) - 1):\n            data, target = get_batch(sequences, i, 1)\n            output = model(data)            \n            forecast_seq = torch.cat((forecast_seq, output[-1].view(-1).cpu()), 0)\n            actual = torch.cat((actual, target[-1].view(-1).cpu()), 0)\n    timed = time.time()-start_timer\n    print(f\"{timed} sec\")\n\n    return forecast_seq, actual\n\nPrepare data for training model\n\ntrain_data, val_data = get_data(logreturn, 0.6) # 60% train, 40% test split\nmodel = TransAm().to(device)\n\n\ncriterion = nn.MSELoss() # Loss function\nlr = 0.00005 # learning rate\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n\nepochs =  10 # Number of epochs 150\n\n\nfor epoch in range(1, epochs + 1):\n    epoch_start_time = time.time()\n    train(train_data)\n    \n    if(epoch % epochs == 0): # Valid model after last training epoch\n        val_loss = evaluate(model, val_data)\n        print('-' * 80)\n        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss: {:5.7f}'.format(epoch, (time.time() - epoch_start_time), val_loss))\n        print('-' * 80)\n\n    else:   \n        print('-' * 80)\n        print('| end of epoch {:3d} | time: {:5.2f}s'.format(epoch, (time.time() - epoch_start_time)))\n        print('-' * 80)\n\n    scheduler.step() \n\n| epoch   1 |    77/  385 batches | lr 0.0000500000 | 57.22 ms | loss 0.0136349\n| epoch   1 |   154/  385 batches | lr 0.0000500000 | 56.69 ms | loss 0.0101929\n| epoch   1 |   231/  385 batches | lr 0.0000500000 | 56.71 ms | loss 0.0057727\n| epoch   1 |   308/  385 batches | lr 0.0000500000 | 56.94 ms | loss 0.0042714\n| epoch   1 |   385/  385 batches | lr 0.0000500000 | 56.60 ms | loss 0.0135543\n--------------------------------------------------------------------------------\n| end of epoch   1 | time: 21.88s\n--------------------------------------------------------------------------------\n| epoch   2 |    77/  385 batches | lr 0.0000451250 | 57.75 ms | loss 0.0058133\n| epoch   2 |   154/  385 batches | lr 0.0000451250 | 57.89 ms | loss 0.0048406\n| epoch   2 |   231/  385 batches | lr 0.0000451250 | 56.89 ms | loss 0.0023565\n| epoch   2 |   308/  385 batches | lr 0.0000451250 | 57.11 ms | loss 0.0025112\n| epoch   2 |   385/  385 batches | lr 0.0000451250 | 56.61 ms | loss 0.0074962\n--------------------------------------------------------------------------------\n| end of epoch   2 | time: 22.04s\n--------------------------------------------------------------------------------\n| epoch   3 |    77/  385 batches | lr 0.0000428687 | 57.97 ms | loss 0.0040375\n| epoch   3 |   154/  385 batches | lr 0.0000428687 | 57.34 ms | loss 0.0036393\n| epoch   3 |   231/  385 batches | lr 0.0000428687 | 57.57 ms | loss 0.0015818\n| epoch   3 |   308/  385 batches | lr 0.0000428687 | 57.03 ms | loss 0.0035247\n| epoch   3 |   385/  385 batches | lr 0.0000428687 | 56.50 ms | loss 0.0056444\n--------------------------------------------------------------------------------\n| end of epoch   3 | time: 22.05s\n--------------------------------------------------------------------------------\n| epoch   4 |    77/  385 batches | lr 0.0000407253 | 57.57 ms | loss 0.0046868\n| epoch   4 |   154/  385 batches | lr 0.0000407253 | 56.73 ms | loss 0.0026414\n| epoch   4 |   231/  385 batches | lr 0.0000407253 | 56.21 ms | loss 0.0018956\n| epoch   4 |   308/  385 batches | lr 0.0000407253 | 58.26 ms | loss 0.0027017\n| epoch   4 |   385/  385 batches | lr 0.0000407253 | 56.28 ms | loss 0.0050008\n--------------------------------------------------------------------------------\n| end of epoch   4 | time: 21.95s\n--------------------------------------------------------------------------------\n| epoch   5 |    77/  385 batches | lr 0.0000386890 | 57.32 ms | loss 0.0052833\n| epoch   5 |   154/  385 batches | lr 0.0000386890 | 56.76 ms | loss 0.0036348\n| epoch   5 |   231/  385 batches | lr 0.0000386890 | 57.19 ms | loss 0.0021735\n| epoch   5 |   308/  385 batches | lr 0.0000386890 | 57.64 ms | loss 0.0020005\n| epoch   5 |   385/  385 batches | lr 0.0000386890 | 56.69 ms | loss 0.0055897\n--------------------------------------------------------------------------------\n| end of epoch   5 | time: 21.99s\n--------------------------------------------------------------------------------\n| epoch   6 |    77/  385 batches | lr 0.0000367546 | 57.75 ms | loss 0.0070904\n| epoch   6 |   154/  385 batches | lr 0.0000367546 | 56.02 ms | loss 0.0022925\n| epoch   6 |   231/  385 batches | lr 0.0000367546 | 56.53 ms | loss 0.0018821\n| epoch   6 |   308/  385 batches | lr 0.0000367546 | 57.27 ms | loss 0.0022888\n| epoch   6 |   385/  385 batches | lr 0.0000367546 | 56.54 ms | loss 0.0090193\n--------------------------------------------------------------------------------\n| end of epoch   6 | time: 21.88s\n--------------------------------------------------------------------------------\n| epoch   7 |    77/  385 batches | lr 0.0000349169 | 57.88 ms | loss 0.0068115\n| epoch   7 |   154/  385 batches | lr 0.0000349169 | 57.39 ms | loss 0.0021631\n| epoch   7 |   231/  385 batches | lr 0.0000349169 | 58.06 ms | loss 0.0018259\n| epoch   7 |   308/  385 batches | lr 0.0000349169 | 57.22 ms | loss 0.0022266\n| epoch   7 |   385/  385 batches | lr 0.0000349169 | 56.09 ms | loss 0.0118806\n--------------------------------------------------------------------------------\n| end of epoch   7 | time: 22.07s\n--------------------------------------------------------------------------------\n| epoch   8 |    77/  385 batches | lr 0.0000331710 | 58.05 ms | loss 0.0077954\n| epoch   8 |   154/  385 batches | lr 0.0000331710 | 56.74 ms | loss 0.0031891\n| epoch   8 |   231/  385 batches | lr 0.0000331710 | 56.75 ms | loss 0.0040912\n| epoch   8 |   308/  385 batches | lr 0.0000331710 | 56.93 ms | loss 0.0023997\n| epoch   8 |   385/  385 batches | lr 0.0000331710 | 56.25 ms | loss 0.0081000\n--------------------------------------------------------------------------------\n| end of epoch   8 | time: 21.92s\n--------------------------------------------------------------------------------\n| epoch   9 |    77/  385 batches | lr 0.0000315125 | 57.79 ms | loss 0.0066046\n| epoch   9 |   154/  385 batches | lr 0.0000315125 | 56.83 ms | loss 0.0039425\n| epoch   9 |   231/  385 batches | lr 0.0000315125 | 56.75 ms | loss 0.0037167\n| epoch   9 |   308/  385 batches | lr 0.0000315125 | 57.20 ms | loss 0.0019323\n| epoch   9 |   385/  385 batches | lr 0.0000315125 | 57.33 ms | loss 0.0132667\n--------------------------------------------------------------------------------\n| end of epoch   9 | time: 22.02s\n--------------------------------------------------------------------------------\n| epoch  10 |    77/  385 batches | lr 0.0000299368 | 58.24 ms | loss 0.0085330\n| epoch  10 |   154/  385 batches | lr 0.0000299368 | 57.27 ms | loss 0.0032725\n| epoch  10 |   231/  385 batches | lr 0.0000299368 | 70.65 ms | loss 0.0034041\n| epoch  10 |   308/  385 batches | lr 0.0000299368 | 70.60 ms | loss 0.0015592\n| epoch  10 |   385/  385 batches | lr 0.0000299368 | 68.78 ms | loss 0.0146613\n--------------------------------------------------------------------------------\n| end of epoch  10 | time: 38.02s | valid loss: 0.2860207\n--------------------------------------------------------------------------------\n\n\n\nmodel\n\nTransAm(\n  (pos_encoder): PositionalEncoding()\n  (encoder_layer): TransformerEncoderLayer(\n    (self_attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=250, out_features=250, bias=True)\n    )\n    (linear1): Linear(in_features=250, out_features=2048, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linear2): Linear(in_features=2048, out_features=250, bias=True)\n    (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n    (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n    (dropout1): Dropout(p=0.1, inplace=False)\n    (dropout2): Dropout(p=0.1, inplace=False)\n  )\n  (transformer_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=250, out_features=250, bias=True)\n        )\n        (linear1): Linear(in_features=250, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=250, bias=True)\n        (norm1): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((250,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): Linear(in_features=250, out_features=1, bias=True)\n)\n\n\n\ntest_result, truth = forecast_seq(model, val_data)\n\n1600.3889005184174 sec\n\n\n- Plot forecasted sequence vs. actual\n\nplt.plot(truth, color='red', alpha=0.7)\nplt.plot(test_result, color='blue', linewidth=0.7)\nplt.title('Actual vs Forecast')\nplt.legend(['Actual', 'Forecast'])\nplt.xlabel('Time Steps')\nplt.show()\n\n\n\n\n\n에폭 수가 너무 적어서 학습이 잘 안된 듯..\n\n- Test random sequence\n\nr = np.random.randint(100000, 160000)\ntest_forecast = model_forecast(model, csum_logreturn[r: r+10]) # random 10 sequence length\n\nprint(f\"forecast sequence: {test_forecast}\")\nprint(f\"Actual sequence: {csum_logreturn[r: r+11]}\")\n\nforecast sequence: [ 0.45956996  0.45955116  0.45804468  0.45834616  0.46043515  0.45947587\n  0.45996502  0.45966405  0.46049154  0.45876053 -0.08363973]\nActual sequence: [0.45956996 0.45955115 0.45804467 0.45834615 0.46043515 0.45947588\n 0.45996503 0.45966404 0.46049154 0.45876053 0.45747915]\n\n\n\nref: https://github.com/ctxj/Time-Series-Transformer-Pytorch/tree/main"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html",
    "href": "posts/7_study/6_as2023/2023-09-11.html",
    "title": "2wk 파이썬",
    "section": "",
    "text": "나머지 연산, 조건문\n\n\ndef main():\n    # a = int(input(\"숫자를 입력하세요.\"))\n    number = 3\n    if num % 2 == 0:\n        # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\n        if number % 10 in [2, 4, 5, 9]:\n            print(f\"{number}는 짝수입니다.\")\n        else:\n            print(f\"{number}은 짝수입니다.\")\n    else:\n        if number % 10 in [2, 4, 5, 9]: # 는\n            print(f\"{number}는 홀수입니다.\")\n        else:\n            print(f\"{number}은 홀수입니다.\")\n            \n# if __name__ == \"__main__\":\n#     main()\n\n\nnumber = 3\nif number % 2 == 0:\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\n    if number % 10 in [2, 4, 5, 9]:\n        print(f\"{number}는 짝수입니다.\")\n    else:\n        print(f\"{number}은 짝수입니다.\")\nelse:\n    if number % 10 in [2, 4, 5, 9]: # 는\n        print(f\"{number}는 홀수입니다.\")\n    else:\n        print(f\"{number}은 홀수입니다.\")\n\n3은 홀수입니다."
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#홀짝-구분",
    "href": "posts/7_study/6_as2023/2023-09-11.html#홀짝-구분",
    "title": "2wk 파이썬",
    "section": "",
    "text": "나머지 연산, 조건문\n\n\ndef main():\n    # a = int(input(\"숫자를 입력하세요.\"))\n    number = 3\n    if num % 2 == 0:\n        # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\n        if number % 10 in [2, 4, 5, 9]:\n            print(f\"{number}는 짝수입니다.\")\n        else:\n            print(f\"{number}은 짝수입니다.\")\n    else:\n        if number % 10 in [2, 4, 5, 9]: # 는\n            print(f\"{number}는 홀수입니다.\")\n        else:\n            print(f\"{number}은 홀수입니다.\")\n            \n# if __name__ == \"__main__\":\n#     main()\n\n\nnumber = 3\nif number % 2 == 0:\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\n    if number % 10 in [2, 4, 5, 9]:\n        print(f\"{number}는 짝수입니다.\")\n    else:\n        print(f\"{number}은 짝수입니다.\")\nelse:\n    if number % 10 in [2, 4, 5, 9]: # 는\n        print(f\"{number}는 홀수입니다.\")\n    else:\n        print(f\"{number}은 홀수입니다.\")\n\n3은 홀수입니다."
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#단위변환-온도-길이-등",
    "href": "posts/7_study/6_as2023/2023-09-11.html#단위변환-온도-길이-등",
    "title": "2wk 파이썬",
    "section": "2. 단위변환 (온도, 길이 등)",
    "text": "2. 단위변환 (온도, 길이 등)\n\n함수, 포맷팅\n\n\ndef f2c(temp_f):\n    return (temp_f - 32) * 5 / 9\n\ndef main():\n    # 두 줄\n    temp_f = 80\n    temp_c = f2c(temp_f)\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\n    \n    # 한 줄\n    print(f\"{temp_f}F =&gt; {f2c(temp_f):.2f}C\")\n\n\ntemp_f = 80\ntemp_c = f2c(temp_f)\nprint(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\n\n# 한 줄\nprint(f\"{temp_f}F =&gt; {f2c(temp_f):.2f}C\")\n\n80F =&gt; 26.67C\n80F =&gt; 26.67C\n\n\n\nshift F6: num 이라고 써있는 것을 한꺼번에 바꿔준다.\n1헥타르 = 10,000 제곱미터 = 대략 3천평\n보통 밭의 크기 300평 = 1,000 제곱미터 = 0.1헥타르 = 10a(아르)"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#소수-판별하기",
    "href": "posts/7_study/6_as2023/2023-09-11.html#소수-판별하기",
    "title": "2wk 파이썬",
    "section": "3. 소수 판별하기",
    "text": "3. 소수 판별하기\n\n함수, 조건문(break 등)\n\n\ndef is_prime(num):\n    if num &lt; 2:\n        return False\n    for i in range(2, num):\n        if num % i == 0:\n            return False\n    return True\n\ndef main():\n    num = 8\n    \n    if is_prime(num):\n        print(f\"{num}은/는 소수입니다.\")\n    else:\n        print(f\"{num}은/는 소수가 아닙니다.\")\n\n\nnum = 8\n\nif is_prime(num):\n    print(f\"{num} 은/는 소수입니다.\")\nelse:\n    print(f\"{num} 은/는 소수가 아닙니다.\")\n\n8 은/는 소수가 아닙니다.\n\n\n- 에라토스테네스의 체"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#소수-구하기",
    "href": "posts/7_study/6_as2023/2023-09-11.html#소수-구하기",
    "title": "2wk 파이썬",
    "section": "4. 소수 구하기",
    "text": "4. 소수 구하기\n\n함수, 반복문\n\n\ndef main():\n    list_prime = [x for x in range(1, 1000) if is_prime(x)]\n    print(f\"1-100까지 중 소수는 {list_prime}입니다.\")\n    print(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")\n\n\nlist_prime = [x for x in range(1, 1000) if is_prime(x)]\nprint(f\"1-100까지 중 소수는 {list_prime}입니다.\")\nprint(f\"1-100까지 중 소수의 개수는 {len(list_prime)}입니다.\")\n\n1-100까지 중 소수는 [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997]입니다.\n1-100까지 중 소수의 개수는 168입니다."
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#팩토리얼-구하기",
    "href": "posts/7_study/6_as2023/2023-09-11.html#팩토리얼-구하기",
    "title": "2wk 파이썬",
    "section": "5. 팩토리얼 구하기",
    "text": "5. 팩토리얼 구하기\n\n재귀함수\n\n\n# factorial\ndef fact(num):\n    result = 1\n    for i in range(1, num+1):\n        result = result * i\n    return result\n\ndef main():\n    print(f\"{10}!은 {fact(10)}입니다.\")\n    \nif __name__ == \"__main__\":\n    main()\n\n10!은 3628800입니다.\n\n\n\ndef fact(num):\n    if num == 1:\n        return 1\n    return num * fact(num - 1) # 재귀식 이용\n\n\nfact(3)\n\n6\n\n\n- 재귀함수\n구글에 Recursion 검색.. &gt; recursion의 의미를 알 수 있음. (구글의 장난)\nchat : 재귀 함수(Recursive function)는 자기 자신을 호출하는 함수입니다. 다시 말해, 함수 내에서 함수 자신을 호출하는 것을 의미합니다\n- 점화식\n\\(F_n = F_{n-1} + F_{n-2}, \\quad F_0=0, F_1=1\\)"
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#까지-짝수-합",
    "href": "posts/7_study/6_as2023/2023-09-11.html#까지-짝수-합",
    "title": "2wk 파이썬",
    "section": "6. 1-100까지 짝수 합",
    "text": "6. 1-100까지 짝수 합\n\n반복문, 조건문\n\n- 지능형 리스트 (=리스트 컴프리헨션)\n [x for x in ~~]\n\ndef is_even(a):\n    return a % 2 == 0\n\ndef main():\n    evens = [x for x in range(1, 101) if is_even(x)]  # 뒤에 if가 붙으면 filter역할\n    sum_even = sum(evens)\n    \n    print(f\"1-100까지 숫자 중 짝수의 합은 {sum_even}입니다.\")\n\n\nmain()\n\n1-100까지 숫자 중 짝수의 합은 2550입니다."
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#예외처리",
    "href": "posts/7_study/6_as2023/2023-09-11.html#예외처리",
    "title": "2wk 파이썬",
    "section": "예외처리",
    "text": "예외처리\n코드 잘 짜고 에러가 안나는데 돌릴 때 에러가 나는 경우.\n\nwhile True:\n    try:\n        x = int(input('Please enter a number: '))\n        break # 에러가 안나면 break!\n    except ValueError:\n        print('Oops! That was no valid number. Try Again.')\n\nPlease enter a number:  d\nPlease enter a number:  1\n\n\nOops! That was no valid number. Try Again."
  },
  {
    "objectID": "posts/7_study/6_as2023/2023-09-11.html#gui",
    "href": "posts/7_study/6_as2023/2023-09-11.html#gui",
    "title": "2wk 파이썬",
    "section": "GUI",
    "text": "GUI\nimport tkinter as tk\nfrom tkinter import simpledialog\n\n# ROOT = tk.Tk()\n\n# ROOT.withdraw()\n\ndef simple_gui_input(text=\"값을 입력하세요.\"): # 값을 입력하지 않으면 디폴트로 표시\n    return simpledialog.askstring(title='GUI 창', # title도 이런식으로 바꿀 수 있다.\n                                   prompt = text)\n\nif __name__ == \"__main__\":\n    user_input = simple_gui_input(\"첫번째 숫자를 입력해주세요\")\n    user_input2 = simple_gui_input(\"두번째 숫자를 입력해주세요\")\n    \n    print(f\"입력된 값은 {user_iuput}와 {user_input2}\")\n\n    \n# USER_INP = simpledialog.askstring(title='GUI 창',\n#                                   prompt = \"숫자를 입력해주세요.\")\n\n# print(f\"입력된 값은 {user_input}\")\n# 소수판별기\ndef main():\n    num = int(input(\"숫자를 입력하세요.\"))\n    \n    if is_prime(num):\n        print(f\"{num}은/는 소수입니다.\")\n        \n    ....\n\nGUI버전으로 바꿔보기.\n\n\nfrom rich import print\n\nprint(\"Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())\n\n# run &gt; edit~~ / edit Emulate? 체크해주면 된다. (pycharm에서) -- 에디터 특성.\n\nHello, [bold magentalWorld/bold magental! 🧛\n{\n    '__name__': '__main__',\n    '__doc__': 'Automatically created module for IPython interactive environment',\n    '__package__': None,\n    '__loader__': None,\n    '__spec__': None,\n    '__builtin__': &lt;module 'builtins' (built-in)&gt;,\n    '__builtins__': &lt;module 'builtins' (built-in)&gt;,\n    '_ih': [\n        '',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}\\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\\n        \\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")\\n        \\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")',\n        'main(5)',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은/는 홀수입니다.\")',\n        'main',\n        'main()',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        if \nnumber % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n            print(f\"{number}는 짝수입니다.\")\\n       \nelse:\\n            print(f\"{number}은 짝수입니다.\\n        print(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        if number % 10 in [2, 4, 5, 9]: # 는\\n            \nprint(f\"{number}는 홀수입니다.)\\n        else:\\n            print(f\"{number}은 홀수입니다.\")\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")',\n        'main',\n        'main(3)',\n        'main',\n        'main.item()',\n        'main',\n        'main.main',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"}',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f C})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}C\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":\\n    main()',\n        'def is_prime(num):\\n    for i in range(2, num):\\n        if num % i == 0:\\n            return False\\n    \nreturn True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n        print(f\"{num}은/는 소수입니다.\")\\n   \nelse:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\n# if __name__ == \"__main__\":\\n#     main()',\n        'number = 3\\nif num % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n    \nif number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은 \n짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 홀수입니다.\")\\n    \nelse:\\n        print(f\"{number}은 홀수입니다.\")',\n        'number = 3\\nif number % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n  \nif number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은 \n짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 홀수입니다.\")\\n    \nelse:\\n        print(f\"{number}은 홀수입니다.\")',\n        'def is_prime(num):\\n    if num &lt; 2:\\n        return False\\n    for i in range(2, num):\\n        if num % i\n== 0:\\n            return False\\n    return True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n        \nprint(f\"{num}은/는 소수입니다.\")\\n    else:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n        'temp_f = 80\\ntemp_c = f2c(temp_f)\\nprint(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n\\n# 한 줄\\nprint(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}C\")',\n        'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num}은/는 소수입니다.\")\\nelse:\\n    print(f\"{num}은/는 소수가 \n아닙니다.\")',\n        'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num} 은/는 소수입니다.\")\\nelse:\\n    print(f\"{num} 은/는 소수가\n아닙니다.\")',\n        'def main():\\n    list_prime = [x for x in range(1, 1000) if is_prime(x)]\\n    print(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\n    print(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n        'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n        'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {len(list_prime)}입니다.\")',\n        'main()',\n        'def fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n    return \nresult',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result',\n        'def fact(num):\\n    if num == 1:\\n        return 1\\n    return num * fact(num - 1)',\n        'fact(3)',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{num}!dms {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{num}!은 {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{10}!은 {fact(10)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        'def is_even(a):\\n    return a % 2 == 0\\n\\ndef main():\\n    evens = [x for x in range(1, 101) if \nis_even(x)]  # 뒤에 if가 붙으면 filter역할\\n    sum_even = sum(evens)\\n    \\n    print(f\"1-100까지 숫자 중 짝수의 \n합은 {sum_even}입니다.\")',\n        'main()',\n        'var1 = 34\\nvar2 = \"p1234n\"\\nvar6 = 35.1\\n\\n# numbers\\nprint(isinstance(var1, int))\\nprint(isinstance(var6,\nfloat))\\nprint(var1 &lt; 35)\\nprint(var1 &lt;= var6)',\n        \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n        \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n        'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()',\n        \"get_ipython().system('pip install tkinter')\",\n        'import tkinter as tk',\n        'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = \ntk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n                                  \nprompt = \"What\\'s your name?:\")\\n\\nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP = \nsimpledialog.askstring(title=\\'Test\\',\\n                                  prompt = \"What\\'s your \nname?:\")\\n\\nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# USER_INP \n= simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your name?:\")\\n\\n# \nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# \nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your \nname?:\")\\n\\n# print(\"Hello\", USER_INP)',\n        'echo $DISPLAY',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n\\ndef \nsimple_gui_input():\\n    return simpledialog.askstring(title=\\'GUI 창\\',\\n                                   prompt\n= \"숫자를 입력해주세요.\")\\n\\nif __name__ == \"__main__\":\\n    user_input = simple_gui_input()\\n    user_input2 = \nsimple_gui_input()\\n    \\n    print(f\"입력된 값은 {user_iuput}와 {user_input2}\")\\n\\n    \\n# USER_INP = \nsimpledialog.askstring(title=\\'GUI 창\\',\\n#                                   prompt = \"숫자를 입력해주세요.\")\\n\\n#\nprint(f\"입력된 값은 {user_input}\")',\n        '# 소수판별기\\ndef main():\\n    num = int(input(',\n        \"get_ipython().system('pip install rich')\",\n        'from rich import print\\n\\nprint(\\'Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())',\n        'from rich import print\\n\\nprint(\"Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())'\n    ],\n    '_oh': {\n        7: &lt;function main at 0x7f808835f4c0&gt;,\n        13: &lt;function main at 0x7f807a7ea430&gt;,\n        15: &lt;function main at 0x7f807a7ea430&gt;,\n        17: &lt;function main at 0x7f807a7ea430&gt;,\n        42: 6\n    },\n    '_dh': [PosixPath('/home/jy/Dropbox/임지윤/Quarto-Blog/posts/5_study')],\n    'In': [\n        '',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}\\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\\n        \\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")\\n        \\n    ',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")',\n        'main(5)',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은/는 홀수입니다.\")',\n        'main',\n        'main()',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        if \nnumber % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n            print(f\"{number}는 짝수입니다.\")\\n       \nelse:\\n            print(f\"{number}은 짝수입니다.\\n        print(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        if number % 10 in [2, 4, 5, 9]: # 는\\n            \nprint(f\"{number}는 홀수입니다.)\\n        else:\\n            print(f\"{number}은 홀수입니다.\")\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")',\n        'main',\n        'main(3)',\n        'main',\n        'main.item()',\n        'main',\n        'main.main',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"}',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f\\'C\"})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f C})',\n        'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}C\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":\\n    main()',\n        'def is_prime(num):\\n    for i in range(2, num):\\n        if num % i == 0:\\n            return False\\n    \nreturn True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n        print(f\"{num}은/는 소수입니다.\")\\n   \nelse:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n        'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        # \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n         \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\n# if __name__ == \"__main__\":\\n#     main()',\n        'number = 3\\nif num % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n    \nif number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은 \n짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 홀수입니다.\")\\n    \nelse:\\n        print(f\"{number}은 홀수입니다.\")',\n        'number = 3\\nif number % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n  \nif number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은 \n짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 홀수입니다.\")\\n    \nelse:\\n        print(f\"{number}은 홀수입니다.\")',\n        'def is_prime(num):\\n    if num &lt; 2:\\n        return False\\n    for i in range(2, num):\\n        if num % i\n== 0:\\n            return False\\n    return True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n        \nprint(f\"{num}은/는 소수입니다.\")\\n    else:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n        'temp_f = 80\\ntemp_c = f2c(temp_f)\\nprint(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n\\n# 한 줄\\nprint(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}C\")',\n        'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num}은/는 소수입니다.\")\\nelse:\\n    print(f\"{num}은/는 소수가 \n아닙니다.\")',\n        'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num} 은/는 소수입니다.\")\\nelse:\\n    print(f\"{num} 은/는 소수가\n아닙니다.\")',\n        'def main():\\n    list_prime = [x for x in range(1, 1000) if is_prime(x)]\\n    print(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\n    print(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n        'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n        'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {len(list_prime)}입니다.\")',\n        'main()',\n        'def fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n    return \nresult',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result',\n        'def fact(num):\\n    if num == 1:\\n        return 1\\n    return num * fact(num - 1)',\n        'fact(3)',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{num}!dms {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{num}!은 {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n \nreturn result\\n\\ndef main():\\n    print(f\"{10}!은 {fact(10)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n    \nmain()',\n        'def is_even(a):\\n    return a % 2 == 0\\n\\ndef main():\\n    evens = [x for x in range(1, 101) if \nis_even(x)]  # 뒤에 if가 붙으면 filter역할\\n    sum_even = sum(evens)\\n    \\n    print(f\"1-100까지 숫자 중 짝수의 \n합은 {sum_even}입니다.\")',\n        'main()',\n        'var1 = 34\\nvar2 = \"p1234n\"\\nvar6 = 35.1\\n\\n# numbers\\nprint(isinstance(var1, int))\\nprint(isinstance(var6,\nfloat))\\nprint(var1 &lt; 35)\\nprint(var1 &lt;= var6)',\n        \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n        \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n        'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()',\n        \"get_ipython().system('pip install tkinter')\",\n        'import tkinter as tk',\n        'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = \ntk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n                                  \nprompt = \"What\\'s your name?:\")\\n\\nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP = \nsimpledialog.askstring(title=\\'Test\\',\\n                                  prompt = \"What\\'s your \nname?:\")\\n\\nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# USER_INP \n= simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your name?:\")\\n\\n# \nprint(\"Hello\", USER_INP)',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# \nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your \nname?:\")\\n\\n# print(\"Hello\", USER_INP)',\n        'echo $DISPLAY',\n        'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n\\ndef \nsimple_gui_input():\\n    return simpledialog.askstring(title=\\'GUI 창\\',\\n                                   prompt\n= \"숫자를 입력해주세요.\")\\n\\nif __name__ == \"__main__\":\\n    user_input = simple_gui_input()\\n    user_input2 = \nsimple_gui_input()\\n    \\n    print(f\"입력된 값은 {user_iuput}와 {user_input2}\")\\n\\n    \\n# USER_INP = \nsimpledialog.askstring(title=\\'GUI 창\\',\\n#                                   prompt = \"숫자를 입력해주세요.\")\\n\\n#\nprint(f\"입력된 값은 {user_input}\")',\n        '# 소수판별기\\ndef main():\\n    num = int(input(',\n        \"get_ipython().system('pip install rich')\",\n        'from rich import print\\n\\nprint(\\'Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())',\n        'from rich import print\\n\\nprint(\"Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())'\n    ],\n    'Out': {\n        7: &lt;function main at 0x7f808835f4c0&gt;,\n        13: &lt;function main at 0x7f807a7ea430&gt;,\n        15: &lt;function main at 0x7f807a7ea430&gt;,\n        17: &lt;function main at 0x7f807a7ea430&gt;,\n        42: 6\n    },\n    'get_ipython': &lt;bound method InteractiveShell.get_ipython of &lt;ipykernel.zmqshell.ZMQInteractiveShell object at \n0x7f8089ca0eb0&gt;&gt;,\n    'exit': &lt;IPython.core.autocall.ZMQExitAutocall object at 0x7f8089ca97f0&gt;,\n    'quit': &lt;IPython.core.autocall.ZMQExitAutocall object at 0x7f8089ca97f0&gt;,\n    'open': &lt;function open at 0x7f808c7f2700&gt;,\n    '_': 6,\n    '__': &lt;function main at 0x7f807a7ea430&gt;,\n    '___': &lt;function main at 0x7f807a7ea430&gt;,\n    '_i': 'from rich import print\\n\\nprint(\\'Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())',\n    '_ii': '!pip install rich',\n    '_iii': '# 소수판별기\\ndef main():\\n    num = int(input(',\n    '_i1': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}\\n    ',\n    '_i2': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    a = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\\n        \\n    ',\n    '_i3': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")\\n        \\n    ',\n    'main': &lt;function main at 0x7f807a3e65e0&gt;,\n    '_i4': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    num = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{num}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{num}은/는 홀수입니다.\")',\n    '_i5': 'main(5)',\n    '_i6': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은/는 홀수입니다.\")',\n    '_i7': 'main',\n    '_7': &lt;function main at 0x7f808835f4c0&gt;,\n    '_i8': 'main()',\n    '_i9': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n        \nif number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n            print(f\"{number}는 짝수입니다.\")\\n    \nelse:\\n            print(f\"{number}은 짝수입니다.\\n        print(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n    '_i10': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n       \n# if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n       \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n        \nprint(f\"{number}은/는 짝수입니다.\")\\n    else:\\n        if number % 10 in [2, 4, 5, 9]: # 는\\n            \nprint(f\"{number}는 홀수입니다.)\\n        else:\\n            print(f\"{number}은 홀수입니다.\")\\n        \nprint(f\"{number}은/는 홀수입니다.\")',\n    '_i11': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n       \n# if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n       \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":',\n    '_i12': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n       \n# if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n       \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")',\n    '_i13': 'main',\n    '_13': &lt;function main at 0x7f807a7ea430&gt;,\n    '_i14': 'main(3)',\n    '_i15': 'main',\n    '_15': &lt;function main at 0x7f807a7ea430&gt;,\n    '_i16': 'main.item()',\n    '_i17': 'main',\n    '_17': &lt;function main at 0x7f807a7ea430&gt;,\n    '_i18': 'main.main',\n    '_i19': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9',\n    'f2c': &lt;function f2c at 0x7f807a4b7d30&gt;,\n    '_i20': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"}',\n    '_i21': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f}:.2f}\\'C\"})',\n    '_i22': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}\\'C\"})',\n    '_i23': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}\\'C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f\\'C\"})',\n    '_i24': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f C})',\n    '_i25': 'def f2c(temp_f):\\n    return (temp_f - 32) * 5 / 9\\n\\ndef main():\\n    # 두 줄\\n    temp_f = 80\\n    \ntemp_c = f2c(temp_f)\\n    print(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n    \\n    # 한 줄\\n    print(f\"{temp_f}F =&gt; \n{f2c(temp_f):.2f}C\")',\n    '_i26': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n       \n# if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n       \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\nif __name__ == \"__main__\":\\n    main()',\n    '_i27': 'def is_prime(num):\\n    for i in range(2, num):\\n        if num % i == 0:\\n            return False\\n \nreturn True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n        print(f\"{num}은/는 소수입니다.\")\\n   \nelse:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n    'is_prime': &lt;function is_prime at 0x7f807a3e6280&gt;,\n    '_i28': 'def main():\\n    # a = int(input(\"숫자를 입력하세요.\"))\\n    number = 3\\n    if num % 2 == 0:\\n       \n# if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n        if number % 10 in [2, 4, 5, 9]:\\n       \nprint(f\"{number}는 짝수입니다.\")\\n        else:\\n            print(f\"{number}은 짝수입니다.\")\\n    else:\\n        \nif number % 10 in [2, 4, 5, 9]: # 는\\n            print(f\"{number}는 홀수입니다.\")\\n        else:\\n            \nprint(f\"{number}은 홀수입니다.\")\\n            \\n# if __name__ == \"__main__\":\\n#     main()',\n    '_i29': 'number = 3\\nif num % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == 5:#는\\n \nif number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        print(f\"{number}은 \n짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 홀수입니다.\")\\n    \nelse:\\n        print(f\"{number}은 홀수입니다.\")',\n    'number': 3,\n    '_i30': 'number = 3\\nif number % 2 == 0:\\n    # if number % 10 == 2 or number % 10 == 4 or number % 10 == \n5:#는\\n    if number % 10 in [2, 4, 5, 9]:\\n        print(f\"{number}는 짝수입니다.\")\\n    else:\\n        \nprint(f\"{number}은 짝수입니다.\")\\nelse:\\n    if number % 10 in [2, 4, 5, 9]: # 는\\n        print(f\"{number}는 \n홀수입니다.\")\\n    else:\\n        print(f\"{number}은 홀수입니다.\")',\n    '_i31': 'def is_prime(num):\\n    if num &lt; 2:\\n        return False\\n    for i in range(2, num):\\n        if num\n% i == 0:\\n            return False\\n    return True\\n\\ndef main():\\n    num = 8\\n    \\n    if is_prime(num):\\n    \nprint(f\"{num}은/는 소수입니다.\")\\n    else:\\n        print(f\"{num}은/는 소수가 아닙니다.\")',\n    '_i32': 'temp_f = 80\\ntemp_c = f2c(temp_f)\\nprint(f\"{temp_f}F =&gt; {temp_c:.2f}C\")\\n\\n# 한 줄\\nprint(f\"{temp_f}F \n=&gt; {f2c(temp_f):.2f}C\")',\n    'temp_f': 80,\n    'temp_c': 26.666666666666668,\n    '_i33': 'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num}은/는 소수입니다.\")\\nelse:\\n    print(f\"{num}은/는 \n소수가 아닙니다.\")',\n    'num': 8,\n    '_i34': 'num = 8\\n\\nif is_prime(num):\\n    print(f\"{num} 은/는 소수입니다.\")\\nelse:\\n    print(f\"{num} 은/는 \n소수가 아닙니다.\")',\n    '_i35': 'def main():\\n    list_prime = [x for x in range(1, 1000) if is_prime(x)]\\n    print(f\"1-100까지 중 \n소수는 {list_prime}입니다.\")\\n    print(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n    '_i36': 'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {list_prime}입니다.\")',\n    'list_prime': [\n        2,\n        3,\n        5,\n        7,\n        11,\n        13,\n        17,\n        19,\n        23,\n        29,\n        31,\n        37,\n        41,\n        43,\n        47,\n        53,\n        59,\n        61,\n        67,\n        71,\n        73,\n        79,\n        83,\n        89,\n        97,\n        101,\n        103,\n        107,\n        109,\n        113,\n        127,\n        131,\n        137,\n        139,\n        149,\n        151,\n        157,\n        163,\n        167,\n        173,\n        179,\n        181,\n        191,\n        193,\n        197,\n        199,\n        211,\n        223,\n        227,\n        229,\n        233,\n        239,\n        241,\n        251,\n        257,\n        263,\n        269,\n        271,\n        277,\n        281,\n        283,\n        293,\n        307,\n        311,\n        313,\n        317,\n        331,\n        337,\n        347,\n        349,\n        353,\n        359,\n        367,\n        373,\n        379,\n        383,\n        389,\n        397,\n        401,\n        409,\n        419,\n        421,\n        431,\n        433,\n        439,\n        443,\n        449,\n        457,\n        461,\n        463,\n        467,\n        479,\n        487,\n        491,\n        499,\n        503,\n        509,\n        521,\n        523,\n        541,\n        547,\n        557,\n        563,\n        569,\n        571,\n        577,\n        587,\n        593,\n        599,\n        601,\n        607,\n        613,\n        617,\n        619,\n        631,\n        641,\n        643,\n        647,\n        653,\n        659,\n        661,\n        673,\n        677,\n        683,\n        691,\n        701,\n        709,\n        719,\n        727,\n        733,\n        739,\n        743,\n        751,\n        757,\n        761,\n        769,\n        773,\n        787,\n        797,\n        809,\n        811,\n        821,\n        823,\n        827,\n        829,\n        839,\n        853,\n        857,\n        859,\n        863,\n        877,\n        881,\n        883,\n        887,\n        907,\n        911,\n        919,\n        929,\n        937,\n        941,\n        947,\n        953,\n        967,\n        971,\n        977,\n        983,\n        991,\n        997\n    ],\n    '_i37': 'list_prime = [x for x in range(1, 1000) if is_prime(x)]\\nprint(f\"1-100까지 중 소수는 \n{list_prime}입니다.\")\\nprint(f\"1-100까지 중 소수의 개수는 {len(list_prime)}입니다.\")',\n    '_i38': 'main()',\n    '_i39': 'def fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * i\\n    return\nresult',\n    'fact': &lt;function fact at 0x7f807a3e6670&gt;,\n    '_i40': '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * \ni\\n    return result',\n    '_i41': 'def fact(num):\\n    if num == 1:\\n        return 1\\n    return num * fact(num - 1)',\n    '_i42': 'fact(3)',\n    '_42': 6,\n    '_i43': '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * \ni\\n    return result\\n\\ndef main():\\n    print(f\"{num}!dms {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n\nmain()',\n    '_i44': '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * \ni\\n    return result\\n\\ndef main():\\n    print(f\"{num}!은 {fact(num)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n \nmain()',\n    '_i45': '# factorial\\ndef fact(num):\\n    result = 1\\n    for i in range(1, num+1):\\n        result = result * \ni\\n    return result\\n\\ndef main():\\n    print(f\"{10}!은 {fact(10)}입니다.\")\\n    \\nif __name__ == \"__main__\":\\n   \nmain()',\n    '_i46': 'def is_even(a):\\n    return a % 2 == 0\\n\\ndef main():\\n    evens = [x for x in range(1, 101) if \nis_even(x)]  # 뒤에 if가 붙으면 filter역할\\n    sum_even = sum(evens)\\n    \\n    print(f\"1-100까지 숫자 중 짝수의 \n합은 {sum_even}입니다.\")',\n    'is_even': &lt;function is_even at 0x7f807a3e6550&gt;,\n    '_i47': 'main()',\n    '_i48': 'var1 = 34\\nvar2 = \"p1234n\"\\nvar6 = 35.1\\n\\n# numbers\\nprint(isinstance(var1, \nint))\\nprint(isinstance(var6, float))\\nprint(var1 &lt; 35)\\nprint(var1 &lt;= var6)',\n    'var1': 34,\n    'var2': 'p1234n',\n    'var6': 35.1,\n    '_i49': \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n    'x': 1,\n    '_i50': \"while True:\\n    try:\\n        x = int(input('Please enter a number: '))\\n        break\\n    except \nValueError:\\n        print('Oops! That was no valid number. Try Again.')\",\n    '_i51': 'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()',\n    'tk': &lt;module 'tkinter' from '/home/jy/anaconda3/envs/torch/lib/python3.8/tkinter/__init__.py'&gt;,\n    '_i52': '!pip install tkinter',\n    '_exit_code': 0,\n    '_i53': 'import tkinter as tk',\n    '_i54': 'import tkinter as tk\\nfrom tkinter import simpledialogpledialog\\n\\nROOT = \ntk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n                                  \nprompt = \"What\\'s your name?:\")\\n\\nprint(\"Hello\", USER_INP)',\n    '_i55': 'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\nROOT.withdraw()\\nUSER_INP \n= simpledialog.askstring(title=\\'Test\\',\\n                                  prompt = \"What\\'s your \nname?:\")\\n\\nprint(\"Hello\", USER_INP)',\n    'simpledialog': &lt;module 'tkinter.simpledialog' from \n'/home/jy/anaconda3/envs/torch/lib/python3.8/tkinter/simpledialog.py'&gt;,\n    '_i56': 'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\nROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# \nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your \nname?:\")\\n\\n# print(\"Hello\", USER_INP)',\n    '_i57': 'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n# \nUSER_INP = simpledialog.askstring(title=\\'Test\\',\\n#                                   prompt = \"What\\'s your \nname?:\")\\n\\n# print(\"Hello\", USER_INP)',\n    '_i58': 'echo $DISPLAY',\n    '_i59': 'import tkinter as tk\\nfrom tkinter import simpledialog\\n\\n# ROOT = tk.Tk()\\n\\n# ROOT.withdraw()\\n\\ndef\nsimple_gui_input():\\n    return simpledialog.askstring(title=\\'GUI 창\\',\\n                                   prompt\n= \"숫자를 입력해주세요.\")\\n\\nif __name__ == \"__main__\":\\n    user_input = simple_gui_input()\\n    user_input2 = \nsimple_gui_input()\\n    \\n    print(f\"입력된 값은 {user_iuput}와 {user_input2}\")\\n\\n    \\n# USER_INP = \nsimpledialog.askstring(title=\\'GUI 창\\',\\n#                                   prompt = \"숫자를 입력해주세요.\")\\n\\n#\nprint(f\"입력된 값은 {user_input}\")',\n    'simple_gui_input': &lt;function simple_gui_input at 0x7f807879a040&gt;,\n    '_i60': '# 소수판별기\\ndef main():\\n    num = int(input(',\n    '_i61': '!pip install rich',\n    '_i62': 'from rich import print\\n\\nprint(\\'Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())',\n    '_i63': 'from rich import print\\n\\nprint(\"Hello, [bold magentalWorld/bold magental!\", \":vampire:\", locals())',\n    'print': &lt;function print at 0x7f807889b9d0&gt;\n}\n\n\n\n예제 6개, 예제 1개?"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-14-graph.html",
    "href": "posts/7_study/7_ds2023/2023-09-14-graph.html",
    "title": "2wk-2 그래프",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-14-graph.html#import",
    "href": "posts/7_study/7_ds2023/2023-09-14-graph.html#import",
    "title": "2wk-2 그래프",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-14-graph.html#막대그래프",
    "href": "posts/7_study/7_ds2023/2023-09-14-graph.html#막대그래프",
    "title": "2wk-2 그래프",
    "section": "막대그래프",
    "text": "막대그래프\n\n# Make a dataset 지게차 톤수별 1회 평균 주유량\noil = [30, 35, 40, 42, 45]\nton = ['5TON', '7TON', '10TON', '11TON', '15TON']\ny_pos = np.arange(len(ton))\n\n# 막대그래프 만들기\nplt.bar(y_pos, oil)\n# 색상 변경 옵션\n# plt.bar(y_pos, oil, color='g')\n# plt.bar(y_pos, oil, color=['black', 'red', 'green', 'blue', 'cyan'])\n\n# 축이름 바꾸기\nplt.xticks(y_pos, ton)\n\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Look at index 4 and 6, which demonstrate overlapping cases.\nx1 = [1, 3, 4, 5, 6, 7, 9]\ny1 = [4, 7, 2, 4, 7, 8, 3]\n\nx2 = [2, 4, 6, 8, 10]\ny2 = [5, 6, 2, 6, 2]\n\n# Colors: https://matplotlib.org/api/colors_api.html\n\nplt.bar(x1, y1, label=\"Blue Bar\", color='b')\nplt.bar(x2, y2, label=\"Green Bar\", color='g')\nplt.plot()\n\nplt.xlabel(\"bar number\")\nplt.ylabel(\"bar height\")\nplt.title(\"Bar Chart Example\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-14-graph.html#선그래프",
    "href": "posts/7_study/7_ds2023/2023-09-14-graph.html#선그래프",
    "title": "2wk-2 그래프",
    "section": "선그래프",
    "text": "선그래프\n\nimport matplotlib.pyplot as plt\n\nx  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\ny1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\ny2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\nplt.plot(x, y1, label=\"line L\")\nplt.plot(x, y2, label=\"line H\")\nplt.plot()\n\nplt.xlabel(\"x axis\")\nplt.ylabel(\"y axis\")\nplt.title(\"Line Graph Example\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/7_study/7_ds2023/2023-09-14-graph.html#중첩-그래프",
    "href": "posts/7_study/7_ds2023/2023-09-14-graph.html#중첩-그래프",
    "title": "2wk-2 그래프",
    "section": "중첩 그래프",
    "text": "중첩 그래프\n\nimport matplotlib.pyplot as plt\n\nidxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\narr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\narr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\narr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n\n# Adding legend for stack plots is tricky.\nplt.plot([], [], color='r', label = 'D 1')\nplt.plot([], [], color='g', label = 'D 2')\nplt.plot([], [], color='b', label = 'D 3')\n\nplt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\nplt.title('Stack Plot Example')\nplt.legend()\nplt.show()\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nys = 200 + np.random.randn(100)\nx = [x for x in range(len(ys))]\n\nplt.plot(x, ys, '-')\nplt.fill_between(x, ys, 195, where=(ys &gt; 195), facecolor='g', alpha=0.6)\n\nplt.title(\"Fills and Alpha Example\")\nplt.show()"
  },
  {
    "objectID": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html",
    "href": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html",
    "title": "[수리통계학] 추정 for JY",
    "section": "",
    "text": "- 비편향추정량(UB)란 \\(\\theta\\)의 추정량 중\n\\[\\forall \\theta\\in \\Theta:~ E(\\hat{\\theta})=\\theta\\]\n를 만족하는 추정량 \\(\\hat{\\theta}\\)을 의미한다.\n- (예시) 아래와 같은 상황을 가정하자.\n\\[X_n \\overset{iid}{\\sim} N(\\theta,1)\\]\n여기에서\n\n\\(\\hat{\\theta}_1=0\\) 은 \\(\\theta=0\\) 일 경우에는 \\(E(\\hat{\\theta})=\\theta\\) 를 만족하지만 그 외의 경우에는 \\(E(\\hat{\\theta})\\neq\\theta\\) 이므로 UB가 아니다.\n\\(\\hat{\\theta}_2=X_1\\) 은 UB이다.\n\\(\\hat{\\theta}_3=\\frac{X_1+X_2}{2}\\) 역시 UB이다.\n\\(\\hat{\\theta}_4=X_1+X_2-X_3\\) 역시 UB이다.\n\\(\\hat{\\theta}_5=-99X_1+100X_2\\) 역시 UB이다.\n\\(\\hat{\\theta}_6=\\frac{X_1+0}{2}\\) 은 1과 동일한 이유로 UB가 아니다.\n\\(\\hat{\\theta}_7=\\bar{X}\\)는 UB이다.\n\\(\\hat{\\theta}_8=w_1X_1+\\dots+w_nX_n\\) ,where \\(\\sum_{i=1}^{n}w_i=1\\) 형태의 estimator는 모두 UB이다.\n\n- 최소분산비편향추정량(MVUE)란 \\(\\theta\\)에 대한 비편향추정량을 모아놓은 집합 \\(\\hat{\\Theta}_{UB}\\) 에서 최소분산을 가지는 추정량을 의미한다. MVUE를 구하는 방법은 아래와 같다.\n\n\\(\\theta\\)에 대한 모든 비편향추정량을 구한다. 즉 집합 \\(\\hat{\\Theta}_{UB}\\)를 구한다. 그리고 \\(\\forall \\hat{\\theta} \\in \\hat{\\Theta}_{UB}\\) 에 대하여 \\(V(\\hat{\\theta})\\) 를 구한 뒤 \\(V(\\hat{\\theta})\\)가 가장 작은 \\(\\hat{\\theta}\\)를 선택한다.\n\n예를들어 위의 예제에서 \\(V(\\hat{\\theta}_2)=1\\) 이고 \\(V(\\hat{\\theta}_3)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}\\) 이므로 \\(\\hat{\\theta}_3\\) 이 더 좋은 추정량이라 볼 수 있다.\n- (의문) 왜 비편향추정량만 모아서 그중에서 최소분산을 구할까?\n\n\\(\\hat{\\theta}_1\\)와 같은 추정량은 \\(V(\\hat{\\theta}_1)=0\\) 이므로 그냥 최소분산을 만족한다. 따라서 이러한 추정량은 제외해야지 게임이 성립함.\n\n- 불만: 아래의 방법으로 구하는건 거의 불가능하지 않나?\n\n\\(\\theta\\)에 대한 모든 비편향추정량을 구한다. 즉 집합 \\(\\hat{\\Theta}_{UB}\\)를 구한다. 그리고 \\(\\forall \\hat{\\theta} \\in \\hat{\\Theta}_{UB}\\) 에 대하여 \\(V(\\hat{\\theta})\\) 를 구한 뒤 \\(V(\\hat{\\theta})\\)가 가장 작은 \\(\\hat{\\theta}\\)를 선택한다.\n\n- 이론: 크래머라오 하한값(편의상 \\(L^\\star\\)이라고 하자)이라고 있는데, 이는 \\({\\Theta}_{UB}\\)에 존재하는 모든 추정량에 대한 분산의 하한값을 제공한다.1 즉 아래가 성립한다.\n\n\\(L^\\star\\) is Cramer-Rao lower bound \\(\\Rightarrow\\) \\(\\forall \\hat{\\theta} \\in {\\Theta}_{UB}:~ V(\\hat{\\theta}) \\geq L^\\star\\)\n\n역은 성립하지 않음을 주의하자. 즉 아래를 만족하는 \\(L\\)이 존재할 수 있다.\n\n\\(V(\\hat{\\theta}) \\geq L &gt; L^\\star\\) for some \\(\\hat{\\theta} \\in \\Theta_{UB}\\)\n\n- 위의 이론을 이용하면 아래의 논리전개를 펼 수 있다.\n\n\\(L^\\star\\)를 구한다.\n왠지 MVUE가 될 것 같은 \\(\\hat{\\theta}\\)을 하나 찍고 그것의 분산 \\(V(\\hat{\\theta})\\)를 구한다.\n만약에 \\(V(\\hat{\\theta})=L^\\star\\)를 만족하면 그 \\(\\hat{\\theta}\\)이 MVUE라고 주장할 수 있다.\n\n- 위의 논리전개에 대한 불만 [@ p.212]\n\n\\(V(\\hat{\\theta})=L^\\star\\) 이길 기도해야함.\n\\(\\forall \\hat{\\theta} \\in {\\Theta}_{UB}:~ V(\\hat{\\theta}) \\geq L &gt; L^\\star\\) 와 같은 \\(L\\)이 존재하는 경우는 쓸 수 없음.\n\n- 또 다른 방법: 완비충분통계량을 이용함\n\n\n아래와 같은 상황을 가정하자.\n\\[ X_1,\\dots,X_n \\overset{iid}{\\sim} P_{\\theta}\\]\n- 충분통계량(SS)의 느낌: “이 값만 기억하면 \\(\\theta\\)를 추정하는데 무난할듯”\n- 예시1: \\(X_1 \\sim N(\\theta,1)\\)\n\n\\(X_1\\)은 \\(\\theta_1\\) 의 SS. (하나밖에 없으니 그거라도 기억해야지)\n즉 \\(\\hat{\\theta}=X_1\\)은 \\(\\theta\\)의 SS\n\n- 예시2: \\(X_1,X_2 \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)는 \\(\\theta\\)의 SS (둘다 기억하면 당연히 \\(\\theta\\)를 추정함에 있어서 충분함)\n그렇지만 좀 더 생각해보면 굳이 값 두개를 기억하기보다 \\(\\frac{1}{2}(X_1+X_2)\\)의 값만 기억해도 왠지 충분할것 같음. 따라서 \\(\\hat{\\theta} = \\frac{1}{2}(X_1+X_2)\\) 역시 \\(\\theta\\)의 SS 일듯\n그런데 좀 더 생각해보니까 \\(X_1+X_2\\)의 값만 기억해도 \\(\\frac{1}{2}(X_1+X_2)\\)를 나중에 만들 수 있음 (1/2만 곱하면 되니까) 따라서 \\(X_1+X_2\\)만 기억해도 왠지 충분할 것 같음. 따라서 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- 예시3: \\(X_1,\\dots,X_n \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2,\\dots,X_n)\\)은 \\(\\theta\\)의 SS.\n하지만 \\(n\\)개의 숫자를 기억할 필요 없이 \\(\\sum_{i=1}^{n} X_i\\) 하나의 숫자만 기억해도 왠지 충분할듯. 그래서 \\(\\hat{\\theta} = \\sum_{i=1}^{n} X_i\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- SS에 대한 직관1\n\n기억할 숫자가 적을수록 유리 -&gt; MSS의 개념\n충분통계량의 1:1은 충분통계량 (\\(\\frac{1}{2}(X_1+X_2)\\)을 기억하면 충분한 상황이라면, \\(X_1+X_2\\)를 기억해도 충분하니까..)\n\n- 예시4: \\(X_1,X_2 \\sim {\\cal B}er(\\theta)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)은 \\(\\theta\\)의 SS.\n그리고 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\) SS 일듯.\n두개보다 한개가 유리하니까 둘다 SS이면 \\((X_1,X_2)\\)보다 \\(X_1+X_2\\)가 더 좋은 SS.\n\\(X_1\\)은 SS가 아닐듯. \\(p\\)를 추정함에 있어서 \\(X_1\\)만 가지고서는 충분하지 않아보임\n\\(X_2\\)도 SS가 아닐듯.\n\n왠지 충분할 것 같은 느낌의 정의\n아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n- 일반적으로\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=\\theta(1-\\theta)\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=\\theta^2\\)\n\n와 같은 확률들은 \\(\\theta\\)가 unknown일 때 하나의 숫자로 정할 수 없다. 예를들어 \\(\\theta=0\\) 이라면 아래와 같을 것이고\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=0\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=0\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=0\\)\n\n\\(\\theta=1/2\\) 이라면 아래와 같을 것이다.\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=1/4\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=1/4\\)\n\n즉 \\(X_1,X_2\\)의 결합확률분포는 \\(\\theta\\)가 변함에 따라 같이 변화한다. 이를 이용해 우리는 \\(X_1,X_2\\)의 결합확률분포에서 관찰한 샘플들을 이용하여 \\(\\theta\\)의 값을 역으로 추론한다.\n- 만약에 어떠한 “특수한 정보를 알고 있을 경우” \\(X_1,X_2\\)의 결합확률분포를 완벽하게 기술할 수 있을 때를 가정해보자.\n- 경우1: \\(\\theta\\)를 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(\\theta=1/2\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=0,X_2=1 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=1 | \\theta=1/2)=1/4\\)\n\n- 경우2: \\(X_1,X_2\\)의 realization을 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(X_1=0,X_2=1\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | X_1=0,X_2=0)=0\\)\n\\(P(X_1=0,X_2=1| X_1=0,X_2=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1,X_2=0)=1\\)\n\\(P(X_1=1,X_2=1| X_1=1,X_2=1)=0\\)\n\n- 경우3: \\((X_1+X_2)(\\omega)\\)의 realization을 알고 있을 경우. 이때도 매우 특이하게 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 있다.\ncase1: \\(X_1+X_2=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=0)=1\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=0)=0\\)\n\ncase2: \\(X_1+X_2=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=1)=0\\)\n\ncase3: \\(X_1+X_2=2\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=2)=1\\)\n\n- 경우4: \\(X_1\\)의 realization만 알고 있을 경우. 이때는 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 없다.\ncase1: \\(X_1=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=0)=1-\\theta\\)\n\\(P(X_1=0,X_2=1| X_1=0)=\\theta\\)\n\\(P(X_1=1,X_2=0| X_1=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1=0)=0\\)\n\ncase2: \\(X_1=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1)=1-\\theta\\)\n\\(P(X_1=1,X_2=1| X_1=1)=\\theta\\)\n\n- 종합해보면 경우1,경우2,경우3은 경우4와 구분되는 어떠한 공통점을 가지고 있다 볼 수 있다. 특징은 결합확률분포가 \\(\\theta\\)에 대한 함수로 표현되지 않는다는 것이다. 하나씩 살펴보면\n\n경우1: 당연히 \\(\\theta\\)를 줬으니까 \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우2: \\(X_1,X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우3: \\(X_1+X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n\n이렇게보면 경우1과 경우2,3은 또 다시 구분된다. 경우1은 \\(\\theta\\)에 대한 완전한 정보를 준 상황이므로 당연히 조인트는 \\(\\theta\\)에 의존하지 않는다. 경우2-3은 \\(\\theta\\)를 주지 않았음에도 조인트가 \\(\\theta\\)에 의존하지 않는 매우 특별해보이는 상황이다. 따라서 이를 통해서 유추하면\n\n경우2에서는 \\((X_1,X_2)\\) 가 경우3에서는 \\(X_1+X_2\\)가 \\(\\theta\\)에 대한 완전한 정보를 대신하고 있는것 아닐까?\n\n라는 생각이 든다. 정리하면\n\n경우2: \\((X_1,X_2)\\)을 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n경우3: \\(X_1+X_2\\)를 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n\n라고 해석할 수 있는데 이를 수식화 하면 아래와 같다.\n- 대충정의: 어떠한 통계량 \\(S\\)의 값을 줬을때, \\((X_1,X_2\\dots,X_n)\\)의 조인트가 \\(\\theta\\)에 의존하지 않으면 그 통계량 \\(S\\)를 \\(\\theta\\)의 충분통계량이라고 한다.\n- 충분통계량 구하는 방법\n\n지수족일때 구하는 방식이 있음! &lt;– 외우세여\n분해정리를 쓰는 경우. &lt;– 거의 안쓰는거같은데..\n1-2로도 잘 모르겠으면 충분통계량일듯한 애를 잡아와서 정의에 넣고 노가다로 때려맞춤. (문제가 디스크릿할때만 쓸것)\n\n\n\n\n- 충분통계량에 대한 realization을 알려주면 \\(\\theta\\)의 값을 그냥 알려주는 효과임. 그래서 충분통계량은 좋은 것임\n- 그런데 충분통계량에도 급이 있음. 아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n이 경우\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n이지만 1은 두개의 숫자를 기억해야하고 2는 하나의 숫자만 기억하면 되니까 2가 더 좋음\n- 예비개념: 상태1과 상태2가 있다고 하자. 상태1에서 상태2로 가는 변화는 쉽지만, 상태2에서 상태1로 가는 변화는 어렵다고 할때, 상태1이 더 좋은 상태이다.\n\n두가지 상태 “500원을 가지고 있음”, “1000원을 가지고 있음” 을 고려하자. 1000원을 500원을 만드는 것은 쉽지만 500원을 1000원으로 만들기는 어렵다. 따라서 1000원이 더 좋은 상태이다.\n\n- 충분통계량의 급을 어떻게 구분할까? 아래의 상황에서\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n1을 이용하면 2를 만들 수 있지만, 2를 이용해서 1을 만들 수는 없음. 즉 \\(1\\to 2\\) 인 변환(=함수)는 가능하지만 \\(2\\to 1\\)로 만드는 변환(=함수)는 가능하지 않음. 예비개념을 잘 이해했다면 2가 더 좋은 상태라고 볼 수 있다.\n- 이를 확장하자. 어떠한 충분 통계량 \\(S^\\star\\)가 있다고 가정하자. 다른 모든 충분통계량 \\(S_1,S_2,S_3 \\dots\\)에서 \\(S^\\star\\)로 만드는 변환은 존재하는데 (함수는 존재하는데) 그 반대는 \\(S^\\star\\)의 전단사인 충분통계량만 가능하다고 하자. 그렇다면 \\(S^\\star\\)는 가장 좋은 충분통계량이라고 하며, 가장 적은 숫자만 기억하면 되는 충분통계량이라 볼 수 있다. 이러한 충분통계량을 MSS 라고 하자.\n\n\n\n- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, @Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다.\n\n\n\n- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, @Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#충분통계량",
    "href": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#충분통계량",
    "title": "[수리통계학] 추정 for JY",
    "section": "",
    "text": "아래와 같은 상황을 가정하자.\n\\[ X_1,\\dots,X_n \\overset{iid}{\\sim} P_{\\theta}\\]\n- 충분통계량(SS)의 느낌: “이 값만 기억하면 \\(\\theta\\)를 추정하는데 무난할듯”\n- 예시1: \\(X_1 \\sim N(\\theta,1)\\)\n\n\\(X_1\\)은 \\(\\theta_1\\) 의 SS. (하나밖에 없으니 그거라도 기억해야지)\n즉 \\(\\hat{\\theta}=X_1\\)은 \\(\\theta\\)의 SS\n\n- 예시2: \\(X_1,X_2 \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)는 \\(\\theta\\)의 SS (둘다 기억하면 당연히 \\(\\theta\\)를 추정함에 있어서 충분함)\n그렇지만 좀 더 생각해보면 굳이 값 두개를 기억하기보다 \\(\\frac{1}{2}(X_1+X_2)\\)의 값만 기억해도 왠지 충분할것 같음. 따라서 \\(\\hat{\\theta} = \\frac{1}{2}(X_1+X_2)\\) 역시 \\(\\theta\\)의 SS 일듯\n그런데 좀 더 생각해보니까 \\(X_1+X_2\\)의 값만 기억해도 \\(\\frac{1}{2}(X_1+X_2)\\)를 나중에 만들 수 있음 (1/2만 곱하면 되니까) 따라서 \\(X_1+X_2\\)만 기억해도 왠지 충분할 것 같음. 따라서 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- 예시3: \\(X_1,\\dots,X_n \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2,\\dots,X_n)\\)은 \\(\\theta\\)의 SS.\n하지만 \\(n\\)개의 숫자를 기억할 필요 없이 \\(\\sum_{i=1}^{n} X_i\\) 하나의 숫자만 기억해도 왠지 충분할듯. 그래서 \\(\\hat{\\theta} = \\sum_{i=1}^{n} X_i\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- SS에 대한 직관1\n\n기억할 숫자가 적을수록 유리 -&gt; MSS의 개념\n충분통계량의 1:1은 충분통계량 (\\(\\frac{1}{2}(X_1+X_2)\\)을 기억하면 충분한 상황이라면, \\(X_1+X_2\\)를 기억해도 충분하니까..)\n\n- 예시4: \\(X_1,X_2 \\sim {\\cal B}er(\\theta)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)은 \\(\\theta\\)의 SS.\n그리고 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\) SS 일듯.\n두개보다 한개가 유리하니까 둘다 SS이면 \\((X_1,X_2)\\)보다 \\(X_1+X_2\\)가 더 좋은 SS.\n\\(X_1\\)은 SS가 아닐듯. \\(p\\)를 추정함에 있어서 \\(X_1\\)만 가지고서는 충분하지 않아보임\n\\(X_2\\)도 SS가 아닐듯.\n\n왠지 충분할 것 같은 느낌의 정의\n아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n- 일반적으로\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=\\theta(1-\\theta)\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=\\theta^2\\)\n\n와 같은 확률들은 \\(\\theta\\)가 unknown일 때 하나의 숫자로 정할 수 없다. 예를들어 \\(\\theta=0\\) 이라면 아래와 같을 것이고\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=0\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=0\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=0\\)\n\n\\(\\theta=1/2\\) 이라면 아래와 같을 것이다.\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=1/4\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=1/4\\)\n\n즉 \\(X_1,X_2\\)의 결합확률분포는 \\(\\theta\\)가 변함에 따라 같이 변화한다. 이를 이용해 우리는 \\(X_1,X_2\\)의 결합확률분포에서 관찰한 샘플들을 이용하여 \\(\\theta\\)의 값을 역으로 추론한다.\n- 만약에 어떠한 “특수한 정보를 알고 있을 경우” \\(X_1,X_2\\)의 결합확률분포를 완벽하게 기술할 수 있을 때를 가정해보자.\n- 경우1: \\(\\theta\\)를 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(\\theta=1/2\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=0,X_2=1 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=1 | \\theta=1/2)=1/4\\)\n\n- 경우2: \\(X_1,X_2\\)의 realization을 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(X_1=0,X_2=1\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | X_1=0,X_2=0)=0\\)\n\\(P(X_1=0,X_2=1| X_1=0,X_2=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1,X_2=0)=1\\)\n\\(P(X_1=1,X_2=1| X_1=1,X_2=1)=0\\)\n\n- 경우3: \\((X_1+X_2)(\\omega)\\)의 realization을 알고 있을 경우. 이때도 매우 특이하게 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 있다.\ncase1: \\(X_1+X_2=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=0)=1\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=0)=0\\)\n\ncase2: \\(X_1+X_2=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=1)=0\\)\n\ncase3: \\(X_1+X_2=2\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=2)=1\\)\n\n- 경우4: \\(X_1\\)의 realization만 알고 있을 경우. 이때는 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 없다.\ncase1: \\(X_1=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=0)=1-\\theta\\)\n\\(P(X_1=0,X_2=1| X_1=0)=\\theta\\)\n\\(P(X_1=1,X_2=0| X_1=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1=0)=0\\)\n\ncase2: \\(X_1=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1)=1-\\theta\\)\n\\(P(X_1=1,X_2=1| X_1=1)=\\theta\\)\n\n- 종합해보면 경우1,경우2,경우3은 경우4와 구분되는 어떠한 공통점을 가지고 있다 볼 수 있다. 특징은 결합확률분포가 \\(\\theta\\)에 대한 함수로 표현되지 않는다는 것이다. 하나씩 살펴보면\n\n경우1: 당연히 \\(\\theta\\)를 줬으니까 \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우2: \\(X_1,X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우3: \\(X_1+X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n\n이렇게보면 경우1과 경우2,3은 또 다시 구분된다. 경우1은 \\(\\theta\\)에 대한 완전한 정보를 준 상황이므로 당연히 조인트는 \\(\\theta\\)에 의존하지 않는다. 경우2-3은 \\(\\theta\\)를 주지 않았음에도 조인트가 \\(\\theta\\)에 의존하지 않는 매우 특별해보이는 상황이다. 따라서 이를 통해서 유추하면\n\n경우2에서는 \\((X_1,X_2)\\) 가 경우3에서는 \\(X_1+X_2\\)가 \\(\\theta\\)에 대한 완전한 정보를 대신하고 있는것 아닐까?\n\n라는 생각이 든다. 정리하면\n\n경우2: \\((X_1,X_2)\\)을 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n경우3: \\(X_1+X_2\\)를 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n\n라고 해석할 수 있는데 이를 수식화 하면 아래와 같다.\n- 대충정의: 어떠한 통계량 \\(S\\)의 값을 줬을때, \\((X_1,X_2\\dots,X_n)\\)의 조인트가 \\(\\theta\\)에 의존하지 않으면 그 통계량 \\(S\\)를 \\(\\theta\\)의 충분통계량이라고 한다.\n- 충분통계량 구하는 방법\n\n지수족일때 구하는 방식이 있음! &lt;– 외우세여\n분해정리를 쓰는 경우. &lt;– 거의 안쓰는거같은데..\n1-2로도 잘 모르겠으면 충분통계량일듯한 애를 잡아와서 정의에 넣고 노가다로 때려맞춤. (문제가 디스크릿할때만 쓸것)"
  },
  {
    "objectID": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#최소충분통계량",
    "href": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#최소충분통계량",
    "title": "[수리통계학] 추정 for JY",
    "section": "",
    "text": "- 충분통계량에 대한 realization을 알려주면 \\(\\theta\\)의 값을 그냥 알려주는 효과임. 그래서 충분통계량은 좋은 것임\n- 그런데 충분통계량에도 급이 있음. 아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n이 경우\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n이지만 1은 두개의 숫자를 기억해야하고 2는 하나의 숫자만 기억하면 되니까 2가 더 좋음\n- 예비개념: 상태1과 상태2가 있다고 하자. 상태1에서 상태2로 가는 변화는 쉽지만, 상태2에서 상태1로 가는 변화는 어렵다고 할때, 상태1이 더 좋은 상태이다.\n\n두가지 상태 “500원을 가지고 있음”, “1000원을 가지고 있음” 을 고려하자. 1000원을 500원을 만드는 것은 쉽지만 500원을 1000원으로 만들기는 어렵다. 따라서 1000원이 더 좋은 상태이다.\n\n- 충분통계량의 급을 어떻게 구분할까? 아래의 상황에서\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n1을 이용하면 2를 만들 수 있지만, 2를 이용해서 1을 만들 수는 없음. 즉 \\(1\\to 2\\) 인 변환(=함수)는 가능하지만 \\(2\\to 1\\)로 만드는 변환(=함수)는 가능하지 않음. 예비개념을 잘 이해했다면 2가 더 좋은 상태라고 볼 수 있다.\n- 이를 확장하자. 어떠한 충분 통계량 \\(S^\\star\\)가 있다고 가정하자. 다른 모든 충분통계량 \\(S_1,S_2,S_3 \\dots\\)에서 \\(S^\\star\\)로 만드는 변환은 존재하는데 (함수는 존재하는데) 그 반대는 \\(S^\\star\\)의 전단사인 충분통계량만 가능하다고 하자. 그렇다면 \\(S^\\star\\)는 가장 좋은 충분통계량이라고 하며, 가장 적은 숫자만 기억하면 되는 충분통계량이라 볼 수 있다. 이러한 충분통계량을 MSS 라고 하자."
  },
  {
    "objectID": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#라오블랙웰",
    "href": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#라오블랙웰",
    "title": "[수리통계학] 추정 for JY",
    "section": "",
    "text": "- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, @Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#레만쉐페정리",
    "href": "posts/7_study/3_Stat/2023-05-13-추정 for JY.html#레만쉐페정리",
    "title": "[수리통계학] 추정 for JY",
    "section": "",
    "text": "- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, @Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html",
    "title": "[Fourier] 푸리에변환 코드실습 (블로그)",
    "section": "",
    "text": "ref: https://towardsdatascience.com/fourier-transform-the-practical-python-implementation-acdd32f1b96a"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#introduction",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#introduction",
    "title": "[Fourier] 푸리에변환 코드실습 (블로그)",
    "section": "Introduction",
    "text": "Introduction\nFourier Transform (FT) relates the time domain of a signal to its frequency domain, where the frequency domain contains the information about the sinusoids (amplitude, frequency, phase) that construct the signal. Since FT is a continuous transform, the Discrete Fourier Transform (DFT) becomes the applicable transform in the digital world that holds the information of signals in the discrete format as a set of samples, where the sampling theorem is the strict rule of discretizing and the signal. The DFT of a signal (xn) with N number of samples is given by the following equation [2]:\n\\(X_k = \\sum_{n=0}^{N-1} x_n \\cdot e^{i2\\pi kn/N}=\\sum_{n=0}^{N-1}\\cdot [\\cos(2\\pi k/N) - i\\sin(2\\pi kn/N)]\\)\n\n\\(N\\) : Number of Samples\n\\(n\\): Current Sample\n\\(k\\): Current frequency where \\(k\\in [0,N-1]\\)\n\\(xn\\): The sine value at sample \\(n\\)\n\\(Xk\\): The DFT which includes information on both amplitude and phase\n\nThe output of the DFT (Xk) is an array of complex numbers that hold the information of frequency components [2]. Applying DFT on signals using the mathematical equation directly demands a heavy computation complexity. Luckily, a Fast Fourier Transform (FFT) was developed [3] to provide a faster implementation of the DFT. The FFT takes advantage of the symmetry nature of the output of the DFT. We will not further discuss how FFT works as it’s like the standard practical application of DFT. But if you want more details, refer to [3]."
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#lets-get-started",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#lets-get-started",
    "title": "[Fourier] 푸리에변환 코드실습 (블로그)",
    "section": "Let’s get started",
    "text": "Let’s get started\n\n# Import the required packages\nimport numpy as np\nfrom scipy.fft import fft, rfft\nfrom scipy.fft import fftfreq, rfftfreq\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nThe signal we will generate using the previous class contains three sinusoids (1, 10, 20) Hz with amplitudes of (3, 1, 0.5), respectively. The sampling rate will be 200 and the duration of the signal is 2 seconds.\n\n# Import the required package\nimport numpy as np\n\n# Building a class Signal for better use.\nclass Signal:\n    def __init__(self, amplitude=1, frequency=10, duration=1, sampling_rate=100.0, phase=0):\n        self.amplitude = amplitude\n        self.frequency = frequency\n        self.duration = duration\n        self.sampling_rate = sampling_rate\n        self.phase = phase\n        self.time_step = 1.0/self.sampling_rate\n        self.time_axis = np.arange(0, self.duration, self.time_step)\n  \n  # Generate sine wave\n    def sine(self):\n        return self.amplitude*np.sin(2*np.pi*self.frequency*self.time_axis+self.phase)\n  \n  # Generate cosine wave\n    def cosine(self):\n        return self.amplitude*np.cos(2*np.pi*self.frequency*self.time_axis+self.phase)\n\n\n# Generate the three signals using Signal class and its method sine()\nsignal_1hz = Signal(amplitude=3, frequency=1, sampling_rate=200, duration=2)\nsine_1hz = signal_1hz.sine()\nsignal_20hz = Signal(amplitude=1, frequency=20, sampling_rate=200, duration=2)\nsine_20hz = signal_20hz.sine()\nsignal_10hz = Signal(amplitude=0.5, frequency=10, sampling_rate=200, duration=2)\nsine_10hz = signal_10hz.sine()\n\n# Sum the three signals to output the signal we want to analyze\nsignal = sine_1hz + sine_20hz + sine_10hz\n\n# Plot the signal\nplt.plot(signal_1hz.time_axis, signal, 'b')\nplt.xlabel('Time [sec]')\nplt.ylabel('Amplitude')\nplt.title('Sum of three signals')\nplt.show()\n\n\n\n\nThe Fourier Transform of this signal can be calculated using (fft) from the scipy package as follows [4]:"
  },
  {
    "objectID": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#fft",
    "href": "posts/7_study/0_Fourier Transformation/2023-06-25-fft-python.html#fft",
    "title": "[Fourier] 푸리에변환 코드실습 (블로그)",
    "section": "FFT",
    "text": "FFT\n\n# Apply the FFT on the signal\nfourier = fft(signal)\n\n# Plot the result (the spectrum |Xk|)\nplt.plot(np.abs(fourier))\nplt.title('The output of the FFT of the signal')\nplt.show()\n\n\n\n\nThe figure above should represent the frequency spectrum of the signal. Notice that the x-axis is the number of samples (instead of the frequency components) and the y-axis should represent the amplitudes of the sinusoids. To get the actual amplitudes of the spectrum, we have to normalize the output of (fft) by N/2 the number of samples.\n\n# Calculate N/2 to normalize the FFT output\nN = len(signal)\nnormalize = N/2\n\n# Plot the normalized FFT (|Xk|)/(N/2)\nplt.plot(np.abs(fourier)/normalize)\nplt.ylabel('Amplitude')\nplt.xlabel('Samples')\nplt.title('Normalized FFT Spectrum')\nplt.show()\n\n\n\n\nTo get the frequency components (x-axis), you can use (fftfreq) from the scipy package. This method needs the number of samples (N) and the sampling rate as input arguments. And it returns a frequency axis with N frequency components [5].\n\n# Get the frequency components of the spectrum\nsampling_rate = 200.0 # It's used as a sample spacing\nfrequency_axis = fftfreq(N, d=1.0/sampling_rate)\nnorm_amplitude = np.abs(fourier)/normalize\n# Plot the results\nplt.plot(frequency_axis, norm_amplitude)\nplt.xlabel('Frequency[Hz]')\nplt.ylabel('Amplitude')\nplt.title('Spectrum')\nplt.show()\n\n\n\n\nThe spectrum with the actual amplitudes and frequency axis\nTo understand what happened in the last code, let’s plot only the frequency axis:\n\n# Plot the frequency axis for more explanation\nplt.plot(frequency_axis)\nplt.ylabel('Frequency[Hz]')\nplt.title('Frequency Axis')\nplt.show()\n\n\n\n\nNotice that the frequency array starts at zero. Then, it begins to increase with (d) step by step to reach its maximum (100Hz). After that, it starts from the negative maximum frequency (-100Hz) to increase back again with (d) step by step. The maximum frequency that can hold information from the signal (100Hz) is half of the sampling rate and this is true according to the Sampling Theorem [2].\nDue to the symmetry of the spectrum for the real-value signals, we only focus on the first half of the spectrum [2]. The Scipy package provides methods to deal with the Fourier transform of the real-value signals, where it takes advantage symmetry nature of the spectrum. Such methods are (rfft [6], rfftfreq [7]). These methods are the same as (fft, fftfreq), respectively. By comparing the time execution between (fft) and (rfft) methods on the same signal, you’ll find that (rfft) is a little bit faster. When dealing with real-value signals, which is most likely the case, using (rfft) is the best choice.\n\n# Calculate the time execution of (fft)\nprint('Execution time of fft function:')\n%timeit fft(signal)\n# Calculate the time execution of (rfft)\nprint('\\nExecution time of rfft function:')\n%timeit rfft(signal)\n\nExecution time of fft function:\n4.08 µs ± 11.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\nExecution time of rfft function:\n4 µs ± 5.39 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nTo conclude our discussion about scaling the amplitudes and generating the frequency axis of the spectrum for real-values signal data that have a symmetry nature in their frequency domain, the code below represents the final form of the spectrum (the actual amplitudes on the right frequencies).\n\n# Plot the actual spectrum of the signal\nplt.plot(rfftfreq(N, d=1/sampling_rate), 2*np.abs(rfft(signal))/N)\nplt.title('Spectrum')\nplt.xlabel('Frequency[Hz]')\nplt.ylabel('Amplitude')\nplt.show()"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "",
    "text": "모르고 살았다면 더 좋았을 내용"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-비상식적인-append",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-비상식적인-append",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제1: 비상식적인 append",
    "text": "예제1: 비상식적인 append\n\n포인트: 이상한 일의 관찰\n\n- 원소의 추가: + 이용\n\na=[1,2,3]\nb=a\na=a+[4]\n\n\na\n\n[1, 2, 3, 4]\n\n\n\nb\n\n[1, 2, 3]\n\n\n- 원소의 추가 .append 이용\n\na=[1,2,3]\nb=a\na.append(4) # a=a+[4]\n\n\na\n\n[1, 2, 3, 4]\n\n\n\nb\n\n[1, 2, 3, 4]"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#append의-동작원리-틀린상상",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#append의-동작원리-틀린상상",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "append의 동작원리: 틀린상상",
    "text": "append의 동작원리: 틀린상상\n- 상자로서의 변수: 변수가 데이터를 저장하는 일종의 상자와 같다. &lt;– 아주 흔한 오해 (Fluent Python)\n\n흔히 비유하는 ‘상자로서의 변수’ 개념이 실제로는 객체지향적 언어에서 참조변수를 이해하는 데 방해가 된다.\n\n- “상자로서의 변수” 관점에서 아래의 코드를 해석하자. (일단 아래의 해석들이 틀린해석이라는 사실을 명심할 것)\na=[1,2,3]\nb=a\na.append(4)\na,b라는 변수들은 메모리에 어떻게 저장이 되어있을까?\n상상력을 조금 발휘하면 아래와 같이 여길 수 있다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\na=[1,2,3]\n\n메모리주소1에 존재하는 방을 a라고 하고, 그 방에 [1,2,3]을 넣는다.\n\n\n아래를 실행하였을 경우\n\nb=a\n\n메모리주소2에 존재하는 방을 b라고 하고, 그 방에 a를 넣어야하는데, a는 [1,2,3]이니까 [1,2,3]을 넣는다.\n\n\n아래를 실행하면\n\na.append(4)\n\n방 a로가서 [1,2,3]을 [1,2,3,4]로 바꾼다.\n그리고 방 b에는 아무것도 하지 않는다.\n\n- R에서는 맞는 비유인데, 파이썬은 적절하지 않은 비유이다.\n\n틀린이유\n\nid(a)\n\n139765704687936\n\n\n\nid(b)\n\n139765704687936"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#append의-동작원리-올바른-상상",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#append의-동작원리-올바른-상상",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "append의 동작원리: 올바른 상상",
    "text": "append의 동작원리: 올바른 상상\n\n파이썬에서의 변수는 자바에서의 참조변수와 같으므로 변수는 객체에 붙은 레이블이라고 생각하는 것이 좋다.\n\n- 파이썬에서는 아래가 더 적절한 비유이다.\n\n메모리는 변수를 담을 방이 여러개 있는 호텔이라고 생각하자.\n아래를 실행하였을 경우\n\na=[1,2,3]\n\n메모리주소 139753545242336에서 [1,2,3]을 생성\n방 139753545242336의 방문에 a라는 포스트잇을 붙인다.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방중에서 a라는 포스트잇이 붙은 방을 찾아가면 된다.\n\n\n아래를 실행하였을 경우\n\nb=a\n\na라는 포스트잇이 지칭하는 객체를 가져옴. 그리고 그 객체에 b라는 포스트잇을 붙인다.\n쉽게말하면 b라는 포스트잇을 방 139753545242336의 방문에 붙인다는 이야기.\n앞으로 [1,2,3]에 접근하기 위해서는 여러 메모리방중에서 a라는 포스트잇이 붙어 있거나 b라는 포스트잇이 붙어있는 방을 찾아가면 된다.\n\n\n아래를 실행하면\n\na.append(4)\n\na라는 포스트잇이 붙어있는 방으로 가서, 그 내용물에 append함수를 적용하여 4를 추가하라. 즉 내용물 [1,2,3]을 [1,2,3,4]로 바꾸라.\n같은방(139753545242336)에 a,b라는 포스트잇이 모두 붙어있음. 따라서 b라는 포스트잇이 붙은 방을 찾아가서 내용물을 열어보면 [1,2,3,4]가 나온다."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-같은-value-다른-id",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-같은-value-다른-id",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제1: 같은 value, 다른 id",
    "text": "예제1: 같은 value, 다른 id\n\n포인트: (1) 포스트잇 개념의 확실한 이해 (2) 할당문을 새로운 시각으로 해석하는 연습 (3) “생성-&gt;할당”과 “참조/에일리어싱”의 구분\n\n\na=[1,2,3] # 우변: 생성된 오브젝트, 좌변: 이름 \nb=a # 우변: 가져온 오브젝트, 좌변: 별명 --&gt; 참조, 에일리어싱(별칭부여)이라고 한다\na.append(4) # a라는 오브젝트를 직접변경\nc=[1,2,3,4] # 우변: 생성된 오브젝트, 좌변: 이름\n\n여기에서 a,b,c는 모두 같은 value를 가진다.\n\na\n\n[1, 2, 3, 4]\n\n\n\nb\n\n[1, 2, 3, 4]\n\n\n\nc\n\n[1, 2, 3, 4]\n\n\n하지만 그 id까지 같은 것은 아니다.\n\nid(a), id(b), id(c)\n\n(140237129249664, 140237129249664, 140237128836544)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제2",
    "text": "예제2\n\n선행지식: “생성-&gt;할당” 과 “참조/에일리어싱”의 구분\n\n\n포인트: 재할당의 이해!!\n\n(관찰)\n\na=[1,2,3] # 생성-&gt;할당\nb=a # 참조/에일리어싱 \na=a+[4] # 생성-&gt;재할당 \nprint('a=',a)\nprint('b=',b)\n\na= [1, 2, 3, 4]\nb= [1, 2, 3]\n\n\n(해설)\n\nid(a),id(b)\n\n(140237129283584, 140237129350912)\n\n\n\n포인트: [1,2,3]+[4] 가 실행되는 순간 새로운 오브젝트가 만들어지고 그 오브젝트를 a라는 이름으로 다시 할당되었음. (재할당)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제1",
    "text": "예제1\n\n선행지식: “생성-&gt;할당,재할당” 과 “참조/에일리어싱”의 구분\n\n\n포인트: 인터닝을 위한 떡밥예제\n\n\na=1+2021\nid(a)\n\n139753546122608\n\n\n\nb=2023-1\nid(b)\n\n139753545299280\n\n\n\nid(2022)\n\n139753545299472\n\n\n\n당연한결과임."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-이제-다-이해했다고-생각했는데..",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-이제-다-이해했다고-생각했는데..",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제2: 이제 다 이해했다고 생각했는데..",
    "text": "예제2: 이제 다 이해했다고 생각했는데..\n\n선행지식: “생성-&gt;할당,재할당” 과 “참조/에일리어싱”의 구분\n\n\n포인트: 인터닝의 이해\n\n\na=1+2 \nid(a)\n\n7394720\n\n\n\nb=4-1\nid(b)\n\n7394720\n\n\n\nid(a)와 id(b)가 왜 똑같지..?\n\n(해설) 파이썬의 경우 효율성을 위해서 -5~256까지의 정수를 미리 저장해둠.\n\nid(3)\n\n7394720\n\n\n\n3은 언제나 7394720에 지박령마냥 밖혀있음"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-1",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제1",
    "text": "예제1\n(관찰) 아래의 예제를 살펴보자. 참조를 제대로 이해했다면 아래의 예제는 자연스럽게 이해가능할 것임.\n\nl1 = [3, [66,55,44]]\nl2 = l1 \nprint('시점1')\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1[0]=4 \nprint('시점2')\nprint('l1=',l1)\nprint('l2=',l2)\n\nl2.append(5)\nprint('시점3')\nprint('l1=',l1)\nprint('l2=',l2)\n\n시점1\nl1= [3, [66, 55, 44]]\nl2= [3, [66, 55, 44]]\n시점2\nl1= [4, [66, 55, 44]]\nl2= [4, [66, 55, 44]]\n시점3\nl1= [4, [66, 55, 44], 5]\nl2= [4, [66, 55, 44], 5]\n\n\n(해설)\n\nl1 = [3, [66,55,44]]\nl2 = l1 \n\n\nid(l1),id(l2)\n\n(140571068242688, 140571068242688)\n\n\n이해는 되지만 우리가 원한건 이런게 아니야"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-r과-같이-를-쓰고-싶다면",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-r과-같이-를-쓰고-싶다면",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제2: R과 같이 = 를 쓰고 싶다면?",
    "text": "예제2: R과 같이 = 를 쓰고 싶다면?\n\n선행지식: “생성-&gt;할당,재할당” 과 “참조/에일리어싱”의 구분\n\n\n포인트: 복사의 사용, 얕은복사의 떡밥\n\n(관찰)\n\nl1 = [3, [66,55,44]]\nl2 = l1.copy()\nprint('시점1')\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1[0]=4 \nprint('시점2')\nprint('l1=',l1)\nprint('l2=',l2)\n\nl2.append(5)\nprint('시점3')\nprint('l1=',l1)\nprint('l2=',l2)\n\n시점1\nl1= [3, [66, 55, 44]]\nl2= [3, [66, 55, 44]]\n시점2\nl1= [4, [66, 55, 44]]\nl2= [3, [66, 55, 44]]\n시점3\nl1= [4, [66, 55, 44]]\nl2= [3, [66, 55, 44], 5]\n\n\n(해설)\n\nl1 = [3, [66,55,44]]\nl2 = l1.copy()\n\n\nid(l1),id(l2) ## 드디어 주소가 달라졌다.\n\n(140571068242688, 140571068242240)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3-이제-다-이해했다고-생각했는데..",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3-이제-다-이해했다고-생각했는데..",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제3: 이제 다 이해했다고 생각했는데..",
    "text": "예제3: 이제 다 이해했다고 생각했는데..\n\n선행지식: “생성-&gt;할당,재할당” 과 “참조/에일리어싱”의 구분, 복사의 사용\n\n\n포인트: 얕은복사를 이해하지 못할때 생기는 개념충돌\n\n(관찰)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\nl1[1].append(33)\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44, 33]]\n\n\n(의문)\n\nid(l1),id(l2)\n\n(140571077644352, 140571068253376)\n\n\n\nl1이랑 l2의 주소도 다르게 나오는데 왜 또 참조한것마냥 l1과 l2가 같이 바뀌고 있지?\n\n나는 진정한 복사(=깊은복사)를 하고싶다"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-2",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-2",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제1",
    "text": "예제1\n\n선행지식: 이전까지 모든것\n\n\n포인트: 0차원 자료형의 메모리 구조 이해, 1차원 자료형의 메모리 구조를 위한 떡밥\n\n(관찰+해설)\n\na=2222\nb=2222\n\n\nid(a),id(b)\n\n(139753545300880, 139753545301808)\n\n\n메모리 상황\n\n2222라는 오브젝트가 어떤공간(139753545300880)에 생성되고 그 공간에 a라는 라벨이 붙음\n2222라는 오브젝트가 어떤공간(139753545301808)에 생성되고 그 공간에 b라는 라벨이 붙음\n\n즉 -5~256 이외의 2개의 메모리 공간을 추가적으로 사용"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-1",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제2",
    "text": "예제2\n\n선행지식: 이전까지 모든것, 0차원 자료형의 메모리저장상태 이해\n\n\n포인트: (1) 1차원 자료형의 메모리 구조 이해 (2) 가변형객체라는 표현의 의미\n\n(관찰)\n\na=[1,2,2222]\nb=[1,2,2222]\na.append(4)\nprint('a=',a)\nprint('b=',b)\n\na= [1, 2, 2222, 4]\nb= [1, 2, 2222]\n\n\n(해설)\n\na=[1,2,2222]\nb=[1,2,2222]\n\n\nid(a), [id(a[0]),id(a[1]),id(a[2])] # a=[1,2,2222]\n\n(140527746917824, [7585472, 7585504, 140528016796752])\n\n\n\nid(b), [id(b[0]),id(b[1]),id(b[2])] # b=[1,2,2222] \n\n(140527746917568, [7585472, 7585504, 140528016796144])\n\n\n\na.append(4)\n\n\na\n\n[1, 2, 2222, 4]\n\n\n\nb\n\n[1, 2, 2222]\n\n\n\nid(a)\n\n140527746917824\n\n\n메모리상황\n\n-5~256까지의 숫자는 미리 메모리에 저장되어 있다. 이중에서 1은 7394656, 2는 7394688에 저장되어있음.\n2222가 공간 139753178093776에서 만들어진다.\n어떠한 리스트오브젝트가 공간 139753182327904에서 만들어지고 원소로 [1,2,2222]를 가진다. 이 공간에 a라는 포스트잇을 붙인다.\n2222가 공간 139753178095568에서 만들어진다.\n어떠한 리스트오브젝트가 공간 139753173818656에서 만들어지고 원소로 [1,2,2222]를 가진다. 이 공간에 b라는 포스트잇을 붙인다.\na라는 포스트잇이 붙은 공간으로 이동하여 원소에 4를 추가시킨다.\n\n즉 -5~256이외에 4개의 메모리 공간을 추가사용 (a,b,a의 2222,b의 2222)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제3",
    "text": "예제3\n\n선행지식: 이전까지 모든 것\n\n\n포인트: l2=l1 와 l2=l1.copy() 의 차이점\n\n(관찰)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\nl1[0] = 7777\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1= [7777, [66, 55, 44]]\nl2= [3, [66, 55, 44]]\n\n\n(해설)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139753183437040, [7394720, 139753183707216])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139753182311120, [7394720, 139753183707216])\n\n\n메모리상황\n\n-5~256까지의 숫자가 메모리에 저장되어 있다.\n저장된 숫자중 66,55,44를 묶어서 리스트로 구성하고 이 리스트를 공간 139753183707216에 저장.\n숫자 3과 공간 139753183707216에 저장된 리스트 [66,55,44]를 하나로 묶어서 새로운 리스트를 구성하고 이를 공간 139753183437040에 저장. 공간 139753183437040에 l1이라는 포스트잇 생성.\n공간 139753182311120에 l1의 원소들을 모아서 새로운 리스트를 구성함. 공간 139753182311120에 l2라는 포스트잇 생성. 그런데 따져보니까 내부구성은 똑같아?\n\n\nl1[0] = 7777\nl1,l2\n\n([7777, [66, 55, 44]], [3, [66, 55, 44]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139753183437040, [139753178092080, 139753183707216])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139753182311120, [7394720, 139753183707216])\n\n\n\nl1[0]은 원래 공간 7394720와 binding 되어 있었음.\n\n그런데 7777이라는 새로운 오브젝트가 공간 139753178092080에 생성되고 l1[0]이 공간 139753178092080와 다시 binding 됨."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제4",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제4",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제4",
    "text": "예제4\n\n선행지식: 이전까지 모든것, .copy()의 동작원리\n\n\n포인트: .copy()의 동작원리 재학습\n\n(관찰)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\nl1.append(7777)\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1= [3, [66, 55, 44], 7777]\nl2= [3, [66, 55, 44]]\n\n\n(해설)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\nl1.append(7777)\n\n\nl1,l2\n\n([3, [66, 55, 44], 7777], [3, [66, 55, 44]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1]), id(l1[2])]\n\n(139753183257056, [7394720, 139753184484240, 139753180268560])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139753183216656, [7394720, 139753184484240])"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제5-우리를-힘들게-했던-그-예제.",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제5-우리를-힘들게-했던-그-예제.",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제5: 우리를 힘들게 했던 그 예제.",
    "text": "예제5: 우리를 힘들게 했던 그 예제.\n\n선행지식: 이전까지 모든것, .copy()의 동작원리\n\n\n포인트: (1) .copy()의 한계, (2) 얕은복사라는 명칭의 유래\n\n(관찰)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\nl1[1].append(7777)\nprint('l1=',l1)\nprint('l2=',l2)\n\nl1= [3, [66, 55, 44, 7777]]\nl2= [3, [66, 55, 44, 7777]]\n\n\n(해설-시점1)\n\nl1 = [3,[66,55,44]]\nl2 = l1.copy()\n\n\nl1,l2\n\n([3, [66, 55, 44]], [3, [66, 55, 44]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139753181411920, [7394720, 139753181409920])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139753181409440, [7394720, 139753181409920])\n\n\n(해설-시점2)\n\nl1[1].append(7777)\n\n\nl1,l2\n\n([3, [66, 55, 44, 7777]], [3, [66, 55, 44, 7777]])\n\n\n\nid(l1), [id(l1[0]), id(l1[1])]\n\n(139753181411920, [7394720, 139753181409920])\n\n\n\nid(l2), [id(l2[0]), id(l2[1])]\n\n(139753181409440, [7394720, 139753181409920])\n\n\n해설: 사실 시점1에서 메모리 주소상황을 잘 이해했다면 신기한 일이 아니다. .copy()는 l1과 l2의 주소만 다르게 만들 뿐 내용물인 l1[0],l1[1]는 동일하니까."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제6-신임교수최규빈이영미",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제6-신임교수최규빈이영미",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제6: 신임교수=[‘최규빈’,‘이영미’]",
    "text": "예제6: 신임교수=[‘최규빈’,‘이영미’]\n\n선행지식: 이전까지의 모든것\n\n\n포인트: 이전까지의 모든것 점검\n\n- 최규빈, 이영미는 신임교수임\n\n신임교수 = ['최규빈','이영미']\n\n\nid(신임교수), id('최규빈'), id('이영미')\n\n(139753182527808, 139753171447312, 139753171447408)\n\n\n- 신임교수를 누군가는 막내들이라고 부르기도 함.\n\n막내들 = 신임교수 \n\n\nid(막내들), id(신임교수)\n\n(139753182527808, 139753182527808)\n\n\n“막내들”이라는 단어와 “신임교수”라는 단어는 사실 같은 말임\n- 새로운 교수 “박혜원”이 뽑혔음.\n\n신임교수.append(\"박혜원\")\n\n\n신임교수, 막내들\n\n(['최규빈', '이영미', '박혜원'], ['최규빈', '이영미', '박혜원'])\n\n\n- 전북대 통계학과에서 R특강팀을 구성하여 방학중 R교육을 실시하고자함. 특강팀은 우선 신임교수들로 구성.\n\nR특강팀 = 신임교수.copy()\nR특강팀 \n\n['최규빈', '이영미', '박혜원']\n\n\n- R특강팀에 최혜미교수님 추가. (그렇지만 최혜미교수님이 막내는 아니야.. // 참조와 shallow copy의 차이점)\n\nR특강팀.append(\"최혜미\") \n\n\nR특강팀, 신임교수, 막내들\n\n(['최규빈', '이영미', '박혜원', '최혜미'], ['최규빈', '이영미', '박혜원'], ['최규빈', '이영미', '박혜원'])\n\n\n- R특강팀에서 양성준 교수를 추가하여 파이썬 특강팀을 구성 (R특강팀의 구분을 위해서 중첩리스트 구조로 만들자)\n\n파이썬특강팀 = [R특강팀, \"양성준\"]\n파이썬특강팀\n\n[['최규빈', '이영미', '박혜원', '최혜미'], '양성준']\n\n\n- 이영미교수는 다른 일이 많아서 R특강 팀에서 제외됨. (그럼 자연히 파이썬에서도 제외됨!!)\n\nR특강팀.remove(\"이영미\")\n\n\nR특강팀, 파이썬특강팀\n\n(['최규빈', '박혜원', '최혜미'], [['최규빈', '박혜원', '최혜미'], '양성준'])\n\n\n하지만 이영미교수는 여전히 신임교수이면서 막내들임\n\n신임교수, 막내들\n\n(['최규빈', '이영미', '박혜원'], ['최규빈', '이영미', '박혜원'])\n\n\n- 새로운 교수로 “손흥민”이 임용됨.\n\n막내들.append(\"손흥민\")\n\n\n막내들, 신임교수\n\n(['최규빈', '이영미', '박혜원', '손흥민'], ['최규빈', '이영미', '박혜원', '손흥민'])\n\n\n- 그렇다고 해서 손흥민 교수가 바로 R이나 파이썬 특강팀에 자동소속되는건 아님\n\nR특강팀, 파이썬특강팀\n\n(['최규빈', '박혜원', '최혜미'], [['최규빈', '박혜원', '최혜미'], '양성준'])"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-motivation-example",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-motivation-example",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제1: Motivation example",
    "text": "예제1: Motivation example\n- 아래의 상황을 다시 생각해보자.\n\n파이썬특강팀 = [\"양성준\",[\"최규빈\",\"이영미\",\"최혜미\"]]\nADSP특강팀 = 파이썬특강팀.copy()\n파이썬특강팀[-1].remove(\"이영미\")\n\n\n파이썬특강팀, ADSP특강팀\n\n(['양성준', ['최규빈', '최혜미']], ['양성준', ['최규빈', '최혜미']])\n\n\n이슈: 이영미교수가 파이썬특강에서 제외되면서 ADSP특강팀에서도 제외되었음. 그런데 사실 이영미교수가 파이썬특강팀에서만 제외되길 원한 것이지 ADSP특강팀에서 제외되길 원한게 아닐수도 있음.\n해결: Deep copy의 사용\n\nimport copy\n\n\n파이썬특강팀 = [\"양성준\",[\"최규빈\",\"이영미\",\"최혜미\"]]\nADSP특강팀 = copy.deepcopy(파이썬특강팀)\n파이썬특강팀[-1].remove(\"이영미\")\n\n\n파이썬특강팀, ADSP특강팀\n\n(['양성준', ['최규빈', '최혜미']], ['양성준', ['최규빈', '이영미', '최혜미']])"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-2",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-2",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제2",
    "text": "예제2\n\n선행지식: 이전까지 모든것, 얕은복사\n\n\n포인트: (1) 깊은복사 (2) 복사의 레벨을 이해 (3) 얕은복사 = 1단계 깊은복사\n\n- deepcopy\n\nl1 = [3,[66,[55,44]]] \nl2 = copy.deepcopy(l1)\n\n\nl2[1][1].append(33)\n\n\nl1,l2\n\n([3, [66, [55, 44]]], [3, [66, [55, 44, 33]]])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 140137133270656\nl2: 140137132727232\n\n\n\n레벨1: l1,l2 의 메모리 주소가 다름을 확인\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 140137133270656 [7585536, 140137133267712]\nl2: 140137132727232 [7585536, 140137133267456]\n\n\n\n레벨2: l1안에 있는 [66,[55,44]]와 l2안에 있는 [66,[55,44]]의 메모리 주소가 다름도 확인.\n\n\nprint('level 3')\nprint('l1:', id(l1), [id(l1[0]),[id(l1[1][0]),id(l1[1][1])]])\nprint('l2:', id(l2), [id(l2[0]),[id(l2[1][0]),id(l2[1][1])]])\n\nlevel 3\nl1: 140137133270656 [7585536, [7587552, 140137133704320]]\nl2: 140137132727232 [7585536, [7587552, 140137137410624]]\n\n\n\n레벨3: l1안의 [66,[55,44]] 안의 [55,44]와 l2안의 [66,[55,44]] 안의 [55,44]의 메모리 주소까지도 다름을 확인.\n\n- 비교를 위한 shallow copy\n\nl1 = [3,[66,[55,44]]] \nl2 = l1.copy()\n\n\nl2[1][1].append(33)\n\n\nl1,l2\n\n([3, [66, [55, 44, 33]]], [3, [66, [55, 44, 33]]])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 140137133470528\nl2: 140137137411136\n\n\n\n레벨1: l1,l2 의 메모리 주소가 다름을 확인\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 140137133470528 [7585536, 140137133703424]\nl2: 140137137411136 [7585536, 140137133703424]\n\n\n\n레벨2: l1안에 있는 [66,[55,44]]와 l2안에 있는 [66,[55,44]]의 메모리 주소는 같음!!\n\n\nprint('level 3')\nprint('l1:', id(l1), [id(l1[0]),[id(l1[1][0]),id(l1[1][1])]])\nprint('l2:', id(l2), [id(l2[0]),[id(l2[1][0]),id(l2[1][1])]])\n\nlevel 3\nl1: 140137133470528 [7585536, [7587552, 140137137410880]]\nl2: 140137137411136 [7585536, [7587552, 140137137410880]]\n\n\n\n레벨3: l1안의 [66,[55,44]] 안의 [55,44]와 l2안의 [66,[55,44]] 안의 [55,44]의 메모리 주소도 같음!!\n\n- 비교를 위한 참조\n\nl1 = [3,[66,[55,44]]] \nl2 = l1\n\n\nl2[1][1].append(33)\n\n\nl1,l2\n\n([3, [66, [55, 44, 33]]], [3, [66, [55, 44, 33]]])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 140137133223232\nl2: 140137133223232\n\n\n\n레벨1: l1,l2 여기서부터 메모리 주소가 같다.\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 140137133223232 [7585536, 140137133698560]\nl2: 140137133223232 [7585536, 140137133698560]\n\n\n\nprint('level 3')\nprint('l1:', id(l1), [id(l1[0]),[id(l1[1][0]),id(l1[1][1])]])\nprint('l2:', id(l2), [id(l2[0]),[id(l2[1][0]),id(l2[1][1])]])\n\nlevel 3\nl1: 140137133223232 [7585536, [7587552, 140137133438144]]\nl2: 140137133223232 [7585536, [7587552, 140137133438144]]\n\n\n\nNote: 문헌에 따라서 shallow copy를 level1 deep copy라고 부르기도 한다."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-3",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-3",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제1",
    "text": "예제1\n\n선행지식: 이전까지 모든것\n\n\n포인트: 얕은복사의 한계점 이해\n\n- 아래의 코드결과를 예측하라. 결과가 나오는 이유를 설명하라.\n\nl1= [3,[66,55,44]]\nl2= l1.copy() \nl1[-1].append(33)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44, 33]]\n\n\n\n포인트: shallow copy (=level 1 deep copy) 이므로 l1안의 [66,55,44]와 l2안의 [66,55,44]는 같은 메모리 주소를 가짐"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-3",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제2-3",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제2",
    "text": "예제2\n\n선행지식: 이전까지 모든것\n\n\n포인트: 재할당의 활용하여 얕은복사의 한계점 극복\n\n- 아래의 코드결과를 예측하라. 결과가 나오는 이유를 설명하라.\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33] \n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44]]\n\n\n\n포인트: l1[-1]+[33]가 실행되는 순간 새로운 오브젝트가 생성되고 이 새로운 오브젝트가 l1의 마지막 원소에 새롭게 할당된다."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3-1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제3-1",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제3",
    "text": "예제3\n\n선행지식: 이전까지 모든것\n\n\n포인트: 재할당의 활용하여 얕은복사의 한계점 극복, 예제4를 위한 떡밥\n\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33] \nl1[-1].remove(33)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44]]\nl2= [3, [66, 55, 44]]\n\n\n\n포인트: 이 상황에서 l1안의 [66,55,44]와 l2안의 [66,55,44]는 서로 다른 메모리 주소를 가진다."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제4-1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제4-1",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제4",
    "text": "예제4\n\n선행지식: 이전까지 모든것\n\n\n포인트: 재할당으로 인해 메모리주소가 틀어짐을 이용한 트릭예제, 예제5의 떡밥예제\n\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33]\nl1[-1].remove(33) \nl1[-1].append(33)\n\n(잘못된 상상) 아래의 코드와 결과가 같을거야!!\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \n# l1[-1] = l1[-1]+[33] \n# l1[-1].remove(33)\nl1[-1].append(33)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44, 33]]\n\n\n(하지만 현실은)\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] = l1[-1]+[33] \nl1[-1].remove(33)\nl1[-1].append(33)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44]]"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제5",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제5",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제5",
    "text": "예제5\n\n선행지식: 이전까지 모든것\n\n\n포인트: +=는 재할당이 아니다.\n\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] += [33] # l1[-1] = l1[-1]+[33] \nl1[-1].remove(33)\nl1[-1].append(33)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44, 33]]\n\n\n+= 연산자의 올바른 이해\n\n??? 예제4랑 예제5는 같은코드가 아니었음!!! a += [1] 는 새로운 오브젝트를 만드는게 아니고, 기존의 오브젝트를 변형하는 스타일의 코드였음! (마치 append 메소드처럼)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-motivation-example-1",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#예제1-motivation-example-1",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "예제1: Motivation example",
    "text": "예제1: Motivation example\n\n선행지식: 이전까지 모든것\n\n\n포인트: +=는 재할당이 아니다.\n\n- 우리는 이제 아래의 내용은 마스터함\n\nl1= [3,[66,55,44]] \nl2= l1.copy() \nl1[-1] += [33] # l1[-1].append(33)이랑 같은거..\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, [66, 55, 44, 33]]\nl2= [3, [66, 55, 44, 33]]\n\n\n- 아래의 결과를 한번 예측해볼까?\n\nl1=[3,(66,55,44)]\nl2=l1.copy()\nl2[1] += (33,)\n\n\nprint('l1=', l1)\nprint('l2=', l2)\n\nl1= [3, (66, 55, 44)]\nl2= [3, (66, 55, 44, 33)]"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#해설",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#해설",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "해설",
    "text": "해설\n(시점1)\n\nl1=[3,(66,55,44)]\nl2=l1.copy()\n\n\nl1,l2\n\n([3, (66, 55, 44)], [3, (66, 55, 44)])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 140006812656640\nl2: 140006812645888\n\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 140006812656640 [7585536, 140006812590400]\nl2: 140006812645888 [7585536, 140006812590400]\n\n\n(시점2)\n\nl2[1] += (33,)\n\n\nl1,l2\n\n([3, (66, 55, 44)], [3, (66, 55, 44, 33)])\n\n\n\nprint('level 1')\nprint('l1:', id(l1))\nprint('l2:', id(l2))\n\nlevel 1\nl1: 140006812656640\nl2: 140006812645888\n\n\n\nprint('level 2')\nprint('l1:', id(l1), [id(l1[0]),id(l1[1])])\nprint('l2:', id(l2), [id(l2[0]),id(l2[1])])\n\nlevel 2\nl1: 140006812656640 [7585536, 140006812590400]\nl2: 140006812645888 [7585536, 140006813422272]\n\n\n주소 140006812590400:(66,55,44)에 있는 값을 바꾸고 싶지만 불변형이라 못바꿈 \\(\\to\\) 그냥 새로 만들자. 그래서 그걸 140006813422272에 저장하자."
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#차원의-실체",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#차원의-실체",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "2차원의 실체",
    "text": "2차원의 실체\n- 2차원 array a,b를 선언하자.\n\na = np.array([[11,22,33,44]]).reshape(2,2)\nb = np.array([[11,22,33,44,55,66]]).reshape(2,3)\nc = np.array([11,22,33,44]).reshape(4,1)\nd = np.array([11,22,33,44]) # 1d\n\n- a,b,c,d 속성비교\n\na.shape, b.shape, c.shape, d.shape ## 차원 \n\n((2, 2), (2, 3), (4, 1), (4,))\n\n\n\na.strides, b.strides, c.strides, d.strides ## 차원이랑 관련이 있어보임.. + 8의 배수 \n\n((16, 8), (24, 8), (8, 8), (8,))\n\n\n- ((16, 8), (24, 8), (8, 8), (8,)) 와 같은 저 숫자들이 도데체 무엇을 의미하는거야?!\n\n사전지식: 컴퓨터는 하나의 숫자를 저장하는데 메모리를 8칸 쓴다.\n가정: 만약에 컴퓨터가 1차원으로만 숫자를 저장한다면??\nstrides의 의미: (다음 행으로 가기위해서 JUMP해야하는 메모리 공간수, 다음 열로 가기위해서 JUMP해야하는 메모리 공간수)\n\n- 통찰: strides의 존재로 인해서 유추할 수 있는 것은 a,b,c,d 는 모두 1차원으로 저장되어있다는 사실이다. (중첩된 리스트꼴이 아니라)\n- 그렇다면.. shallow copy = deep copy?!\n\nA1=[[1,2],[3,4]]\nA2=A1.copy()\nB1=np.array([[1,2],[3,4]])\nB2=B1.copy()\n\n\nB1\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nA2[0][0]=11\nB2[0][0]=11\n\n\nA1,A2\n\n([[11, 2], [3, 4]], [[11, 2], [3, 4]])\n\n\n\nB1,B2\n\n(array([[1, 2],\n        [3, 4]]),\n array([[11,  2],\n        [ 3,  4]]))\n\n\n- 해방: 넘파이를 쓰면 copy.deepcopy()를 쓰지 않아도 된다.\n- 용어정리: (필요할까..?)\n\nnumpy 한정 .copy() 는 copy모듈의 deep copy와 동등한 효과를 준다. 하지만 실제로는 shallow copy 이다. 공식문서에는 “Note that np.copy is a shallow copy and will not copy object elements within arrays.” 라고 명시되어 있음.\n일부 블로그에서 deep copy라고 주장하기도 함. 블로그1, 블로그2, 블로그3 // 블로그2의 경우 참조와 shallow copy도 구분못함..\n이따가 view라는 개념도 나올텐데 .copy()를 deep copy라고 주장하는 블로거들 대부분 .view()를 shallow copy 혹은 참조라고 주장한다. 하지만 copy와 view를 설명하는 공식문서에서는 view가 shallow copy라는 말을 찾아볼 수 없음.\n\n- 정리 (넘파이한정)\n\nnparray.copy(): 실제로는 shallow copy, 그런데 느낌은 deep copy\nnparray.view(): 실제로는 shallow copy 보다 더 얕은 단계의 카피, 그런데 느낌은 shallow copy"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#참조",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#참조",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "참조",
    "text": "참조\n- a를 선언, b는 a의 참조\n\na=np.array([[1,2],[3,4]])\nb=a ## 참조 \n\n\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n\na.shape\n\n(2, 2)\n\n\n\nb.shape\n\n(2, 2)\n\n\n- a의 shape을 바꾸어보자 \\(\\to\\) b도 같이 바뀐다\n\na.shape = (4,)\n\n\na\n\narray([1, 2, 3, 4])\n\n\n\nb\n\narray([1, 2, 3, 4])\n\n\n\nid(a),id(b)\n\n(139680327738544, 139680327738544)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#view",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#view",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "view",
    "text": "view\n- a를 선언, b는 a의 view\n\na=np.array([[1,2],[3,4]]) \nb=a.view() ## 어떤 블로그등에서는 shallow copy라고 부르기도 한다. 그렇게 공부하지 마세여..\n\n\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n\na.shape\n\n(2, 2)\n\n\n\nb.shape\n\n(2, 2)\n\n\n\na.shape= (4,1)\n\n\na\n\narray([[1],\n       [2],\n       [3],\n       [4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nid(a), id(b)\n\n(139679960161232, 139679932937872)\n\n\n- 그런데..\n\na[0]=100\n\n\na\n\narray([[100],\n       [  2],\n       [  3],\n       [  4]])\n\n\n\nb\n\narray([[100,   2],\n       [  3,   4]])\n\n\n- 출생의 비밀\n\nb\n\narray([[100,   2],\n       [  3,   4]])\n\n\n\nb.base\n\narray([[100],\n       [  2],\n       [  3],\n       [  4]])\n\n\n\n? 이거 바뀐 a아니야?\n\n\nid(b.base), id(a)\n\n(139679960161232, 139679960161232)\n\n\n- View\n\nb가 a의 뷰라는 의미는, b가 a를 소스로하여 만들어진 오브젝트란 의미이다.\n따라서 이때 b.base는 a가 된다.\nb는 자체적으로 데이터를 가지고 있지 않으며 a와 공유한다.\n\nnote1 원본 ndarray의 일 경우는 .base가 None으로 나온다.\n\na.base\n\nnote2 b.base의 shpae과 b의 shape은 아무 관련없다.\n\nb.shape\n\n(2, 2)\n\n\n\nb.base.shape # a.shape과 같음\n\n(4, 1)\n\n\n- numpy에서 view를 사용하는 예시 (transpose)\n\nX = np.random.normal(size=[100,2])\nid((X.T).base), id(X)\n\n(139679932937584, 139679932937584)\n\n\n\nX.T 는 X의 view 이다.\n\n\nX.T @ X ## 실제로 X.T를 메모리공간에 새로 만들어 숫자를 저장하지않고 X.T @ X를 계산할 수 있음 (R과차이점) \n\narray([[124.15127928,  -0.45772606],\n       [ -0.45772606,  79.17005817]])"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#copy",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#copy",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "copy",
    "text": "copy\n- a를 선언, b는 a의 copy\n\na=np.array([[1,2],[3,4]])\nb=a.copy() # 껍데기를 새로 생성 (strides, shape) + 데이터도 a와 독립적으로 새로 생성하여 따로 메모리에 저장함. \n\n\nid(a),id(b)\n\n(139680327737776, 139679932938832)\n\n\n- a의 shape을 바꿔도 b에는 적용되지 않음\n\na.shape = (4,1)\na\n\narray([[1],\n       [2],\n       [3],\n       [4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n- 그리고 a[0]의 값을 바꿔도 b에는 적용되지 않음.\n\na[0]=100\n\n\na\n\narray([[100],\n       [  2],\n       [  3],\n       [  4]])\n\n\n\nb\n\narray([[1, 2],\n       [3, 4]])\n\n\n- b의 출생을 조사해보니..\n\na.base,b.base\n\n(None, None)\n\n\n출생의 비밀은 없었다. 둘다 원본.\n- .view() 는 껍데기만 새로생성 // .copy() 는 껍데기와 데이터를 모두 새로 생성\n\nAppendix: .copy의 한계(?)\n(관찰)\n\na=np.array([1,[1,2]],dtype='O')\nb=a.copy()\nprint('시점1')\nprint('a=',a)\nprint('b=',b)\n\na[0]=222\nprint('시점2')\nprint('a=',a)\nprint('b=',b)\n\na[1][0]=333\nprint('시점2')\nprint('a=',a)\nprint('b=',b)\n\n시점1\na= [1 list([1, 2])]\nb= [1 list([1, 2])]\n시점2\na= [222 list([1, 2])]\nb= [1 list([1, 2])]\n시점2\na= [222 list([333, 2])]\nb= [1 list([333, 2])]\n\n\n\n왜 또 시점2에서는 a와 b가 같이 움직여?\n\n해결책: 더 깊은 복사\n\nimport copy\n\n\na=np.array([1,[1,2]],dtype='O')\nb=copy.deepcopy(a)\nprint('시점1')\nprint('a=',a)\nprint('b=',b)\n\na[0]=222\nprint('시점2')\nprint('a=',a)\nprint('b=',b)\n\na[1][0]=333\nprint('시점2')\nprint('a=',a)\nprint('b=',b)\n\n시점1\na= [1 list([1, 2])]\nb= [1 list([1, 2])]\n시점2\na= [222 list([1, 2])]\nb= [1 list([1, 2])]\n시점2\na= [222 list([333, 2])]\nb= [1 list([1, 2])]\n\n\n- 중간요약\n\n사실 b=a.copy()는 에서 .copy()는 사실 온전한 deep-copy가 아니다.\n그래서 a의 데이터가 중첩구조를 가지는 경우는 온전한 deep-copy가 수행되지 않는다.\n그런데 일반적으로 넘파이를 이용할때 자주 사용하는 데이터 구조인 행렬, 텐서등은 데이터가 중첩구조를 가지지 않는다. (1차원 array로만 저장되어 있음)\n따라서 행렬, 텐서에 한정하면 .copy()는 온전한 deep-copy라고 이해해도 무방하다. &lt;– 이것만 기억해!"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#요약",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#요약",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "요약",
    "text": "요약\n아래를 구분할 수 있으면 잘 이해한 것!!\narr = np.array(...) # arr -- [arr.shape, arr.strides, arr.base, ... ] \narr2 = arr \narr2 = arr.view()\narr2 = arr.copy()\narr2 = copy.deepcopy(arr)"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#별명-뷰-카피",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#별명-뷰-카피",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "별명, 뷰, 카피",
    "text": "별명, 뷰, 카피\n- test 함수 작성\n\ndef test(a,b): \n    if id(a) == id(b): \n        print(\"별명\")\n    elif id(a) == id(b.base) or id(a.base)==id(b): \n        print(\"뷰\")\n    elif (id(a.base)!=id(None) and id(b.base)!=id(None)) and id(a.base) == id(b.base):\n        print(\"공통의 base를 가짐\")\n    else: \n        print(\"카피, 혹은 아무 관련없는 오브젝트\") \n\n- 잘 동작하나?\n(테스트1)\n\na=np.array([1,2,3,4])\nb=a\n\n\ntest(a,b)\n\n별명\n\n\n(테스트2)\n\na=np.array([1,2,3,4])\nb=a.view()\n\n\ntest(a,b)\n\n뷰\n\n\n(테스트3)\n\na=np.array([1,2,3,4])\nb=a.view()\nc=a.view()\n\n\ntest(b,c)\n\n공통의 base를 가짐\n\n\n\ntest(a,b)\n\n뷰\n\n\n\ntest(a,c)\n\n뷰\n\n\n(테스트4)\n\na=np.array([1,2,3,4])\nb=a.copy()\n\n\ntest(a,b)\n\n카피, 혹은 아무 관련없는 오브젝트"
  },
  {
    "objectID": "posts/1_IP2022/2023-06-21-13wk-1.html#결론",
    "href": "posts/1_IP2022/2023-06-21-13wk-1.html#결론",
    "title": "13wk-1: 깊은복사와 얕은복사",
    "section": "결론",
    "text": "결론\n- 참조, 뷰, 카피의 개념을 잘 알고 있고 때에 따라 메모리를 아끼면서 이들을 적절하게 사용하고 싶을것 같음. 하지만 이건 불가능한 소망임.\n- 우리가 사용했던 어떠한 것들이 뷰가 나올지 카피가 나올지 잘 모른다. (그래서 원리를 이해해도 대응할 방법이 사실없음)\n\n예시1\n\na=np.array([1,2,3,4])\nb=a[:3]\n\n\na\n\narray([1, 2, 3, 4])\n\n\n\nb\n\narray([1, 2, 3])\n\n\n\ntest(a,b)\n\n뷰\n\n\n\nc=a[[0,1,2]]\nc\n\narray([1, 2, 3])\n\n\n\ntest(a,c)\n\n카피, 혹은 아무 관련없는 오브젝트\n\n\n\n\n예시2\n\na=np.array([[1,2],[3,4]])\na\n\narray([[1, 2],\n       [3, 4]])\n\n\n\nb=a.flatten() # 플래튼은 펼치는건데..\nc=a.ravel() # 라벨도 펼치라는 뜻인데..\nd=a.reshape(-1) # 이것도 뜻은 펼치는건데..\n# 걍 다 똑같은거아냐?\n\n\ntest(a,b)\n\n카피, 혹은 아무 관련없는 오브젝트\n\n\n\ntest(a,c)\n\n뷰\n\n\n\ntest(a,d)\n\n뷰\n\n\n\ntest(c,d)\n\n공통의 base를 가짐\n\n\n\ntest(b,c)\n\n카피, 혹은 아무 관련없는 오브젝트\n\n\n- 심지어 copy인줄 알았던것이 사실 view라서 원치않는 side effect이 생길수 있음. \\(\\to\\) 그냥 방어적 프로그래밍이 최선인듯"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class6.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class6.html",
    "title": "class 6단계",
    "section": "",
    "text": "상속, 사용자정의 자료형\n\n\n\n- 아래와 같은 클래스를 만들자.\n\n이름, 직급, 연봉에 대한 정보가 있다.\n연봉을 올려주는 메소드가 존재함.\n\n\nclass Employee:\n    def __init__(self, name,position=None, pay=0):\n        self.name = name\n        self.position = position\n        self.pay = pay\n    def _repr_html_(self):\n        html_str = \"\"\"\n        이름: {} &lt;br/&gt;\n        직급: {} &lt;br/&gt;\n        연봉: {} &lt;br/&gt;\n        \"\"\".format(self.name, self.position, self.pay)\n        return html_str\n    def giveraise(self, pct):\n        self.pay = self.pay * (1+pct)\n\n- 확인\n\niu = Employee('iu', position = 'staff', pay = 5000)\nhynn = Employee('hynn', position = 'staff', pay = 4000)\nhd = Employee('hodong', position = 'mgr', pay = 8000)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5000 \n        \n\n\n\niu.giveraise(0.1)\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn.giveraise(0.2)\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4800.0 \n        \n\n\n- 회사의 모든 직원의 연봉을 \\(10\\%\\)씩 올려보자.\n\niu = Employee('iu', position = 'staff', pay = 5000)\nhynn = Employee('hynn', position = 'staff', pay = 4000)\nhd = Employee('hodong', position = 'mgr', pay = 8000)\n\n\nfor i in [iu, hynn, hd]:\n    i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 8800.0 \n        \n\n\n- 매니저직은 일반직원들의 상승분에서 \\(5\\%\\)의 보너스가 추가되어 상승한다고 가정하고 모든 직원의 연봉을 \\(10\\%\\)씩 올리는 코드를 구현해보자.\n\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Employee('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu, hynn, hd]:\n    if i.position == 'mgr':\n        i.giveraise(0.1 + 0.05)\n    else:\n        i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.0 \n        \n\n\n\n\n\n\nclass Manager:\n    def __init__(self, name, position=None, pay=0):\n        self.name = name\n        self.position = position\n        self.pay = pay\n    def _repr_html_(self):\n        html_str = \"\"\"\n        이름: {} &lt;br/&gt;\n        직급: {} &lt;br/&gt;\n        연봉: {} &lt;br/&gt;\n        \"\"\".format(self.name, self.position, self.pay)\n        return html_str\n    def giveraise(self,pct):\n        self.pay = self.pay * (1+pct+0.05)\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Manager('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu,hynn,hd]:\n    i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.000000000002 \n        \n\n\n\n\n\n\nclass Manager(Employee):\n    def giveraise(self,pct):\n        self.pay = self.pay * (1+pct+0.05)\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Manager('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu,hynn,hd]:\n    i.giveraise(0.1) \n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.000000000002 \n        \n\n\n- 요약: 이미 만들어진 클래스에서 대부분의 기능은 그대로 쓰지만 일부기능만 변경 혹은 추가하고 싶다면 클래스를 상속하면 된다!\n\n\n\n\nref: http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791165213190\n\n- list와 비슷한데 멤버들의 빈도가 계산되는 메소드를 포함하는 새로운 나만의 list를 만들고 싶다.\n\nlst = ['a','b','a','c','b','a','d']\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n- 아래와 같은 딕셔너리를 만들고 싶다.\n\nfreq = {'a':3, 'b':2, 'c':1, 'd':1} \nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n\nlst.frequency()를 입력하면 위의 기능이 수행되도록 변형된 list를 쓰고 싶다.\n\n- 구현\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = {'a':0, 'b':0, 'c':0, 'd':0}\nfreq\n\n{'a': 0, 'b': 0, 'c': 0, 'd': 0}\n\n\n\nfor item in lst:\n    freq[item] = freq[item] + 1\n\n\nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = dict()\nfreq\n\n{}\n\n\n\nfor item in lst:\n    freq[item] = freq[item] + 1\n\nKeyError: 'a'\n\n\n에러이유? freq['a']를 호출할 수 없다. \\(\\to\\) freq.get('a',0) 이용\n\nfreq['a']\n\nKeyError: 'a'\n\n\n\nfreq.get?\n\n\nSignature: freq.get(key, default=None, /)\nDocstring: Return the value for key if key is in the dictionary, else default.\nType:      builtin_function_or_method\n\n\n\n\nkey에 대응하는 값이 있으면 그 값을 리턴하고 없으면 default를 리턴\n\n\nfreq.get('a') # freq['a']에 해당하는 자료가 없어도 에러가 나지 않음\n\n\nfreq.get('a',0) # freq['a']에 해당하는 자료가 없어도 에러가 나지 않음 + freq['a']에 해당하는 자료가 없으면 0을 리턴\n\n0\n\n\n\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = dict()\nfreq\n\n{}\n\n\n\nfor item in lst:\n    freq[item] = freq.get(item,0) + 1\n\n\nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n- 이것을 내가 정의하는 새로은 list의 메소드로 넣고 싶다.\n\nclass L(list):\n    def frequency(self):\n        freq = dict()\n        for item in self:\n            freq[item] = freq.get(item,0) + 1\n        return freq\n\n\nlst = L([1,1,1,2,2,3])\n\n\nlst # 원래 list에 있는 repr 기능을 상속받아서 이루어지는 결과\n\n[1, 1, 1, 2, 2, 3]\n\n\n\n_lst = L([4,5,6])\nlst + _lst  # L자료형끼리의 덧셈\n\n[1, 1, 1, 2, 2, 3, 4, 5, 6]\n\n\n\nlst + [4,5,6] # lst + [4,5,6] # L자료형과 list자료형의 덧셈도 가능\n\n[1, 1, 1, 2, 2, 3, 4, 5, 6]\n\n\n\nL자료형의 덧셈은 list의 덧셈과 완전히 같음\n\n\nlst.append(10) # append 함수도 그대로 쓸 수 있음.\n\n\nlst\n\n[1, 1, 1, 2, 2, 3, 10]\n\n\n- 기존 리스트에서 추가로 frequency() 메소드가 존재함.\n\nlst.frequency()\n\n{1: 3, 2: 2, 3: 1, 10: 1}\n\n\n\n\n\n\n\n- 사용자정의 자료형이 어떤 경우에는 유용할 수 있다.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nyear = ['2016','2017','2017','2017',2017,2018,2018,2019,2019] \nvalue = np.random.randn(9)\n\n\ndf = pd.DataFrame({'year':year, 'value':value})\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n-0.140139\n\n\n1\n2017\n1.412758\n\n\n2\n2017\n-0.065478\n\n\n3\n2017\n0.107847\n\n\n4\n2017\n0.824112\n\n\n5\n2018\n0.061573\n\n\n6\n2018\n-0.463060\n\n\n7\n2019\n-0.808921\n\n\n8\n2019\n0.389417\n\n\n\n\n\n\n\n\nplt.plot(df.year, df.value)\n\nTypeError: 'value' must be an instance of str or bytes, not a int\n\n\n\n\n\n에러의 이유: df.year에 str,int가 동시에 있음.\n\nnp.array(df.year)\n\narray(['2016', '2017', '2017', '2017', 2017, 2018, 2018, 2019, 2019],\n      dtype=object)\n\n\n자료형을 바꿔주면 해결할 수 있다.\n\nnp.array(df.year, dtype=np.float64)\n#np.array(df.year).astype(np.float64)\n#df.year.astype(np.float64)\n\narray([2016., 2017., 2017., 2017., 2017., 2018., 2018., 2019., 2019.])\n\n\n\nplt.plot(df.year.astype(np.float64),df.value,'.')\n\n\n\n\n\n\n\n\nyear = ['2016','2017','2017','2017년','2017년',2018,2018,2019,2019] \nvalue = np.random.randn(9)\n\n\ndf= pd.DataFrame({'year':year,'value':value})\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n0.127739\n\n\n1\n2017\n1.437921\n\n\n2\n2017\n-1.137349\n\n\n3\n2017년\n-0.178713\n\n\n4\n2017년\n-0.276401\n\n\n5\n2018\n2.467760\n\n\n6\n2018\n-1.068202\n\n\n7\n2019\n-0.313908\n\n\n8\n2019\n1.049837\n\n\n\n\n\n\n\n\nnp.array(df.year,dtype=np.float64) # 타입을 일괄적으로 바꾸기 어렵다. \n\nValueError: could not convert string to float: '2017년'\n\n\n\nL(df.year).frequency()\n\n{'2016': 1, '2017': 2, '2017년': 2, 2018: 2, 2019: 2}\n\n\n\n’2016’와 같은 형태, ’2017년’와 같은 형태, 숫자형이 혼합 \\(\\to\\) 맞춤형 변환이 필요함\n\n\n'2017년'.replace('년','')\n\n'2017'\n\n\n\ndef f(a): ## 사실 데이터의 구조를 모르면 이런 함수를 짤 수 없음 --&gt; 자료의 구조를 확인해준다는 의미에서 freq가 있다면 편리하다. \n    if type(a) is str:\n        if '년' in a:\n            return int(a.replace('년',''))\n        else:\n            return int(a)\n    else:\n        return a\n\n\n[f(a) for a in df.year]\n\n[2016, 2017, 2017, 2017, 2017, 2018, 2018, 2019, 2019]\n\n\n\ndf.year = [f(a) for a in df.year]\n\n\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n0.127739\n\n\n1\n2017\n1.437921\n\n\n2\n2017\n-1.137349\n\n\n3\n2017\n-0.178713\n\n\n4\n2017\n-0.276401\n\n\n5\n2018\n2.467760\n\n\n6\n2018\n-1.068202\n\n\n7\n2019\n-0.313908\n\n\n8\n2019\n1.049837\n\n\n\n\n\n\n\n\nplt.plot(df.year, df.value,'.')"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class6.html#인사관리-예제",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class6.html#인사관리-예제",
    "title": "class 6단계",
    "section": "",
    "text": "- 아래와 같은 클래스를 만들자.\n\n이름, 직급, 연봉에 대한 정보가 있다.\n연봉을 올려주는 메소드가 존재함.\n\n\nclass Employee:\n    def __init__(self, name,position=None, pay=0):\n        self.name = name\n        self.position = position\n        self.pay = pay\n    def _repr_html_(self):\n        html_str = \"\"\"\n        이름: {} &lt;br/&gt;\n        직급: {} &lt;br/&gt;\n        연봉: {} &lt;br/&gt;\n        \"\"\".format(self.name, self.position, self.pay)\n        return html_str\n    def giveraise(self, pct):\n        self.pay = self.pay * (1+pct)\n\n- 확인\n\niu = Employee('iu', position = 'staff', pay = 5000)\nhynn = Employee('hynn', position = 'staff', pay = 4000)\nhd = Employee('hodong', position = 'mgr', pay = 8000)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5000 \n        \n\n\n\niu.giveraise(0.1)\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn.giveraise(0.2)\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4800.0 \n        \n\n\n- 회사의 모든 직원의 연봉을 \\(10\\%\\)씩 올려보자.\n\niu = Employee('iu', position = 'staff', pay = 5000)\nhynn = Employee('hynn', position = 'staff', pay = 4000)\nhd = Employee('hodong', position = 'mgr', pay = 8000)\n\n\nfor i in [iu, hynn, hd]:\n    i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 8800.0 \n        \n\n\n- 매니저직은 일반직원들의 상승분에서 \\(5\\%\\)의 보너스가 추가되어 상승한다고 가정하고 모든 직원의 연봉을 \\(10\\%\\)씩 올리는 코드를 구현해보자.\n\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Employee('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu, hynn, hd]:\n    if i.position == 'mgr':\n        i.giveraise(0.1 + 0.05)\n    else:\n        i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.0 \n        \n\n\n\n\n\n\nclass Manager:\n    def __init__(self, name, position=None, pay=0):\n        self.name = name\n        self.position = position\n        self.pay = pay\n    def _repr_html_(self):\n        html_str = \"\"\"\n        이름: {} &lt;br/&gt;\n        직급: {} &lt;br/&gt;\n        연봉: {} &lt;br/&gt;\n        \"\"\".format(self.name, self.position, self.pay)\n        return html_str\n    def giveraise(self,pct):\n        self.pay = self.pay * (1+pct+0.05)\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Manager('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu,hynn,hd]:\n    i.giveraise(0.1)\n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.000000000002 \n        \n\n\n\n\n\n\nclass Manager(Employee):\n    def giveraise(self,pct):\n        self.pay = self.pay * (1+pct+0.05)\n\n\niu=Employee('iu',position='staff',pay=5000)\nhynn=Employee('hynn',position='staff',pay=4000)\nhd=Manager('hodong',position='mgr',pay=8000)\n\n\nfor i in [iu,hynn,hd]:\n    i.giveraise(0.1) \n\n\niu\n\n\n        이름: iu \n        직급: staff \n        연봉: 5500.0 \n        \n\n\n\nhynn\n\n\n        이름: hynn \n        직급: staff \n        연봉: 4400.0 \n        \n\n\n\nhd\n\n\n        이름: hodong \n        직급: mgr \n        연봉: 9200.000000000002 \n        \n\n\n- 요약: 이미 만들어진 클래스에서 대부분의 기능은 그대로 쓰지만 일부기능만 변경 혹은 추가하고 싶다면 클래스를 상속하면 된다!\n\n\n\n\nref: http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9791165213190\n\n- list와 비슷한데 멤버들의 빈도가 계산되는 메소드를 포함하는 새로운 나만의 list를 만들고 싶다.\n\nlst = ['a','b','a','c','b','a','d']\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n- 아래와 같은 딕셔너리를 만들고 싶다.\n\nfreq = {'a':3, 'b':2, 'c':1, 'd':1} \nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n\nlst.frequency()를 입력하면 위의 기능이 수행되도록 변형된 list를 쓰고 싶다.\n\n- 구현\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = {'a':0, 'b':0, 'c':0, 'd':0}\nfreq\n\n{'a': 0, 'b': 0, 'c': 0, 'd': 0}\n\n\n\nfor item in lst:\n    freq[item] = freq[item] + 1\n\n\nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = dict()\nfreq\n\n{}\n\n\n\nfor item in lst:\n    freq[item] = freq[item] + 1\n\nKeyError: 'a'\n\n\n에러이유? freq['a']를 호출할 수 없다. \\(\\to\\) freq.get('a',0) 이용\n\nfreq['a']\n\nKeyError: 'a'\n\n\n\nfreq.get?\n\n\nSignature: freq.get(key, default=None, /)\nDocstring: Return the value for key if key is in the dictionary, else default.\nType:      builtin_function_or_method\n\n\n\n\nkey에 대응하는 값이 있으면 그 값을 리턴하고 없으면 default를 리턴\n\n\nfreq.get('a') # freq['a']에 해당하는 자료가 없어도 에러가 나지 않음\n\n\nfreq.get('a',0) # freq['a']에 해당하는 자료가 없어도 에러가 나지 않음 + freq['a']에 해당하는 자료가 없으면 0을 리턴\n\n0\n\n\n\n\n\n\nlst\n\n['a', 'b', 'a', 'c', 'b', 'a', 'd']\n\n\n\nfreq = dict()\nfreq\n\n{}\n\n\n\nfor item in lst:\n    freq[item] = freq.get(item,0) + 1\n\n\nfreq\n\n{'a': 3, 'b': 2, 'c': 1, 'd': 1}\n\n\n- 이것을 내가 정의하는 새로은 list의 메소드로 넣고 싶다.\n\nclass L(list):\n    def frequency(self):\n        freq = dict()\n        for item in self:\n            freq[item] = freq.get(item,0) + 1\n        return freq\n\n\nlst = L([1,1,1,2,2,3])\n\n\nlst # 원래 list에 있는 repr 기능을 상속받아서 이루어지는 결과\n\n[1, 1, 1, 2, 2, 3]\n\n\n\n_lst = L([4,5,6])\nlst + _lst  # L자료형끼리의 덧셈\n\n[1, 1, 1, 2, 2, 3, 4, 5, 6]\n\n\n\nlst + [4,5,6] # lst + [4,5,6] # L자료형과 list자료형의 덧셈도 가능\n\n[1, 1, 1, 2, 2, 3, 4, 5, 6]\n\n\n\nL자료형의 덧셈은 list의 덧셈과 완전히 같음\n\n\nlst.append(10) # append 함수도 그대로 쓸 수 있음.\n\n\nlst\n\n[1, 1, 1, 2, 2, 3, 10]\n\n\n- 기존 리스트에서 추가로 frequency() 메소드가 존재함.\n\nlst.frequency()\n\n{1: 3, 2: 2, 3: 1, 10: 1}"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class6.html#appendix-사용자정의-자료형의-유용함",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class6.html#appendix-사용자정의-자료형의-유용함",
    "title": "class 6단계",
    "section": "",
    "text": "- 사용자정의 자료형이 어떤 경우에는 유용할 수 있다.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nyear = ['2016','2017','2017','2017',2017,2018,2018,2019,2019] \nvalue = np.random.randn(9)\n\n\ndf = pd.DataFrame({'year':year, 'value':value})\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n-0.140139\n\n\n1\n2017\n1.412758\n\n\n2\n2017\n-0.065478\n\n\n3\n2017\n0.107847\n\n\n4\n2017\n0.824112\n\n\n5\n2018\n0.061573\n\n\n6\n2018\n-0.463060\n\n\n7\n2019\n-0.808921\n\n\n8\n2019\n0.389417\n\n\n\n\n\n\n\n\nplt.plot(df.year, df.value)\n\nTypeError: 'value' must be an instance of str or bytes, not a int\n\n\n\n\n\n에러의 이유: df.year에 str,int가 동시에 있음.\n\nnp.array(df.year)\n\narray(['2016', '2017', '2017', '2017', 2017, 2018, 2018, 2019, 2019],\n      dtype=object)\n\n\n자료형을 바꿔주면 해결할 수 있다.\n\nnp.array(df.year, dtype=np.float64)\n#np.array(df.year).astype(np.float64)\n#df.year.astype(np.float64)\n\narray([2016., 2017., 2017., 2017., 2017., 2018., 2018., 2019., 2019.])\n\n\n\nplt.plot(df.year.astype(np.float64),df.value,'.')\n\n\n\n\n\n\n\n\nyear = ['2016','2017','2017','2017년','2017년',2018,2018,2019,2019] \nvalue = np.random.randn(9)\n\n\ndf= pd.DataFrame({'year':year,'value':value})\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n0.127739\n\n\n1\n2017\n1.437921\n\n\n2\n2017\n-1.137349\n\n\n3\n2017년\n-0.178713\n\n\n4\n2017년\n-0.276401\n\n\n5\n2018\n2.467760\n\n\n6\n2018\n-1.068202\n\n\n7\n2019\n-0.313908\n\n\n8\n2019\n1.049837\n\n\n\n\n\n\n\n\nnp.array(df.year,dtype=np.float64) # 타입을 일괄적으로 바꾸기 어렵다. \n\nValueError: could not convert string to float: '2017년'\n\n\n\nL(df.year).frequency()\n\n{'2016': 1, '2017': 2, '2017년': 2, 2018: 2, 2019: 2}\n\n\n\n’2016’와 같은 형태, ’2017년’와 같은 형태, 숫자형이 혼합 \\(\\to\\) 맞춤형 변환이 필요함\n\n\n'2017년'.replace('년','')\n\n'2017'\n\n\n\ndef f(a): ## 사실 데이터의 구조를 모르면 이런 함수를 짤 수 없음 --&gt; 자료의 구조를 확인해준다는 의미에서 freq가 있다면 편리하다. \n    if type(a) is str:\n        if '년' in a:\n            return int(a.replace('년',''))\n        else:\n            return int(a)\n    else:\n        return a\n\n\n[f(a) for a in df.year]\n\n[2016, 2017, 2017, 2017, 2017, 2018, 2018, 2019, 2019]\n\n\n\ndf.year = [f(a) for a in df.year]\n\n\ndf\n\n\n\n\n\n\n\n\nyear\nvalue\n\n\n\n\n0\n2016\n0.127739\n\n\n1\n2017\n1.437921\n\n\n2\n2017\n-1.137349\n\n\n3\n2017\n-0.178713\n\n\n4\n2017\n-0.276401\n\n\n5\n2018\n2.467760\n\n\n6\n2018\n-1.068202\n\n\n7\n2019\n-0.313908\n\n\n8\n2019\n1.049837\n\n\n\n\n\n\n\n\nplt.plot(df.year, df.value,'.')"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class8.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class8.html",
    "title": "class 8단계",
    "section": "",
    "text": "for문 복습, iterable object\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\n\n- 아래와 같은 예제들을 관찰하여 for문을 복습하자.\n(예제1)\n\nfor i in [1,2,3,4]:\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제2)\n\nfor i in (1,2,3,4):\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제3)\n\nfor i in '1234':\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제4)\n\na=5\nfor i in a:\n    print(i)\n\nTypeError: 'int' object is not iterable\n\n\n\n5라고 출력되어야 하지 않나?\n\n- 의문1:\nfor i in ???:\n    print(i)\n에서 ???자리에 올 수 있는 것이 무엇일까?\n(예제5)\n상황1\n\nlst = [[1,2,3,4],[3,4,5,6]]\nlst\n\n[[1, 2, 3, 4], [3, 4, 5, 6]]\n\n\n\nfor l in lst:\n    print(l)\n\n[1, 2, 3, 4]\n[3, 4, 5, 6]\n\n\n상황2\n\ndf = pd.DataFrame(lst)\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nfor i in df:\n    print(i)\n\n0\n1\n2\n3\n\n\n칼럼이름들이 나오는 것 같음 \\(\\to\\) 확인해보자.\n\ndf.columns = pd.Index(['X'+str(i) for i in range(1,5)])\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nfor i in df:\n    print(i)\n\nX1\nX2\nX3\nX4\n\n\n- 의문2: for의 출력결과는 어떻게 예측할 수 있을까?\n\n\n\n- 의문1의 해결: 아래의 ??? 자리에 올 수 있는 것은 dir() 하여 __iter__ 가 있는 object이다.\nfor i in ???:\n    print(i)\n- 확인\n\na = [1,2,3] # list\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na = 1,2,3 # tuple\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na = '123' # string\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na=3\nset(dir(a)) & {'__iter__'}\n\nset()\n\n\niterable 하지 않다라는 것은dir()을 쳤을 때 __iter__ 라는 메소드가 없다는 것을 의미\n\n예상대로 예제1~예제4에서는 int클래스의 instance만 __iter__가 없다.\nfor문 뒤 ??? 자리에 올 수 있는 것은 iterable object만 올 수 있다.\n\n- __iter__의 역할: iterable object를 iterator로 만들 수 있다.\n\nlst = [1,2,3]\nlst\n\n[1, 2, 3]\n\n\n\nlst[1] # 충실한 리스트\n\n2\n\n\n\nltor = iter(lst) # 아래와 같은 표현 (a.__str__() = str(a)가 같은 것처럼)\n#ltor = lst.__iter__() # list iterator\nltor\n\n&lt;list_iterator at 0x7f2dfc4c51f0&gt;\n\n\n\nltor[1] # 더이상 리스트가 아니다.\n\nTypeError: 'list_iterator' object is not subscriptable\n\n\n\nltor?\n\n\nType:        list_iterator\nString form: &lt;list_iterator object at 0x7f2dfc4c51f0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n- iterator가 되면 무엇이 좋은가? \\(\\to\\) 숨겨진 기능 __next__가 열린다.\n\nlst\n\n[1, 2, 3]\n\n\n\nset(dir(lst)) & {'__next__'}, set(dir(ltor)) & {'__next__'}\n\n(set(), {'__next__'})\n\n\n\nlst에는 __next__ 가 없지만 ltor에는 있다.\n\n- 그래서 __next__의 기능은? \\(\\to\\) 원소를 차례대로 꺼내준다. + 더 이상 꺼낼 원소가 없으면 Stopiteration Error 발생시킨다.\n\nlst\n\n[1, 2, 3]\n\n\n\nltor.__next__()\n\n1\n\n\n\nltor.__next__()\n\n2\n\n\n\nltor.__next__()\n\n3\n\n\n\nltor.__next__()\n\nStopIteration: \n\n\n- for문의 동작원리\nfor i in lst:\n    print(i)\n\nlst.__iter__() 혹은 iter(lst)를 이용하여 lst를 iterator로 만든다. (iterable object를 iterator object로 만든다.)\niterator에서 .__next__() 함수를 호출하고 결과를 i에 저장한 뒤에 for문 블락안에 있는 내용 (들여쓰기 된 내용)을 실행한다. \\(\\to\\) 반복\nStopIteration 에러가 발생하면 for문을 멈춘다.\n\n- 아래의 ??? 자리에 올 수 있는 것이 iterable object 가 아니라 iterator 자체여도 for문이 돌아갈까? (당연히 돌아가야 할 것 같음)\nfor i in ???:\n    print(i)\n\nfor i in [1,2,3]: # iterable object\n    print(i)\n\n1\n2\n3\n\n\n\n당연히 가능!\n\n- a가 iterator일때 iter(a)의 출력결과가 a와 같도록 조정한다면 for문의 동작원리 (1)-(3)을 수행하지 않아도 좋다. \\(\\to\\) 실제로 이렇게 동작한다.\n- 요약\n\niterable object는 숨겨진 기능으로 __iter__를 가진다.\niterator object는 숨겨진 기능으로 __iter__와 __next__를 가진다. (즉 iterator는 그 자체로 iterable object가 된다!)\n\n\nlst = [1,2,3]\nltor = iter(lst)\n\n\nset(dir(lst)) & {'__iter__','__next__'}\n\n{'__iter__'}\n\n\n\nset(dir(ltor)) & {'__iter__', '__next__'}\n\n{'__iter__', '__next__'}\n\n\n- 의문2의 해결: for의 출력결과는 어떻게 예측할 수 있을까? iterator를 만들어서 .__next__()의 출력값을 확인하면 알 수 있다.\n\nfor i in df:\n    print(i)\n\nX1\nX2\nX3\nX4\n\n\n\ndftor = iter(df)\ndftor?\n\n\nType:        map\nString form: &lt;map object at 0x7f2dfc4c85b0&gt;\nDocstring:  \nmap(func, *iterables) --&gt; map object\nMake an iterator that computes the function using arguments from\neach of the iterables.  Stops when the shortest iterable is exhausted.\n\n\n\n\ndftor.__next__()\n\n'X1'\n\n\n\ndftor.__next__()\n\n'X2'\n\n\n\ndftor.__next__()\n\n'X3'\n\n\n\ndftor.__next__()\n\n'X4'\n\n\n\ndftor.__next__()\n\nStopIteration: \n\n\n\n\n\n- 파이썬에서 for문을 처음 배울 때: range(5)를 써라!\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\nrange(5)가 도대체 무엇이길래? ## iterator 아니면 iterable object 일건데..\n\n\nrange(5)\n\nrange(0, 5)\n\n\n\nrepr(range(5))\n\n'range(0, 5)'\n\n\n- range(5)의 정체는 그냥 iterable object이다.\n\nset(dir(range(5))) & {'__iter__', '__next__'}\n\n{'__iter__'}\n\n\n__next__ 는 갖고있지 않은데 __iter__만 갖고있으니까 range(5)는 iterable object\n- 그래서 언제든지 iterator로 바꿀 수 있다.\n\nrtor = iter(range(5))\n\n\nrtor?\n\n\nType:        range_iterator\nString form: &lt;range_iterator object at 0x7f2dfc4c56c0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\nset(dir(rtor)) & {'__iter__','__next__'}\n\n{'__iter__', '__next__'}\n\n\n- for문에서 range(5)가 행동하는 방법?\n\nrtor.__next__()\n\n0\n\n\n\nrtor.__next__()\n\n1\n\n\n\nrtor.__next__()\n\n2\n\n\n\nrtor.__next__()\n\n3\n\n\n\nrtor.__next__()\n\n4\n\n\n\nrtor.__next__()\n\nStopIteration: \n\n\n\n\n\n- 이터레이터의 개념을 알면 for문에 대한 이해도가 대폭 상승한다.\n\nfor i in zip([1,2,3],'abc'):\n    print(i)\n\n(1, 'a')\n(2, 'b')\n(3, 'c')\n\n\n\nzip은 뭐지???\n\n\nzip([1,2,3],'abc')\n\n&lt;zip at 0x7f2dfc4e8340&gt;\n\n\n- 어차피 for i in ????: 의 ???? 자리는 iterable object(iterator)의 자리이다.\n\nset(dir(zip([1,2,3],'abc'))) & {'__iter__','__next__'}\n\n{'__iter__', '__next__'}\n\n\n\n__next__() 함수가 있음 \\(\\to\\) zip([1,2,3],'abc')는 그자체로 iterator 였다!\n\n\nz = zip([1,2,3],'abc')\n\n\nz.__next__()\n\n(1, 'a')\n\n\n\nz.__next__()\n\n(2, 'b')\n\n\n\nz.__next__()\n\n(3, 'c')\n\n\n\nz.__next__()\n\nStopIteration: \n\n\n\n\n\n- 내가 이터레이터를 만들어보자.\n\nclass Klass: # 찌를 내는 순간 for문이 멈추도록 하는 이터레이터를 만들자.\n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        if action == '찌':\n            print('찌가 나와서 for문을 멈춥니다.')\n            raise StopIteration\n        else:\n            return action\n\n\na = Klass() # 클래스로부터 인스턴스 만들기\n\n\na?\n\n\nType:        Klass\nString form: &lt;__main__.Klass object at 0x7f2dfc373af0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\nset(dir(a)) & {'__iter__', '__next__'} # a는 이터레이터!\n\n{'__iter__', '__next__'}\n\n\n\na.__next__()\n\n'빠'\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n찌가 나와서 for문을 멈춥니다.\n\n\nStopIteration: \n\n\n\nfor i in a:\n    print(i)\n\n빠\n묵\n묵\n빠\n찌가 나와서 for문을 멈춥니다.\n\n\n\n\n\n\n파이썬의 비밀1: 자료형은 클래스의 이름이다.\n파이썬의 비밀2: 클래스에는 __str__ 처럼 숨겨진 매서드가 존재한다. 이를 이용하여 파이썬 내부의 기능을 가로챌 수 있다.\n파이썬의 비밀3: 주피터노트북(대화형 콘솔)에서는 “오브젝트이름 + 엔터”를 쳐서 나오는 출력은 __repr__로 가로챌 수 있다. (주피터의 비밀)\n파이썬의 비밀4: 함수와 클래스는 숨겨진 메소드에 __call__을 가진 오브젝트일 뿐이다.\n파이썬의 비밀5: for문의 비밀(iterable object, iterator, StopIteration Error)"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class8.html#import",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class8.html#import",
    "title": "class 8단계",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class8.html#for문의-복습",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class8.html#for문의-복습",
    "title": "class 8단계",
    "section": "",
    "text": "- 아래와 같은 예제들을 관찰하여 for문을 복습하자.\n(예제1)\n\nfor i in [1,2,3,4]:\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제2)\n\nfor i in (1,2,3,4):\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제3)\n\nfor i in '1234':\n    print(i)\n\n1\n2\n3\n4\n\n\n(예제4)\n\na=5\nfor i in a:\n    print(i)\n\nTypeError: 'int' object is not iterable\n\n\n\n5라고 출력되어야 하지 않나?\n\n- 의문1:\nfor i in ???:\n    print(i)\n에서 ???자리에 올 수 있는 것이 무엇일까?\n(예제5)\n상황1\n\nlst = [[1,2,3,4],[3,4,5,6]]\nlst\n\n[[1, 2, 3, 4], [3, 4, 5, 6]]\n\n\n\nfor l in lst:\n    print(l)\n\n[1, 2, 3, 4]\n[3, 4, 5, 6]\n\n\n상황2\n\ndf = pd.DataFrame(lst)\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nfor i in df:\n    print(i)\n\n0\n1\n2\n3\n\n\n칼럼이름들이 나오는 것 같음 \\(\\to\\) 확인해보자.\n\ndf.columns = pd.Index(['X'+str(i) for i in range(1,5)])\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\n0\n1\n2\n3\n4\n\n\n1\n3\n4\n5\n6\n\n\n\n\n\n\n\n\nfor i in df:\n    print(i)\n\nX1\nX2\nX3\nX4\n\n\n- 의문2: for의 출력결과는 어떻게 예측할 수 있을까?"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class8.html#for문의-동작원리",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class8.html#for문의-동작원리",
    "title": "class 8단계",
    "section": "",
    "text": "- 의문1의 해결: 아래의 ??? 자리에 올 수 있는 것은 dir() 하여 __iter__ 가 있는 object이다.\nfor i in ???:\n    print(i)\n- 확인\n\na = [1,2,3] # list\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na = 1,2,3 # tuple\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na = '123' # string\nset(dir(a)) & {'__iter__'}\n\n{'__iter__'}\n\n\n\na=3\nset(dir(a)) & {'__iter__'}\n\nset()\n\n\niterable 하지 않다라는 것은dir()을 쳤을 때 __iter__ 라는 메소드가 없다는 것을 의미\n\n예상대로 예제1~예제4에서는 int클래스의 instance만 __iter__가 없다.\nfor문 뒤 ??? 자리에 올 수 있는 것은 iterable object만 올 수 있다.\n\n- __iter__의 역할: iterable object를 iterator로 만들 수 있다.\n\nlst = [1,2,3]\nlst\n\n[1, 2, 3]\n\n\n\nlst[1] # 충실한 리스트\n\n2\n\n\n\nltor = iter(lst) # 아래와 같은 표현 (a.__str__() = str(a)가 같은 것처럼)\n#ltor = lst.__iter__() # list iterator\nltor\n\n&lt;list_iterator at 0x7f2dfc4c51f0&gt;\n\n\n\nltor[1] # 더이상 리스트가 아니다.\n\nTypeError: 'list_iterator' object is not subscriptable\n\n\n\nltor?\n\n\nType:        list_iterator\nString form: &lt;list_iterator object at 0x7f2dfc4c51f0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n- iterator가 되면 무엇이 좋은가? \\(\\to\\) 숨겨진 기능 __next__가 열린다.\n\nlst\n\n[1, 2, 3]\n\n\n\nset(dir(lst)) & {'__next__'}, set(dir(ltor)) & {'__next__'}\n\n(set(), {'__next__'})\n\n\n\nlst에는 __next__ 가 없지만 ltor에는 있다.\n\n- 그래서 __next__의 기능은? \\(\\to\\) 원소를 차례대로 꺼내준다. + 더 이상 꺼낼 원소가 없으면 Stopiteration Error 발생시킨다.\n\nlst\n\n[1, 2, 3]\n\n\n\nltor.__next__()\n\n1\n\n\n\nltor.__next__()\n\n2\n\n\n\nltor.__next__()\n\n3\n\n\n\nltor.__next__()\n\nStopIteration: \n\n\n- for문의 동작원리\nfor i in lst:\n    print(i)\n\nlst.__iter__() 혹은 iter(lst)를 이용하여 lst를 iterator로 만든다. (iterable object를 iterator object로 만든다.)\niterator에서 .__next__() 함수를 호출하고 결과를 i에 저장한 뒤에 for문 블락안에 있는 내용 (들여쓰기 된 내용)을 실행한다. \\(\\to\\) 반복\nStopIteration 에러가 발생하면 for문을 멈춘다.\n\n- 아래의 ??? 자리에 올 수 있는 것이 iterable object 가 아니라 iterator 자체여도 for문이 돌아갈까? (당연히 돌아가야 할 것 같음)\nfor i in ???:\n    print(i)\n\nfor i in [1,2,3]: # iterable object\n    print(i)\n\n1\n2\n3\n\n\n\n당연히 가능!\n\n- a가 iterator일때 iter(a)의 출력결과가 a와 같도록 조정한다면 for문의 동작원리 (1)-(3)을 수행하지 않아도 좋다. \\(\\to\\) 실제로 이렇게 동작한다.\n- 요약\n\niterable object는 숨겨진 기능으로 __iter__를 가진다.\niterator object는 숨겨진 기능으로 __iter__와 __next__를 가진다. (즉 iterator는 그 자체로 iterable object가 된다!)\n\n\nlst = [1,2,3]\nltor = iter(lst)\n\n\nset(dir(lst)) & {'__iter__','__next__'}\n\n{'__iter__'}\n\n\n\nset(dir(ltor)) & {'__iter__', '__next__'}\n\n{'__iter__', '__next__'}\n\n\n- 의문2의 해결: for의 출력결과는 어떻게 예측할 수 있을까? iterator를 만들어서 .__next__()의 출력값을 확인하면 알 수 있다.\n\nfor i in df:\n    print(i)\n\nX1\nX2\nX3\nX4\n\n\n\ndftor = iter(df)\ndftor?\n\n\nType:        map\nString form: &lt;map object at 0x7f2dfc4c85b0&gt;\nDocstring:  \nmap(func, *iterables) --&gt; map object\nMake an iterator that computes the function using arguments from\neach of the iterables.  Stops when the shortest iterable is exhausted.\n\n\n\n\ndftor.__next__()\n\n'X1'\n\n\n\ndftor.__next__()\n\n'X2'\n\n\n\ndftor.__next__()\n\n'X3'\n\n\n\ndftor.__next__()\n\n'X4'\n\n\n\ndftor.__next__()\n\nStopIteration:"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class8.html#range",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class8.html#range",
    "title": "class 8단계",
    "section": "",
    "text": "- 파이썬에서 for문을 처음 배울 때: range(5)를 써라!\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\nrange(5)가 도대체 무엇이길래? ## iterator 아니면 iterable object 일건데..\n\n\nrange(5)\n\nrange(0, 5)\n\n\n\nrepr(range(5))\n\n'range(0, 5)'\n\n\n- range(5)의 정체는 그냥 iterable object이다.\n\nset(dir(range(5))) & {'__iter__', '__next__'}\n\n{'__iter__'}\n\n\n__next__ 는 갖고있지 않은데 __iter__만 갖고있으니까 range(5)는 iterable object\n- 그래서 언제든지 iterator로 바꿀 수 있다.\n\nrtor = iter(range(5))\n\n\nrtor?\n\n\nType:        range_iterator\nString form: &lt;range_iterator object at 0x7f2dfc4c56c0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\nset(dir(rtor)) & {'__iter__','__next__'}\n\n{'__iter__', '__next__'}\n\n\n- for문에서 range(5)가 행동하는 방법?\n\nrtor.__next__()\n\n0\n\n\n\nrtor.__next__()\n\n1\n\n\n\nrtor.__next__()\n\n2\n\n\n\nrtor.__next__()\n\n3\n\n\n\nrtor.__next__()\n\n4\n\n\n\nrtor.__next__()\n\nStopIteration:"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class8.html#zip",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class8.html#zip",
    "title": "class 8단계",
    "section": "",
    "text": "- 이터레이터의 개념을 알면 for문에 대한 이해도가 대폭 상승한다.\n\nfor i in zip([1,2,3],'abc'):\n    print(i)\n\n(1, 'a')\n(2, 'b')\n(3, 'c')\n\n\n\nzip은 뭐지???\n\n\nzip([1,2,3],'abc')\n\n&lt;zip at 0x7f2dfc4e8340&gt;\n\n\n- 어차피 for i in ????: 의 ???? 자리는 iterable object(iterator)의 자리이다.\n\nset(dir(zip([1,2,3],'abc'))) & {'__iter__','__next__'}\n\n{'__iter__', '__next__'}\n\n\n\n__next__() 함수가 있음 \\(\\to\\) zip([1,2,3],'abc')는 그자체로 iterator 였다!\n\n\nz = zip([1,2,3],'abc')\n\n\nz.__next__()\n\n(1, 'a')\n\n\n\nz.__next__()\n\n(2, 'b')\n\n\n\nz.__next__()\n\n(3, 'c')\n\n\n\nz.__next__()\n\nStopIteration:"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class8.html#사용자정의-이터레이터",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class8.html#사용자정의-이터레이터",
    "title": "class 8단계",
    "section": "",
    "text": "- 내가 이터레이터를 만들어보자.\n\nclass Klass: # 찌를 내는 순간 for문이 멈추도록 하는 이터레이터를 만들자.\n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        if action == '찌':\n            print('찌가 나와서 for문을 멈춥니다.')\n            raise StopIteration\n        else:\n            return action\n\n\na = Klass() # 클래스로부터 인스턴스 만들기\n\n\na?\n\n\nType:        Klass\nString form: &lt;__main__.Klass object at 0x7f2dfc373af0&gt;\nDocstring:   &lt;no docstring&gt;\n\n\n\n\nset(dir(a)) & {'__iter__', '__next__'} # a는 이터레이터!\n\n{'__iter__', '__next__'}\n\n\n\na.__next__()\n\n'빠'\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n'묵'\n\n\n\na.__next__()\n\n찌가 나와서 for문을 멈춥니다.\n\n\nStopIteration: \n\n\n\nfor i in a:\n    print(i)\n\n빠\n묵\n묵\n빠\n찌가 나와서 for문을 멈춥니다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class8.html#파이썬의-비밀-15",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class8.html#파이썬의-비밀-15",
    "title": "class 8단계",
    "section": "",
    "text": "파이썬의 비밀1: 자료형은 클래스의 이름이다.\n파이썬의 비밀2: 클래스에는 __str__ 처럼 숨겨진 매서드가 존재한다. 이를 이용하여 파이썬 내부의 기능을 가로챌 수 있다.\n파이썬의 비밀3: 주피터노트북(대화형 콘솔)에서는 “오브젝트이름 + 엔터”를 쳐서 나오는 출력은 __repr__로 가로챌 수 있다. (주피터의 비밀)\n파이썬의 비밀4: 함수와 클래스는 숨겨진 메소드에 __call__을 가진 오브젝트일 뿐이다.\n파이썬의 비밀5: for문의 비밀(iterable object, iterator, StopIteration Error)"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class1.html",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class1.html",
    "title": "class 1단계",
    "section": "",
    "text": "클래스 선언 및 사용 예시\n\n\n\n1. 이미지 자료 불러오기 (PIL 이용)\n2. 클래스 성능 정리\n3. 연습문제\n\n\n\n- 예제1\n\n# 이미지 출력을 위한 패키지 불러오기\nimport requests\nfrom PIL import Image\n\n\nurl= 'https://stat.jbnu.ac.kr/sites/stat/images/intro_about_02.jpg'\n\n\nImage.open(Image.io.BytesIO(requests.get(url).content))\n\n\n\n\n- 예제2\n\nurl1 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\nurl2 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop2.png?raw=true' \n\n\nImage.open(Image.io.BytesIO(requests.get(url1).content))\n\n\n\n\n\nImage.open(Image.io.BytesIO(requests.get(url2).content))\n\n\n\n\n\n\n\nclass STOOOP:\n    title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n                \n\n\n규칙1 : 메소드(=class 안에서 정의된 함수)의 첫번째 인자는 무조건 self\n규칙2 : 메소드에서 class 안에 정의된 변수들 (title, url, end)을 사용하려면 self.변수이름 과 같은 형식으로 쓴다.\n\n즉, self.title, self.url, self.end 와 같은 방식으로 써야한다.\n\n(참고) : 규칙2에서 가끔 self 자리에 STOOOP.title, STOOOP.url, STOOOP.end 와 같이 클래스의 이름을 쓰기도 한다.\n\n\n\n\n\n\n\nschool = STOOOP()\n\n\nschool.stop()\n\n학교폭력!\n멈춰~~~~\n\n\n\n\n\n\n\n\n\nkospi = STOOOP()\n\n\nkospi.title = 'KOSPI 하락'\n\n\nkospi.stop()\n\nKOSPI 하락\n멈춰~~~~\n\n\n\n\n\n\n\n\n\n\n\n\nschool = STOOOP()\nkospi = STOOOP()\n\n\n함수의 사용법과 비슷하다.\n클래스 이름을 쓰고, 콘텐츠를 구체화하는 과정에서 필요한 입력1, 입력2를 ()에 넣는다. 이때는 STOOOP(입력1, 입력2) 와 같이 생성\n위의 예시는 따로 입력이 없으므로 비워둔 상태이다. 즉, STOOOP() 와 같은 식으로 생성\n\n\n\n\n\nschool.title # 출력\n\n'학교폭력!'\n\n\n\nkospi.title # 출력\n\n'학교폭력!'\n\n\n\nkospi.title = '코스피하락' # 변경\n\n\nkospi.title\n\n'코스피하락'\n\n\n\n\n\n\nschool.stop()\n\n학교폭력!\n멈춰~~~~\n\n\n\n\n\n\nkospi.stop()\n\n코스피하락\n멈춰~~~~\n\n\n\n\n\n\n\n\n\n\n\n\n- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 True이다.\n- 클래스에는 show()라는 메소드가 있다. show() 기능은 a의 값을 print하는 기능을 한다.\n\nclass Klass1:\n    a = True # 초기값\n    def show(self):\n        print(self.a)\n\n\nex1 = Klass1()\n\n\nex1.a # 초기값\n\nTrue\n\n\n\nex1.show()\n\nTrue\n\n\n\n\n\n- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 1이다.\n- 클래스에는 up()이라는 메소드가 있다. up()의 기능은 a의 값을 1증가시키는 기능을 한다.\n\nclass Klass2:\n    a = 1 # 초깃값\n    def up(self):\n        self.a = self.a + 1\n\n\nex2 = Klass2()\nex2.a\n\n1\n\n\n\nex2.up()\nex2.a\n\n2\n\n\n\nex2.up()\nex2.a\n\n3\n\n\n\nex2.up()\nex2.a\n\n4\n\n\n\n\n\n- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 \\(0\\) 이다.\n- 클래스에는 up(), down(), show() 라는 메소드가 있다. 각각은 a의 값을 1증가, a값을 1감소, a의 값을 print하는 기능을 한다.\n\nclass Klass3:\n    a = 0\n    def up(self):\n        self.a  = self.a + 1\n    def down(self):\n        self.a = self.a - 1\n    def show(self):\n        print(self.a)\n\n\nex3 = Klass3()\n\n\nex3.show()\n\n0\n\n\n\nex3.up()\nex3.show()\n\n1\n\n\n\nex3.up()\nex3.up()\nex3.show()\n\n3\n\n\n\nex3.down()\nex3.show()\n\n2\n\n\n\n\n\n- 클래스 내에는 변수 url이 있음. url의 초기값은 다음과 같다. https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true\n- 클래스에는 show() 라는 메소드(클래스 안에 정의된 함수)를 가지는데, 메소드는 아래와 같은 기능을 한다. - 기능1: url의 그림을 출력 - 기능2: ‘당신은 이 그림을 \\(n\\) 번 보았습니다.’ 출력. (여기에서 \\(n\\)은 그림을 본 횟수)\n\nclass Klass4:\n    n = 1 # 초기값\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 그림을 {}번 보았습니다.'.format(self.n))\n        self.n = self.n + 1\n\n\nex4 = Klass4()\nex4.show()\n\n\n\n\n당신은 이 그림을 1번 보았습니다.\n\n\n\nex4.show()\n\n\n\n\n당신은 이 그림을 2번 보았습니다.\n\n\n\n# url 변환 (학교 폭력 이미지 말고, SNL 이미지로 출력되게 바꿔보자.)\nex4_1 = Klass4()\nex4_1.url = url2 # SNL image link\n\n\nex4_1.show()\n\n\n\n\n당신은 이 그림을 1번 보았습니다.\n\n\n\n\n\n\n- 클래스를 선언하라. [‘가위’, ‘바위’, ‘보’] 중 하나를 골라서 내는 메소드를 정의하라.\n\n# hint\nimport numpy as np\nnp.random.choice(['가위', '바위', '보'])\n\n'가위'\n\n\n\nclass Klass5:\n    def game(self):\n        print(np.random.choice(['가위','바위','보']))\n\n\nex5 = Klass5()\n\n\nex5.game()\n\n보"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class1.html#contents",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class1.html#contents",
    "title": "class 1단계",
    "section": "",
    "text": "1. 이미지 자료 불러오기 (PIL 이용)\n2. 클래스 성능 정리\n3. 연습문제"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class1.html#이미지-자료-불러오기",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class1.html#이미지-자료-불러오기",
    "title": "class 1단계",
    "section": "",
    "text": "- 예제1\n\n# 이미지 출력을 위한 패키지 불러오기\nimport requests\nfrom PIL import Image\n\n\nurl= 'https://stat.jbnu.ac.kr/sites/stat/images/intro_about_02.jpg'\n\n\nImage.open(Image.io.BytesIO(requests.get(url).content))\n\n\n\n\n- 예제2\n\nurl1 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\nurl2 = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop2.png?raw=true' \n\n\nImage.open(Image.io.BytesIO(requests.get(url1).content))\n\n\n\n\n\nImage.open(Image.io.BytesIO(requests.get(url2).content))\n\n\n\n\n\n\n\nclass STOOOP:\n    title = '학교폭력!'\n    url = url1\n    end = '멈춰~~~~'\n    def stop(self):\n        print(self.title)\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print(self.end)\n                \n\n\n규칙1 : 메소드(=class 안에서 정의된 함수)의 첫번째 인자는 무조건 self\n규칙2 : 메소드에서 class 안에 정의된 변수들 (title, url, end)을 사용하려면 self.변수이름 과 같은 형식으로 쓴다.\n\n즉, self.title, self.url, self.end 와 같은 방식으로 써야한다.\n\n(참고) : 규칙2에서 가끔 self 자리에 STOOOP.title, STOOOP.url, STOOOP.end 와 같이 클래스의 이름을 쓰기도 한다.\n\n\n\n\n\n\n\nschool = STOOOP()\n\n\nschool.stop()\n\n학교폭력!\n멈춰~~~~\n\n\n\n\n\n\n\n\n\nkospi = STOOOP()\n\n\nkospi.title = 'KOSPI 하락'\n\n\nkospi.stop()\n\nKOSPI 하락\n멈춰~~~~\n\n\n\n\n\n\n\n\n\n\n\n\nschool = STOOOP()\nkospi = STOOOP()\n\n\n함수의 사용법과 비슷하다.\n클래스 이름을 쓰고, 콘텐츠를 구체화하는 과정에서 필요한 입력1, 입력2를 ()에 넣는다. 이때는 STOOOP(입력1, 입력2) 와 같이 생성\n위의 예시는 따로 입력이 없으므로 비워둔 상태이다. 즉, STOOOP() 와 같은 식으로 생성\n\n\n\n\n\nschool.title # 출력\n\n'학교폭력!'\n\n\n\nkospi.title # 출력\n\n'학교폭력!'\n\n\n\nkospi.title = '코스피하락' # 변경\n\n\nkospi.title\n\n'코스피하락'\n\n\n\n\n\n\nschool.stop()\n\n학교폭력!\n멈춰~~~~\n\n\n\n\n\n\nkospi.stop()\n\n코스피하락\n멈춰~~~~"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class1.html#연습문제",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class1.html#연습문제",
    "title": "class 1단계",
    "section": "",
    "text": "- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 True이다.\n- 클래스에는 show()라는 메소드가 있다. show() 기능은 a의 값을 print하는 기능을 한다.\n\nclass Klass1:\n    a = True # 초기값\n    def show(self):\n        print(self.a)\n\n\nex1 = Klass1()\n\n\nex1.a # 초기값\n\nTrue\n\n\n\nex1.show()\n\nTrue\n\n\n\n\n\n- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 1이다.\n- 클래스에는 up()이라는 메소드가 있다. up()의 기능은 a의 값을 1증가시키는 기능을 한다.\n\nclass Klass2:\n    a = 1 # 초깃값\n    def up(self):\n        self.a = self.a + 1\n\n\nex2 = Klass2()\nex2.a\n\n1\n\n\n\nex2.up()\nex2.a\n\n2\n\n\n\nex2.up()\nex2.a\n\n3\n\n\n\nex2.up()\nex2.a\n\n4\n\n\n\n\n\n- 클래스 내에는 변수 a가 있다. 변수 a의 초기값은 \\(0\\) 이다.\n- 클래스에는 up(), down(), show() 라는 메소드가 있다. 각각은 a의 값을 1증가, a값을 1감소, a의 값을 print하는 기능을 한다.\n\nclass Klass3:\n    a = 0\n    def up(self):\n        self.a  = self.a + 1\n    def down(self):\n        self.a = self.a - 1\n    def show(self):\n        print(self.a)\n\n\nex3 = Klass3()\n\n\nex3.show()\n\n0\n\n\n\nex3.up()\nex3.show()\n\n1\n\n\n\nex3.up()\nex3.up()\nex3.show()\n\n3\n\n\n\nex3.down()\nex3.show()\n\n2\n\n\n\n\n\n- 클래스 내에는 변수 url이 있음. url의 초기값은 다음과 같다. https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true\n- 클래스에는 show() 라는 메소드(클래스 안에 정의된 함수)를 가지는데, 메소드는 아래와 같은 기능을 한다. - 기능1: url의 그림을 출력 - 기능2: ‘당신은 이 그림을 \\(n\\) 번 보았습니다.’ 출력. (여기에서 \\(n\\)은 그림을 본 횟수)\n\nclass Klass4:\n    n = 1 # 초기값\n    url = 'https://github.com/guebin/IP2022/blob/master/_notebooks/2022-05-07-stop1.jpeg?raw=true'\n    def show(self):\n        display(Image.open(Image.io.BytesIO(requests.get(self.url).content)))\n        print('당신은 이 그림을 {}번 보았습니다.'.format(self.n))\n        self.n = self.n + 1\n\n\nex4 = Klass4()\nex4.show()\n\n\n\n\n당신은 이 그림을 1번 보았습니다.\n\n\n\nex4.show()\n\n\n\n\n당신은 이 그림을 2번 보았습니다.\n\n\n\n# url 변환 (학교 폭력 이미지 말고, SNL 이미지로 출력되게 바꿔보자.)\nex4_1 = Klass4()\nex4_1.url = url2 # SNL image link\n\n\nex4_1.show()\n\n\n\n\n당신은 이 그림을 1번 보았습니다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-15-class1.html#homework",
    "href": "posts/1_IP2022/03_Class/2023-02-15-class1.html#homework",
    "title": "class 1단계",
    "section": "",
    "text": "- 클래스를 선언하라. [‘가위’, ‘바위’, ‘보’] 중 하나를 골라서 내는 메소드를 정의하라.\n\n# hint\nimport numpy as np\nnp.random.choice(['가위', '바위', '보'])\n\n'가위'\n\n\n\nclass Klass5:\n    def game(self):\n        print(np.random.choice(['가위','바위','보']))\n\n\nex5 = Klass5()\n\n\nex5.game()\n\n보"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class7.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class7.html",
    "title": "class 7단계",
    "section": "",
    "text": "함수형 프로그래밍, callable object, 파이썬의 비밀"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제1-숫자입력-함수출력",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제1-숫자입력-함수출력",
    "title": "class 7단계",
    "section": "(예제1) 숫자입력, 함수출력",
    "text": "(예제1) 숫자입력, 함수출력\n\ndef f(a):\n    def _f(x):\n        return (x-a)**2\n    return _f\n\n\ng = f(10) # g(x) = (x-10)**2\n\n\ng(2) # (2-10)**2 = 64\n\n64\n\n\n\n해석: \\(f(a)\\)는 \\(a\\)를 입력으로 받고 \\(g(x)=(x-a)^2\\)를 함수를 리턴해주는 함수"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제1의-다른-표현-익명함수-lambda",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제1의-다른-표현-익명함수-lambda",
    "title": "class 7단계",
    "section": "(예제1)의 다른 표현: 익명함수 lambda",
    "text": "(예제1)의 다른 표현: 익명함수 lambda\n\n표현1\n\ndef f(a):\n    _f = lambda x: (x-a)**2 ### lambda x: (x-a)**2 가 실행되는 순간 함수오브젝트가 만들어지고 그것이 _f 로 저장됨 \n    return _f\n\n\ng = f(10) # g(x) = (x-10)**2\n\n\ng(3)\n\n49\n\n\n\n\n표현2\n\ndef f(a):\n    return lambda x: (x-a)**2\n\n\ng = f(10)\n\n\ng(3)\n\n49\n\n\n\nlambda x: (x-a)**2는 \\(\\text{lambda}(x) = (x-a)^2\\)의 느낌으로 기억하면 외우기 쉽다.\nlambda x: (x-a)**2는 “아직 이름이 없는 함수 오브젝트를 (가칭 lambda라고 하자) 만들고 기능은 x를 입력으로 하고 (x-2)**2를 출력하도록 하자” 라는 뜻으로 해석하면 된다."
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제2-함수입력-숫자출력",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제2-함수입력-숫자출력",
    "title": "class 7단계",
    "section": "(예제2) 함수입력, 숫자출력",
    "text": "(예제2) 함수입력, 숫자출력\n\ndef f(x):\n    return x**2\n\n\ndef d(f,x): # 함수를 입력을 받는 함수를 정의\n    h=0.000000000001\n    return (f(x+h)-f(x))/h \n\n\\[f'(x)\\approx \\frac{f(x+h)-f(x)}{h}\\]\n\n\\(h\\)의 값이 점점 0에 가까울수록 등호에 가까워짐.\n\n\nd(f,4) # f'(4) = 2*4 = 8\n\n8.000711204658728"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제3-함수입력-함수출력",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제3-함수입력-함수출력",
    "title": "class 7단계",
    "section": "(예제3) 함수입력, 함수출력",
    "text": "(예제3) 함수입력, 함수출력\n\ndef f(x): \n    return x**2 \n\n\ndef derivate(f): \n    def df(x): \n        h=0.000000000001\n        return (f(x+h)-f(x))/h \n    return df\n\n\nff = derivate(f)\n\n\nff(7) # f의 도함수\n\n14.004797321831575\n\n\n원래함수 시각화\n\nx = np.linspace(-1,1,100)\nplt.plot(x,f(x))\n\n\n\n\n도함수 시각화\n\nx = np.linspace(-1,1,100)\nplt.plot(x, ff(x))"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제3의-다른-표현",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제3의-다른-표현",
    "title": "class 7단계",
    "section": "(예제3)의 다른 표현",
    "text": "(예제3)의 다른 표현\n\ndef f(x): \n    return x**2\n\n\ndef derivate(f): \n    h=0.000000000001\n    return lambda x: (f(x+h)-f(x))/h \n\n\nff = derivate(f)\n\n\nff(10)\n\n20.00888343900442"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제4-함수들의-리스트",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class7.html#예제4-함수들의-리스트",
    "title": "class 7단계",
    "section": "(예제4) 함수들의 리스트",
    "text": "(예제4) 함수들의 리스트\n[오브젝트, 오브젝트, 오브젝트]\n\nflst = [lambda x: x, lambda x: x**2, lambda x: x**3]  # [함수오브젝트,함수오브젝트,함수오브젝트]\nflst # 이것의 타입은 function\n\n[&lt;function __main__.&lt;lambda&gt;(x)&gt;,\n &lt;function __main__.&lt;lambda&gt;(x)&gt;,\n &lt;function __main__.&lt;lambda&gt;(x)&gt;]\n\n\n\nfor f in flst:\n    print(f(2))\n\n2\n4\n8\n\n\n\n첫번째 함수에 적용될 때는 2출력, 2번째 함수에 적용될 때는 4출력, 3번째 함수에 적용될 때는 8출력\n\n\nfor f in flst:\n    plt.plot(x,f(x),'--')\n\n\n\n\n위의 코드는 아래와 같음.\n\nplt.plot(x, (lambda x: x)(x),'--')\nplt.plot(x, (lambda x: x**2)(x),'--')\nplt.plot(x, (lambda x: x**3)(x),'--')"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class5.html",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class5.html",
    "title": "class 5단계",
    "section": "",
    "text": "특정 자료형에 한정하여 print 이외에 파이썬 내부기능을 재정의해보자.\n\n- 지난시간까지 배운 것: RPC자료형에 한정해서 print() 등의 기능을 조작할 수 있었다. (재정의 할 수 있었다.)\n- 이번시간에 배울 것: 특정 자료형에 한정하여 print 이외에 파이썬 내부기능을 조작하여 보자. (재정의하여 보자.)\n\nimport numpy as np\n\n\n\n- 아래의 연산구조를 관찰하자.\n\na = 1\nb = 2\n\n\na?? # a는 int class에서 만들어진 인스턴스다.\n\n\nType:        int\nString form: 1\nDocstring:  \nint([x]) -&gt; integer\nint(x, base=10) -&gt; integer\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n&gt;&gt;&gt; int('0b100', base=0)\n4\n\n\n\n\na + b\n\n3\n\n\n\na라는 인스턴스와 b라는 인스턴스를 +라는 기호가 연결하고 있다.\n\n- 이번에는 아래의 연산구조를 관찰하자.\n\na = [1,2]\nb = [3,4]\na+b\n\n[1, 2, 3, 4]\n\n\n\na라는 인스턴스와 b라는 인스턴스를 +라는 기호가 연결하고 있다.\n\n- 동작이 다른 이유?\n\n클래스를 배우기 이전: int자료형의 +는 “정수의 덧셈”을 의미하고 list자료형의 +는 “자료의 추가”를 의미한다.\n클래스를 배운 이후: 아마 클래스는 + 라는 연산을 정의하는 숨겨진 메소드가 있을 것이다. (print가 그랬듯이) 그런데 int 클래스에서는 그 메소드를 “정수의 덧셈”이 되도록 정의하였고, list클래스에서는 그 메소드를 “자료의 추가”를 의마하도록 정의하였을 것이다.\n\n- 아래의 결과를 관찰\n\na = 1\nb = 2\n\n\nset(dir(a)) & {'__add__'}\n\n{'__add__'}\n\n\n\na.__add__(b)\n\n3\n\n\n\nb.__add__(a)\n\n3\n\n\n\na = [1,2]\nb = [3,4]\n\n\na.__add__(b)\n\n[1, 2, 3, 4]\n\n\n\nb.__add__(a)\n\n[3, 4, 1, 2]\n\n\n- a+b는 사실 내부적으로 a.__add(b)의 축약구문이다. 따라서 만약 a.__add__(b)의 기능을 바꾸면 (재정의 하면) a+b의 기능도 바뀔 것이다.\n\n\n- 학생예제\n\nclass Student: # student class를 만들어보자. (student 자료형인것.)\n    def __init__(self, age = 20.0, semester = 0):\n        self.age = age\n        self.semester = semester\n        print('입학을 축하합니다. 당신의 나이는 {}이고 현재 학기는 {}학기입니다.'.format(self.age, self.semester))\n    def __add__(self, val):\n        # val == 0: 휴학\n        # val == 1: 등록\n        if val == 0:\n            self.age = self.age + 0.5\n        elif val == 1:\n            self.age = self.age + 0.5\n            self.semester = self.semester + 1\n    def _repr_html_(self):\n        html_str = \"\"\"\n        나이: {} &lt;br/&gt;\n        학기: {} &lt;br/&gt;\n        \"\"\"\n        return html_str.format(self.age, self.semester)\n\n\niu = Student()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기입니다.\n\n\n\niu\n\n\n        나이: 20.0 \n        학기: 0 \n        \n\n\n\niu + 1 ## 1학년 1학기 등록\niu\n\n\n        나이: 20.5 \n        학기: 1 \n        \n\n\n\niu + 0 ## 휴학함\niu\n\n\n        나이: 21.0 \n        학기: 1 \n        \n\n\n\niu.__add__(1)\n\n\niu\n\n\n        나이: 21.5 \n        학기: 2 \n        \n\n\n- 연산을 연속으로 하고 싶다.\n\niu + 1 + 0 + 0 + 0 + 0\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n- 에러의 이유?\n(되는코드)\n\n(1+1)+1 # 1+1+1은 이렇게 볼 수 있다.\n\n3\n\n\n\n_a = (1+1)\ntype(_a)\n\nint\n\n\n\n_a+1 # 이 연산은 int 인스턴스 + int인스턴스\n\n3\n\n\n(안되는코드)\n\niu + 1 + 1\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n\n_a = iu + 1\ntype(_a)\n\nNoneType\n\n\n\n_a + 1\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n- 에러를 해결하는 방법: iu + 1의 결과로 Student클래스가 리턴되면 된다.\n\nclass Student: # student class를 만들어보자. (student 자료형인것.)\n    def __init__(self, age = 20.0, semester = 0):\n        self.age = age\n        self.semester = semester\n        print('입학을 축하합니다. 당신의 나이는 {}이고 현재 학기는 {}학기입니다.'.format(self.age, self.semester))\n    def __add__(self, val):\n        # val == 0: 휴학\n        # val == 1: 등록\n        if val == 0:\n            self.age = self.age + 0.5\n        elif val == 1:\n            self.age = self.age + 0.5\n            self.semester = self.semester + 1\n        return self\n    def _repr_html_(self):\n        html_str = \"\"\"\n        나이: {} &lt;br/&gt;\n        학기: {} &lt;br/&gt;\n        \"\"\"\n        return html_str.format(self.age, self.semester)\n\n\niu = Student()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기입니다.\n\n\n\niu+1  # __add__의 return에 Student 클래스의 인스턴스가 리턴되면서 자동으로 _repr_html_() 실행\n\n\n        나이: 20.5 \n        학기: 1 \n        \n\n\n\niu + 1 + 0 + 0 + 0 + 0\n\n\n        나이: 23.0 \n        학기: 2 \n        \n\n\n\n\n\n\na = 1\nb = 0\na*b\n\n0\n\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n        self.results = list()\n    def __mul__(self, other):\n        self.choose()\n        other.choose()\n        if self.actions[-1] == '가위' and other.actions[-1]=='가위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '가위' and other.actions[-1]=='바위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '가위' and other.actions[-1]=='보':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='가위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='바위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '바위' and other.actions[-1]=='보':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='가위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='바위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '보' and other.actions[-1]=='보':\n            self.results.append(0)\n            other.results.append(0)\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        액션: {} &lt;br/&gt;\n        승패: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions, self.results)\n\n\na = RPC()\nb = RPC()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\na*b\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: ['보'] \n        승패: [-1]\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: ['가위'] \n        승패: [1]\n        \n\n\n\nfor i in range(50000):\n    a*b\n\n\n#a\n\n\n#b\n\n\nsum(a.results), sum(b.results)\n\n(175, -175)\n\n\n\nsum(a.results)/50000\n\n0.0035\n\n\n\nsum(b.results)/50000\n\n-0.0035\n\n\n\n\n\n\nRPC클래스에서 Player a와 Player b를 만들어라. - Player a는 [‘가위’,‘보’] 중에 하나를 낼 수 있다. - 그리고 Player b는 [‘가위’,‘바위’] 중에 하나를 낼 수 있다. - 두 Player는 가지고 있는 패를 (같은 확률로) 랜덤으로 낸다. (즉, Player a가 가위만 내거나 보만 내는 경우는 없다.)\n\n누가 더 유리한가? 이유를 스스로 생각해보라.\n\n\n비슷하지 않을까?\n\n\n50000번을 시뮬레이션을 해보고 결과를 분석해보라.\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n        self.results = list()\n    def __mul__(self, other):\n        self.choose()\n        other.choose()\n        if self.actions[-1] == '가위' and other.actions[-1]=='가위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '가위' and other.actions[-1]=='바위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '가위' and other.actions[-1]=='보':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='가위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='바위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '바위' and other.actions[-1]=='보':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='가위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='바위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '보' and other.actions[-1]=='보':\n            self.results.append(0)\n            other.results.append(0)\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        액션: {} &lt;br/&gt;\n        승패: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions, self.results)\n\n\nplayer_a = RPC(['가위', '보'])\nplayer_b = RPC(['가위', '바위'])\n\n\nplayer_a\n\n\n        낼 수 있는 패: ['가위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\nplayer_b\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        액션: [] \n        승패: []\n        \n\n\n\nplayer_a*player_b\n\n\nplayer_a\n\n\n        낼 수 있는 패: ['가위', '보'] \n        액션: ['보'] \n        승패: [1]\n        \n\n\n\nplayer_b\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        액션: ['바위'] \n        승패: [-1]\n        \n\n\n\nfor i in range(50000):\n    player_a*player_b\n\n\nsum(player_a.results), sum(player_b.results)\n\n(-12279, 12279)\n\n\n\nsum(player_a.results)/50000, sum(player_b.results)/50000\n\n(-0.24558, 0.24558)"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class5.html#motive",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class5.html#motive",
    "title": "class 5단계",
    "section": "",
    "text": "- 아래의 연산구조를 관찰하자.\n\na = 1\nb = 2\n\n\na?? # a는 int class에서 만들어진 인스턴스다.\n\n\nType:        int\nString form: 1\nDocstring:  \nint([x]) -&gt; integer\nint(x, base=10) -&gt; integer\nConvert a number or string to an integer, or return 0 if no arguments\nare given.  If x is a number, return x.__int__().  For floating point\nnumbers, this truncates towards zero.\nIf x is not a number or if base is given, then x must be a string,\nbytes, or bytearray instance representing an integer literal in the\ngiven base.  The literal can be preceded by '+' or '-' and be surrounded\nby whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\nBase 0 means to interpret the base from the string as an integer literal.\n&gt;&gt;&gt; int('0b100', base=0)\n4\n\n\n\n\na + b\n\n3\n\n\n\na라는 인스턴스와 b라는 인스턴스를 +라는 기호가 연결하고 있다.\n\n- 이번에는 아래의 연산구조를 관찰하자.\n\na = [1,2]\nb = [3,4]\na+b\n\n[1, 2, 3, 4]\n\n\n\na라는 인스턴스와 b라는 인스턴스를 +라는 기호가 연결하고 있다.\n\n- 동작이 다른 이유?\n\n클래스를 배우기 이전: int자료형의 +는 “정수의 덧셈”을 의미하고 list자료형의 +는 “자료의 추가”를 의미한다.\n클래스를 배운 이후: 아마 클래스는 + 라는 연산을 정의하는 숨겨진 메소드가 있을 것이다. (print가 그랬듯이) 그런데 int 클래스에서는 그 메소드를 “정수의 덧셈”이 되도록 정의하였고, list클래스에서는 그 메소드를 “자료의 추가”를 의마하도록 정의하였을 것이다.\n\n- 아래의 결과를 관찰\n\na = 1\nb = 2\n\n\nset(dir(a)) & {'__add__'}\n\n{'__add__'}\n\n\n\na.__add__(b)\n\n3\n\n\n\nb.__add__(a)\n\n3\n\n\n\na = [1,2]\nb = [3,4]\n\n\na.__add__(b)\n\n[1, 2, 3, 4]\n\n\n\nb.__add__(a)\n\n[3, 4, 1, 2]\n\n\n- a+b는 사실 내부적으로 a.__add(b)의 축약구문이다. 따라서 만약 a.__add__(b)의 기능을 바꾸면 (재정의 하면) a+b의 기능도 바뀔 것이다.\n\n\n- 학생예제\n\nclass Student: # student class를 만들어보자. (student 자료형인것.)\n    def __init__(self, age = 20.0, semester = 0):\n        self.age = age\n        self.semester = semester\n        print('입학을 축하합니다. 당신의 나이는 {}이고 현재 학기는 {}학기입니다.'.format(self.age, self.semester))\n    def __add__(self, val):\n        # val == 0: 휴학\n        # val == 1: 등록\n        if val == 0:\n            self.age = self.age + 0.5\n        elif val == 1:\n            self.age = self.age + 0.5\n            self.semester = self.semester + 1\n    def _repr_html_(self):\n        html_str = \"\"\"\n        나이: {} &lt;br/&gt;\n        학기: {} &lt;br/&gt;\n        \"\"\"\n        return html_str.format(self.age, self.semester)\n\n\niu = Student()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기입니다.\n\n\n\niu\n\n\n        나이: 20.0 \n        학기: 0 \n        \n\n\n\niu + 1 ## 1학년 1학기 등록\niu\n\n\n        나이: 20.5 \n        학기: 1 \n        \n\n\n\niu + 0 ## 휴학함\niu\n\n\n        나이: 21.0 \n        학기: 1 \n        \n\n\n\niu.__add__(1)\n\n\niu\n\n\n        나이: 21.5 \n        학기: 2 \n        \n\n\n- 연산을 연속으로 하고 싶다.\n\niu + 1 + 0 + 0 + 0 + 0\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n- 에러의 이유?\n(되는코드)\n\n(1+1)+1 # 1+1+1은 이렇게 볼 수 있다.\n\n3\n\n\n\n_a = (1+1)\ntype(_a)\n\nint\n\n\n\n_a+1 # 이 연산은 int 인스턴스 + int인스턴스\n\n3\n\n\n(안되는코드)\n\niu + 1 + 1\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n\n_a = iu + 1\ntype(_a)\n\nNoneType\n\n\n\n_a + 1\n\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\n\n- 에러를 해결하는 방법: iu + 1의 결과로 Student클래스가 리턴되면 된다.\n\nclass Student: # student class를 만들어보자. (student 자료형인것.)\n    def __init__(self, age = 20.0, semester = 0):\n        self.age = age\n        self.semester = semester\n        print('입학을 축하합니다. 당신의 나이는 {}이고 현재 학기는 {}학기입니다.'.format(self.age, self.semester))\n    def __add__(self, val):\n        # val == 0: 휴학\n        # val == 1: 등록\n        if val == 0:\n            self.age = self.age + 0.5\n        elif val == 1:\n            self.age = self.age + 0.5\n            self.semester = self.semester + 1\n        return self\n    def _repr_html_(self):\n        html_str = \"\"\"\n        나이: {} &lt;br/&gt;\n        학기: {} &lt;br/&gt;\n        \"\"\"\n        return html_str.format(self.age, self.semester)\n\n\niu = Student()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기입니다.\n\n\n\niu+1  # __add__의 return에 Student 클래스의 인스턴스가 리턴되면서 자동으로 _repr_html_() 실행\n\n\n        나이: 20.5 \n        학기: 1 \n        \n\n\n\niu + 1 + 0 + 0 + 0 + 0\n\n\n        나이: 23.0 \n        학기: 2 \n        \n\n\n\n\n\n\na = 1\nb = 0\na*b\n\n0\n\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n        self.results = list()\n    def __mul__(self, other):\n        self.choose()\n        other.choose()\n        if self.actions[-1] == '가위' and other.actions[-1]=='가위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '가위' and other.actions[-1]=='바위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '가위' and other.actions[-1]=='보':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='가위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='바위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '바위' and other.actions[-1]=='보':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='가위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='바위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '보' and other.actions[-1]=='보':\n            self.results.append(0)\n            other.results.append(0)\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        액션: {} &lt;br/&gt;\n        승패: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions, self.results)\n\n\na = RPC()\nb = RPC()\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\na*b\n\n\na\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: ['보'] \n        승패: [-1]\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '바위', '보'] \n        액션: ['가위'] \n        승패: [1]\n        \n\n\n\nfor i in range(50000):\n    a*b\n\n\n#a\n\n\n#b\n\n\nsum(a.results), sum(b.results)\n\n(175, -175)\n\n\n\nsum(a.results)/50000\n\n0.0035\n\n\n\nsum(b.results)/50000\n\n-0.0035"
  },
  {
    "objectID": "posts/1_IP2022/03_Class/2023-02-23-class5.html#숙제",
    "href": "posts/1_IP2022/03_Class/2023-02-23-class5.html#숙제",
    "title": "class 5단계",
    "section": "",
    "text": "RPC클래스에서 Player a와 Player b를 만들어라. - Player a는 [‘가위’,‘보’] 중에 하나를 낼 수 있다. - 그리고 Player b는 [‘가위’,‘바위’] 중에 하나를 낼 수 있다. - 두 Player는 가지고 있는 패를 (같은 확률로) 랜덤으로 낸다. (즉, Player a가 가위만 내거나 보만 내는 경우는 없다.)\n\n누가 더 유리한가? 이유를 스스로 생각해보라.\n\n\n비슷하지 않을까?\n\n\n50000번을 시뮬레이션을 해보고 결과를 분석해보라.\n\n\nclass RPC:\n    def __init__(self, candidate=['가위','바위','보']):\n        self.candidate = candidate\n        self.actions = list()\n        self.results = list()\n    def __mul__(self, other):\n        self.choose()\n        other.choose()\n        if self.actions[-1] == '가위' and other.actions[-1]=='가위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '가위' and other.actions[-1]=='바위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '가위' and other.actions[-1]=='보':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='가위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '바위' and other.actions[-1]=='바위':\n            self.results.append(0)\n            other.results.append(0)\n        if self.actions[-1] == '바위' and other.actions[-1]=='보':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='가위':\n            self.results.append(-1)\n            other.results.append(1)\n        if self.actions[-1] == '보' and other.actions[-1]=='바위':\n            self.results.append(1)\n            other.results.append(-1)\n        if self.actions[-1] == '보' and other.actions[-1]=='보':\n            self.results.append(0)\n            other.results.append(0)\n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate))\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt;\n        액션: {} &lt;br/&gt;\n        승패: {}\n        \"\"\"\n        return html_str.format(self.candidate, self.actions, self.results)\n\n\nplayer_a = RPC(['가위', '보'])\nplayer_b = RPC(['가위', '바위'])\n\n\nplayer_a\n\n\n        낼 수 있는 패: ['가위', '보'] \n        액션: [] \n        승패: []\n        \n\n\n\nplayer_b\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        액션: [] \n        승패: []\n        \n\n\n\nplayer_a*player_b\n\n\nplayer_a\n\n\n        낼 수 있는 패: ['가위', '보'] \n        액션: ['보'] \n        승패: [1]\n        \n\n\n\nplayer_b\n\n\n        낼 수 있는 패: ['가위', '바위'] \n        액션: ['바위'] \n        승패: [-1]\n        \n\n\n\nfor i in range(50000):\n    player_a*player_b\n\n\nsum(player_a.results), sum(player_b.results)\n\n(-12279, 12279)\n\n\n\nsum(player_a.results)/50000, sum(player_b.results)/50000\n\n(-0.24558, 0.24558)"
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022-06-13-final.html",
    "href": "posts/1_IP2022/04_시험/2022-06-13-final.html",
    "title": "2022 final exam",
    "section": "",
    "text": "ref: 기말고사 풀이 링크\n\n\n\n아래코드를 이용하여 numpy, matplotlib, pandas를 import하라.\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd\nfrom IPython.display import HTML\n\n\n\n\n(1) 도함수를 구하는 함수 derivate를 선언하라. 이 함수를 이용하여 \\(f(x)=x^2\\)의 그래프와 \\(f'(x)=2x\\)의 그래프를 \\(x \\in (-1,1)\\)의 범위에서 그려라.\n\ndef f(x):\n    return x**2\n\n\ndef derivate(f): \n    def df(x): \n        h=0.00000000000001\n        return (f(x+h)-f(x))/h \n    return df\n\n\nx = np.linspace(-1,1,100)\nplt.plot(x, f(x))  ## f(x)=x**2\nplt.plot(x, derivate(f)(x)) ## f'(x)=2*x\n\n\n\n\n(2) 적당한 클래스 정의하여 인스턴스 a를 만들고 print(a)의 출력결과가 본인의 학번이 나오도록 하라.\n## 코드예시\nclass Klass:\n    ???\n    ???\na=Klass()\nprint(a)\n## 출력결과\n2022-43052\n\nclass Klass:\n    def __str__(self):\n        return('12345678')\n\n\na = Klass()\n\n\nprint(a)\n\n12345678\n\n\n(3) for문이 실행될때마다 [묵,찌,빠] 중에 하나를 내며 빠를 누적 3회 낼경우 for문이 멈추는 이터레이터를 생성하라.\n(나의풀이)\n\nclass Klass: # 빠를 누적 3회 낼 경우 for문이 멈추는 이터레이터를 만들자.\n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n        self.n = 0\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        if action == '빠':\n            self.n += 1\n            print(action,self.n)\n            if self.n == 3:\n                print('빠가 누적3회 나와서 for문을 멈춥니다.')\n                raise StopIteration\n            else:\n                return action\n        else:\n            return action\n\n\na = Klass()\n\n\nfor i in a:\n    print(i)\n\n찌\n찌\n묵\n찌\n찌\n묵\n빠 1\n빠\n찌\n빠 2\n빠\n빠 3\n빠가 누적3회 나와서 for문을 멈춥니다.\n\n\n(모범답안)\n\nclass Klass: \n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n        self.dic = {'묵':0,'찌':0,'빠':0}\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        self.dic[action] += 1\n        if self.dic['빠'] == 3:\n            print('빠가 3번 누적되어 for문을 멈춥니다.')\n            raise StopIteration\n        else:\n            return action\n\n\na = Klass()\nfor i in a:\n    print(i)\n\n묵\n빠\n찌\n찌\n빠\n빠가 3번 누적되어 for문을 멈춥니다.\n\n\n(4)-(6)\nclass GS25: \n    n=0 \n    total_number_of_guests = 0 \n    def __init__(self):\n        self.number_of_guests = 0 \n(4) 위의 클래스를 수정하여 아래와 같이 GS25에서 새로운 인스턴스가 생성될때마다\nGS25의 점포수가 ?개로 늘었습니다.\n라는 메시지가 출력되도록 하라.\n(5) 함수 come를 인스턴스 메소드로 정의하라. 이 메소드가 실행될때마다 각 점포의 손님 인스턴스 변수 number_of_guests와 클래스변수 total_number_of_guests를 1씩 증가시키고 아래의 메시지를 출력하라.\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 n명입니다. \n현재 GS25 점포를 방문한 손님수는 m명입니다. \n(6) 새로운 클래스메서드 show를 만들고 아래와 같은 메시지를 출력하도록 하라.\nGS25의 점포수: ??\nGS25를 방문한 총 손님수: ??\n(사용예시) (4)-(6)을 모두 적용한 경우 사용예시는 아래와 같다.\n\nclass GS25:\n    n = 0\n    total_numer_of_guests = 0\n    def __init__(self):\n        self.number_of_guests = 0\n        GS25.n += 1\n        print('GS25의 점포수가 {}개로 늘었습니다.'.format(GS25.n))\n    def come(self):\n        GS25.total_number_of_guests += 1\n        self.number_of_guests += 1\n        print('새로운 손님이 오셨습니다.')\n        print('GS25를 방문한 총 손님 수는 {}명입니다.'.format(GS25.total_number_of_guests))\n        print('현재 GS25 점포를 방문한 손님수는 {}명입니다.'.format(self.number_of_guests))\n    @classmethod\n    def show(cls):\n        print('GS25의 점포수:{}'.format(cls.n))\n        print('GS를 방문한 총 손님 수: {}'.format(cls.total_number_of_guests))\n\n\na = GS25()\n\nGS25의 점포수가 5개로 늘었습니다.\n\n\n\na=GS25() ## (4)의 사용예시\n\nGS25의 점포수가 1개로 늘었습니다.\n\n\n\nb=GS25() ## (4)의 사용예시\n\nGS25의 점포수가 2개로 늘었습니다.\n\n\n\na.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 1명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다. \n\n\n\na.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 2명입니다.\n현재 GS25 점포를 방문한 손님수는 2명입니다. \n\n\n\nb.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 3명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다. \n\n\n\nGS25.show() ## (6)의 사용예시\n\nGS25의 점포수: 2\nGS25를 방문한 총 손님수: 3\n\n\n(풀이시작)\n\nclass GS25: \n    n=0 \n    total_number_of_guests = 0 \n    def __init__(self):\n        self.number_of_guests = 0\n        GS25.n += 1\n        print('GS25의 점포수가 {}개로 늘었습니다.'.format(GS25.n))\n    def come(self):\n        self.number_of_guests += 1\n        GS25.total_number_of_guests += 1\n        print('새로운 손님이 오셨습니다!')\n        print('GS25를 방문한 총 손님수는 {}명입니다.'.format(GS25.total_number_of_guests))\n        print('현재 GS25 점포를 방문한 손님수는 {}명입니다.'.format(self.number_of_guests))\n    @classmethod\n    def show(cls):\n        print('GS25의 점포수: {}'.format(cls.n))\n        print('GS25를 방문한 총 손님수: {}'.format(cls.total_number_of_guests))\n\n\na = GS25()\n\nGS25의 점포수가 1개로 늘었습니다.\n\n\n\nb = GS25()\n\nGS25의 점포수가 2개로 늘었습니다.\n\n\n\na.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 1명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다.\n\n\n\na.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 2명입니다.\n현재 GS25 점포를 방문한 손님수는 2명입니다.\n\n\n\nb.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 3명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다.\n\n\n\nGS25.show()\n\nGS25의 점포수: 2\nGS25를 방문한 총 손님수: 3\n\n\n(7) __eq__는 연산 == 를 재정의하는 메소드이다. 클래스 RPS_BASE를 상속하여 새로운 클래스 RPS5를 만들라. 연산 ==를 재정의하여 RPS5의 두 인스턴스의 action이 같은 경우 true를 리턴하는 기능을 구현하라.\n\nclass RPS_BASE:\n    def __init__(self):\n        self.action = np.random.choice(['가위','바위','보'])\n\nhint: Appendix를 참고할 것\nhint: RPS5의 선언부분은 아래와 같은 형태를 가지고 있다.\nclass RPS5(???):\n    def __eq__(self,other):\n        return ??????\nhint: RPS5클래스의 사용예시는 아래와 같다.\n\na=RPS5()\na.action\n\n'바위'\n\n\n\nb=RPS5()\nb.action\n\n'보'\n\n\n\na==b\n\nFalse\n\n\n(풀이시작)\n(8) __gt__는 연산 &gt; 를 재정의하는 메소드이다. 클래스 RPS_BASE를 상속하여 새로운 클래스 RPS6를 만들라. 연산 &gt;를 재정의하여 RPS6의 두 인스턴스 a,b의 action이 각각 (‘가위’,‘보’), (‘바위’,‘가위’), (‘보’,‘바위’) 인 경우 true를 리턴하는 기능을 구현하라.\nhint: Appendix를 참고할 것\nhint: RPS6클래스의 사용예시는 아래와 같다.\n\na=RPS6()\na.action\n\n'바위'\n\n\n\nb=RPS6()\nb.action\n\n'보'\n\n\n\na&gt;b, a&lt;b\n\n(False, True)\n\n\n(9)-(10)\n아래와 같은 데이터프레임을 선언하고 물음에 답하라.\n\nnp.random.seed(43052)\ndf=pd.DataFrame({'type':np.random.choice(['A','B'],100), 'score':np.random.randint(40,95,100)})\ndf\n\n\n\n\n\n\n\n\ntype\nscore\n\n\n\n\n0\nB\n45\n\n\n1\nA\n40\n\n\n2\nB\n79\n\n\n3\nB\n46\n\n\n4\nB\n57\n\n\n...\n...\n...\n\n\n95\nB\n69\n\n\n96\nA\n71\n\n\n97\nA\n93\n\n\n98\nA\n63\n\n\n99\nA\n82\n\n\n\n\n100 rows × 2 columns\n\n\n\n(9) type==’A’의 평균score를 구하는 코드를 작성하라.\n(10) type==’A’의 평균score보다 같거나 큰 값을 가지는 행을 출력하라.\n\n\n\n(1) 플레이어A는 (가위,가위) 중 하나를 선택할 수 있고 플레이어B는 (가위,바위) 중 하나를 선택할 수 있다. 각 플레이어는 각 패 중 하나를 랜덤으로 선택하는 액션을 한다고 가정하자. 아래에 해당하는 확률을 시뮬레이션을 이용하여 추정하라.\n\n플레이어A가 승리할 확률:\n플레이어B가 승리할 확률:\n플레이어A와 플레이어B가 비길 확률:\n\nhint: 50% 확률로 b가 승리하고 50% 확률로 비긴다.\n(2) 문제 (1)과 같이 아래의 상황을 가정하자.\n\n\n\n\n플레이어A\n플레이어B\n\n\n\n\n각 플레이어가 낼 수 있는 패 (candidate)\n(가위,가위)\n(가위,바위)\n\n\n각 패를 선택할 확률 (prob)\n(0.5,0.5)\n(0.5,0.5)\n\n\n\n각 플레이어는 아래와 같은 규칙으로 가위바위보 결과에 따른 보상점수를 적립한다고 하자. - 승리: 보상점수 2점 적립 - 무승부: 보상점수 1점 적립 - 패배: 보상점수 0점 적립\n100번째 대결까지 시뮬레이션을 시행하고 플레이어B가 가위를 낼 경우 얻은 보상점수의 총합과 바위를 낼 경우 얻은 보상점수의 총합을 각각 구하라. 플레이어B는 가위를 내는것이 유리한가? 바위를 내는것이 유리한가?\nhint: 플레이어B는 바위를 내는 것이 유리하다.\nhint: 플레이어B가 100번중에 49번 가위를 내고 51번 바위를 낸다면 플레이어B가 적립할 보상점수는 각각 아래와 같다. - 가위를 내었을 경우: 49 * 1 = 49점 - 바위를 내었을 경우: 51 * 2 = 102점 - 총 보상점수 = 49점 + 102점 = 151점\n(3) (2)에서 얻은 데이터를 학습하여 플레이어B가 “가위” 혹은 “바위” 를 선택할 확률을 매시점 조금씩 조정한다고 가정하자. 구체적으로는 현재시점까지 얻은 보상점수의 비율로 확률을 결정한다. 예를들어 플레이어B가 100회의 대결동안 누적한 보상점수의 총합이 아래와 같다고 하자.\n\n가위를 내었을 경우 보상점수 총합 = 50점\n바위를 내었을 경우 보상점수 총합 = 100점\n\n그렇다면 플레이어B는 각각 (50/150,100/150) 의 확률로 (가위,바위) 중 하나를 선택한다. 101번째 대결에 플레이어B가 가위를 내서 비겼다면 이후에는 (51/151,100/151) 의 확률로 (가위,바위) 중 하나를 선택한다. 102번째 대결에 플레이어B가 바위를 내서 이겼다면 이후에는 각각 (51/153,102/153) 의 확률로 (가위,바위) 중 하나를 선택한다. 이러한 상황을 요약하여 표로 정리하면 아래와 같다.\n\n\n\n\n\n\n\n\n\n시점\n플레이어B가 가위를 냈을 경우 얻은 점수 총합\n플레이어B가 바위를 냈을 경우 얻은 점수 총합\nt+1시점에서 플레이어B가 (가위,바위)를 낼 확률\n\n\n\n\nt=100\n50\n100\n(50/150, 100/150)\n\n\nt=101\n51\n100\n(51/151, 100/151)\n\n\nt=102\n51\n102\n(51/153, 102/153)\n\n\n\n이러한 방식으로 500회까지 게임을 진행하며 확률을 수정하였을 경우 501번째 대결에서 플레이어B가 (가위,바위)를 낼 확률은 각각 얼마인가?\nhint: 시간이 지날수록 플레이어B는 (가위,바위)중 바위를 내는 쪽이 유리하다는 것을 알게 될 것이다.\n\n앞으로 아래와 같은 용어를 사용한다. - (정의) 어떠한 플레이어가 양손 중 하나를 선택하는 확률을 데이터를 바탕으로 매 순간 업데이트 한다면 그 플레이어는 “학습모드 상태이다”고 표현한다. - (정의) 반대로 어떠한 플레이어가 양손 중 하나를 항상 동일한 확률로 낸다면 그 플레이어는 “학습모드 상태가 아니다”라고 표현한다.\n\n(4) 새로운 두명의 플레이어C와 플레이어D를 만들어라. 두 플레이어는 모두 동일하게 (가위,바위) 중 하나를 선택할 수 있다. 두 명의 플레이어는 100번째 대결까지는 두 가지 패중 하나를 랜덤하게 선택하고 101번째 대결부터 500번째 대결까지는 문제(3)의 플레이어B와 같은 방식으로 확률을 업데이트 하여 두 가지 패를 서로 다른 확률로 낸다고 하자. 즉 100번째 대결까지는 두 플레이어가 모두 학습모드 상태가 아니고 101번째부터 500번째 대결까지는 두 플레이어가 모두 학습모드 상태이다. 500번째 대결까지의 학습이 끝났을 경우 플레이어 C와 플레이어D가 각 패를 낼 확률은 각각 얼마인가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어C가 (가위,바위)를 낼 확률\n플레이어D가 (가위,바위)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n대결 데이터를 학습하여 수정한 확률\n대결 데이터를 학습하여 수정한 확률\n양쪽 플레이어 모두 학습모드임\n\n\n\nhint: 시간이 지날수록 두 플레이어 모두 바위를 내는 쪽이 유리하다는 것을 알게 될 것이다.\n(5) 새로운 플레이어 E와 F를 생각하자. 플레이어E와 플레이어F는 각각 (가위,바위) 그리고 (가위,보) 중 하나를 선택할 수 있다고 가정하자. 시뮬레이션 대결결과를 이용하여 아래의 확률을 근사적으로 추정하라.\n\n플레이어E가 승리할 확률:\n플레이어F가 승리할 확률:\n플레이어E와 플레이어F가 비길 확률:\n\nhint: 플레이어E가 가위를 낸다면 최소한 지지는 않기 때문에 플레이어E가 좀 더 유리한 패를 가지고 있다. 따라서 플레이어E의 결과가 더 좋을 것이다.\n(6) (5)와 동일한 두 명의 플레이어E, F를 생각하자. 두 플레이어는 100회까지는 랜덤으로 자신의 패를 선택한다. 그리고 101회부터 500회까지는 플레이어F만 데이터로 부터 학습을 하여 수정된 확률을 사용한다. 500번의 대결이 끝나고 플레이어F가 (가위,보)를 선택하는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\n\nhint: 플레이어F는 보를 내는 것이 낫다고 생각할 것이다. (가위를 내면 지거나 비기지만 보를 내면 지거나 이긴다.)\n(7) (6)번의 플레이어E와 플레이어F가 500회~1000회까지 추가로 게임을 한다. 이번에는 플레이어E만 데이터로부터 학습한다. 1000회까지 대결을 끝낸 이후 플레이어E가 (가위,바위)를 내는 확률은 어떻게 업데이트 되었는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\nt=500시점에 업데이트된 확률\n플레이어E는 학습모드 / 플레이어F는 학습모드아님\n\n\n\nhint: 플레이어F는 보를 내도록 학습되어 있다. 따라서 플레이어E가 바위를 내면 지고 가위를 내면 이길것이다. 따라서 플레이어E는 가위가 유리하다고 생각할 것이다.\n(8) (7)번의 플레이어E와 플레이어F가 1000회~30000회까지 추가로 게임을 한다. 이번에는 플레이어F만 데이터로부터 학습한다. 30000회까지 대결을 끝낸 이후 플레이어F가 (가위,보)를 내는 확률은 어떻게 업데이트 되었는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\nt=500시점에 업데이트된 확률\n플레이어E는 학습모드 / 플레이어F는 학습모드아님\n\n\nt &lt;= 30000\nt=1000시점에 업데이트된 확률\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\n\nhint: 플레이어F는 원래 보가 유리하다고 생각하여 보를 자주 내도록 학습되었다. 하지만 플레이어E가 그러한 플레이어F의 성향을 파악하고 가위를 주로 내도록 학습하였다. 플레이어F는 그러한 플레이어E의 성향을 다시 파악하여 이번에는 가위을 자주 내는 것이 유리하다고 생각할 것이다.\n(9) 플레이어E와 플레이어F의 대결기록을 초기화 한다. 이번에는 플레이어F가 항상 (3/4)의 확률로 가위를 (1/4)의 확률로 보를 낸다고 가정한다. 플레이어E는 100번의 대결까지는 랜덤으로 (가위,바위)중 하나를 내고 101번째 대결부터 1000번째 대결까지는 대결 데이터를 학습하여 수정한 확률을 사용한다고 하자. 1000번째 대결이후에 플레이어E가 (가위,바위)를 내는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(3/4, 1/4)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\n(3/4, 1/4)\n플레이어E는 학습모드 / 플레이어F는 학습모드 아님\n\n\n\n(10) 플레이어E와 플레이어F의 대결기록을 초기화 한다. 이번에는 플레이어F가 항상 (2/3)의 확률로 가위를 (1/3)의 확률로 보를 낸다고 가정한다. 플레이어E는 100번의 대결까지는 랜덤으로 (가위,바위)중 하나를 내고 101번째 대결부터 1000번째 대결까지는 대결 데이터를 학습하여 수정한 확률을 사용한다고 하자. 1000번째 대결이후에 플레이어E가 (가위,바위)를 내는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(2/3, 1/3)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\n(2/3, 1/3)\n플레이어E는 학습모드 / 플레이어F는 학습모드 아님\n\n\n\n\n\n\n- 아래의 클래스를 참고하여 문제1,2을 풀어라. (5월25일 강의노트에 소개된 클래스를 약간 정리한 것) - 참고하지 않아도 감점은 없음\n\nclass RPS:\n    def __init__(self,candidate):\n        self.candidate = candidate\n        self.actions = list() \n        self.rewards = list()\n        self.prob = [0.5,0.5]\n\n    def __eq__(self,other): # 연산 == 를 재정의 \n        return self.actions[-1] == other.actions[-1] \n        #note: 둘의 액션이 같으면 무승부 \n    \n    def __gt__(self,other): # 연산 &gt; 를 재정의 \n        pair = self.actions[-1], other.actions[-1]\n        return pair == ('가위','보') or pair == ('바위','가위') or pair == ('보','바위') \n        #note: 가위&gt;보, 바위&gt;가위, 보&gt;가위 \n    \n    def __mul__(self,other):\n        # step1: 각자의 패를 선택 \n        self.choose()\n        other.choose()\n        \n        # step2: 승패 판단 + upate reward\n        if self == other: # 무승부일경우 \n            self.rewards.append(1)\n            other.rewards.append(1)\n        elif self &gt; other: # self의 승리 \n            self.rewards.append(2)\n            other.rewards.append(0)\n        else: # other의 승리 \n            self.rewards.append(0)\n            other.rewards.append(2)\n        \n        # step3: update data\n        self.update_data()\n        other.update_data()\n    \n    def update_data(self):\n        self.data = pd.DataFrame({'actions':self.actions, 'rewards':self.rewards})\n    \n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt; \n        데이터: &lt;br/&gt;\n        {}\n        \"\"\"        \n        return html_str.format(self.candidate,self.data._repr_html_())\n    \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate,p=self.prob))\n\n- 사용예시\n\na=RPS(['가위','가위'])\nb=RPS(['가위','보'])\n\n\nfor i in range(5):\n    a*b\n\n\na\n\n\n        낼 수 있는 패: ['가위', '가위']  \n        데이터: \n        \n\n\n\n\n\n\nactions\nrewards\n\n\n\n\n0\n가위\n2\n\n\n1\n가위\n2\n\n\n2\n가위\n1\n\n\n3\n가위\n2\n\n\n4\n가위\n2\n\n\n\n\n\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '보']  \n        데이터: \n        \n\n\n\n\n\n\nactions\nrewards\n\n\n\n\n0\n보\n0\n\n\n1\n보\n0\n\n\n2\n가위\n1\n\n\n3\n보\n0\n\n\n4\n보\n0"
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022-06-13-final.html#imports",
    "href": "posts/1_IP2022/04_시험/2022-06-13-final.html#imports",
    "title": "2022 final exam",
    "section": "",
    "text": "아래코드를 이용하여 numpy, matplotlib, pandas를 import하라.\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd\nfrom IPython.display import HTML"
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022-06-13-final.html#기본문제-50점",
    "href": "posts/1_IP2022/04_시험/2022-06-13-final.html#기본문제-50점",
    "title": "2022 final exam",
    "section": "",
    "text": "(1) 도함수를 구하는 함수 derivate를 선언하라. 이 함수를 이용하여 \\(f(x)=x^2\\)의 그래프와 \\(f'(x)=2x\\)의 그래프를 \\(x \\in (-1,1)\\)의 범위에서 그려라.\n\ndef f(x):\n    return x**2\n\n\ndef derivate(f): \n    def df(x): \n        h=0.00000000000001\n        return (f(x+h)-f(x))/h \n    return df\n\n\nx = np.linspace(-1,1,100)\nplt.plot(x, f(x))  ## f(x)=x**2\nplt.plot(x, derivate(f)(x)) ## f'(x)=2*x\n\n\n\n\n(2) 적당한 클래스 정의하여 인스턴스 a를 만들고 print(a)의 출력결과가 본인의 학번이 나오도록 하라.\n## 코드예시\nclass Klass:\n    ???\n    ???\na=Klass()\nprint(a)\n## 출력결과\n2022-43052\n\nclass Klass:\n    def __str__(self):\n        return('12345678')\n\n\na = Klass()\n\n\nprint(a)\n\n12345678\n\n\n(3) for문이 실행될때마다 [묵,찌,빠] 중에 하나를 내며 빠를 누적 3회 낼경우 for문이 멈추는 이터레이터를 생성하라.\n(나의풀이)\n\nclass Klass: # 빠를 누적 3회 낼 경우 for문이 멈추는 이터레이터를 만들자.\n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n        self.n = 0\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        if action == '빠':\n            self.n += 1\n            print(action,self.n)\n            if self.n == 3:\n                print('빠가 누적3회 나와서 for문을 멈춥니다.')\n                raise StopIteration\n            else:\n                return action\n        else:\n            return action\n\n\na = Klass()\n\n\nfor i in a:\n    print(i)\n\n찌\n찌\n묵\n찌\n찌\n묵\n빠 1\n빠\n찌\n빠 2\n빠\n빠 3\n빠가 누적3회 나와서 for문을 멈춥니다.\n\n\n(모범답안)\n\nclass Klass: \n    def __init__(self):\n        self.candidate = ['묵','찌','빠']\n        self.dic = {'묵':0,'찌':0,'빠':0}\n    def __iter__(self):\n        return self\n    def __next__(self):\n        action = np.random.choice(self.candidate)\n        self.dic[action] += 1\n        if self.dic['빠'] == 3:\n            print('빠가 3번 누적되어 for문을 멈춥니다.')\n            raise StopIteration\n        else:\n            return action\n\n\na = Klass()\nfor i in a:\n    print(i)\n\n묵\n빠\n찌\n찌\n빠\n빠가 3번 누적되어 for문을 멈춥니다.\n\n\n(4)-(6)\nclass GS25: \n    n=0 \n    total_number_of_guests = 0 \n    def __init__(self):\n        self.number_of_guests = 0 \n(4) 위의 클래스를 수정하여 아래와 같이 GS25에서 새로운 인스턴스가 생성될때마다\nGS25의 점포수가 ?개로 늘었습니다.\n라는 메시지가 출력되도록 하라.\n(5) 함수 come를 인스턴스 메소드로 정의하라. 이 메소드가 실행될때마다 각 점포의 손님 인스턴스 변수 number_of_guests와 클래스변수 total_number_of_guests를 1씩 증가시키고 아래의 메시지를 출력하라.\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 n명입니다. \n현재 GS25 점포를 방문한 손님수는 m명입니다. \n(6) 새로운 클래스메서드 show를 만들고 아래와 같은 메시지를 출력하도록 하라.\nGS25의 점포수: ??\nGS25를 방문한 총 손님수: ??\n(사용예시) (4)-(6)을 모두 적용한 경우 사용예시는 아래와 같다.\n\nclass GS25:\n    n = 0\n    total_numer_of_guests = 0\n    def __init__(self):\n        self.number_of_guests = 0\n        GS25.n += 1\n        print('GS25의 점포수가 {}개로 늘었습니다.'.format(GS25.n))\n    def come(self):\n        GS25.total_number_of_guests += 1\n        self.number_of_guests += 1\n        print('새로운 손님이 오셨습니다.')\n        print('GS25를 방문한 총 손님 수는 {}명입니다.'.format(GS25.total_number_of_guests))\n        print('현재 GS25 점포를 방문한 손님수는 {}명입니다.'.format(self.number_of_guests))\n    @classmethod\n    def show(cls):\n        print('GS25의 점포수:{}'.format(cls.n))\n        print('GS를 방문한 총 손님 수: {}'.format(cls.total_number_of_guests))\n\n\na = GS25()\n\nGS25의 점포수가 5개로 늘었습니다.\n\n\n\na=GS25() ## (4)의 사용예시\n\nGS25의 점포수가 1개로 늘었습니다.\n\n\n\nb=GS25() ## (4)의 사용예시\n\nGS25의 점포수가 2개로 늘었습니다.\n\n\n\na.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 1명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다. \n\n\n\na.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 2명입니다.\n현재 GS25 점포를 방문한 손님수는 2명입니다. \n\n\n\nb.come() ## (5)의 사용예시\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 모든 손님수는 3명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다. \n\n\n\nGS25.show() ## (6)의 사용예시\n\nGS25의 점포수: 2\nGS25를 방문한 총 손님수: 3\n\n\n(풀이시작)\n\nclass GS25: \n    n=0 \n    total_number_of_guests = 0 \n    def __init__(self):\n        self.number_of_guests = 0\n        GS25.n += 1\n        print('GS25의 점포수가 {}개로 늘었습니다.'.format(GS25.n))\n    def come(self):\n        self.number_of_guests += 1\n        GS25.total_number_of_guests += 1\n        print('새로운 손님이 오셨습니다!')\n        print('GS25를 방문한 총 손님수는 {}명입니다.'.format(GS25.total_number_of_guests))\n        print('현재 GS25 점포를 방문한 손님수는 {}명입니다.'.format(self.number_of_guests))\n    @classmethod\n    def show(cls):\n        print('GS25의 점포수: {}'.format(cls.n))\n        print('GS25를 방문한 총 손님수: {}'.format(cls.total_number_of_guests))\n\n\na = GS25()\n\nGS25의 점포수가 1개로 늘었습니다.\n\n\n\nb = GS25()\n\nGS25의 점포수가 2개로 늘었습니다.\n\n\n\na.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 1명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다.\n\n\n\na.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 2명입니다.\n현재 GS25 점포를 방문한 손님수는 2명입니다.\n\n\n\nb.come()\n\n새로운 손님이 오셨습니다!\nGS25를 방문한 총 손님수는 3명입니다.\n현재 GS25 점포를 방문한 손님수는 1명입니다.\n\n\n\nGS25.show()\n\nGS25의 점포수: 2\nGS25를 방문한 총 손님수: 3\n\n\n(7) __eq__는 연산 == 를 재정의하는 메소드이다. 클래스 RPS_BASE를 상속하여 새로운 클래스 RPS5를 만들라. 연산 ==를 재정의하여 RPS5의 두 인스턴스의 action이 같은 경우 true를 리턴하는 기능을 구현하라.\n\nclass RPS_BASE:\n    def __init__(self):\n        self.action = np.random.choice(['가위','바위','보'])\n\nhint: Appendix를 참고할 것\nhint: RPS5의 선언부분은 아래와 같은 형태를 가지고 있다.\nclass RPS5(???):\n    def __eq__(self,other):\n        return ??????\nhint: RPS5클래스의 사용예시는 아래와 같다.\n\na=RPS5()\na.action\n\n'바위'\n\n\n\nb=RPS5()\nb.action\n\n'보'\n\n\n\na==b\n\nFalse\n\n\n(풀이시작)\n(8) __gt__는 연산 &gt; 를 재정의하는 메소드이다. 클래스 RPS_BASE를 상속하여 새로운 클래스 RPS6를 만들라. 연산 &gt;를 재정의하여 RPS6의 두 인스턴스 a,b의 action이 각각 (‘가위’,‘보’), (‘바위’,‘가위’), (‘보’,‘바위’) 인 경우 true를 리턴하는 기능을 구현하라.\nhint: Appendix를 참고할 것\nhint: RPS6클래스의 사용예시는 아래와 같다.\n\na=RPS6()\na.action\n\n'바위'\n\n\n\nb=RPS6()\nb.action\n\n'보'\n\n\n\na&gt;b, a&lt;b\n\n(False, True)\n\n\n(9)-(10)\n아래와 같은 데이터프레임을 선언하고 물음에 답하라.\n\nnp.random.seed(43052)\ndf=pd.DataFrame({'type':np.random.choice(['A','B'],100), 'score':np.random.randint(40,95,100)})\ndf\n\n\n\n\n\n\n\n\ntype\nscore\n\n\n\n\n0\nB\n45\n\n\n1\nA\n40\n\n\n2\nB\n79\n\n\n3\nB\n46\n\n\n4\nB\n57\n\n\n...\n...\n...\n\n\n95\nB\n69\n\n\n96\nA\n71\n\n\n97\nA\n93\n\n\n98\nA\n63\n\n\n99\nA\n82\n\n\n\n\n100 rows × 2 columns\n\n\n\n(9) type==’A’의 평균score를 구하는 코드를 작성하라.\n(10) type==’A’의 평균score보다 같거나 큰 값을 가지는 행을 출력하라."
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022-06-13-final.html#가위-바위-보-하나빼기-150점",
    "href": "posts/1_IP2022/04_시험/2022-06-13-final.html#가위-바위-보-하나빼기-150점",
    "title": "2022 final exam",
    "section": "",
    "text": "(1) 플레이어A는 (가위,가위) 중 하나를 선택할 수 있고 플레이어B는 (가위,바위) 중 하나를 선택할 수 있다. 각 플레이어는 각 패 중 하나를 랜덤으로 선택하는 액션을 한다고 가정하자. 아래에 해당하는 확률을 시뮬레이션을 이용하여 추정하라.\n\n플레이어A가 승리할 확률:\n플레이어B가 승리할 확률:\n플레이어A와 플레이어B가 비길 확률:\n\nhint: 50% 확률로 b가 승리하고 50% 확률로 비긴다.\n(2) 문제 (1)과 같이 아래의 상황을 가정하자.\n\n\n\n\n플레이어A\n플레이어B\n\n\n\n\n각 플레이어가 낼 수 있는 패 (candidate)\n(가위,가위)\n(가위,바위)\n\n\n각 패를 선택할 확률 (prob)\n(0.5,0.5)\n(0.5,0.5)\n\n\n\n각 플레이어는 아래와 같은 규칙으로 가위바위보 결과에 따른 보상점수를 적립한다고 하자. - 승리: 보상점수 2점 적립 - 무승부: 보상점수 1점 적립 - 패배: 보상점수 0점 적립\n100번째 대결까지 시뮬레이션을 시행하고 플레이어B가 가위를 낼 경우 얻은 보상점수의 총합과 바위를 낼 경우 얻은 보상점수의 총합을 각각 구하라. 플레이어B는 가위를 내는것이 유리한가? 바위를 내는것이 유리한가?\nhint: 플레이어B는 바위를 내는 것이 유리하다.\nhint: 플레이어B가 100번중에 49번 가위를 내고 51번 바위를 낸다면 플레이어B가 적립할 보상점수는 각각 아래와 같다. - 가위를 내었을 경우: 49 * 1 = 49점 - 바위를 내었을 경우: 51 * 2 = 102점 - 총 보상점수 = 49점 + 102점 = 151점\n(3) (2)에서 얻은 데이터를 학습하여 플레이어B가 “가위” 혹은 “바위” 를 선택할 확률을 매시점 조금씩 조정한다고 가정하자. 구체적으로는 현재시점까지 얻은 보상점수의 비율로 확률을 결정한다. 예를들어 플레이어B가 100회의 대결동안 누적한 보상점수의 총합이 아래와 같다고 하자.\n\n가위를 내었을 경우 보상점수 총합 = 50점\n바위를 내었을 경우 보상점수 총합 = 100점\n\n그렇다면 플레이어B는 각각 (50/150,100/150) 의 확률로 (가위,바위) 중 하나를 선택한다. 101번째 대결에 플레이어B가 가위를 내서 비겼다면 이후에는 (51/151,100/151) 의 확률로 (가위,바위) 중 하나를 선택한다. 102번째 대결에 플레이어B가 바위를 내서 이겼다면 이후에는 각각 (51/153,102/153) 의 확률로 (가위,바위) 중 하나를 선택한다. 이러한 상황을 요약하여 표로 정리하면 아래와 같다.\n\n\n\n\n\n\n\n\n\n시점\n플레이어B가 가위를 냈을 경우 얻은 점수 총합\n플레이어B가 바위를 냈을 경우 얻은 점수 총합\nt+1시점에서 플레이어B가 (가위,바위)를 낼 확률\n\n\n\n\nt=100\n50\n100\n(50/150, 100/150)\n\n\nt=101\n51\n100\n(51/151, 100/151)\n\n\nt=102\n51\n102\n(51/153, 102/153)\n\n\n\n이러한 방식으로 500회까지 게임을 진행하며 확률을 수정하였을 경우 501번째 대결에서 플레이어B가 (가위,바위)를 낼 확률은 각각 얼마인가?\nhint: 시간이 지날수록 플레이어B는 (가위,바위)중 바위를 내는 쪽이 유리하다는 것을 알게 될 것이다.\n\n앞으로 아래와 같은 용어를 사용한다. - (정의) 어떠한 플레이어가 양손 중 하나를 선택하는 확률을 데이터를 바탕으로 매 순간 업데이트 한다면 그 플레이어는 “학습모드 상태이다”고 표현한다. - (정의) 반대로 어떠한 플레이어가 양손 중 하나를 항상 동일한 확률로 낸다면 그 플레이어는 “학습모드 상태가 아니다”라고 표현한다.\n\n(4) 새로운 두명의 플레이어C와 플레이어D를 만들어라. 두 플레이어는 모두 동일하게 (가위,바위) 중 하나를 선택할 수 있다. 두 명의 플레이어는 100번째 대결까지는 두 가지 패중 하나를 랜덤하게 선택하고 101번째 대결부터 500번째 대결까지는 문제(3)의 플레이어B와 같은 방식으로 확률을 업데이트 하여 두 가지 패를 서로 다른 확률로 낸다고 하자. 즉 100번째 대결까지는 두 플레이어가 모두 학습모드 상태가 아니고 101번째부터 500번째 대결까지는 두 플레이어가 모두 학습모드 상태이다. 500번째 대결까지의 학습이 끝났을 경우 플레이어 C와 플레이어D가 각 패를 낼 확률은 각각 얼마인가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어C가 (가위,바위)를 낼 확률\n플레이어D가 (가위,바위)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n대결 데이터를 학습하여 수정한 확률\n대결 데이터를 학습하여 수정한 확률\n양쪽 플레이어 모두 학습모드임\n\n\n\nhint: 시간이 지날수록 두 플레이어 모두 바위를 내는 쪽이 유리하다는 것을 알게 될 것이다.\n(5) 새로운 플레이어 E와 F를 생각하자. 플레이어E와 플레이어F는 각각 (가위,바위) 그리고 (가위,보) 중 하나를 선택할 수 있다고 가정하자. 시뮬레이션 대결결과를 이용하여 아래의 확률을 근사적으로 추정하라.\n\n플레이어E가 승리할 확률:\n플레이어F가 승리할 확률:\n플레이어E와 플레이어F가 비길 확률:\n\nhint: 플레이어E가 가위를 낸다면 최소한 지지는 않기 때문에 플레이어E가 좀 더 유리한 패를 가지고 있다. 따라서 플레이어E의 결과가 더 좋을 것이다.\n(6) (5)와 동일한 두 명의 플레이어E, F를 생각하자. 두 플레이어는 100회까지는 랜덤으로 자신의 패를 선택한다. 그리고 101회부터 500회까지는 플레이어F만 데이터로 부터 학습을 하여 수정된 확률을 사용한다. 500번의 대결이 끝나고 플레이어F가 (가위,보)를 선택하는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\n\nhint: 플레이어F는 보를 내는 것이 낫다고 생각할 것이다. (가위를 내면 지거나 비기지만 보를 내면 지거나 이긴다.)\n(7) (6)번의 플레이어E와 플레이어F가 500회~1000회까지 추가로 게임을 한다. 이번에는 플레이어E만 데이터로부터 학습한다. 1000회까지 대결을 끝낸 이후 플레이어E가 (가위,바위)를 내는 확률은 어떻게 업데이트 되었는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\nt=500시점에 업데이트된 확률\n플레이어E는 학습모드 / 플레이어F는 학습모드아님\n\n\n\nhint: 플레이어F는 보를 내도록 학습되어 있다. 따라서 플레이어E가 바위를 내면 지고 가위를 내면 이길것이다. 따라서 플레이어E는 가위가 유리하다고 생각할 것이다.\n(8) (7)번의 플레이어E와 플레이어F가 1000회~30000회까지 추가로 게임을 한다. 이번에는 플레이어F만 데이터로부터 학습한다. 30000회까지 대결을 끝낸 이후 플레이어F가 (가위,보)를 내는 확률은 어떻게 업데이트 되었는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(1/2, 1/2)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 500\n(1/2, 1/2)\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\nt=500시점에 업데이트된 확률\n플레이어E는 학습모드 / 플레이어F는 학습모드아님\n\n\nt &lt;= 30000\nt=1000시점에 업데이트된 확률\n데이터를 학습하여 수정한 확률\n플레이어E는 학습모드아님 / 플레이어F는 학습모드\n\n\n\nhint: 플레이어F는 원래 보가 유리하다고 생각하여 보를 자주 내도록 학습되었다. 하지만 플레이어E가 그러한 플레이어F의 성향을 파악하고 가위를 주로 내도록 학습하였다. 플레이어F는 그러한 플레이어E의 성향을 다시 파악하여 이번에는 가위을 자주 내는 것이 유리하다고 생각할 것이다.\n(9) 플레이어E와 플레이어F의 대결기록을 초기화 한다. 이번에는 플레이어F가 항상 (3/4)의 확률로 가위를 (1/4)의 확률로 보를 낸다고 가정한다. 플레이어E는 100번의 대결까지는 랜덤으로 (가위,바위)중 하나를 내고 101번째 대결부터 1000번째 대결까지는 대결 데이터를 학습하여 수정한 확률을 사용한다고 하자. 1000번째 대결이후에 플레이어E가 (가위,바위)를 내는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(3/4, 1/4)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\n(3/4, 1/4)\n플레이어E는 학습모드 / 플레이어F는 학습모드 아님\n\n\n\n(10) 플레이어E와 플레이어F의 대결기록을 초기화 한다. 이번에는 플레이어F가 항상 (2/3)의 확률로 가위를 (1/3)의 확률로 보를 낸다고 가정한다. 플레이어E는 100번의 대결까지는 랜덤으로 (가위,바위)중 하나를 내고 101번째 대결부터 1000번째 대결까지는 대결 데이터를 학습하여 수정한 확률을 사용한다고 하자. 1000번째 대결이후에 플레이어E가 (가위,바위)를 내는 확률이 어떻게 업데이트 되어있는가?\n\n\n\n\n\n\n\n\n\n시점\n플레이어E가 (가위,바위)를 낼 확률\n플레이어F가 (가위,보)를 낼 확률\n비고\n\n\n\n\nt &lt;= 100\n(1/2, 1/2)\n(2/3, 1/3)\n양쪽 플레이어 모두 학습모드가 아님\n\n\nt &lt;= 1000\n데이터를 학습하여 수정한 확률\n(2/3, 1/3)\n플레이어E는 학습모드 / 플레이어F는 학습모드 아님"
  },
  {
    "objectID": "posts/1_IP2022/04_시험/2022-06-13-final.html#appendix",
    "href": "posts/1_IP2022/04_시험/2022-06-13-final.html#appendix",
    "title": "2022 final exam",
    "section": "",
    "text": "- 아래의 클래스를 참고하여 문제1,2을 풀어라. (5월25일 강의노트에 소개된 클래스를 약간 정리한 것) - 참고하지 않아도 감점은 없음\n\nclass RPS:\n    def __init__(self,candidate):\n        self.candidate = candidate\n        self.actions = list() \n        self.rewards = list()\n        self.prob = [0.5,0.5]\n\n    def __eq__(self,other): # 연산 == 를 재정의 \n        return self.actions[-1] == other.actions[-1] \n        #note: 둘의 액션이 같으면 무승부 \n    \n    def __gt__(self,other): # 연산 &gt; 를 재정의 \n        pair = self.actions[-1], other.actions[-1]\n        return pair == ('가위','보') or pair == ('바위','가위') or pair == ('보','바위') \n        #note: 가위&gt;보, 바위&gt;가위, 보&gt;가위 \n    \n    def __mul__(self,other):\n        # step1: 각자의 패를 선택 \n        self.choose()\n        other.choose()\n        \n        # step2: 승패 판단 + upate reward\n        if self == other: # 무승부일경우 \n            self.rewards.append(1)\n            other.rewards.append(1)\n        elif self &gt; other: # self의 승리 \n            self.rewards.append(2)\n            other.rewards.append(0)\n        else: # other의 승리 \n            self.rewards.append(0)\n            other.rewards.append(2)\n        \n        # step3: update data\n        self.update_data()\n        other.update_data()\n    \n    def update_data(self):\n        self.data = pd.DataFrame({'actions':self.actions, 'rewards':self.rewards})\n    \n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} &lt;br/&gt; \n        데이터: &lt;br/&gt;\n        {}\n        \"\"\"        \n        return html_str.format(self.candidate,self.data._repr_html_())\n    \n    def choose(self):\n        self.actions.append(np.random.choice(self.candidate,p=self.prob))\n\n- 사용예시\n\na=RPS(['가위','가위'])\nb=RPS(['가위','보'])\n\n\nfor i in range(5):\n    a*b\n\n\na\n\n\n        낼 수 있는 패: ['가위', '가위']  \n        데이터: \n        \n\n\n\n\n\n\nactions\nrewards\n\n\n\n\n0\n가위\n2\n\n\n1\n가위\n2\n\n\n2\n가위\n1\n\n\n3\n가위\n2\n\n\n4\n가위\n2\n\n\n\n\n\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['가위', '보']  \n        데이터: \n        \n\n\n\n\n\n\nactions\nrewards\n\n\n\n\n0\n보\n0\n\n\n1\n보\n0\n\n\n2\n가위\n1\n\n\n3\n보\n0\n\n\n4\n보\n0"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html",
    "href": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html",
    "title": "Pandas 0단계",
    "section": "",
    "text": "판다스를 왜 써야할까?, pandas 개발동기\n\n\n\nimport numpy as np\nimport pandas as pd\n\n\n\n\n\n\n- 예제1: 기본인덱싱\n\na = 'asdf'\na[2]\n\n'd'\n\n\n\na[-1]\n\n'f'\n\n\n- 예제2: 슬라이싱\n\na='asdf'\na[1:3]\n\n'sd'\n\n\n- 예제3: 스트라이딩\n\na='asdf'\na[::2] # 1번째, 3번째 원소 출력\n\n'ad'\n\n\n- 예제4: 불가능한 것\n\na = 'asdf'\na[[1,2]] # 정수인덱스를 리스트화 시켜서 인덱싱하는 것을 불가능\n\nTypeError: string indices must be integers\n\n\n\n\n\n- 예제1: 인덱스의 리스트 (혹은 ndarray)를 전달\n\na = np.arange(5)\na,a[[1,2,-1]]\n\n(array([0, 1, 2, 3, 4]), array([1, 2, 4]))\n\n\n\na = np.arange(55,61)\na, a[[1,2,-1]]\n\n(array([55, 56, 57, 58, 59, 60]), array([56, 57, 60]))\n\n\n- 예제2: bool로 이루어진 리스트 (혹은 ndarray)를 전달\n\na[[True, True, False, False, False, False]]\n\narray([55, 56])\n\n\n\na[np.array([True, True, False, False, False, False])] # 꼭 리스트로 전달할 필요는 없음.\n\narray([55, 56])\n\n\n\na[a&lt;58]\n\narray([55, 56, 57])\n\n\n\n\n\n- 예제1\n\na = np.arange(4*3).reshape(4,3)\na\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])\n\n\n\na[0:2,1]\n\narray([1, 4])\n\n\n- 예제2: 차원을 유지하면서 인덱싱을 하고 싶으면?\n\na = np.arange(4*3).reshape(4,3)\na[0:2, [1]]\n\narray([[1],\n       [4]])\n\n\n\n\n\n- 예제1: (key, value)o\n\nd = {'att':65, 'rep':45, 'mid':30, 'fin':100}\nd\n\n{'att': 65, 'rep': 45, 'mid': 30, 'fin': 100}\n\n\n\nd['att'] # key를 넣으면 value가 리턴\n\n65\n\n\n- 예제2: numpy와 비교\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,200)\nrep = np.random.choice(np.arange(5,21)*5,200)\nmid = np.random.choice(np.arange(0,21)*5,200)\nfin = np.random.choice(np.arange(0,21)*5,200)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),200,replace=False)]\ntest_dic = {key[i] : {'att':att[i], 'rep':rep[i], 'mid':mid[i], 'fin':fin[i]} for i in range(200)}\ntest_ndarray = np.array([key,att,rep,mid,fin],dtype=np.int64).T\ndel(att);del(rep);del(mid);del(fin);del(key)\n\n\n#test_dic\n\n학번 202212460에 해당하는 학생의 출석점수를 알고 싶다면?\n- (풀이1)\n\ntest_dic['202212460']['att'] ## 가독성이 좋음.\n\n55\n\n\n- (풀이2)\n\ntest_ndarray[test_ndarray[:,0] == 202212460, 1] ## 가독성이 떨어짐.\n\narray([55])\n\n\n정보를 뽑을 때 Numpy indexing을 이용하는 것보다 딕셔너리를 이용하고 hash 타입으로 접근하는것이 편리할 때가 많이 있다.\n(풀이2)가 (풀이1)에 비하여 불편한 점\n\ntest_ndarray의 첫칼럼은 student id 이고 두번째 칼럼은 att라는 사실을 암기하고 있어야 한다.\nstudent id가 아니고 만약에 학생이름을 써서 데이터를 정리한다면 모든 자료형은 문자형이 되어야 한다.\n작성한 코드의 가독성이 없다. (위치로 접근하기 때문)\n\n- 요약: hash 스타일로 정보를 추출하는 것이 유용할 때가 있다. 그리고 보통 hash 스타일로 정보를 뽑는 것이 유리하다. (사실 Numpy는 정보추출을 위해 개발된 자료형이 아니라 행렬 및 벡터의 수학연산을 지원하기 위해 개발된 자료형이다.)\n- 소망: 정보를 추출할때는 hash 스타일도 유용하다는 것은 이해함 \\(\\to\\) 하지만 나는 넘파이스타일로 정보를 뽑고 싶은걸? 그리고 딕셔너리 형태가 아니고 엑셀처럼(행렬처럼) 데이터를 보고 싶은걸? \\(\\to\\) pandas의 개발\n\n\n\n\n\n\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\ntest_dic = {key[i] : {'att':att[i], 'rep':rep[i], 'mid':mid[i], 'fin':fin[i]} for i in range(20)}\n\n\ntest_dic\n\n{'202212380': {'att': 65, 'rep': 55, 'mid': 50, 'fin': 40},\n '202212370': {'att': 95, 'rep': 100, 'mid': 50, 'fin': 80},\n '202212363': {'att': 65, 'rep': 90, 'mid': 60, 'fin': 30},\n '202212488': {'att': 55, 'rep': 80, 'mid': 75, 'fin': 80},\n '202212312': {'att': 80, 'rep': 30, 'mid': 30, 'fin': 100},\n '202212377': {'att': 75, 'rep': 40, 'mid': 100, 'fin': 15},\n '202212463': {'att': 65, 'rep': 45, 'mid': 45, 'fin': 90},\n '202212471': {'att': 60, 'rep': 60, 'mid': 25, 'fin': 0},\n '202212400': {'att': 95, 'rep': 65, 'mid': 20, 'fin': 10},\n '202212469': {'att': 90, 'rep': 80, 'mid': 80, 'fin': 20},\n '202212318': {'att': 55, 'rep': 75, 'mid': 35, 'fin': 25},\n '202212432': {'att': 95, 'rep': 95, 'mid': 45, 'fin': 0},\n '202212443': {'att': 95, 'rep': 55, 'mid': 15, 'fin': 35},\n '202212367': {'att': 50, 'rep': 80, 'mid': 40, 'fin': 30},\n '202212458': {'att': 50, 'rep': 55, 'mid': 15, 'fin': 85},\n '202212396': {'att': 95, 'rep': 30, 'mid': 30, 'fin': 95},\n '202212482': {'att': 50, 'rep': 50, 'mid': 45, 'fin': 10},\n '202212452': {'att': 65, 'rep': 55, 'mid': 15, 'fin': 45},\n '202212387': {'att': 70, 'rep': 70, 'mid': 40, 'fin': 35},\n '202212354': {'att': 90, 'rep': 90, 'mid': 80, 'fin': 90}}\n\n\n\n테이블형태로 보고싶다.\n\n(방법1) – 행렬이기는 하지만 방법 2,3,4,5에 비하여 우리가 원하는 만큼 가독성을 주는 형태는 아님.\n\ntest_ndarray = np.array([key,att,rep,mid,fin],dtype=np.int64).T\ntest_ndarray\n\narray([[202212380,        65,        55,        50,        40],\n       [202212370,        95,       100,        50,        80],\n       [202212363,        65,        90,        60,        30],\n       [202212488,        55,        80,        75,        80],\n       [202212312,        80,        30,        30,       100],\n       [202212377,        75,        40,       100,        15],\n       [202212463,        65,        45,        45,        90],\n       [202212471,        60,        60,        25,         0],\n       [202212400,        95,        65,        20,        10],\n       [202212469,        90,        80,        80,        20],\n       [202212318,        55,        75,        35,        25],\n       [202212432,        95,        95,        45,         0],\n       [202212443,        95,        55,        15,        35],\n       [202212367,        50,        80,        40,        30],\n       [202212458,        50,        55,        15,        85],\n       [202212396,        95,        30,        30,        95],\n       [202212482,        50,        50,        45,        10],\n       [202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n(방법2)\n\npd.DataFrame(test_dic).T\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법3)\n\ntest_dic2 = {'att':{key[i]:att[i] for i in range(20)},\n             'rep':{key[i]:rep[i] for i in range(20)},\n             'mid':{key[i]:mid[i] for i in range(20)},\n             'fin':{key[i]:fin[i] for i in range(20)}}\n\n\npd.DataFrame(test_dic2)\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법4)\n\ndf = pd.DataFrame({'att':att, 'rep':rep, 'mid':mid, 'fin':fin}, index=key)\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법5)\n\ndf = pd.DataFrame({'att':att, 'rep':rep, 'mid':mid, 'fin':fin})\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n0\n65\n55\n50\n40\n\n\n1\n95\n100\n50\n80\n\n\n2\n65\n90\n60\n30\n\n\n3\n55\n80\n75\n80\n\n\n4\n80\n30\n30\n100\n\n\n5\n75\n40\n100\n15\n\n\n6\n65\n45\n45\n90\n\n\n7\n60\n60\n25\n0\n\n\n8\n95\n65\n20\n10\n\n\n9\n90\n80\n80\n20\n\n\n10\n55\n75\n35\n25\n\n\n11\n95\n95\n45\n0\n\n\n12\n95\n55\n15\n35\n\n\n13\n50\n80\n40\n30\n\n\n14\n50\n55\n15\n85\n\n\n15\n95\n30\n30\n95\n\n\n16\n50\n50\n45\n10\n\n\n17\n65\n55\n15\n45\n\n\n18\n70\n70\n40\n35\n\n\n19\n90\n90\n80\n90\n\n\n\n\n\n\n\n\ndf = df.set_index([key])\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\n\n\n- 예제1: 출석점수를 출력\n\ntest_dic2['att']\n\n{'202212380': 65,\n '202212370': 95,\n '202212363': 65,\n '202212488': 55,\n '202212312': 80,\n '202212377': 75,\n '202212463': 65,\n '202212471': 60,\n '202212400': 95,\n '202212469': 90,\n '202212318': 55,\n '202212432': 95,\n '202212443': 95,\n '202212367': 50,\n '202212458': 50,\n '202212396': 95,\n '202212482': 50,\n '202212452': 65,\n '202212387': 70,\n '202212354': 90}\n\n\n\ndf['att']\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 예제2: 학번 202212380의 출석점수 출력\n\ntest_dic2['att']['202212380']\n\n65\n\n\n\ndf['att']['202212380']\n\n65\n\n\n\n\n\n- 예제1: 첫번째 학생의 기말고사 성적을 출력하고 싶다.\n\ntest_ndarray[0,-1]\n\n40\n\n\n\ndf.iloc[0,-1]\n\n40\n\n\n\n벼락치기: df에서 iloc이라는 특수기능을 이용하면 넘파이 인덱싱처럼 원소출력이 가능하다.\n\n- 예제2: 홀수번째 학생의 점수를 뽑고 싶다.\n\ntest_ndarray[::2]\n\narray([[202212380,        65,        55,        50,        40],\n       [202212363,        65,        90,        60,        30],\n       [202212312,        80,        30,        30,       100],\n       [202212463,        65,        45,        45,        90],\n       [202212400,        95,        65,        20,        10],\n       [202212318,        55,        75,        35,        25],\n       [202212443,        95,        55,        15,        35],\n       [202212458,        50,        55,        15,        85],\n       [202212482,        50,        50,        45,        10],\n       [202212387,        70,        70,        40,        35]])\n\n\n\ndf.iloc[::2]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n202212312\n80\n30\n30\n100\n\n\n202212463\n65\n45\n45\n90\n\n\n202212400\n95\n65\n20\n10\n\n\n202212318\n55\n75\n35\n25\n\n\n202212443\n95\n55\n15\n35\n\n\n202212458\n50\n55\n15\n85\n\n\n202212482\n50\n50\n45\n10\n\n\n202212387\n70\n70\n40\n35\n\n\n\n\n\n\n\n- 예제3: 맨 끝에서 3명의 점수를 출력하고 싶다.\n\ntest_ndarray[-3:]\n\narray([[202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n\ndf.iloc[-3:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n- 예제4: 맨 끝에서 3명의 점수 중 마지막 2개의 칼럼만 출력하고 싶다.\n\ntest_ndarray[-3:,-2:]\n\narray([[15, 45],\n       [40, 35],\n       [80, 90]])\n\n\n\ndf.iloc[-3:,-2:]\n\n\n\n\n\n\n\n\nmid\nfin\n\n\n\n\n202212452\n15\n45\n\n\n202212387\n40\n35\n\n\n202212354\n80\n90\n\n\n\n\n\n\n\n\n\n\n- 예제1: 중간고사 점수가 20점 이상이면서 동시에 출석점수가 60점미만인 학생들의 기말고사 점수를 출력\n\ndf.query('mid &gt;= 20 and att &lt; 60')\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212488\n55\n80\n75\n80\n\n\n202212318\n55\n75\n35\n25\n\n\n202212367\n50\n80\n40\n30\n\n\n202212482\n50\n50\n45\n10\n\n\n\n\n\n\n\n\ndf.query('mid &gt;= 20 and att &lt; 60')['fin']\n\n202212488    80\n202212318    25\n202212367    30\n202212482    10\nName: fin, dtype: int64\n\n\n(방법2) 넘파이 스타일이라면?\n\ntest_ndarray\n\narray([[202212380,        65,        55,        50,        40],\n       [202212370,        95,       100,        50,        80],\n       [202212363,        65,        90,        60,        30],\n       [202212488,        55,        80,        75,        80],\n       [202212312,        80,        30,        30,       100],\n       [202212377,        75,        40,       100,        15],\n       [202212463,        65,        45,        45,        90],\n       [202212471,        60,        60,        25,         0],\n       [202212400,        95,        65,        20,        10],\n       [202212469,        90,        80,        80,        20],\n       [202212318,        55,        75,        35,        25],\n       [202212432,        95,        95,        45,         0],\n       [202212443,        95,        55,        15,        35],\n       [202212367,        50,        80,        40,        30],\n       [202212458,        50,        55,        15,        85],\n       [202212396,        95,        30,        30,        95],\n       [202212482,        50,        50,        45,        10],\n       [202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n\ntest_ndarray[:,3] &gt;= 20 ## 중간고사가 20점이상\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True, False,  True, False,  True,  True, False,\n        True,  True])\n\n\n\ntest_ndarray[:,1] &lt; 60 ## 출석이 60미만 \n\narray([False, False, False,  True, False, False, False, False, False,\n       False,  True, False, False,  True,  True, False,  True, False,\n       False, False])\n\n\n\n(test_ndarray[:,3] &gt;= 20) & (test_ndarray[:,1] &lt; 60)\n\narray([False, False, False,  True, False, False, False, False, False,\n       False,  True, False, False,  True, False, False,  True, False,\n       False, False])\n\n\n\nnote: test_ndarray[:,3] &gt;= 20 & test_ndarray[:,1] &gt;= 60와 같이 하면 에러가 난다. 조심하자! 괄호!!!\n\n\ntest_ndarray[(test_ndarray[:,3] &gt;= 20) & (test_ndarray[:,1] &lt; 60),-1]\n\narray([80, 25, 30, 10])\n\n\n\n구현난이도 어려움, 가독성 꽝..\n\n- 예제2: 중간고사점수&lt;기말고사점수인 학생들의 출석점수 평균을 구하자.\n\ndf.query('mid &lt; fin')\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212370\n95\n100\n50\n80\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212463\n65\n45\n45\n90\n\n\n202212443\n95\n55\n15\n35\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212452\n65\n55\n15\n45\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\ndf.query('mid &lt; fin')['att'].mean()\n\n76.66666666666667\n\n\n\n\n\n\n\n- 방법1: dictionary에서 만든다.\n\npd.DataFrame({'att':[30,40,50], 'mid':[50,60,70]}"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#import",
    "href": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#import",
    "title": "Pandas 0단계",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#부분-데이터-꺼내기-판다스를-왜-써야할까",
    "href": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#부분-데이터-꺼내기-판다스를-왜-써야할까",
    "title": "Pandas 0단계",
    "section": "",
    "text": "- 예제1: 기본인덱싱\n\na = 'asdf'\na[2]\n\n'd'\n\n\n\na[-1]\n\n'f'\n\n\n- 예제2: 슬라이싱\n\na='asdf'\na[1:3]\n\n'sd'\n\n\n- 예제3: 스트라이딩\n\na='asdf'\na[::2] # 1번째, 3번째 원소 출력\n\n'ad'\n\n\n- 예제4: 불가능한 것\n\na = 'asdf'\na[[1,2]] # 정수인덱스를 리스트화 시켜서 인덱싱하는 것을 불가능\n\nTypeError: string indices must be integers\n\n\n\n\n\n- 예제1: 인덱스의 리스트 (혹은 ndarray)를 전달\n\na = np.arange(5)\na,a[[1,2,-1]]\n\n(array([0, 1, 2, 3, 4]), array([1, 2, 4]))\n\n\n\na = np.arange(55,61)\na, a[[1,2,-1]]\n\n(array([55, 56, 57, 58, 59, 60]), array([56, 57, 60]))\n\n\n- 예제2: bool로 이루어진 리스트 (혹은 ndarray)를 전달\n\na[[True, True, False, False, False, False]]\n\narray([55, 56])\n\n\n\na[np.array([True, True, False, False, False, False])] # 꼭 리스트로 전달할 필요는 없음.\n\narray([55, 56])\n\n\n\na[a&lt;58]\n\narray([55, 56, 57])\n\n\n\n\n\n- 예제1\n\na = np.arange(4*3).reshape(4,3)\na\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])\n\n\n\na[0:2,1]\n\narray([1, 4])\n\n\n- 예제2: 차원을 유지하면서 인덱싱을 하고 싶으면?\n\na = np.arange(4*3).reshape(4,3)\na[0:2, [1]]\n\narray([[1],\n       [4]])\n\n\n\n\n\n- 예제1: (key, value)o\n\nd = {'att':65, 'rep':45, 'mid':30, 'fin':100}\nd\n\n{'att': 65, 'rep': 45, 'mid': 30, 'fin': 100}\n\n\n\nd['att'] # key를 넣으면 value가 리턴\n\n65\n\n\n- 예제2: numpy와 비교\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,200)\nrep = np.random.choice(np.arange(5,21)*5,200)\nmid = np.random.choice(np.arange(0,21)*5,200)\nfin = np.random.choice(np.arange(0,21)*5,200)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),200,replace=False)]\ntest_dic = {key[i] : {'att':att[i], 'rep':rep[i], 'mid':mid[i], 'fin':fin[i]} for i in range(200)}\ntest_ndarray = np.array([key,att,rep,mid,fin],dtype=np.int64).T\ndel(att);del(rep);del(mid);del(fin);del(key)\n\n\n#test_dic\n\n학번 202212460에 해당하는 학생의 출석점수를 알고 싶다면?\n- (풀이1)\n\ntest_dic['202212460']['att'] ## 가독성이 좋음.\n\n55\n\n\n- (풀이2)\n\ntest_ndarray[test_ndarray[:,0] == 202212460, 1] ## 가독성이 떨어짐.\n\narray([55])\n\n\n정보를 뽑을 때 Numpy indexing을 이용하는 것보다 딕셔너리를 이용하고 hash 타입으로 접근하는것이 편리할 때가 많이 있다.\n(풀이2)가 (풀이1)에 비하여 불편한 점\n\ntest_ndarray의 첫칼럼은 student id 이고 두번째 칼럼은 att라는 사실을 암기하고 있어야 한다.\nstudent id가 아니고 만약에 학생이름을 써서 데이터를 정리한다면 모든 자료형은 문자형이 되어야 한다.\n작성한 코드의 가독성이 없다. (위치로 접근하기 때문)\n\n- 요약: hash 스타일로 정보를 추출하는 것이 유용할 때가 있다. 그리고 보통 hash 스타일로 정보를 뽑는 것이 유리하다. (사실 Numpy는 정보추출을 위해 개발된 자료형이 아니라 행렬 및 벡터의 수학연산을 지원하기 위해 개발된 자료형이다.)\n- 소망: 정보를 추출할때는 hash 스타일도 유용하다는 것은 이해함 \\(\\to\\) 하지만 나는 넘파이스타일로 정보를 뽑고 싶은걸? 그리고 딕셔너리 형태가 아니고 엑셀처럼(행렬처럼) 데이터를 보고 싶은걸? \\(\\to\\) pandas의 개발"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#pandas-개발동기",
    "href": "posts/1_IP2022/02_DataScience/2023-03-12-pandas0.html#pandas-개발동기",
    "title": "Pandas 0단계",
    "section": "",
    "text": "np.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\ntest_dic = {key[i] : {'att':att[i], 'rep':rep[i], 'mid':mid[i], 'fin':fin[i]} for i in range(20)}\n\n\ntest_dic\n\n{'202212380': {'att': 65, 'rep': 55, 'mid': 50, 'fin': 40},\n '202212370': {'att': 95, 'rep': 100, 'mid': 50, 'fin': 80},\n '202212363': {'att': 65, 'rep': 90, 'mid': 60, 'fin': 30},\n '202212488': {'att': 55, 'rep': 80, 'mid': 75, 'fin': 80},\n '202212312': {'att': 80, 'rep': 30, 'mid': 30, 'fin': 100},\n '202212377': {'att': 75, 'rep': 40, 'mid': 100, 'fin': 15},\n '202212463': {'att': 65, 'rep': 45, 'mid': 45, 'fin': 90},\n '202212471': {'att': 60, 'rep': 60, 'mid': 25, 'fin': 0},\n '202212400': {'att': 95, 'rep': 65, 'mid': 20, 'fin': 10},\n '202212469': {'att': 90, 'rep': 80, 'mid': 80, 'fin': 20},\n '202212318': {'att': 55, 'rep': 75, 'mid': 35, 'fin': 25},\n '202212432': {'att': 95, 'rep': 95, 'mid': 45, 'fin': 0},\n '202212443': {'att': 95, 'rep': 55, 'mid': 15, 'fin': 35},\n '202212367': {'att': 50, 'rep': 80, 'mid': 40, 'fin': 30},\n '202212458': {'att': 50, 'rep': 55, 'mid': 15, 'fin': 85},\n '202212396': {'att': 95, 'rep': 30, 'mid': 30, 'fin': 95},\n '202212482': {'att': 50, 'rep': 50, 'mid': 45, 'fin': 10},\n '202212452': {'att': 65, 'rep': 55, 'mid': 15, 'fin': 45},\n '202212387': {'att': 70, 'rep': 70, 'mid': 40, 'fin': 35},\n '202212354': {'att': 90, 'rep': 90, 'mid': 80, 'fin': 90}}\n\n\n\n테이블형태로 보고싶다.\n\n(방법1) – 행렬이기는 하지만 방법 2,3,4,5에 비하여 우리가 원하는 만큼 가독성을 주는 형태는 아님.\n\ntest_ndarray = np.array([key,att,rep,mid,fin],dtype=np.int64).T\ntest_ndarray\n\narray([[202212380,        65,        55,        50,        40],\n       [202212370,        95,       100,        50,        80],\n       [202212363,        65,        90,        60,        30],\n       [202212488,        55,        80,        75,        80],\n       [202212312,        80,        30,        30,       100],\n       [202212377,        75,        40,       100,        15],\n       [202212463,        65,        45,        45,        90],\n       [202212471,        60,        60,        25,         0],\n       [202212400,        95,        65,        20,        10],\n       [202212469,        90,        80,        80,        20],\n       [202212318,        55,        75,        35,        25],\n       [202212432,        95,        95,        45,         0],\n       [202212443,        95,        55,        15,        35],\n       [202212367,        50,        80,        40,        30],\n       [202212458,        50,        55,        15,        85],\n       [202212396,        95,        30,        30,        95],\n       [202212482,        50,        50,        45,        10],\n       [202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n(방법2)\n\npd.DataFrame(test_dic).T\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법3)\n\ntest_dic2 = {'att':{key[i]:att[i] for i in range(20)},\n             'rep':{key[i]:rep[i] for i in range(20)},\n             'mid':{key[i]:mid[i] for i in range(20)},\n             'fin':{key[i]:fin[i] for i in range(20)}}\n\n\npd.DataFrame(test_dic2)\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법4)\n\ndf = pd.DataFrame({'att':att, 'rep':rep, 'mid':mid, 'fin':fin}, index=key)\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n(방법5)\n\ndf = pd.DataFrame({'att':att, 'rep':rep, 'mid':mid, 'fin':fin})\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n0\n65\n55\n50\n40\n\n\n1\n95\n100\n50\n80\n\n\n2\n65\n90\n60\n30\n\n\n3\n55\n80\n75\n80\n\n\n4\n80\n30\n30\n100\n\n\n5\n75\n40\n100\n15\n\n\n6\n65\n45\n45\n90\n\n\n7\n60\n60\n25\n0\n\n\n8\n95\n65\n20\n10\n\n\n9\n90\n80\n80\n20\n\n\n10\n55\n75\n35\n25\n\n\n11\n95\n95\n45\n0\n\n\n12\n95\n55\n15\n35\n\n\n13\n50\n80\n40\n30\n\n\n14\n50\n55\n15\n85\n\n\n15\n95\n30\n30\n95\n\n\n16\n50\n50\n45\n10\n\n\n17\n65\n55\n15\n45\n\n\n18\n70\n70\n40\n35\n\n\n19\n90\n90\n80\n90\n\n\n\n\n\n\n\n\ndf = df.set_index([key])\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\n\n\n- 예제1: 출석점수를 출력\n\ntest_dic2['att']\n\n{'202212380': 65,\n '202212370': 95,\n '202212363': 65,\n '202212488': 55,\n '202212312': 80,\n '202212377': 75,\n '202212463': 65,\n '202212471': 60,\n '202212400': 95,\n '202212469': 90,\n '202212318': 55,\n '202212432': 95,\n '202212443': 95,\n '202212367': 50,\n '202212458': 50,\n '202212396': 95,\n '202212482': 50,\n '202212452': 65,\n '202212387': 70,\n '202212354': 90}\n\n\n\ndf['att']\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 예제2: 학번 202212380의 출석점수 출력\n\ntest_dic2['att']['202212380']\n\n65\n\n\n\ndf['att']['202212380']\n\n65\n\n\n\n\n\n- 예제1: 첫번째 학생의 기말고사 성적을 출력하고 싶다.\n\ntest_ndarray[0,-1]\n\n40\n\n\n\ndf.iloc[0,-1]\n\n40\n\n\n\n벼락치기: df에서 iloc이라는 특수기능을 이용하면 넘파이 인덱싱처럼 원소출력이 가능하다.\n\n- 예제2: 홀수번째 학생의 점수를 뽑고 싶다.\n\ntest_ndarray[::2]\n\narray([[202212380,        65,        55,        50,        40],\n       [202212363,        65,        90,        60,        30],\n       [202212312,        80,        30,        30,       100],\n       [202212463,        65,        45,        45,        90],\n       [202212400,        95,        65,        20,        10],\n       [202212318,        55,        75,        35,        25],\n       [202212443,        95,        55,        15,        35],\n       [202212458,        50,        55,        15,        85],\n       [202212482,        50,        50,        45,        10],\n       [202212387,        70,        70,        40,        35]])\n\n\n\ndf.iloc[::2]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n202212312\n80\n30\n30\n100\n\n\n202212463\n65\n45\n45\n90\n\n\n202212400\n95\n65\n20\n10\n\n\n202212318\n55\n75\n35\n25\n\n\n202212443\n95\n55\n15\n35\n\n\n202212458\n50\n55\n15\n85\n\n\n202212482\n50\n50\n45\n10\n\n\n202212387\n70\n70\n40\n35\n\n\n\n\n\n\n\n- 예제3: 맨 끝에서 3명의 점수를 출력하고 싶다.\n\ntest_ndarray[-3:]\n\narray([[202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n\ndf.iloc[-3:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n- 예제4: 맨 끝에서 3명의 점수 중 마지막 2개의 칼럼만 출력하고 싶다.\n\ntest_ndarray[-3:,-2:]\n\narray([[15, 45],\n       [40, 35],\n       [80, 90]])\n\n\n\ndf.iloc[-3:,-2:]\n\n\n\n\n\n\n\n\nmid\nfin\n\n\n\n\n202212452\n15\n45\n\n\n202212387\n40\n35\n\n\n202212354\n80\n90\n\n\n\n\n\n\n\n\n\n\n- 예제1: 중간고사 점수가 20점 이상이면서 동시에 출석점수가 60점미만인 학생들의 기말고사 점수를 출력\n\ndf.query('mid &gt;= 20 and att &lt; 60')\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212488\n55\n80\n75\n80\n\n\n202212318\n55\n75\n35\n25\n\n\n202212367\n50\n80\n40\n30\n\n\n202212482\n50\n50\n45\n10\n\n\n\n\n\n\n\n\ndf.query('mid &gt;= 20 and att &lt; 60')['fin']\n\n202212488    80\n202212318    25\n202212367    30\n202212482    10\nName: fin, dtype: int64\n\n\n(방법2) 넘파이 스타일이라면?\n\ntest_ndarray\n\narray([[202212380,        65,        55,        50,        40],\n       [202212370,        95,       100,        50,        80],\n       [202212363,        65,        90,        60,        30],\n       [202212488,        55,        80,        75,        80],\n       [202212312,        80,        30,        30,       100],\n       [202212377,        75,        40,       100,        15],\n       [202212463,        65,        45,        45,        90],\n       [202212471,        60,        60,        25,         0],\n       [202212400,        95,        65,        20,        10],\n       [202212469,        90,        80,        80,        20],\n       [202212318,        55,        75,        35,        25],\n       [202212432,        95,        95,        45,         0],\n       [202212443,        95,        55,        15,        35],\n       [202212367,        50,        80,        40,        30],\n       [202212458,        50,        55,        15,        85],\n       [202212396,        95,        30,        30,        95],\n       [202212482,        50,        50,        45,        10],\n       [202212452,        65,        55,        15,        45],\n       [202212387,        70,        70,        40,        35],\n       [202212354,        90,        90,        80,        90]])\n\n\n\ntest_ndarray[:,3] &gt;= 20 ## 중간고사가 20점이상\n\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True, False,  True, False,  True,  True, False,\n        True,  True])\n\n\n\ntest_ndarray[:,1] &lt; 60 ## 출석이 60미만 \n\narray([False, False, False,  True, False, False, False, False, False,\n       False,  True, False, False,  True,  True, False,  True, False,\n       False, False])\n\n\n\n(test_ndarray[:,3] &gt;= 20) & (test_ndarray[:,1] &lt; 60)\n\narray([False, False, False,  True, False, False, False, False, False,\n       False,  True, False, False,  True, False, False,  True, False,\n       False, False])\n\n\n\nnote: test_ndarray[:,3] &gt;= 20 & test_ndarray[:,1] &gt;= 60와 같이 하면 에러가 난다. 조심하자! 괄호!!!\n\n\ntest_ndarray[(test_ndarray[:,3] &gt;= 20) & (test_ndarray[:,1] &lt; 60),-1]\n\narray([80, 25, 30, 10])\n\n\n\n구현난이도 어려움, 가독성 꽝..\n\n- 예제2: 중간고사점수&lt;기말고사점수인 학생들의 출석점수 평균을 구하자.\n\ndf.query('mid &lt; fin')\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212370\n95\n100\n50\n80\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212463\n65\n45\n45\n90\n\n\n202212443\n95\n55\n15\n35\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212452\n65\n55\n15\n45\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\ndf.query('mid &lt; fin')['att'].mean()\n\n76.66666666666667\n\n\n\n\n\n\n\n- 방법1: dictionary에서 만든다.\n\npd.DataFrame({'att':[30,40,50], 'mid':[50,60,70]}"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-14-pandas2.html",
    "href": "posts/1_IP2022/02_DataScience/2023-03-14-pandas2.html",
    "title": "Pandas 2단계",
    "section": "",
    "text": "하나 혹은 여러개의 col\\(\\cdot\\)row 선택하는 법\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n- 데이터\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\n\n\ndf=pd.DataFrame({'att':att,'rep':rep,'mid':mid,'fin':fin},index=key)\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.att\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 방법2: dict 스타일\n\ndf['att']\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n\ntype(df['att'])\n\npandas.core.series.Series\n\n\n- 방법3: dict 스타일\n\ndf[['att']]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ntype(df[['att']])\n\npandas.core.frame.DataFrame\n\n\n\ndf.att나 df['att']는 series를 리턴하고 df[['att']]는 dataframe을 리턴한다.\n\n- 방법4: ndarray스타일\n\ndf.iloc[:,0] \n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n\ntype(df.iloc[:,0] )\n\npandas.core.series.Series\n\n\n- 방법5: ndarray 스타일\n\ndf.iloc[:,[0]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ntype(df.iloc[:,[0]])\n\npandas.core.frame.DataFrame\n\n\n\ndf.iloc[:,0]은 series를 리턴하고 df.iloc[:,[0]]은 dataframe을 리턴한다.\n\n- 방법6: ndarray 스타일과 dict스타일의 혼합\n\ndf.loc[:,'att'] \n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 방법7: ndarray 스타일과 dict스타일의 혼합\n\ndf.loc[:,['att']] \n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ndf.loc[:,'att']은 series를 리턴하고 df.loc[:,['att']] 은 dataframe을 리턴한다.\n\n- 방법7: ndarray 스타일 + bool 인덱싱\n\ndf.iloc[:,[True,False,False,False]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n- 방법8: ndarray와 dict의 혼합형 + bool 인덱싱\n\ndf.loc[:,[True,False,False,False]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\n\n\n- 방법1: dict 스타일\n\ndf[['att','fin']]\n\n\n\n\n\n\n\n\natt\nfin\n\n\n\n\n202212380\n65\n40\n\n\n202212370\n95\n80\n\n\n202212363\n65\n30\n\n\n202212488\n55\n80\n\n\n202212312\n80\n100\n\n\n202212377\n75\n15\n\n\n202212463\n65\n90\n\n\n202212471\n60\n0\n\n\n202212400\n95\n10\n\n\n202212469\n90\n20\n\n\n202212318\n55\n25\n\n\n202212432\n95\n0\n\n\n202212443\n95\n35\n\n\n202212367\n50\n30\n\n\n202212458\n50\n85\n\n\n202212396\n95\n95\n\n\n202212482\n50\n10\n\n\n202212452\n65\n45\n\n\n202212387\n70\n35\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n- 방법2: ndarray 스타일 (정수리스트로 인덱싱, 슬라이싱, 스트라이딩)\n\ndf.iloc[:,[0,1]] # 정수의 리스트를 전달하여 컬럼추출\n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,range(2)] \n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,:2]  # 슬라이싱 , 0,1,2에서 마지막 2는 제외되고 0,1에 해당하는 것만 추출\n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,::2]  # 스트라이딩\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n- 방법3: ndarray와 dict의 혼합형\n\ndf.loc[:,['att','mid']] \n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,'att':'mid']  # 마지막의 mid도 포함된다. \n\n\n\n\n\n\n\n\natt\nrep\nmid\n\n\n\n\n202212380\n65\n55\n50\n\n\n202212370\n95\n100\n50\n\n\n202212363\n65\n90\n60\n\n\n202212488\n55\n80\n75\n\n\n202212312\n80\n30\n30\n\n\n202212377\n75\n40\n100\n\n\n202212463\n65\n45\n45\n\n\n202212471\n60\n60\n25\n\n\n202212400\n95\n65\n20\n\n\n202212469\n90\n80\n80\n\n\n202212318\n55\n75\n35\n\n\n202212432\n95\n95\n45\n\n\n202212443\n95\n55\n15\n\n\n202212367\n50\n80\n40\n\n\n202212458\n50\n55\n15\n\n\n202212396\n95\n30\n30\n\n\n202212482\n50\n50\n45\n\n\n202212452\n65\n55\n15\n\n\n202212387\n70\n70\n40\n\n\n202212354\n90\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,'rep':] \n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212370\n100\n50\n80\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212312\n30\n30\n100\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212400\n65\n20\n10\n\n\n202212469\n80\n80\n20\n\n\n202212318\n75\n35\n25\n\n\n202212432\n95\n45\n0\n\n\n202212443\n55\n15\n35\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212396\n30\n30\n95\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n202212354\n90\n80\n90\n\n\n\n\n\n\n\n- 방법4: bool을 이용한 인덱싱\n\ndf.iloc[:,[True,False,True,False]]\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,[True,False,True,False]]\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.iloc[0]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.iloc[0])\n\npandas.core.series.Series\n\n\n- 방법2\n\ndf.iloc[[0]]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.iloc[[0]])\n\npandas.core.frame.DataFrame\n\n\n- 방법3\n\ndf.iloc[0,:]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.iloc[0,:])\n\npandas.core.series.Series\n\n\n- 방법4\n\ndf.iloc[[0],:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.iloc[[0],:])\n\npandas.core.frame.DataFrame\n\n\n- 방법5\n\ndf.loc['202212380']\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.loc['202212380'])\n\npandas.core.series.Series\n\n\n- 방법6\n\ndf.loc[['202212380']]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.loc[['202212380']])\n\npandas.core.frame.DataFrame\n\n\n- 방법7\n\ndf.loc['202212380',:]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.loc['202212380',:])\n\npandas.core.series.Series\n\n\n- 방법8\n\ndf.loc[['202212380'],:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.loc[['202212380'],:])\n\npandas.core.frame.DataFrame\n\n\n- 방법9\n\nlen(df)\n\n20\n\n\n\n_lst = [True]+[False]*19\n\n\ndf.iloc[_lst] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.iloc[_lst,:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.loc[_lst] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.loc[_lst,:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.iloc[[0,2]]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n\ndf.iloc[[0,2],:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n- 방법2\n\ndf.loc[['202212380','202212363']] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n\ndf.loc[['202212380','202212363'],:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n- 그 밖의 방법들\n\ndf.iloc[::3] # 스트라이딩\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212488\n55\n80\n75\n80\n\n\n202212463\n65\n45\n45\n90\n\n\n202212469\n90\n80\n80\n20\n\n\n202212443\n95\n55\n15\n35\n\n\n202212396\n95\n30\n30\n95\n\n\n202212387\n70\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.iloc[:5]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ndf.loc[:'202212312']\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ndf.loc[list(df.att&lt;80),'rep':]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.loc[df.att&lt;80,'rep':]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.iloc[list(df.att&lt;80),1:]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n- 아래는 에러가 난다 주의!\n\ndf.iloc[df.att&lt;80, 1:]\n\nValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types"
  },
  {
    "objectID": "posts/1_IP2022/02_DataScience/2023-03-14-pandas2.html#pandas-공부-2단계",
    "href": "posts/1_IP2022/02_DataScience/2023-03-14-pandas2.html#pandas-공부-2단계",
    "title": "Pandas 2단계",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n- 데이터\n\nnp.random.seed(43052)\natt = np.random.choice(np.arange(10,21)*5,20)\nrep = np.random.choice(np.arange(5,21)*5,20)\nmid = np.random.choice(np.arange(0,21)*5,20)\nfin = np.random.choice(np.arange(0,21)*5,20)\nkey = ['202212'+str(s) for s in np.random.choice(np.arange(300,501),20,replace=False)]\n\n\ndf=pd.DataFrame({'att':att,'rep':rep,'mid':mid,'fin':fin},index=key)\ndf\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n202212377\n75\n40\n100\n15\n\n\n202212463\n65\n45\n45\n90\n\n\n202212471\n60\n60\n25\n0\n\n\n202212400\n95\n65\n20\n10\n\n\n202212469\n90\n80\n80\n20\n\n\n202212318\n55\n75\n35\n25\n\n\n202212432\n95\n95\n45\n0\n\n\n202212443\n95\n55\n15\n35\n\n\n202212367\n50\n80\n40\n30\n\n\n202212458\n50\n55\n15\n85\n\n\n202212396\n95\n30\n30\n95\n\n\n202212482\n50\n50\n45\n10\n\n\n202212452\n65\n55\n15\n45\n\n\n202212387\n70\n70\n40\n35\n\n\n202212354\n90\n90\n80\n90\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.att\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 방법2: dict 스타일\n\ndf['att']\n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n\ntype(df['att'])\n\npandas.core.series.Series\n\n\n- 방법3: dict 스타일\n\ndf[['att']]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ntype(df[['att']])\n\npandas.core.frame.DataFrame\n\n\n\ndf.att나 df['att']는 series를 리턴하고 df[['att']]는 dataframe을 리턴한다.\n\n- 방법4: ndarray스타일\n\ndf.iloc[:,0] \n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n\ntype(df.iloc[:,0] )\n\npandas.core.series.Series\n\n\n- 방법5: ndarray 스타일\n\ndf.iloc[:,[0]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ntype(df.iloc[:,[0]])\n\npandas.core.frame.DataFrame\n\n\n\ndf.iloc[:,0]은 series를 리턴하고 df.iloc[:,[0]]은 dataframe을 리턴한다.\n\n- 방법6: ndarray 스타일과 dict스타일의 혼합\n\ndf.loc[:,'att'] \n\n202212380    65\n202212370    95\n202212363    65\n202212488    55\n202212312    80\n202212377    75\n202212463    65\n202212471    60\n202212400    95\n202212469    90\n202212318    55\n202212432    95\n202212443    95\n202212367    50\n202212458    50\n202212396    95\n202212482    50\n202212452    65\n202212387    70\n202212354    90\nName: att, dtype: int64\n\n\n- 방법7: ndarray 스타일과 dict스타일의 혼합\n\ndf.loc[:,['att']] \n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\ndf.loc[:,'att']은 series를 리턴하고 df.loc[:,['att']] 은 dataframe을 리턴한다.\n\n- 방법7: ndarray 스타일 + bool 인덱싱\n\ndf.iloc[:,[True,False,False,False]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n- 방법8: ndarray와 dict의 혼합형 + bool 인덱싱\n\ndf.loc[:,[True,False,False,False]]\n\n\n\n\n\n\n\n\natt\n\n\n\n\n202212380\n65\n\n\n202212370\n95\n\n\n202212363\n65\n\n\n202212488\n55\n\n\n202212312\n80\n\n\n202212377\n75\n\n\n202212463\n65\n\n\n202212471\n60\n\n\n202212400\n95\n\n\n202212469\n90\n\n\n202212318\n55\n\n\n202212432\n95\n\n\n202212443\n95\n\n\n202212367\n50\n\n\n202212458\n50\n\n\n202212396\n95\n\n\n202212482\n50\n\n\n202212452\n65\n\n\n202212387\n70\n\n\n202212354\n90\n\n\n\n\n\n\n\n\n\n\n- 방법1: dict 스타일\n\ndf[['att','fin']]\n\n\n\n\n\n\n\n\natt\nfin\n\n\n\n\n202212380\n65\n40\n\n\n202212370\n95\n80\n\n\n202212363\n65\n30\n\n\n202212488\n55\n80\n\n\n202212312\n80\n100\n\n\n202212377\n75\n15\n\n\n202212463\n65\n90\n\n\n202212471\n60\n0\n\n\n202212400\n95\n10\n\n\n202212469\n90\n20\n\n\n202212318\n55\n25\n\n\n202212432\n95\n0\n\n\n202212443\n95\n35\n\n\n202212367\n50\n30\n\n\n202212458\n50\n85\n\n\n202212396\n95\n95\n\n\n202212482\n50\n10\n\n\n202212452\n65\n45\n\n\n202212387\n70\n35\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n- 방법2: ndarray 스타일 (정수리스트로 인덱싱, 슬라이싱, 스트라이딩)\n\ndf.iloc[:,[0,1]] # 정수의 리스트를 전달하여 컬럼추출\n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,range(2)] \n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,:2]  # 슬라이싱 , 0,1,2에서 마지막 2는 제외되고 0,1에 해당하는 것만 추출\n\n\n\n\n\n\n\n\natt\nrep\n\n\n\n\n202212380\n65\n55\n\n\n202212370\n95\n100\n\n\n202212363\n65\n90\n\n\n202212488\n55\n80\n\n\n202212312\n80\n30\n\n\n202212377\n75\n40\n\n\n202212463\n65\n45\n\n\n202212471\n60\n60\n\n\n202212400\n95\n65\n\n\n202212469\n90\n80\n\n\n202212318\n55\n75\n\n\n202212432\n95\n95\n\n\n202212443\n95\n55\n\n\n202212367\n50\n80\n\n\n202212458\n50\n55\n\n\n202212396\n95\n30\n\n\n202212482\n50\n50\n\n\n202212452\n65\n55\n\n\n202212387\n70\n70\n\n\n202212354\n90\n90\n\n\n\n\n\n\n\n\ndf.iloc[:,::2]  # 스트라이딩\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n- 방법3: ndarray와 dict의 혼합형\n\ndf.loc[:,['att','mid']] \n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,'att':'mid']  # 마지막의 mid도 포함된다. \n\n\n\n\n\n\n\n\natt\nrep\nmid\n\n\n\n\n202212380\n65\n55\n50\n\n\n202212370\n95\n100\n50\n\n\n202212363\n65\n90\n60\n\n\n202212488\n55\n80\n75\n\n\n202212312\n80\n30\n30\n\n\n202212377\n75\n40\n100\n\n\n202212463\n65\n45\n45\n\n\n202212471\n60\n60\n25\n\n\n202212400\n95\n65\n20\n\n\n202212469\n90\n80\n80\n\n\n202212318\n55\n75\n35\n\n\n202212432\n95\n95\n45\n\n\n202212443\n95\n55\n15\n\n\n202212367\n50\n80\n40\n\n\n202212458\n50\n55\n15\n\n\n202212396\n95\n30\n30\n\n\n202212482\n50\n50\n45\n\n\n202212452\n65\n55\n15\n\n\n202212387\n70\n70\n40\n\n\n202212354\n90\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,'rep':] \n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212370\n100\n50\n80\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212312\n30\n30\n100\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212400\n65\n20\n10\n\n\n202212469\n80\n80\n20\n\n\n202212318\n75\n35\n25\n\n\n202212432\n95\n45\n0\n\n\n202212443\n55\n15\n35\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212396\n30\n30\n95\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n202212354\n90\n80\n90\n\n\n\n\n\n\n\n- 방법4: bool을 이용한 인덱싱\n\ndf.iloc[:,[True,False,True,False]]\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\ndf.loc[:,[True,False,True,False]]\n\n\n\n\n\n\n\n\natt\nmid\n\n\n\n\n202212380\n65\n50\n\n\n202212370\n95\n50\n\n\n202212363\n65\n60\n\n\n202212488\n55\n75\n\n\n202212312\n80\n30\n\n\n202212377\n75\n100\n\n\n202212463\n65\n45\n\n\n202212471\n60\n25\n\n\n202212400\n95\n20\n\n\n202212469\n90\n80\n\n\n202212318\n55\n35\n\n\n202212432\n95\n45\n\n\n202212443\n95\n15\n\n\n202212367\n50\n40\n\n\n202212458\n50\n15\n\n\n202212396\n95\n30\n\n\n202212482\n50\n45\n\n\n202212452\n65\n15\n\n\n202212387\n70\n40\n\n\n202212354\n90\n80\n\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.iloc[0]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.iloc[0])\n\npandas.core.series.Series\n\n\n- 방법2\n\ndf.iloc[[0]]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.iloc[[0]])\n\npandas.core.frame.DataFrame\n\n\n- 방법3\n\ndf.iloc[0,:]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.iloc[0,:])\n\npandas.core.series.Series\n\n\n- 방법4\n\ndf.iloc[[0],:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.iloc[[0],:])\n\npandas.core.frame.DataFrame\n\n\n- 방법5\n\ndf.loc['202212380']\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.loc['202212380'])\n\npandas.core.series.Series\n\n\n- 방법6\n\ndf.loc[['202212380']]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.loc[['202212380']])\n\npandas.core.frame.DataFrame\n\n\n- 방법7\n\ndf.loc['202212380',:]\n\natt    65\nrep    55\nmid    50\nfin    40\nName: 202212380, dtype: int64\n\n\n\ntype(df.loc['202212380',:])\n\npandas.core.series.Series\n\n\n- 방법8\n\ndf.loc[['202212380'],:]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ntype(df.loc[['202212380'],:])\n\npandas.core.frame.DataFrame\n\n\n- 방법9\n\nlen(df)\n\n20\n\n\n\n_lst = [True]+[False]*19\n\n\ndf.iloc[_lst] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.iloc[_lst,:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.loc[_lst] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\ndf.loc[_lst,:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n\n\n\n\n\n\n\n\n- 방법1\n\ndf.iloc[[0,2]]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n\ndf.iloc[[0,2],:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n- 방법2\n\ndf.loc[['202212380','202212363']] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n\ndf.loc[['202212380','202212363'],:] \n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212363\n65\n90\n60\n30\n\n\n\n\n\n\n\n- 그 밖의 방법들\n\ndf.iloc[::3] # 스트라이딩\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212488\n55\n80\n75\n80\n\n\n202212463\n65\n45\n45\n90\n\n\n202212469\n90\n80\n80\n20\n\n\n202212443\n95\n55\n15\n35\n\n\n202212396\n95\n30\n30\n95\n\n\n202212387\n70\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.iloc[:5]\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ndf.loc[:'202212312']\n\n\n\n\n\n\n\n\natt\nrep\nmid\nfin\n\n\n\n\n202212380\n65\n55\n50\n40\n\n\n202212370\n95\n100\n50\n80\n\n\n202212363\n65\n90\n60\n30\n\n\n202212488\n55\n80\n75\n80\n\n\n202212312\n80\n30\n30\n100\n\n\n\n\n\n\n\n\ndf.loc[list(df.att&lt;80),'rep':]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.loc[df.att&lt;80,'rep':]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n\ndf.iloc[list(df.att&lt;80),1:]\n\n\n\n\n\n\n\n\nrep\nmid\nfin\n\n\n\n\n202212380\n55\n50\n40\n\n\n202212363\n90\n60\n30\n\n\n202212488\n80\n75\n80\n\n\n202212377\n40\n100\n15\n\n\n202212463\n45\n45\n90\n\n\n202212471\n60\n25\n0\n\n\n202212318\n75\n35\n25\n\n\n202212367\n80\n40\n30\n\n\n202212458\n55\n15\n85\n\n\n202212482\n50\n45\n10\n\n\n202212452\n55\n15\n45\n\n\n202212387\n70\n40\n35\n\n\n\n\n\n\n\n- 아래는 에러가 난다 주의!\n\ndf.iloc[df.att&lt;80, 1:]\n\nValueError: Location based indexing can only have [integer, integer slice (START point is INCLUDED, END point is EXCLUDED), listlike of integers, boolean array] types"
  },
  {
    "objectID": "6_note.html",
    "href": "6_note.html",
    "title": "NOTE",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 13, 2023\n\n\nAutoGluon TS\n\n\nJiyunLim \n\n\n\n\nSep 9, 2023\n\n\n연습장2\n\n\nJiyunLim \n\n\n\n\nSep 3, 2023\n\n\n연습장1\n\n\nJiyunLim \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "2023 study blog",
    "section": "",
    "text": "Transformers for Timeseries\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n2wk-2\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\nAutoGluon TS\n\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n2wk-2\n\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n02wk-007: 타이타닉, Autogluon (Fsize,Drop)\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-006: 타이타닉, Autogluon (Fsize)\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] Quarto Blog 만들기\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-003: 타이타닉, 첫 제출\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-008: 타이타닉, Autogluon (best_quality)\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-004: 타이타닉, Alexis Cook의 코드\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-005: 타이타닉, Autogluon\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n02wk-008: 타이타닉, Autogluon (best_quality)\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n2wk\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[DL] 2wk. Applied Math and Machine Learning Basics\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n2wk\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n학습 파이프라인\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n타이타닉 튜토리얼\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n연습장2\n\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n01wk-002: 타이타닉, 데이터의 이해\n\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\ntitanic\n\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n연습장1\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Transformere (Stock Price)\n\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\nAttention is all you need\n\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 12wk. CNN / 모형성능 향상을 위한 노력들\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[Fourier] 퓨리에변환(detailed)\n\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[Fourier] 푸리에변환 코드실습 (블로그)\n\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[Fourier] 퓨리에변환4jy\n\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\n신록예찬\n\n\n\n\n\n\n  \n\n\n\n\n13wk-1: 깊은복사와 얕은복사\n\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n[수리통계학] 추정 for JY\n\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\n신록예찬\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 11wk. MaxPool2D, Conv2D\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 10wk. Softmax / 다양한 평가지표\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 9wk-2. 경사하강법 / 확률적경사하강법\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 9wk. Likelihood function\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 중간고사\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 7wk. Piece-wise LR / Logistic Regression\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 6wk. 회귀모형 적합 with keras\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 5wk. optimizer를 이용한 최적화\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 4wk. 미분 / 경사하강법\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 3wk. 텐서플로우 intro2 (tf.GradientTape())\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 2wk. 텐서플로우 intro1 (tf.constant선언, tnp사용법)\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n[STBDA] 1wk. 강의소개 및 단순선형회귀\n\n\n\n\n\n\n\n빅데이터분석특강\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n07wk-2 아이스크림을 많이 먹으면 걸리는 병(2)\n\n\n\n\n\n\n\n통계와 시각화\n\n\nplotnine\n\n\n아이스크림을 많이 먹으면 걸리는 병\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2023\n\n\njiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n10wk-2 심슨의 역설\n\n\n\n\n\n\n\n통계와 시각화\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\njiyunLim\n\n\n\n\n\n\n  \n\n\n\n\nPandas 2단계\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nPandas 1단계\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nPandas 0단계\n\n\n\n\n\n\n\nPython\n\n\nPandas\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\n2022 final exam\n\n\n\n\n\n\n\npython\n\n\nclass\n\n\ntest\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nclass 6단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nclass 9단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nclass 8단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nclass 10단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nclass 7단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nclass 5단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nNumpy 4단계(concat, stack)\n\n\n\n\n\n\n\nPython\n\n\nNumpy\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nsimultaneous equation\n\n\n\n\n\n\n\nR\n\n\nlinear algebra\n\n\nbasic\n\n\n\n\nimplementation with R\n\n\n\n\n\n\nFeb 19, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nclass 3단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nclass 1단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nclass 2단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nclass 4단계\n\n\n\n\n\n\n\nclass\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\njiyun Lim\n\n\n\n\n\n\n  \n\n\n\n\nJupyter\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\nJiyunLim\n\n\n\n\n\n\n  \n\n\n\n\n05wk-2\n\n\n\n\n\n\n\n훌륭한 시각화\n\n\nplotnine\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2022\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n05wk-1\n\n\n\n\n\n\n\nseaborn\n\n\nmatplotlib\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\n최규빈\n\n\n\n\n\n\n  \n\n\n\n\n2021 final exam solution\n\n\n\n\n\n\n\npython\n\n\nclass\n\n\ntest\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2022\n\n\nGuebinChoi\n\n\n\n\n\n\n  \n\n\n\n\n07wk-1 [Pandas] 새로운 열 할당, 아이스크림을 많이 먹으면 걸리는 병(1)\n\n\n\n\n\n\n\npandas\n\n\n통계와 시각화\n\n\n아이스크림을 많이 먹으면 걸리는 병\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2022\n\n\nJiyunLim\n\n\n\n\n\n\nNo matching items"
  }
]