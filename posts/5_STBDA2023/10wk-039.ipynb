{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f5e8aeea-f8f9-4af2-9636-8f4f84e87e23",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"10wk-039: 의사결정나무 Discussion\"\n",
    "author: \"JiyunLim\"\n",
    "date: \"11/10/2023\"\n",
    "bibliography: ref.bib\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e231c0-beed-4b0b-af73-333ea8915dec",
   "metadata": {},
   "source": [
    "# 1. 강의영상 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d3b50-e75f-4b8c-adb9-a0b0dc53ee3b",
   "metadata": {},
   "source": [
    "{{<video https://youtu.be/playlist?list=PLQqh36zP38-y7ZQE5CtHiEraKV2eZslti&si=QYNee59zfsGgXL_X >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa9ae49-2da0-4ba9-ab33-9fa42e8008e0",
   "metadata": {},
   "source": [
    "# 2. 의사결정나무 Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2a5f1-eb51-47e9-b19b-f070ab3722cb",
   "metadata": {},
   "source": [
    "`-` 의사결정나무 vs 선형모형\n",
    "\n",
    "1. 아이스크림+축제: 이상치에 강했음. \n",
    "2. 운동+보조제: 교호작용을 고려하지 않아도 괜찮았음. \n",
    "3. 토익유사점수: 다중공선성문제가 발생하는 경우에도 모형이 덜 망함. \n",
    "4. 밸런스게임: 필요없는 변수가 있을 경우에도 모형이 덜 망함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d42777f-61bd-4ba4-827d-7901abad8fbf",
   "metadata": {},
   "source": [
    "`-` 의사결정나무의 장점들 \n",
    "\n",
    "1. 시각화가 유리하다. 설명력이 좋다. \n",
    "2. 특성(feature)의 중요도를 파악하기 용이하다. \n",
    "3. ${\\bf y} \\sim {\\bf X}$ 사이에 존재하는 비선형성을 쉽게 모델링 할 수 있다. $\\to$ 쉽게 말해서 잘 맞춘다는 소리에요\n",
    "4. 모형에 대한 가정들이 필요 없다. (넌파라메트릭 모형 특징)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f317f5c-b033-4850-a4df-1a81186f7bea",
   "metadata": {},
   "source": [
    "`-` 의사결정나무의 단점: 오버피팅이 일어나기 너무 쉽다. (모형이 너무 흔들려..)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22eee56-7448-49ad-8637-a4be90dab148",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 의사결정나무에 대한 자잘한 개념들 (자격증에서 잘 물어봄) \n",
    "\n",
    "**최소 샘플 분할(Min Samples Split):**\n",
    "\n",
    "  - 노드를 분할하기 위한 최소 샘플 수.\n",
    "  - 적절한 설정으로 과소적합 및 과적합 조절 가능.\n",
    "\n",
    "**가지치기(Pruning):**\n",
    "\n",
    "  - 트리의 불필요한 부분을 제거.\n",
    "  - 과적합 방지 및 모델 성능 향상에 도움.\n",
    "\n",
    "**정보 이득(Information Gain):**\n",
    "\n",
    "  - 분할 전후의 엔트로피 차이를 측정.\n",
    "  - 높은 정보 이득은 더 좋은 분할을 의미.\n",
    "  \n",
    "**지니 불순도(Gini Impurity):**\n",
    "\n",
    "  - 노드의 순도 측정 지표.\n",
    "  - 낮은 지니 불순도는 높은 클래스 순도를 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f66bf-ea2e-4114-806d-d5e1197a29ec",
   "metadata": {},
   "source": [
    "> 결국 \"트리를 어디까지 성장시킬래?\"라는 물음에 대답하기 위해 고안된 개념들이다. 근본적으로 \"트리를 어디까지 성장시킬래?\"에 대한 이론적인 명확한 기술은 없다. 이는 넌파라메트릭 모형이 가지는 공통적인 특징임. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d4cd7-3d85-4c4b-8f5c-a721d79d25a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "`-` 의사결정나무는 오버피팅을 잡기위해서 지루한 싸움을 시작함. \n",
    "\n",
    "- 발전과정: 의사결정나무 $\\to$ 배깅, 랜덤포레스트, 부스팅\n",
    "- 의사결정나무를 응용한 다양한 방법들이 개발되었다. (너무 많아요 진짜) $\\to$ 모든 방법들의 원리를 세세하게 파헤치는건 비효율적이다. \n",
    "- 그러한 다양한 방법들을 적덩히 분류해보면 대체로 배깅, 랜덤포레스트, 부스팅 계열로 나뉜다.^[모든 방법들이 세개의 카테고리중 하나에만 들어가는건 아니다] $\\to$ 배깅, 랜덤포레스트, 부스팅에 대한 공통적 아이디어를 파악하는건 효율적이다. \n",
    "- 현재 최고로 (state of the art, SOTA) 로 평가받는 알고리즘은 부스팅계열의 `XGBoost`, `LightGBM`, `CatBoost` 이다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
