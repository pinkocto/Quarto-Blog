{"title":"**[STBDA]** 1wk. 강의소개 및 단순선형회귀","markdown":{"yaml":{"title":"**[STBDA]** 1wk. 강의소개 및 단순선형회귀","author":"JiyunLim","date":"05/08/2023","categories":["빅데이터분석특강"]},"headingText":"(1주차) 3월7일","containsRefs":false,"markdown":"\n\n\n### 강의영상 \n\n> youtube: https://youtube.com/playlist?list=PLQqh36zP38-yKGpQh49tnRrA-o2Odea8r\n\n### 강의보충자료 \n\n`-` https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp1.pdf\n\n`-` https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp2.pdf\n\n### 로드맵 \n\n`-` 오늘수업할내용: 단순선형회귀 \n\n`-` 단순선형회귀를 배우는 이유? \n\n- 우리가 배우고싶은것: 심층신경망(DNN) $\\to$ 합성곱신경망(CNN) $\\to$ 적대적생성신경망(GAN) \n- 심층신경망을 바로 이해하기 어려움 \n- 다음의 과정으로 이해해야함: (선형대수학 $\\to$) 회귀분석 $\\to$ 로지스틱회귀분석 $\\to$ 심층신경망 \n\n### 선형회귀 \n\n`-` 상황극\n- 나는 동네에 커피점을 하나 차렸음. \n- 장사를 하다보니까 날이 더울수록 아이스아메리카노의 판매량이 증가한다는 사실을 깨달았다. \n- 일기예보는 미리 나와있으니까 그 정보를 잘 이용하면 '온도 -> 아이스아메리카노 판매량 예측' 이 가능할것 같다. (내가 앞으로 얼마나 벌지 예측가능) \n\n`-` 가짜자료 생성 \n\n온도 ${\\bf x}$가 아래와 같다고 하자. \n\n아이스아메리카노의 판매량 ${\\bf y}$이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자) \n\n$${\\bf y} \\approx 10.2 +2.2 {\\bf x}$$ \n\n- 여기에서 10.2, 2.2 의 숫자는 제가 임의로 정한것임 \n- 식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림 \n- 물결의의미: 현실반영. 세상은 꼭 수식대로 정확하게 이루어지지 않음. \n\n`-` 우리는 아래와 같은 자료를 모았다고 생각하자.  \n\n`-` 그려보자. \n\n`-` 우리의 목표: 파란색점 $\\to$ 주황색점선을 추론 // 데이터를 바탕으로 세상의 법칙을 추론 \n\n`-` 아이디어: 데이터를 보니까 $x$와 $y$가 선형의 관계에 있는듯 보인다. 즉 모든 $i=1,2,\\dots, 10$에 대하여 아래를 만족하는 적당한 a,b (혹은 $\\beta_0,\\beta_1$) 가 존재할것 같다. \n- $y_{i} \\approx ax_{i}+b$\n- $y_{i} \\approx \\beta_1 x_{i}+\\beta_0$\n\n`-` 어림짐작으로 $a,b$를 알아내보자. \n\n데이터를 살펴보자. \n\n적당히 `왼쪽*2+15 = 오른쪽`의 관계가 성립하는것 같다. \n\n따라서 $a=2, b=15$ 혹은 $\\beta_0=15, \\beta_1=2$ 로 추론할 수 있겠다. \n\n`-` 누군가가 $(\\beta_0,\\beta_1)=(14,2)$ 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까) \n\n`-` 새로운 주장으로 인해서 $(\\beta_0,\\beta_1)=(15,2)$ 로 볼 수도 있고 $(\\beta_0,\\beta_1)=(14,2)$ 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? \n- 후보1: $(\\beta_0,\\beta_1)=(15,2)$ \n- 후보2: $(\\beta_0,\\beta_1)=(14,2)$\n\n`-` 가능한 $y_i \\approx \\beta_0 + \\beta_1 x_i$ 이 되도록 만드는 $(\\beta_0,\\beta_1)$ 이 좋을 것이다. $\\to$ 후보 1,2를 비교해보자. \n\n(관찰에 의한 비교) \n\n후보1에 대해서 $i=1,2$를 넣고 관찰하여 보자. \n\n후보2에 대하여 $i=1,2$를 넣고 관찰하여 보자. \n\n$i=1$인 경우에는 후보1이 더 잘맞는것 같은데 $i=2$인 경우는 후보2가 더 잘맞는것 같다. \n\n(좀 더 체계적인 비교) \n\n$i=1,2,3, \\dots, 10$ 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자. \n\n후보 1,2에 대하여 $\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2$를 계산하여 비교해보자. \n\n후보1이 더 $\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2$의 값이 작다. \n\n후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다. \n\n`-` 그런데 이 알고리즘은 현실적으로 구현이 불가능하다. (무한번 계산하기도 힘들고, 언제 멈출지도 애매함) \n\n`-` 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 $\\beta_0,\\beta_1$을 찾으면 된다. \n\n$\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2$\n\n그런데 결국 $\\beta_0, \\beta_1$에 대한 이차식인데 이 식을 최소화하는 $\\beta_0,\\beta_1$을 구하기 위해서는 아래를 연립하여 풀면된다. \n\n$\\begin{cases}\n\\frac{\\partial}{\\partial \\beta_0}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0 \\\\ \n\\frac{\\partial}{\\partial \\beta_1}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0\n\\end{cases}$\n\n`-` 풀어보자. \n\n$\\begin{cases}\n\\sum_{i=1}^{10} -2(y_i -\\beta_0 -\\beta_1 x_i)=0 \\\\ \n\\sum_{i=1}^{10} -2x_i(y_i -\\beta_0 -\\beta_1 x_i)=0\n\\end{cases}$\n\n정리하면 \n\n$$\\hat{\\beta}_0= \\bar{y}-\\hat{\\beta}_1 \\bar{x}$$\n\n$$\\hat{\\beta}_1= \\frac{S_{xy}}{S_{xx}}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}$$\n\n`-` 따라서 최적의 추정치 $(\\hat{\\beta}_0,\\hat{\\beta}_1)$를 이용한 추세선을 아래와 같이 계산할 수 있음.\n\n> Note: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다. \n\n`-` 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다. \n\n(1) 공식이 좀 복잡함.. \n\n(2) $x$가 여러개일 경우 확장이 어려움 \n\n`-` 단점을 극복하기 위해서 우리가 지금까지 했던논의를 매트릭스로 바꾸어서 다시 써보자. \n\n`-` 모형의 매트릭스화 \n\n우리의 모형은 아래와 같다. \n\n$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10$ \n\n풀어서 쓰면 \n\n$\\begin{cases}\ny_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ \ny_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \n\\dots \\\\ \ny_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \n\\end{cases}$\n\n아래와 같이 쓸 수 있다.\n\n$\\begin{bmatrix} \ny_1 \\\\ \ny_2 \\\\ \n\\dots \\\\\ny_{10} \n\\end{bmatrix} \n= \\begin{bmatrix} \n1 & x_1 \\\\ \n1 & x_2 \\\\ \n\\dots & \\dots \\\\\n1 & x_{10} \n\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} \n\\epsilon_1 \\\\ \n\\epsilon_2 \\\\ \n\\dots \\\\\n\\epsilon_{10} \n\\end{bmatrix} $\n\n벡터와 매트릭스 형태로 정리하면 \n\n${\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}$ \n\n`-` 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다. \n\n$loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2$\n\n이것을 벡터표현으로 하면 아래와 같다. \n\n$loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})$\n\n풀어보면 \n\n$loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}$\n\n`-` 미분하는 과정의 매트릭스화 \n\nloss를 최소화하는 ${\\boldsymbol \\beta}$를 구해야하므로 loss를 ${\\boldsymbol \\beta}$로 미분한식을 0이라고 놓고 풀면 된다. \n\n$\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}$ \n\n$= 0 - {\\bf X}^\\top {\\bf y}- {\\bf X}^\\top {\\bf y} + 2{\\bf X}^\\top {\\bf X}{\\boldsymbol\\beta} $\n\n따라서 $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0$을 풀면 아래와 같다. \n\n$\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y} $\n\n`-` 공식도 매트릭스로 표현하면: $\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y} $ <-- 외우세요 \n\n`-` 적용을 해보자. \n\n(X를 만드는 방법1) \n\n(X를 만드는 방법2)\n\n`-` 잘 구해진다. \n\n`-` 그런데.. \n\n값이 좀 다르다..?\n\n`-` 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다. \n\n### 앞으로 할것 \n\n`-` 선형대수학의 미분이론.. \n\n`-` 실습 (tensorflow에서 매트릭스를 자유롭게 다루기!) \n","srcMarkdownNoYaml":"\n\n# (1주차) 3월7일 \n\n### 강의영상 \n\n> youtube: https://youtube.com/playlist?list=PLQqh36zP38-yKGpQh49tnRrA-o2Odea8r\n\n### 강의보충자료 \n\n`-` https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp1.pdf\n\n`-` https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp2.pdf\n\n### 로드맵 \n\n`-` 오늘수업할내용: 단순선형회귀 \n\n`-` 단순선형회귀를 배우는 이유? \n\n- 우리가 배우고싶은것: 심층신경망(DNN) $\\to$ 합성곱신경망(CNN) $\\to$ 적대적생성신경망(GAN) \n- 심층신경망을 바로 이해하기 어려움 \n- 다음의 과정으로 이해해야함: (선형대수학 $\\to$) 회귀분석 $\\to$ 로지스틱회귀분석 $\\to$ 심층신경망 \n\n### 선형회귀 \n\n`-` 상황극\n- 나는 동네에 커피점을 하나 차렸음. \n- 장사를 하다보니까 날이 더울수록 아이스아메리카노의 판매량이 증가한다는 사실을 깨달았다. \n- 일기예보는 미리 나와있으니까 그 정보를 잘 이용하면 '온도 -> 아이스아메리카노 판매량 예측' 이 가능할것 같다. (내가 앞으로 얼마나 벌지 예측가능) \n\n`-` 가짜자료 생성 \n\n온도 ${\\bf x}$가 아래와 같다고 하자. \n\n아이스아메리카노의 판매량 ${\\bf y}$이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자) \n\n$${\\bf y} \\approx 10.2 +2.2 {\\bf x}$$ \n\n- 여기에서 10.2, 2.2 의 숫자는 제가 임의로 정한것임 \n- 식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림 \n- 물결의의미: 현실반영. 세상은 꼭 수식대로 정확하게 이루어지지 않음. \n\n`-` 우리는 아래와 같은 자료를 모았다고 생각하자.  \n\n`-` 그려보자. \n\n`-` 우리의 목표: 파란색점 $\\to$ 주황색점선을 추론 // 데이터를 바탕으로 세상의 법칙을 추론 \n\n`-` 아이디어: 데이터를 보니까 $x$와 $y$가 선형의 관계에 있는듯 보인다. 즉 모든 $i=1,2,\\dots, 10$에 대하여 아래를 만족하는 적당한 a,b (혹은 $\\beta_0,\\beta_1$) 가 존재할것 같다. \n- $y_{i} \\approx ax_{i}+b$\n- $y_{i} \\approx \\beta_1 x_{i}+\\beta_0$\n\n`-` 어림짐작으로 $a,b$를 알아내보자. \n\n데이터를 살펴보자. \n\n적당히 `왼쪽*2+15 = 오른쪽`의 관계가 성립하는것 같다. \n\n따라서 $a=2, b=15$ 혹은 $\\beta_0=15, \\beta_1=2$ 로 추론할 수 있겠다. \n\n`-` 누군가가 $(\\beta_0,\\beta_1)=(14,2)$ 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까) \n\n`-` 새로운 주장으로 인해서 $(\\beta_0,\\beta_1)=(15,2)$ 로 볼 수도 있고 $(\\beta_0,\\beta_1)=(14,2)$ 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? \n- 후보1: $(\\beta_0,\\beta_1)=(15,2)$ \n- 후보2: $(\\beta_0,\\beta_1)=(14,2)$\n\n`-` 가능한 $y_i \\approx \\beta_0 + \\beta_1 x_i$ 이 되도록 만드는 $(\\beta_0,\\beta_1)$ 이 좋을 것이다. $\\to$ 후보 1,2를 비교해보자. \n\n(관찰에 의한 비교) \n\n후보1에 대해서 $i=1,2$를 넣고 관찰하여 보자. \n\n후보2에 대하여 $i=1,2$를 넣고 관찰하여 보자. \n\n$i=1$인 경우에는 후보1이 더 잘맞는것 같은데 $i=2$인 경우는 후보2가 더 잘맞는것 같다. \n\n(좀 더 체계적인 비교) \n\n$i=1,2,3, \\dots, 10$ 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자. \n\n후보 1,2에 대하여 $\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2$를 계산하여 비교해보자. \n\n후보1이 더 $\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2$의 값이 작다. \n\n후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다. \n\n`-` 그런데 이 알고리즘은 현실적으로 구현이 불가능하다. (무한번 계산하기도 힘들고, 언제 멈출지도 애매함) \n\n`-` 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 $\\beta_0,\\beta_1$을 찾으면 된다. \n\n$\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2$\n\n그런데 결국 $\\beta_0, \\beta_1$에 대한 이차식인데 이 식을 최소화하는 $\\beta_0,\\beta_1$을 구하기 위해서는 아래를 연립하여 풀면된다. \n\n$\\begin{cases}\n\\frac{\\partial}{\\partial \\beta_0}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0 \\\\ \n\\frac{\\partial}{\\partial \\beta_1}\\sum_{i=1}^{10} (y_i -\\beta_0 -\\beta_1 x_i)^2=0\n\\end{cases}$\n\n`-` 풀어보자. \n\n$\\begin{cases}\n\\sum_{i=1}^{10} -2(y_i -\\beta_0 -\\beta_1 x_i)=0 \\\\ \n\\sum_{i=1}^{10} -2x_i(y_i -\\beta_0 -\\beta_1 x_i)=0\n\\end{cases}$\n\n정리하면 \n\n$$\\hat{\\beta}_0= \\bar{y}-\\hat{\\beta}_1 \\bar{x}$$\n\n$$\\hat{\\beta}_1= \\frac{S_{xy}}{S_{xx}}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}$$\n\n`-` 따라서 최적의 추정치 $(\\hat{\\beta}_0,\\hat{\\beta}_1)$를 이용한 추세선을 아래와 같이 계산할 수 있음.\n\n> Note: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다. \n\n`-` 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다. \n\n(1) 공식이 좀 복잡함.. \n\n(2) $x$가 여러개일 경우 확장이 어려움 \n\n`-` 단점을 극복하기 위해서 우리가 지금까지 했던논의를 매트릭스로 바꾸어서 다시 써보자. \n\n`-` 모형의 매트릭스화 \n\n우리의 모형은 아래와 같다. \n\n$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10$ \n\n풀어서 쓰면 \n\n$\\begin{cases}\ny_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ \ny_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \n\\dots \\\\ \ny_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \n\\end{cases}$\n\n아래와 같이 쓸 수 있다.\n\n$\\begin{bmatrix} \ny_1 \\\\ \ny_2 \\\\ \n\\dots \\\\\ny_{10} \n\\end{bmatrix} \n= \\begin{bmatrix} \n1 & x_1 \\\\ \n1 & x_2 \\\\ \n\\dots & \\dots \\\\\n1 & x_{10} \n\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix} \n\\epsilon_1 \\\\ \n\\epsilon_2 \\\\ \n\\dots \\\\\n\\epsilon_{10} \n\\end{bmatrix} $\n\n벡터와 매트릭스 형태로 정리하면 \n\n${\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}$ \n\n`-` 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다. \n\n$loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2$\n\n이것을 벡터표현으로 하면 아래와 같다. \n\n$loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})$\n\n풀어보면 \n\n$loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}$\n\n`-` 미분하는 과정의 매트릭스화 \n\nloss를 최소화하는 ${\\boldsymbol \\beta}$를 구해야하므로 loss를 ${\\boldsymbol \\beta}$로 미분한식을 0이라고 놓고 풀면 된다. \n\n$\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}$ \n\n$= 0 - {\\bf X}^\\top {\\bf y}- {\\bf X}^\\top {\\bf y} + 2{\\bf X}^\\top {\\bf X}{\\boldsymbol\\beta} $\n\n따라서 $\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0$을 풀면 아래와 같다. \n\n$\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y} $\n\n`-` 공식도 매트릭스로 표현하면: $\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y} $ <-- 외우세요 \n\n`-` 적용을 해보자. \n\n(X를 만드는 방법1) \n\n(X를 만드는 방법2)\n\n`-` 잘 구해진다. \n\n`-` 그런데.. \n\n값이 좀 다르다..?\n\n`-` 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다. \n\n### 앞으로 할것 \n\n`-` 선형대수학의 미분이론.. \n\n`-` 실습 (tensorflow에서 매트릭스를 자유롭게 다루기!) \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022_03_07_(1주차)_3월7일.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.315","theme":"cosmo","code-copy":true,"title-block-banner":true,"title":"**[STBDA]** 1wk. 강의소개 및 단순선형회귀","author":"JiyunLim","date":"05/08/2023","categories":["빅데이터분석특강"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}