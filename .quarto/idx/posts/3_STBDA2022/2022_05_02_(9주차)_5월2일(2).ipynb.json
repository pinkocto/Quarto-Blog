{"title":"**[STBDA]** 9wk-2. 경사하강법 / 확률적경사하강법","markdown":{"yaml":{"title":"**[STBDA]** 9wk-2. 경사하강법 / 확률적경사하강법 ","author":"JiyunLim","date":"05/26/2023","categories":["빅데이터분석특강"]},"headingText":"강의영상","containsRefs":false,"markdown":"\n\n\n> youtube: https://youtube.com/playlist?list=PLQqh36zP38-wvV9xuYHvx0Gn7KDGNJbwj\n\n## import\n\n### <font color='red'>basis가 orthogonal하냐?</font>\n\n<font color='red'>overfitting 이슈는 변수가 많다고해서 무조건 발생하는 것은 아님! 변수가 많이 있어도 orthogonal하게 잘 넣으면 심지어 무한대의 basis를 갖고있어도 overfitting이슈가 발생하지 않는다. 이렇게 맞추는 것을 semi-parametric modeling 이라고 한다!</font>\n\n- ex. 직선의 basis: 절편과 기울기\n\n## 중간고사 관련 잡담\n\n### 중간고사 3번문제\n\n`-` 특이한모형: 오버핏이 일어날 수 없는 모형이다.\n- 유의미한 coef: 상수항(bias), $\\cos(t)$의 계수, $\\cos(2t)$의 계수, $\\cos(5t)$의 계수.\n- 유의미하지 않은 coef: $\\cos(3t)$의 계수, $\\cos(4t)$의 계수\n- 유의미하지 않은 계수는 $n%$이 커질수록 0으로 추정된다 =  $\\cos(3t)$와 $\\cos(5t)$는 사용자가 임의로 제외하지 않아도 결국 모형에서 알아서 제거된다 = overfit이 일어나지 않는다. 모형이 알아서 유의미한 변수만 뽑아서 fit하는 느낌\n\n`-` 3번문제는 overfit이 일어나지 않는다. 이러한 신기한 일이 일어나는 이유는 모든 설명변수가 직교하기 때문임.\n- 이런 모형의 장점: overfit이 일어날 위험이 없으므로 train/test로 나누어 학습할 이유가 없다. (샘플만 버리는 꼴, test에 빼둔 observation까지 모아서 학습해 $\\beta$를 좀 더 정확히 추론하는게 차라리 더 이득)\n- 이러한 모형에서 할일: 추정된 계수들이 0인지 아닌지만 test하면 된다. (이것을 유의성검정이라고 한다)\n\n`-` 직교기저의 예시\n- 빨강과 파랑을 255,255만큼 섞으면 보라색이 된다.\n- 빨강과 파랑과 노랑을 각각 255,255,255만큼 섞으면 검은색이 된다.\n- 임의의 어떠한 색도 빨강,파랑,노랑의 조합으로 표현가능하다. 즉 $\\text{color}= \\text{red}*\\beta_1 + \\text{blue}*\\beta_2 + \\text{yellow}*\\beta_3$ 이다.\n- (빨,파,노)는 색을 표현하는 basis이다. (적절한 $\\beta_1,\\beta_2,\\beta_3$을 구하기만 하면 임의의 색도 표현가능)\n- (빨,보,노)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요할때 보라색-빨간색을 하면되니까)\n- (빨,보,검)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요하면 보라색-빨간색을 하면되고, 노란색이 필요하면 검정색-보라색을 하면 되니까)\n- (빨,파,노)는 직교기저이다.\n\n`-` 3번에서 알아둘 것: (1) 직교기저의 개념 (추후 재설명) (2) 임의의 색을 표현하려면 3개의 basis가 필요함\n\n### 중간고사 1-(3)번 문제\n\n`-` 그림을 그려보자.\n\n`-` 저것 꼭 10000개 다 모아서 loss계산해야할까?\n\n`-` 대충 이정도만 모아서 해도 비슷하지 않을까? $\\to$ 해보자!\n\n- 주황색만가지고 기울기, 절편 추론을 해보자.\n\n## 경사하강법과 확률적경사하강법\n\n원래 확률적경사하강법이 딥러닝을 하려고 만든것이 아니다.(만들어진 의도와 사용이 다름) 그런데 거기에 맞게 진화를 한 것. 그래서 되게 헷갈린다...\n\n### ver1: 모든 샘플을 사용하여 slope계산 (gradient descent)\n\n`-` 단순회귀분석에서 샘플 10개 관측: $(x_1,y_1),\\dots,(x_{10},y_{10})$.\n\n(epoch1) $loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n(epoch2) $loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n...\n\nfor문 이 3번 돌아감.\n\n### ver2: 하나의 샘플만 사용하여 slope계산\n\n(epoch1)\n- $loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- ...\n- $loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n(epoch2)\n- $loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- ...\n- $loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n...\n\nfor문이 30번 돌아감.\n\n### ver3: $m(\\leq n)$개의 샘플만 사용하여 slope계산 (mini-batch)\n\n$m=3$이라고 하자.\n\n(epoch1)\n- $loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n1,2,3번 observation들만 뽑아서 loss를 계산해서 그것만가지고 update $\\to$ 4,5,6먼만 가지고 loss계산해서 update $\\to$ 7,8,9를가지고 loss구하고 업데이트 $\\to$ 남은 하나가지고 loss구하고 업데이트... // 한 에폭 끝!\n\n\n(epoch2)\n- $loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n...\n\n### 용어의 정리\n\n#### 옛날 (좀 더 엄밀)\n\n`-` ver1: gradient descent, batch gradient descent\n\n\n`-` ver2: stochastic gradient descent\n\n`-` ver3: mini-batch gradient descent, mini-batch stochastic gradient descent\n\n#### 요즘\n\n`-` ver1: gradient descent\n\n\n`-` ver2: stochastic gradient descent with batch size = 1\n\n`-` ver3: stochastic gradient descent\n- https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고.\n\n![](attachment:b26b0736-6f75-4fa0-992b-ab485b5851b2.png)\n\n지금은 mini-batch가 포함된 방법을 stochastic gradient descent라고 부른다. 왜냐하면 1,2는 사장된 방법. 버전 3만 쓴다.(유명한 사람들이 학회에서 그렇게 부르기 시작했음.)\n\nnote: 이렇게 많이 쓰는 이유? ver1,2는 사실상 없는 방법이므로\n\n### ver1,2,3 이외에 좀 더 지저분한 것들이 있다.\n\n`-` ver2,3에서 샘플을 셔플할 수도 있다.\n\n`-` ver3에서 일부 샘플이 학습에 참여 안하는 버전도 있다.\n\n`-` 개인적 생각: 크게3개정도만 알면 괜찮고 나머지는 그렇게 유의미하지 않아보인다.\n\n### Discussion\n\n`-` 핵심개념\n\n\n- **메모리사용량^[한 번 업데이트 할 때 드는 계산량]: ver1 > ver3 > ver2**\n- 계산속도: ver1 > ver3 > ver2\n- local-min에 갇힘: ver1 > ver3 > ver2\n\n로컬미니멈에 갇힌가는 건 로컬미니멈을 잘 찾는다라고 생각 (정신이 제대로 박힌애)\n\n대충대충 학습하면 로컬미니멈에서 딱 멈춰야하는데 대충대충 계산해서 기울기가 딱 $0$이 안나오는 것. (운 좋게 탈출하는 경우도 있음.)\n\n`-` 본질: GPU 메모리가 한정되어 있어서 ver1을 쓰지는 못한다. GPU 메모리를 가장 적게쓰는것은 ver2인데 이것은 너무 불안정하다.\n\n`-` 틀리진 않지만 어색한 블로그 정리 내용들\n\n\n- 경사하강법은 종종 국소최소점에 갇히는 문제가 있다. 이를 해결하기 위해서 등장한 방법이 확률적 경사하강법이다. --> 영 틀린말은 아니지만 그걸 의도하고 만든건 아님 (이건 side effect)\n\n- 경사하강법은 계산시간이 오래걸린다. 계산을 빠르게 하기 위해서 등장한 방법이 확률적 경사하강법이다. --> 1회 업데이트는 빠르게 계산함. 하지만 그것이 최적의 $\\beta$를 빠르게 얻을 수 있다는 의미는 아님\n\n\n<font color='blue'>원래 경사하강법은 local minimum에 빠지기 쉬운 알고리즘! 그런데 그나마 둘 중에 비교를 하자면 확률적으로 하면 로컬 미니멈에 빠졌다가 어쩌다 운좋아서 튀어 나가는 경우가 있다.</font>\n\n\n동일한 컴퓨터 자원으로 수렴을 더 빨리 시킬 수 있냐? 그건 아님 (그건 모름).\n\n그럼 왜 쓰냐???\n\n메모리 사용량만 보면 됩니다!\n\n## fashion_mnist 모듈\n\n### tf.keras.datasets.fashion_mnist.load_data()\n\n`-` tf.keras.datasets.fashion_mnist.load_data 의 리턴값 조사\n\n### 데이터생성 및 탐색\n\n`-` tf.keras.datasets.fashion_mnist.load_data()를 이용한 데이터 생성\n\n`-` 차원확인\n\n- 60000은 obs숫자인듯\n- (28,28)은 28픽셀,28픽셀을 의미하는듯\n- train/test는 6:1로 나눈것 같음\n\n`-` 첫번째 obs\n\n- 첫번쨰 obs에 대응하는 라벨\n\n`-` 첫번째 obs와 동일한 라벨을 가지는 그림을 찾아보자.\n\n### 데이터구조\n\n`-` ${\\bf X}$: (n,28,28)\n\n`-` ${\\bf y}$: (n,) , $y=0,1,2,3,\\dots,9$\n\n## 예제1\n\n### 데이터 정리\n\n`-` y=0,1에 대응하는 이미지만 정리하자. (우리가 배운건 로지스틱이니까)\n\n### 풀이1: 은닉층을 포함한 신경망  // epochs=100\n\n### 풀이2: 옵티마이저 개선\n\n### 풀이3: 컴파일시 metrics=['accuracy'] 추가\n\n### 풀이4: 확률적경사하강법 이용 // epochs=10\n","srcMarkdownNoYaml":"\n\n## 강의영상\n\n> youtube: https://youtube.com/playlist?list=PLQqh36zP38-wvV9xuYHvx0Gn7KDGNJbwj\n\n## import\n\n### <font color='red'>basis가 orthogonal하냐?</font>\n\n<font color='red'>overfitting 이슈는 변수가 많다고해서 무조건 발생하는 것은 아님! 변수가 많이 있어도 orthogonal하게 잘 넣으면 심지어 무한대의 basis를 갖고있어도 overfitting이슈가 발생하지 않는다. 이렇게 맞추는 것을 semi-parametric modeling 이라고 한다!</font>\n\n- ex. 직선의 basis: 절편과 기울기\n\n## 중간고사 관련 잡담\n\n### 중간고사 3번문제\n\n`-` 특이한모형: 오버핏이 일어날 수 없는 모형이다.\n- 유의미한 coef: 상수항(bias), $\\cos(t)$의 계수, $\\cos(2t)$의 계수, $\\cos(5t)$의 계수.\n- 유의미하지 않은 coef: $\\cos(3t)$의 계수, $\\cos(4t)$의 계수\n- 유의미하지 않은 계수는 $n%$이 커질수록 0으로 추정된다 =  $\\cos(3t)$와 $\\cos(5t)$는 사용자가 임의로 제외하지 않아도 결국 모형에서 알아서 제거된다 = overfit이 일어나지 않는다. 모형이 알아서 유의미한 변수만 뽑아서 fit하는 느낌\n\n`-` 3번문제는 overfit이 일어나지 않는다. 이러한 신기한 일이 일어나는 이유는 모든 설명변수가 직교하기 때문임.\n- 이런 모형의 장점: overfit이 일어날 위험이 없으므로 train/test로 나누어 학습할 이유가 없다. (샘플만 버리는 꼴, test에 빼둔 observation까지 모아서 학습해 $\\beta$를 좀 더 정확히 추론하는게 차라리 더 이득)\n- 이러한 모형에서 할일: 추정된 계수들이 0인지 아닌지만 test하면 된다. (이것을 유의성검정이라고 한다)\n\n`-` 직교기저의 예시\n- 빨강과 파랑을 255,255만큼 섞으면 보라색이 된다.\n- 빨강과 파랑과 노랑을 각각 255,255,255만큼 섞으면 검은색이 된다.\n- 임의의 어떠한 색도 빨강,파랑,노랑의 조합으로 표현가능하다. 즉 $\\text{color}= \\text{red}*\\beta_1 + \\text{blue}*\\beta_2 + \\text{yellow}*\\beta_3$ 이다.\n- (빨,파,노)는 색을 표현하는 basis이다. (적절한 $\\beta_1,\\beta_2,\\beta_3$을 구하기만 하면 임의의 색도 표현가능)\n- (빨,보,노)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요할때 보라색-빨간색을 하면되니까)\n- (빨,보,검)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요하면 보라색-빨간색을 하면되고, 노란색이 필요하면 검정색-보라색을 하면 되니까)\n- (빨,파,노)는 직교기저이다.\n\n`-` 3번에서 알아둘 것: (1) 직교기저의 개념 (추후 재설명) (2) 임의의 색을 표현하려면 3개의 basis가 필요함\n\n### 중간고사 1-(3)번 문제\n\n`-` 그림을 그려보자.\n\n`-` 저것 꼭 10000개 다 모아서 loss계산해야할까?\n\n`-` 대충 이정도만 모아서 해도 비슷하지 않을까? $\\to$ 해보자!\n\n- 주황색만가지고 기울기, 절편 추론을 해보자.\n\n## 경사하강법과 확률적경사하강법\n\n원래 확률적경사하강법이 딥러닝을 하려고 만든것이 아니다.(만들어진 의도와 사용이 다름) 그런데 거기에 맞게 진화를 한 것. 그래서 되게 헷갈린다...\n\n### ver1: 모든 샘플을 사용하여 slope계산 (gradient descent)\n\n`-` 단순회귀분석에서 샘플 10개 관측: $(x_1,y_1),\\dots,(x_{10},y_{10})$.\n\n(epoch1) $loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n(epoch2) $loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n...\n\nfor문 이 3번 돌아감.\n\n### ver2: 하나의 샘플만 사용하여 slope계산\n\n(epoch1)\n- $loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- ...\n- $loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n(epoch2)\n- $loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- ...\n- $loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n...\n\nfor문이 30번 돌아감.\n\n### ver3: $m(\\leq n)$개의 샘플만 사용하여 slope계산 (mini-batch)\n\n$m=3$이라고 하자.\n\n(epoch1)\n- $loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n1,2,3번 observation들만 뽑아서 loss를 계산해서 그것만가지고 update $\\to$ 4,5,6먼만 가지고 loss계산해서 update $\\to$ 7,8,9를가지고 loss구하고 업데이트 $\\to$ 남은 하나가지고 loss구하고 업데이트... // 한 에폭 끝!\n\n\n(epoch2)\n- $loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n- $loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update$\n\n...\n\n### 용어의 정리\n\n#### 옛날 (좀 더 엄밀)\n\n`-` ver1: gradient descent, batch gradient descent\n\n\n`-` ver2: stochastic gradient descent\n\n`-` ver3: mini-batch gradient descent, mini-batch stochastic gradient descent\n\n#### 요즘\n\n`-` ver1: gradient descent\n\n\n`-` ver2: stochastic gradient descent with batch size = 1\n\n`-` ver3: stochastic gradient descent\n- https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고.\n\n![](attachment:b26b0736-6f75-4fa0-992b-ab485b5851b2.png)\n\n지금은 mini-batch가 포함된 방법을 stochastic gradient descent라고 부른다. 왜냐하면 1,2는 사장된 방법. 버전 3만 쓴다.(유명한 사람들이 학회에서 그렇게 부르기 시작했음.)\n\nnote: 이렇게 많이 쓰는 이유? ver1,2는 사실상 없는 방법이므로\n\n### ver1,2,3 이외에 좀 더 지저분한 것들이 있다.\n\n`-` ver2,3에서 샘플을 셔플할 수도 있다.\n\n`-` ver3에서 일부 샘플이 학습에 참여 안하는 버전도 있다.\n\n`-` 개인적 생각: 크게3개정도만 알면 괜찮고 나머지는 그렇게 유의미하지 않아보인다.\n\n### Discussion\n\n`-` 핵심개념\n\n\n- **메모리사용량^[한 번 업데이트 할 때 드는 계산량]: ver1 > ver3 > ver2**\n- 계산속도: ver1 > ver3 > ver2\n- local-min에 갇힘: ver1 > ver3 > ver2\n\n로컬미니멈에 갇힌가는 건 로컬미니멈을 잘 찾는다라고 생각 (정신이 제대로 박힌애)\n\n대충대충 학습하면 로컬미니멈에서 딱 멈춰야하는데 대충대충 계산해서 기울기가 딱 $0$이 안나오는 것. (운 좋게 탈출하는 경우도 있음.)\n\n`-` 본질: GPU 메모리가 한정되어 있어서 ver1을 쓰지는 못한다. GPU 메모리를 가장 적게쓰는것은 ver2인데 이것은 너무 불안정하다.\n\n`-` 틀리진 않지만 어색한 블로그 정리 내용들\n\n\n- 경사하강법은 종종 국소최소점에 갇히는 문제가 있다. 이를 해결하기 위해서 등장한 방법이 확률적 경사하강법이다. --> 영 틀린말은 아니지만 그걸 의도하고 만든건 아님 (이건 side effect)\n\n- 경사하강법은 계산시간이 오래걸린다. 계산을 빠르게 하기 위해서 등장한 방법이 확률적 경사하강법이다. --> 1회 업데이트는 빠르게 계산함. 하지만 그것이 최적의 $\\beta$를 빠르게 얻을 수 있다는 의미는 아님\n\n\n<font color='blue'>원래 경사하강법은 local minimum에 빠지기 쉬운 알고리즘! 그런데 그나마 둘 중에 비교를 하자면 확률적으로 하면 로컬 미니멈에 빠졌다가 어쩌다 운좋아서 튀어 나가는 경우가 있다.</font>\n\n\n동일한 컴퓨터 자원으로 수렴을 더 빨리 시킬 수 있냐? 그건 아님 (그건 모름).\n\n그럼 왜 쓰냐???\n\n메모리 사용량만 보면 됩니다!\n\n## fashion_mnist 모듈\n\n### tf.keras.datasets.fashion_mnist.load_data()\n\n`-` tf.keras.datasets.fashion_mnist.load_data 의 리턴값 조사\n\n### 데이터생성 및 탐색\n\n`-` tf.keras.datasets.fashion_mnist.load_data()를 이용한 데이터 생성\n\n`-` 차원확인\n\n- 60000은 obs숫자인듯\n- (28,28)은 28픽셀,28픽셀을 의미하는듯\n- train/test는 6:1로 나눈것 같음\n\n`-` 첫번째 obs\n\n- 첫번쨰 obs에 대응하는 라벨\n\n`-` 첫번째 obs와 동일한 라벨을 가지는 그림을 찾아보자.\n\n### 데이터구조\n\n`-` ${\\bf X}$: (n,28,28)\n\n`-` ${\\bf y}$: (n,) , $y=0,1,2,3,\\dots,9$\n\n## 예제1\n\n### 데이터 정리\n\n`-` y=0,1에 대응하는 이미지만 정리하자. (우리가 배운건 로지스틱이니까)\n\n### 풀이1: 은닉층을 포함한 신경망  // epochs=100\n\n### 풀이2: 옵티마이저 개선\n\n### 풀이3: 컴파일시 metrics=['accuracy'] 추가\n\n### 풀이4: 확률적경사하강법 이용 // epochs=10\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2022_05_02_(9주차)_5월2일(2).html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.315","theme":"cosmo","code-copy":true,"title-block-banner":true,"title":"**[STBDA]** 9wk-2. 경사하강법 / 확률적경사하강법 ","author":"JiyunLim","date":"05/26/2023","categories":["빅데이터분석특강"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}