{"title":"**[DL]** 2wk. Applied Math and Machine Learning Basics","markdown":{"yaml":{"title":"**[DL]** 2wk. Applied Math and Machine Learning Basics","author":"JiyunLim","date":"09/11/2023"},"headingText":"Notations","containsRefs":false,"markdown":"\n\n> 이번주 선형대수 // 다음주 확률 // optimization\n\n\n`-` Scalar\n\nSingle number. (real number or natural number)\n\n`-` Vector\n\n숫자들이 순서대로 배열된 1차원 배열.\n\n`-` Matrix\n\n2차원 배열. (벡터를 쌓아놓은 것)\n\n$${\\bf A} = \\begin{bmatrix}A_{1,1} & A_{1,2} \\\\ A_{2,1} & A_{2,2} \\end{bmatrix}$$\n\n`-` Tensor\n\n3차원 이상의 배열.\n\n## Transpose & Multiplication\n\n`-` **1. Transpose**\n\n${\\bf A}_{i,j}^\\top = {\\bf A}_{j,i}^\\top$\n\n`-` **2. Addition**\n\n- ${\\bf A} + {\\bf B} = {\\bf C}$ where $C_{i,j} = A_{i,j} + B_{i,j}$\n- $a\\cdot {\\bf A} + c = {\\bf G}$ where $G_{i,j} = a\\cdot A_{i,j}+c$\n\n$\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}=\\begin{bmatrix} a_{11} + b_1 & a_{12} + b_2 \\\\ a_{21} + b_1 & a_{22} + b_2 \\end{bmatrix}$\n\n> inner product는 차원을 줄이는 것이라면 outer product는 차원을 늘리는 것.\n\n`-` **3. Multiplication**\n\n**Multiplication**\n\nmatrices ${\\bf A} \\in \\mathbb{R}^{a_1\\times b}$ and ${\\bf B} \\in \\mathbb{R}^{b\\times a_2}$의 곱은 아래와 같다.\n\n$${\\bf C} = {\\bf A}{\\bf B}$$\n\nwhere $C_{i,j} = \\sum_{l}A_{i,l}B_{l,j} (i,j) = \\{1,\\dots,a_1\\}\\times \\{1,\\dots, a_2\\}$.\n\n**Hadamard product**\n\nHadamard product (element-wise product) of $A \\in \\mathbb{R}^{a\\times b}$ and $B^{a\\times b}$\n\n$${\\bf C} = {\\bf A} \\odot {\\bf B}$$\n\nwhere $C_{i,j} = A_{i,j}B_{i,j}.$\n\n아다마르 곱(또는 요소별 곱셈, Hadamard product)은 다양한 분야에서 사용되며 주로 두 개의 행렬 또는 벡터 간의 요소별 연산을 나타냅니다. 아다마르 곱은 다음과 같은 상황에서 쓰입니다:\n\n- 신호 처리: 아다마르 곱은 디지털 신호 처리에서 자주 사용됩니다. 예를 들어, 두 시계열 데이터를 아다마르 곱하여 두 신호 간의 상관 관계를 계산하거나 신호를 필터링하는 데 사용될 수 있습니다.\n\n- 이미지 처리: 컴퓨터 비전 및 이미지 처리에서 두 이미지나 이미지와 마스크(필터) 사이의 요소별 곱셈은 특정 이미지 처리 작업에 사용됩니다. 예를 들어, 이미지를 선명하게 만들거나 특정 부분을 강조하는 데 유용합니다.\n\n- 뉴럴 네트워크: 인공 신경망에서 아다마르 곱은 활성화 함수와 가중치 간의 요소별 연산에 사용됩니다. 이를 통해 네트워크의 비선형성을 증가시키고 특정 기능을 강조할 수 있습니다.\n\n- 행렬 연산: 다른 행렬 연산과 결합하여 특정 형태의 계산을 수행할 때 아다마르 곱이 사용될 수 있습니다. 예를 들어, 고유값 분해(Eigendecomposition)와 같은 행렬 분해 기술에서도 사용됩니다.\n\n- 요소별 연산: 두 개의 행렬 또는 벡터 사이의 각 요소를 독립적으로 처리하고 싶을 때 아다마르 곱을 사용합니다. 이렇게 하면 각 요소 간의 관계를 보존하면서 연산을 수행할 수 있습니다.\n\n아다마르 곱은 행렬 곱셈과는 다르며, 두 개의 행렬 또는 벡터의 크기가 동일해야 합니다. 따라서 요소별 연산을 수행하려면 같은 크기의 입력이 필요합니다.\n\n**4. Properties of multiplication**\n\n**5. Linear equation**\n\n${\\bf A}{\\bf x} = {\\bf b}$ where ${\\bf A} \\in \\mathbb{R}^{m\\times n}, {\\bf b} \\in \\mathbb{R}^{m\\times 1}$\n\n## Identity and Inverse Matrix\n\nIdentity matrix: all diagonal terms are one, others are zero such that ${\\bf I}_n \\in \\mathbb{R}^{n\\times n}$\n\n`-` 해가 엄청나게 많은 상황\n\n`-` 해를 못구하는 상황\n\n## Norms and Special Kinds of Matrices and Vectors\n\n### 1. Norm\n\n`-` Definition of $L^p$ norm\n\n$$||{\\bf x}||_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}$$\n\nNorm function $f$ 는 다음을 만족한다.\n\n1. $f(x) = 0 \\Rightarrow {\\bf x} = 0$\n2. $f(x+y) \\leq f(x) + f(y)$ (triangle inequality)\n3. $\\forall \\alpha \\in \\mathbb{R}, f(\\alpha {\\bf x}) = |\\alpha|f(x)$\n\n`-` $L^1$ and $L^0$ norm are considered to address the above.\n\n$$||{\\bf x}||_1 = \\sum_i |x_i|$$\n\n$$||{\\bf x}||_0 = \\sum_i \\mathbb{I} (x_i \\neq 0)$$\n\nhigh dimension에서는 $L^0$나 $L^1$을 많이 쓴다.\n\n`-` $L^2$ norm은 $0$과 $0$이 아닌 기계학습 구별에서 너무 큰 값을 제공한다.\n\n`-` Other norms such as **max norm** and **Frobenius norm**\n\n$$||{\\bf x}||_{\\infty} = \\max|x_i|$$\n\n$$||{\\bf A}||_F = \\sqrt{\\sum_{i,j}{\\bf A}^2_{i,j}}$$\n\n`-` Definition of an angle between two vectors.\n\n$\\cos \\theta = \\frac{x^\\top y}{||x|| ||y||}$\n\n$w_1(1,0,0) + w_1(0,1,0) + w_3(0,0,1) = (w_1, w_2, w_3)$\n\n## Linear Dependence and Span\n\n## Decompositions\n\n### 1. Eigendecomposition\n\n$${\\bf A} = {\\bf V}\\text{diag}({\\lambda}){\\bf V}^{-1}$$\n\n![](attachment:cd7cd15d-e0ae-4898-9e64-572c121393d5.png)\n\n데이터의 correaltion이 강할때 오른쪽 그림과 같다.\n\n### 2. Singular value decomposition\n\nDecomposition for a non-square matrix ${\\bf A} \\in \\mathbb{R}^{n\\times m}$\n\n$${\\bf A} = {\\bf U}{\\bf D}{\\bf V}^\\top$$\n\nwhere ${\\bf U} \\in \\mathbb{R}^{n\\times n}, {\\bf D} \\in \\mathbb{R}^{n\\times m}$, and ${\\bf V}\\in \\mathbb{R}^{m\\times m}$.\n\n## The Moore-Penrose Pseudoinverse, Trace, and Determinant\n\n- linear equation의 해가 너무 많아서 못푸는 문제 같은 경우는 대안이 있지 않을까?\n\n변환시킨 경우 $n$차원 중 어떤 한 축이 0이면 $n-1$ 차원. 그럼 볼륨이 0이된다.\n\n## Example: PCA\n\n통계: 분산이 가장 큰 축을 찾는다.\n\ninput data는 엄청 큰데 작은 차원으로 줄어들면서 다시 커지는 과정을 PCA가 하고 있다. 단, 제약이 있음.\n\n인코딩 디코딩 개념 알아두자.\n\n- $f(x)$: 인코딩 (관측을 하는 큰 차원에서 관측하지 못하는 작은 차원으로)\n\n- $g(f(x))$: 디코딩 (관측하지 못하는 작은 차원에서 관측을 하는 큰차원으로)\n","srcMarkdownNoYaml":"\n\n> 이번주 선형대수 // 다음주 확률 // optimization\n\n## Notations\n\n`-` Scalar\n\nSingle number. (real number or natural number)\n\n`-` Vector\n\n숫자들이 순서대로 배열된 1차원 배열.\n\n`-` Matrix\n\n2차원 배열. (벡터를 쌓아놓은 것)\n\n$${\\bf A} = \\begin{bmatrix}A_{1,1} & A_{1,2} \\\\ A_{2,1} & A_{2,2} \\end{bmatrix}$$\n\n`-` Tensor\n\n3차원 이상의 배열.\n\n## Transpose & Multiplication\n\n`-` **1. Transpose**\n\n${\\bf A}_{i,j}^\\top = {\\bf A}_{j,i}^\\top$\n\n`-` **2. Addition**\n\n- ${\\bf A} + {\\bf B} = {\\bf C}$ where $C_{i,j} = A_{i,j} + B_{i,j}$\n- $a\\cdot {\\bf A} + c = {\\bf G}$ where $G_{i,j} = a\\cdot A_{i,j}+c$\n\n$\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}=\\begin{bmatrix} a_{11} + b_1 & a_{12} + b_2 \\\\ a_{21} + b_1 & a_{22} + b_2 \\end{bmatrix}$\n\n> inner product는 차원을 줄이는 것이라면 outer product는 차원을 늘리는 것.\n\n`-` **3. Multiplication**\n\n**Multiplication**\n\nmatrices ${\\bf A} \\in \\mathbb{R}^{a_1\\times b}$ and ${\\bf B} \\in \\mathbb{R}^{b\\times a_2}$의 곱은 아래와 같다.\n\n$${\\bf C} = {\\bf A}{\\bf B}$$\n\nwhere $C_{i,j} = \\sum_{l}A_{i,l}B_{l,j} (i,j) = \\{1,\\dots,a_1\\}\\times \\{1,\\dots, a_2\\}$.\n\n**Hadamard product**\n\nHadamard product (element-wise product) of $A \\in \\mathbb{R}^{a\\times b}$ and $B^{a\\times b}$\n\n$${\\bf C} = {\\bf A} \\odot {\\bf B}$$\n\nwhere $C_{i,j} = A_{i,j}B_{i,j}.$\n\n아다마르 곱(또는 요소별 곱셈, Hadamard product)은 다양한 분야에서 사용되며 주로 두 개의 행렬 또는 벡터 간의 요소별 연산을 나타냅니다. 아다마르 곱은 다음과 같은 상황에서 쓰입니다:\n\n- 신호 처리: 아다마르 곱은 디지털 신호 처리에서 자주 사용됩니다. 예를 들어, 두 시계열 데이터를 아다마르 곱하여 두 신호 간의 상관 관계를 계산하거나 신호를 필터링하는 데 사용될 수 있습니다.\n\n- 이미지 처리: 컴퓨터 비전 및 이미지 처리에서 두 이미지나 이미지와 마스크(필터) 사이의 요소별 곱셈은 특정 이미지 처리 작업에 사용됩니다. 예를 들어, 이미지를 선명하게 만들거나 특정 부분을 강조하는 데 유용합니다.\n\n- 뉴럴 네트워크: 인공 신경망에서 아다마르 곱은 활성화 함수와 가중치 간의 요소별 연산에 사용됩니다. 이를 통해 네트워크의 비선형성을 증가시키고 특정 기능을 강조할 수 있습니다.\n\n- 행렬 연산: 다른 행렬 연산과 결합하여 특정 형태의 계산을 수행할 때 아다마르 곱이 사용될 수 있습니다. 예를 들어, 고유값 분해(Eigendecomposition)와 같은 행렬 분해 기술에서도 사용됩니다.\n\n- 요소별 연산: 두 개의 행렬 또는 벡터 사이의 각 요소를 독립적으로 처리하고 싶을 때 아다마르 곱을 사용합니다. 이렇게 하면 각 요소 간의 관계를 보존하면서 연산을 수행할 수 있습니다.\n\n아다마르 곱은 행렬 곱셈과는 다르며, 두 개의 행렬 또는 벡터의 크기가 동일해야 합니다. 따라서 요소별 연산을 수행하려면 같은 크기의 입력이 필요합니다.\n\n**4. Properties of multiplication**\n\n**5. Linear equation**\n\n${\\bf A}{\\bf x} = {\\bf b}$ where ${\\bf A} \\in \\mathbb{R}^{m\\times n}, {\\bf b} \\in \\mathbb{R}^{m\\times 1}$\n\n## Identity and Inverse Matrix\n\nIdentity matrix: all diagonal terms are one, others are zero such that ${\\bf I}_n \\in \\mathbb{R}^{n\\times n}$\n\n`-` 해가 엄청나게 많은 상황\n\n`-` 해를 못구하는 상황\n\n## Norms and Special Kinds of Matrices and Vectors\n\n### 1. Norm\n\n`-` Definition of $L^p$ norm\n\n$$||{\\bf x}||_p = \\left(\\sum_i |x_i|^p\\right)^{1/p}$$\n\nNorm function $f$ 는 다음을 만족한다.\n\n1. $f(x) = 0 \\Rightarrow {\\bf x} = 0$\n2. $f(x+y) \\leq f(x) + f(y)$ (triangle inequality)\n3. $\\forall \\alpha \\in \\mathbb{R}, f(\\alpha {\\bf x}) = |\\alpha|f(x)$\n\n`-` $L^1$ and $L^0$ norm are considered to address the above.\n\n$$||{\\bf x}||_1 = \\sum_i |x_i|$$\n\n$$||{\\bf x}||_0 = \\sum_i \\mathbb{I} (x_i \\neq 0)$$\n\nhigh dimension에서는 $L^0$나 $L^1$을 많이 쓴다.\n\n`-` $L^2$ norm은 $0$과 $0$이 아닌 기계학습 구별에서 너무 큰 값을 제공한다.\n\n`-` Other norms such as **max norm** and **Frobenius norm**\n\n$$||{\\bf x}||_{\\infty} = \\max|x_i|$$\n\n$$||{\\bf A}||_F = \\sqrt{\\sum_{i,j}{\\bf A}^2_{i,j}}$$\n\n`-` Definition of an angle between two vectors.\n\n$\\cos \\theta = \\frac{x^\\top y}{||x|| ||y||}$\n\n$w_1(1,0,0) + w_1(0,1,0) + w_3(0,0,1) = (w_1, w_2, w_3)$\n\n## Linear Dependence and Span\n\n## Decompositions\n\n### 1. Eigendecomposition\n\n$${\\bf A} = {\\bf V}\\text{diag}({\\lambda}){\\bf V}^{-1}$$\n\n![](attachment:cd7cd15d-e0ae-4898-9e64-572c121393d5.png)\n\n데이터의 correaltion이 강할때 오른쪽 그림과 같다.\n\n### 2. Singular value decomposition\n\nDecomposition for a non-square matrix ${\\bf A} \\in \\mathbb{R}^{n\\times m}$\n\n$${\\bf A} = {\\bf U}{\\bf D}{\\bf V}^\\top$$\n\nwhere ${\\bf U} \\in \\mathbb{R}^{n\\times n}, {\\bf D} \\in \\mathbb{R}^{n\\times m}$, and ${\\bf V}\\in \\mathbb{R}^{m\\times m}$.\n\n## The Moore-Penrose Pseudoinverse, Trace, and Determinant\n\n- linear equation의 해가 너무 많아서 못푸는 문제 같은 경우는 대안이 있지 않을까?\n\n변환시킨 경우 $n$차원 중 어떤 한 축이 0이면 $n-1$ 차원. 그럼 볼륨이 0이된다.\n\n## Example: PCA\n\n통계: 분산이 가장 큰 축을 찾는다.\n\ninput data는 엄청 큰데 작은 차원으로 줄어들면서 다시 커지는 과정을 PCA가 하고 있다. 단, 제약이 있음.\n\n인코딩 디코딩 개념 알아두자.\n\n- $f(x)$: 인코딩 (관측을 하는 큰 차원에서 관측하지 못하는 작은 차원으로)\n\n- $g(f(x))$: 디코딩 (관측하지 못하는 작은 차원에서 관측을 하는 큰차원으로)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"2023-09-11-DL-2wk.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.315","theme":"cosmo","code-copy":true,"title-block-banner":true,"title":"**[DL]** 2wk. Applied Math and Machine Learning Basics","author":"JiyunLim","date":"09/11/2023"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}